{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7439658,"sourceType":"datasetVersion","datasetId":4330046},{"sourceId":7476209,"sourceType":"datasetVersion","datasetId":4351914},{"sourceId":7512101,"sourceType":"datasetVersion","datasetId":4375343},{"sourceId":7661903,"sourceType":"datasetVersion","datasetId":4467677},{"sourceId":7987664,"sourceType":"datasetVersion","datasetId":4702006}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 原跟车训练测试过程\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nwaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# car_following_data = HighD_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # HighD(12541, 4, 375)\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts  # (max_len - 1)\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# Define data-driven car-following models\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3, hidden_size = 256):\n        super(nn_model, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n            nn.Tanh(),\n            )\n\n    def forward(self, x):\n        out = ACC_LIMIT*self.encoder(x)\n        return out\n\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nnn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n        nn.init.normal_(self.linear1.weight, 0, 0.02)\n        nn.init.constant_(self.linear1.bias, 0.0)\n        nn.init.normal_(self.linear2.weight, 0, 0.02)\n        nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\n\n# Train\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = split_train(car_following_data,0.8,0.1)\n# train_data = Lyft_train\n# test_data = Lyft_test\n# val_data = Lyft_val\ntrain_data = NGSIM_train\ntest_data = NGSIM_test\nval_data = NGSIM_val\nprint(train_data.shape, test_data.shape, val_data.shape)\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nbatch_size = 32\ntotal_epochs = 30\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\n# lr = 0.01\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\n# 根据名称定义模型\nif model_type == 'nn':\n    model = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    model = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\nmodel_state = list(model.parameters())\nprint(model_state)\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\nprint(\"----\")\n# 训练过程\nfor epoch in tqdm(range(total_epochs)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    model.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = model(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        model_optim.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    model.eval()\n    error_list = []\n    for i, item in enumerate(validation_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n            acc_pre = model(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n        model_optim.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n        with open(save, 'wb') as f:\n            torch.save(model, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# 用该数据集训练得到的模型去对其他数据集进行测试\n# Ts = 0.04\n# max_len = 375\n# # car_following_data = NGSIM_data\n# # print(car_following_data.shape)\n# # train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# test_data = HighD_test\n# # 创建测试集 DataLoader\n# test_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\n# test_loader = DataLoader(\n#         test_dataset,\n#         batch_size=batch_size,\n#         shuffle=False,\n#         num_workers=1,\n#         drop_last=True)\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.04\n# max_len = 375\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n#             MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 只含有test和指标计算代码部分，用于使用在不同数据集下训练的模型用在其他数据集上测试\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nwaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\n# car_following_data = NGSIM_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # (12541, 4, 375)\n\n# # 除HighD的数据集需要进行连接\n# def car_stack(data):\n#     # 创建一个空的目标数组，形状为 (1930, 4, 600)\n#     target_shape = (data.shape[0], data.shape[1], data[0, 0].shape[0])\n#     target_array = np.empty(target_shape)\n\n#     # 遍历原始数组，将每个元素转换为 (4, 600) 的子数组，并放入目标数组\n#     for i in range(data.shape[0]):\n#         for j in range(data.shape[1]):\n#             target_array[i, j, :] = data[i, j]\n#     # 最终 target_array 的形状为 (1930, 4, 600)\n#     return target_array\n\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的除最后一项的前 max_len-1 个时间步\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# Define data-driven car-following models\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3, hidden_size = 256):\n        super(nn_model, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n            nn.Tanh(),\n            )\n\n    def forward(self, x):\n        out = ACC_LIMIT*self.encoder(x)\n        return out\n\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nnn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n        nn.init.normal_(self.linear1.weight, 0, 0.02)\n        nn.init.constant_(self.linear1.bias, 0.0)\n        nn.init.normal_(self.linear2.weight, 0, 0.02)\n        nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\n\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = Lyft_train, Lyft_test, Lyft_val\ntrain_data = NGSIM_train\ntest_data = NGSIM_test\nval_data = NGSIM_val\nprint(test_data.shape)\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nbatch_size = 32\ntotal_epochs = 20\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\n# 根据名称定义模型\nif model_type == 'nn':\n    model = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    model = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=MyDevice)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_)\n\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 求一些值用\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s   15/150s\n# max_len = 375 # for HighD dataset is 375 for others are 150\nTs = 0.1\nmax_len = 150\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 获取每个跟车事件的时间步长\ndef DataMin(dataset):\n    data_min = []\n    for i in range(dataset.shape[0]):\n        data_min.append(dataset[i][0].size)\n    return data_min\n\ndatamin = DataMin(NGSIM_data)\n# print(datamin)\nprint(\"最小时间步：\", np.min(datamin))\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# 求原测试集中的平均jerk\nTs = 0.1\nmax_len = 150\ndataset_test = Lyft_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\nhis_horizon = 10\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n#     x_data, y_data = x_data.transpose(0, 2).float(), y_data.transpose(0, 1).float()\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B = y_data.shape # (total steps, batch_size, d) as the shape of the data\n#     print(T,B)\n    a = np.diff(y_data)/Ts\n    b = np.abs(a)\n    jerk =np.mean(b)\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"jerk\", np.mean(jerk_set))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T01:21:49.760181Z","iopub.execute_input":"2024-02-19T01:21:49.760557Z","iopub.status.idle":"2024-02-19T01:22:00.390929Z","shell.execute_reply.started":"2024-02-19T01:21:49.760523Z","shell.execute_reply":"2024-02-19T01:22:00.389827Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"cuda:0\n最小时间步： 150\n","output_type":"stream"},{"name":"stderr","text":"2240it [00:07, 288.08it/s]","output_type":"stream"},{"name":"stdout","text":"jerk 0.7705402415347494\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车  version1\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        # 根据索引划分数据集\n        data_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]\n        data_indices2 = shuffled_indices[data_num:data_ratio]\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# 定义模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(input_size,256)),\n            ('relu1',nn.ReLU()),\n            ('l2',nn.Linear(256,256)),\n            ('relu2',nn.ReLU()),\n            ('l3',nn.Linear(256,1))\n        ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def argforward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net.argforward(x, temp_weights).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len)\n        return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = int(self.k/2)\n\n    def inner_loop(self, task):\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程\n        loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n        grads = torch.autograd.grad(loss, temp_weights)  # 计算损失对参数的梯度\n        temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # 临时参数更新 梯度下降\n        dataloader2 = DataLoader(\n                dataset_loader2,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n        return metaloss\n\n    def outer_loop(self, num_epochs):  # epoch 500\n        total_loss = 0\n        for epoch in range(1, num_epochs + 1):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()\n            for i in tasks:\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights)  # 计算元学习损失对参数的梯度\n            # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'nn'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=400)\n\n\nmaml.outer_loop(num_epochs=10000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn.Sequential(OrderedDict([\n        ('l1', nn.Linear(his_horizon*3,256)),\n        ('relu1', nn.ReLU()),\n        ('l2', nn.Linear(256,256)),\n        ('relu2', nn.ReLU()),\n        ('l3', nn.Linear(256,1))\n    ]))\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=5\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.SGD\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n#         opt.zero_grad()\n#         torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n#         opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T01:31:20.594435Z","iopub.execute_input":"2024-03-08T01:31:20.594910Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cpu\n1/10000. loss: 0.030780471861362457\n2/10000. loss: 0.04516826073328654\n3/10000. loss: 0.3769957224527995\n4/10000. loss: 0.3960939645767212\n5/10000. loss: 0.34749897321065265\n6/10000. loss: 0.03875022133191427\n7/10000. loss: 0.6340972979863485\n8/10000. loss: 1.103097677230835\n9/10000. loss: 0.7345333099365234\n10/10000. loss: 0.8408072789510092\n11/10000. loss: 0.4542378584543864\n12/10000. loss: 0.1158681611220042\n13/10000. loss: 0.06863734622796376\n14/10000. loss: 0.7657466729482015\n15/10000. loss: 0.19987616936365762\n16/10000. loss: 1.2442971070607503\n17/10000. loss: 0.14375762144724527\n18/10000. loss: 0.98065718015035\n19/10000. loss: 1.2417702674865723\n20/10000. loss: 0.20725069443384805\n21/10000. loss: 0.11335354050000508\n22/10000. loss: 0.15266165137290955\n23/10000. loss: 1.0211774508158367\n24/10000. loss: 1.0980748335520427\n25/10000. loss: 0.8192783196767172\n26/10000. loss: 0.6264430284500122\n27/10000. loss: 0.504382848739624\n28/10000. loss: 0.38451504707336426\n29/10000. loss: 0.1360906958580017\n30/10000. loss: 0.12474276622136433\n31/10000. loss: 0.12997093796730042\n32/10000. loss: 0.3321178952852885\n33/10000. loss: 0.3175383408864339\n34/10000. loss: 0.2705320517222087\n35/10000. loss: 0.3122217655181885\n36/10000. loss: 0.18445678551991782\n37/10000. loss: 0.27397119998931885\n38/10000. loss: 0.243138055006663\n39/10000. loss: 0.22698434193929037\n40/10000. loss: 0.18202861150105795\n41/10000. loss: 0.2219420870145162\n42/10000. loss: 0.19615912437438965\n43/10000. loss: 0.288410743077596\n44/10000. loss: 0.10028127829233806\n45/10000. loss: 0.10038862625757854\n46/10000. loss: 0.12347403168678284\n47/10000. loss: 0.15024463335673013\n48/10000. loss: 0.12395741542180379\n49/10000. loss: 0.12782380978266397\n50/10000. loss: 0.15330479542414346\n51/10000. loss: 0.12315595149993896\n52/10000. loss: 0.18812302748362222\n53/10000. loss: 0.1263024906317393\n54/10000. loss: 0.12220177054405212\n55/10000. loss: 0.10237505038579305\n56/10000. loss: 0.13946601748466492\n57/10000. loss: 0.1746010184288025\n58/10000. loss: 0.1360509196917216\n59/10000. loss: 0.09431007504463196\n60/10000. loss: 0.05752186973889669\n61/10000. loss: 0.13718934853871664\n62/10000. loss: 0.13804481426874796\n63/10000. loss: 0.11205055316289265\n64/10000. loss: 0.10126383105913798\n65/10000. loss: 0.1109372079372406\n66/10000. loss: 0.08317519724369049\n67/10000. loss: 0.0831543505191803\n68/10000. loss: 0.0633540153503418\n69/10000. loss: 0.09294744332631429\n70/10000. loss: 0.08310100436210632\n71/10000. loss: 0.06207134326299032\n72/10000. loss: 0.06151878833770752\n73/10000. loss: 0.10621529817581177\n74/10000. loss: 0.037563701470692955\n75/10000. loss: 0.05839581787586212\n76/10000. loss: 0.06452501813570659\n77/10000. loss: 0.04233876864115397\n78/10000. loss: 0.0547116349140803\n79/10000. loss: 0.03507260729869207\n80/10000. loss: 0.037377183636029564\n81/10000. loss: 0.036514535546302795\n82/10000. loss: 0.04188318053881327\n83/10000. loss: 0.027873230477174122\n84/10000. loss: 0.02856268733739853\n85/10000. loss: 0.02346936861673991\n86/10000. loss: 0.0228446622689565\n87/10000. loss: 0.020900989572207134\n88/10000. loss: 0.016984352221091587\n89/10000. loss: 0.02153753737608592\n90/10000. loss: 0.019228039930264156\n91/10000. loss: 0.029676832258701324\n92/10000. loss: 0.012274066607157389\n93/10000. loss: 0.021771493057409923\n94/10000. loss: 0.02576011170943578\n95/10000. loss: 0.01716308792432149\n96/10000. loss: 0.013868536800146103\n97/10000. loss: 0.010404416670401892\n98/10000. loss: 0.011926131943861643\n99/10000. loss: 0.012373042603333792\n100/10000. loss: 0.014536798000335693\n101/10000. loss: 0.013183513035376867\n102/10000. loss: 0.01110409696896871\n103/10000. loss: 0.011629564066727957\n104/10000. loss: 0.010401589795947075\n105/10000. loss: 0.009749958912531534\n106/10000. loss: 0.012787846227486929\n107/10000. loss: 0.011129585405190786\n108/10000. loss: 0.00829542800784111\n109/10000. loss: 0.007229112088680267\n110/10000. loss: 0.011319290846586227\n111/10000. loss: 0.015557395915190378\n112/10000. loss: 0.008968010544776917\n113/10000. loss: 0.0144721120595932\n114/10000. loss: 0.013522079835335413\n115/10000. loss: 0.010947869469722113\n116/10000. loss: 0.009269607563813528\n117/10000. loss: 0.008470458288987478\n118/10000. loss: 0.007664629568656285\n119/10000. loss: 0.0071311021844546\n120/10000. loss: 0.005792911474903424\n121/10000. loss: 0.008416699866453806\n122/10000. loss: 0.007151767611503601\n123/10000. loss: 0.0069066789001226425\n124/10000. loss: 0.007444720094402631\n125/10000. loss: 0.00927892824014028\n126/10000. loss: 0.011488984028498331\n127/10000. loss: 0.00811196118593216\n128/10000. loss: 0.009725059072176615\n129/10000. loss: 0.006300296013553937\n130/10000. loss: 0.004587997371951739\n131/10000. loss: 0.008583924422661463\n132/10000. loss: 0.007284481078386307\n133/10000. loss: 0.006219994276762009\n134/10000. loss: 0.005828443914651871\n135/10000. loss: 0.0067285578697919846\n136/10000. loss: 0.006125390529632568\n137/10000. loss: 0.004259626691540082\n138/10000. loss: 0.005872676149010658\n139/10000. loss: 0.0061950286229451495\n140/10000. loss: 0.008766892676552137\n141/10000. loss: 0.005286309868097305\n142/10000. loss: 0.005281340330839157\n143/10000. loss: 0.0062134650846322375\n144/10000. loss: 0.005885706593592961\n145/10000. loss: 0.00492656013617913\n146/10000. loss: 0.007272087037563324\n147/10000. loss: 0.004539353772997856\n148/10000. loss: 0.003115115687251091\n149/10000. loss: 0.005752951527635257\n150/10000. loss: 0.002841624431312084\n151/10000. loss: 0.003096065173546473\n152/10000. loss: 0.004322901057700316\n153/10000. loss: 0.0038682265828053155\n154/10000. loss: 0.004648126972218354\n155/10000. loss: 0.005397118007143338\n156/10000. loss: 0.003999367045859496\n157/10000. loss: 0.00470776700725158\n158/10000. loss: 0.003550206000606219\n159/10000. loss: 0.002860461672147115\n160/10000. loss: 0.003065311349928379\n161/10000. loss: 0.0020579053089022636\n162/10000. loss: 0.0025554153447349868\n163/10000. loss: 0.002373156137764454\n164/10000. loss: 0.0019953028919796147\n165/10000. loss: 0.005494608233372371\n166/10000. loss: 0.0029520258928338685\n167/10000. loss: 0.0028984639793634415\n168/10000. loss: 0.002502742223441601\n169/10000. loss: 0.0035937512293457985\n170/10000. loss: 0.003047395497560501\n171/10000. loss: 0.001690679540236791\n172/10000. loss: 0.0035089440643787384\n173/10000. loss: 0.0025982285539309182\n174/10000. loss: 0.0035415968547264733\n175/10000. loss: 0.002674019274612268\n176/10000. loss: 0.0019512055441737175\n177/10000. loss: 0.002147058335443338\n178/10000. loss: 0.0023837070912122726\n179/10000. loss: 0.00273426187535127\n180/10000. loss: 0.00294740063448747\n181/10000. loss: 0.0027571329846978188\n182/10000. loss: 0.002830350771546364\n183/10000. loss: 0.0017937988353272278\n184/10000. loss: 0.001557646319270134\n185/10000. loss: 0.0016003701214989026\n186/10000. loss: 0.0014499199266235034\n187/10000. loss: 0.002012966355929772\n188/10000. loss: 0.0018954221159219742\n189/10000. loss: 0.0017705896558860938\n190/10000. loss: 0.0015144259668886662\n191/10000. loss: 0.00238934438675642\n192/10000. loss: 0.0030811280012130737\n193/10000. loss: 0.003122964253028234\n194/10000. loss: 0.002331335097551346\n195/10000. loss: 0.002232526894658804\n196/10000. loss: 0.0013243692616621654\n197/10000. loss: 0.0017432126527031262\n198/10000. loss: 0.002161927210787932\n199/10000. loss: 0.0012569059617817402\n200/10000. loss: 0.002322730297843615\n201/10000. loss: 0.002163671267529329\n202/10000. loss: 0.0025973329320549965\n203/10000. loss: 0.0014890365613003571\n204/10000. loss: 0.001788125994304816\n205/10000. loss: 0.0020764564784864583\n206/10000. loss: 0.0012455872880915801\n207/10000. loss: 0.001959351201852163\n208/10000. loss: 0.0024563043067852655\n209/10000. loss: 0.0015092094739278157\n210/10000. loss: 0.0015846178866922855\n211/10000. loss: 0.0021899652977784476\n212/10000. loss: 0.0018817273279031117\n213/10000. loss: 0.0016632756839195888\n214/10000. loss: 0.0016879883284370105\n215/10000. loss: 0.002323427858452002\n216/10000. loss: 0.0019664911863704524\n217/10000. loss: 0.0011976505629718304\n218/10000. loss: 0.0017336327582597733\n219/10000. loss: 0.002171479475994905\n220/10000. loss: 0.0017174432675043743\n221/10000. loss: 0.001870622547964255\n222/10000. loss: 0.001024694259588917\n223/10000. loss: 0.002766987308859825\n224/10000. loss: 0.0011873421414444845\n225/10000. loss: 0.0012539434246718884\n226/10000. loss: 0.001303812178472678\n227/10000. loss: 0.0020703126986821494\n228/10000. loss: 0.0021612485870718956\n229/10000. loss: 0.0009874793856094282\n230/10000. loss: 0.0019298708066344261\n231/10000. loss: 0.0017987356210748355\n232/10000. loss: 0.002377110533416271\n233/10000. loss: 0.0018941421682635944\n234/10000. loss: 0.0018805544823408127\n235/10000. loss: 0.0015136331009368102\n236/10000. loss: 0.002071139713128408\n237/10000. loss: 0.001507008137802283\n238/10000. loss: 0.0018153226313491662\n239/10000. loss: 0.002248848012338082\n240/10000. loss: 0.0014505207849045594\n241/10000. loss: 0.0020189622106651464\n242/10000. loss: 0.0011042901314795017\n243/10000. loss: 0.0016733685818811257\n244/10000. loss: 0.0013014387028912704\n245/10000. loss: 0.0011063284861544769\n246/10000. loss: 0.002268776452789704\n247/10000. loss: 0.0018428539236386616\n248/10000. loss: 0.0016893820526699226\n249/10000. loss: 0.0014297782133022945\n250/10000. loss: 0.00173859562103947\n251/10000. loss: 0.0016450360417366028\n252/10000. loss: 0.0010114157727609079\n253/10000. loss: 0.0013474894997974236\n254/10000. loss: 0.0011785272508859634\n255/10000. loss: 0.001061013899743557\n256/10000. loss: 0.001240200052658717\n257/10000. loss: 0.0013101914276679356\n258/10000. loss: 0.00239397119730711\n259/10000. loss: 0.002226792275905609\n260/10000. loss: 0.0019531560440858207\n261/10000. loss: 0.0013387979318698247\n262/10000. loss: 0.002660832367837429\n263/10000. loss: 0.001090029642606775\n264/10000. loss: 0.0016610678285360336\n265/10000. loss: 0.0009932967368513346\n266/10000. loss: 0.0017092159638802211\n267/10000. loss: 0.001658118950823943\n268/10000. loss: 0.0010034890534977119\n269/10000. loss: 0.0014842478558421135\n270/10000. loss: 0.0015050203849871953\n271/10000. loss: 0.001656437913576762\n272/10000. loss: 0.001244541723281145\n273/10000. loss: 0.0016619718323151271\n274/10000. loss: 0.0015225362343092759\n275/10000. loss: 0.0013210158795118332\n276/10000. loss: 0.0016362393895785015\n277/10000. loss: 0.0008395101564625899\n278/10000. loss: 0.0015725092962384224\n279/10000. loss: 0.0013586211328705151\n280/10000. loss: 0.001216491839538018\n281/10000. loss: 0.001353134245922168\n282/10000. loss: 0.002105241951843103\n283/10000. loss: 0.0017428947612643242\n284/10000. loss: 0.001810968853533268\n285/10000. loss: 0.0014504107336203258\n286/10000. loss: 0.0015976813932259877\n287/10000. loss: 0.0013616204572220643\n288/10000. loss: 0.001551586203277111\n289/10000. loss: 0.0016194071310261886\n290/10000. loss: 0.001890324056148529\n291/10000. loss: 0.0014062200983365376\n292/10000. loss: 0.0012724646367132664\n293/10000. loss: 0.0009842850267887115\n294/10000. loss: 0.0009767028192679088\n295/10000. loss: 0.0013194374429682891\n296/10000. loss: 0.0008573657833039761\n297/10000. loss: 0.0008888500742614269\n298/10000. loss: 0.0010864267436166604\n299/10000. loss: 0.0013898983597755432\n300/10000. loss: 0.0013675357525547345\n301/10000. loss: 0.0016118007091184456\n302/10000. loss: 0.0022553357606132827\n303/10000. loss: 0.0007949825376272202\n304/10000. loss: 0.0010437427554279566\n305/10000. loss: 0.0007384871132671833\n306/10000. loss: 0.001468454332401355\n307/10000. loss: 0.001565091156711181\n308/10000. loss: 0.0009189618285745382\n309/10000. loss: 0.0008708262660851082\n310/10000. loss: 0.0017213428703447182\n311/10000. loss: 0.0012413025833666325\n312/10000. loss: 0.0009992675234874089\n313/10000. loss: 0.000702517805621028\n314/10000. loss: 0.0010124669255067904\n315/10000. loss: 0.0010871371099104483\n316/10000. loss: 0.0011032276476422946\n317/10000. loss: 0.0007691738040496906\n318/10000. loss: 0.00125398983558019\n319/10000. loss: 0.001938657679905494\n320/10000. loss: 0.0010859320561091106\n321/10000. loss: 0.001641155065347751\n322/10000. loss: 0.0012626979344834883\n323/10000. loss: 0.0009815750333170097\n324/10000. loss: 0.0010029035620391369\n325/10000. loss: 0.0014502075500786304\n326/10000. loss: 0.0009997113763044279\n327/10000. loss: 0.0013723969459533691\n328/10000. loss: 0.0013056847577293713\n329/10000. loss: 0.0011348103483517964\n330/10000. loss: 0.0009706157725304365\n331/10000. loss: 0.0009639174677431583\n332/10000. loss: 0.0013965005055069923\n333/10000. loss: 0.0014222973647216957\n334/10000. loss: 0.001101169114311536\n335/10000. loss: 0.0016991542652249336\n336/10000. loss: 0.0016003741572300594\n337/10000. loss: 0.0009051500043521324\n338/10000. loss: 0.001379136461764574\n339/10000. loss: 0.0011216052807867527\n340/10000. loss: 0.0011821261917551358\n341/10000. loss: 0.001295106407875816\n342/10000. loss: 0.0013054635686179001\n343/10000. loss: 0.0015076071334381898\n344/10000. loss: 0.00120585427309076\n345/10000. loss: 0.0017652617146571477\n346/10000. loss: 0.0013363974479337533\n347/10000. loss: 0.0011508118671675522\n348/10000. loss: 0.001661204732954502\n349/10000. loss: 0.001809803768992424\n350/10000. loss: 0.0015775275727113087\n351/10000. loss: 0.001966670465966066\n352/10000. loss: 0.0012625088760008414\n353/10000. loss: 0.001056994078680873\n354/10000. loss: 0.0010532958743472893\n355/10000. loss: 0.0012616094512244065\n356/10000. loss: 0.0013969217737515767\n357/10000. loss: 0.0014495295472443104\n358/10000. loss: 0.0021014466571311155\n359/10000. loss: 0.0009249305973450342\n360/10000. loss: 0.0012870017283906539\n361/10000. loss: 0.0011156902182847261\n362/10000. loss: 0.0018827235326170921\n363/10000. loss: 0.0008180040555695692\n364/10000. loss: 0.0009777368977665901\n365/10000. loss: 0.0018047560006380081\n366/10000. loss: 0.0011820630946507056\n367/10000. loss: 0.0017396692807475727\n368/10000. loss: 0.0012828251346945763\n369/10000. loss: 0.0013854031761487324\n370/10000. loss: 0.0011211501744886239\n371/10000. loss: 0.0018690256401896477\n372/10000. loss: 0.0016547680522004764\n373/10000. loss: 0.001420949740956227\n374/10000. loss: 0.0016861225788791974\n375/10000. loss: 0.0020289594928423562\n376/10000. loss: 0.0011137464704612892\n377/10000. loss: 0.001303676205376784\n378/10000. loss: 0.0016214102506637573\n379/10000. loss: 0.0010001350504656632\n380/10000. loss: 0.0014012505610783894\n381/10000. loss: 0.001351978008945783\n382/10000. loss: 0.001305945838491122\n383/10000. loss: 0.0012790953430036704\n384/10000. loss: 0.001562219113111496\n385/10000. loss: 0.0007328620801369349\n386/10000. loss: 0.0009054255982240041\n387/10000. loss: 0.0009531867690384388\n388/10000. loss: 0.0010909334135552247\n389/10000. loss: 0.0010889661498367786\n390/10000. loss: 0.000904894977187117\n391/10000. loss: 0.0013285147336622078\n392/10000. loss: 0.0013514285286267598\n393/10000. loss: 0.0012055146507918835\n394/10000. loss: 0.0007156785577535629\n395/10000. loss: 0.0014222245663404465\n396/10000. loss: 0.0010322478289405506\n397/10000. loss: 0.001207747186223666\n398/10000. loss: 0.0016112032656868298\n399/10000. loss: 0.0012914206211765606\n400/10000. loss: 0.0010235062800347805\n401/10000. loss: 0.0016467319801449776\n402/10000. loss: 0.0010247087726990383\n403/10000. loss: 0.0012997832770148914\n404/10000. loss: 0.001146679666514198\n405/10000. loss: 0.0017248308286070824\n406/10000. loss: 0.0009777309217800696\n407/10000. loss: 0.0015093252683679264\n408/10000. loss: 0.0017077942999700706\n409/10000. loss: 0.0017773006111383438\n410/10000. loss: 0.0012921444916476805\n411/10000. loss: 0.0013272132103641827\n412/10000. loss: 0.0018644418256978195\n413/10000. loss: 0.0014248501198987167\n414/10000. loss: 0.0014187654790778954\n415/10000. loss: 0.0015507318700353305\n416/10000. loss: 0.0016765566542744637\n417/10000. loss: 0.001740686905880769\n418/10000. loss: 0.0021426724269986153\n419/10000. loss: 0.002968675767381986\n420/10000. loss: 0.0017405042114357154\n421/10000. loss: 0.0013150510688622792\n422/10000. loss: 0.0015322898204127948\n423/10000. loss: 0.001946317342420419\n424/10000. loss: 0.0022915921484430632\n425/10000. loss: 0.001671702756235997\n426/10000. loss: 0.002005509721736113\n427/10000. loss: 0.0016148836972812812\n428/10000. loss: 0.0014630967440704505\n429/10000. loss: 0.0017144063798089821\n430/10000. loss: 0.001839626890917619\n431/10000. loss: 0.0011879249941557646\n432/10000. loss: 0.0013359198346734047\n433/10000. loss: 0.0017879341418544452\n434/10000. loss: 0.0014315253744522731\n435/10000. loss: 0.002919707757731279\n436/10000. loss: 0.0014760545454919338\n437/10000. loss: 0.0015135191691418488\n438/10000. loss: 0.0017959281491736572\n439/10000. loss: 0.001696366195877393\n440/10000. loss: 0.0015113487218817074\n441/10000. loss: 0.0021822828178604445\n442/10000. loss: 0.0015277994486192863\n443/10000. loss: 0.0013332761203249295\n444/10000. loss: 0.0015748776495456696\n445/10000. loss: 0.0015227772916356723\n446/10000. loss: 0.0012924432133634884\n447/10000. loss: 0.0010400842875242233\n448/10000. loss: 0.0015041666726271312\n449/10000. loss: 0.0016326559707522392\n450/10000. loss: 0.001563016635676225\n451/10000. loss: 0.0016515182020763557\n452/10000. loss: 0.0014394714186588924\n453/10000. loss: 0.001959062647074461\n454/10000. loss: 0.0009424023640652498\n455/10000. loss: 0.0009199882236619791\n456/10000. loss: 0.0009575539734214544\n457/10000. loss: 0.0015897209135194619\n458/10000. loss: 0.001151939621195197\n459/10000. loss: 0.0015084993404646714\n460/10000. loss: 0.0008679155725985765\n461/10000. loss: 0.0010870158051451047\n462/10000. loss: 0.001091527131696542\n463/10000. loss: 0.0008645475221176943\n464/10000. loss: 0.0017167550201217334\n465/10000. loss: 0.0014899475499987602\n466/10000. loss: 0.0008585931888471047\n467/10000. loss: 0.0013296438070635002\n468/10000. loss: 0.0013091669728358586\n469/10000. loss: 0.0008358028717339039\n470/10000. loss: 0.0016564105947812398\n471/10000. loss: 0.0013803900219500065\n472/10000. loss: 0.0017463552455107372\n473/10000. loss: 0.0011937576346099377\n474/10000. loss: 0.0022702814700702825\n475/10000. loss: 0.0011280692803363006\n476/10000. loss: 0.0008452643329898516\n477/10000. loss: 0.0016717324033379555\n478/10000. loss: 0.0013946965336799622\n479/10000. loss: 0.0007448570492366949\n480/10000. loss: 0.0016481041287382443\n481/10000. loss: 0.001034894570087393\n482/10000. loss: 0.0008948406515022119\n483/10000. loss: 0.0013803352291385333\n484/10000. loss: 0.0008085422838727633\n485/10000. loss: 0.0012994525022804737\n486/10000. loss: 0.0008163327972094218\n487/10000. loss: 0.0008821548738827308\n488/10000. loss: 0.0013413617076973121\n489/10000. loss: 0.000730741381024321\n490/10000. loss: 0.0008746442229797443\n491/10000. loss: 0.0014740102924406528\n492/10000. loss: 0.0008902829140424728\n493/10000. loss: 0.001263656032582124\n494/10000. loss: 0.0015860886002580326\n495/10000. loss: 0.0017817461242278416\n496/10000. loss: 0.0014205037926634152\n497/10000. loss: 0.0007856468049188455\n498/10000. loss: 0.0009168963103244702\n499/10000. loss: 0.001614417415112257\n500/10000. loss: 0.0013513220474123955\n501/10000. loss: 0.0009298312943428755\n502/10000. loss: 0.0016582848814626534\n503/10000. loss: 0.0015856496368845303\n504/10000. loss: 0.001196338174243768\n505/10000. loss: 0.0014300905168056488\n506/10000. loss: 0.001966124555716912\n507/10000. loss: 0.001138548832386732\n508/10000. loss: 0.0016139065846800804\n509/10000. loss: 0.0013079003741343815\n510/10000. loss: 0.0018095462583005428\n511/10000. loss: 0.0014030088980992634\n512/10000. loss: 0.0011370470747351646\n513/10000. loss: 0.0017478208368023236\n514/10000. loss: 0.0013602420998116334\n515/10000. loss: 0.0015415670350193977\n516/10000. loss: 0.0012229007358352344\n517/10000. loss: 0.0013500115213294823\n518/10000. loss: 0.0018994590888420741\n519/10000. loss: 0.0018325097238024075\n520/10000. loss: 0.0011263963921616476\n521/10000. loss: 0.0013295464838544528\n522/10000. loss: 0.0024132486432790756\n523/10000. loss: 0.001467456730703513\n524/10000. loss: 0.0018186659241716068\n525/10000. loss: 0.0023473730931679406\n526/10000. loss: 0.001873043520996968\n527/10000. loss: 0.0014337160003681977\n528/10000. loss: 0.0026022111997008324\n529/10000. loss: 0.002392615812520186\n530/10000. loss: 0.0014575696550309658\n531/10000. loss: 0.0018876800313591957\n532/10000. loss: 0.0025660350608328977\n533/10000. loss: 0.0018662144429981709\n534/10000. loss: 0.0017873561009764671\n535/10000. loss: 0.0026432760059833527\n536/10000. loss: 0.0023342398926615715\n537/10000. loss: 0.0021615315539141497\n538/10000. loss: 0.0015997765585780144\n539/10000. loss: 0.0016247519912819068\n540/10000. loss: 0.001590301903585593\n541/10000. loss: 0.002465828787535429\n542/10000. loss: 0.002757192278901736\n543/10000. loss: 0.001794988289475441\n544/10000. loss: 0.0012589394270131986\n545/10000. loss: 0.0014650688196221988\n546/10000. loss: 0.0010570645487556856\n547/10000. loss: 0.002323993636916081\n548/10000. loss: 0.0009876621576646964\n549/10000. loss: 0.0016869034928580124\n550/10000. loss: 0.0016260227809349697\n551/10000. loss: 0.002728956751525402\n552/10000. loss: 0.002270412320892016\n553/10000. loss: 0.001815443392843008\n554/10000. loss: 0.001312298234552145\n555/10000. loss: 0.0017525323977073033\n556/10000. loss: 0.0014688942270974319\n557/10000. loss: 0.0011009750111649434\n558/10000. loss: 0.0017046357194582622\n559/10000. loss: 0.001073997700586915\n560/10000. loss: 0.0015163449570536613\n561/10000. loss: 0.0008129005630811056\n562/10000. loss: 0.0019023797164360683\n563/10000. loss: 0.0021954143109420934\n564/10000. loss: 0.001334750869621833\n565/10000. loss: 0.0013948396469155948\n566/10000. loss: 0.0009438399380693833\n567/10000. loss: 0.001283796348919471\n568/10000. loss: 0.001138037924344341\n569/10000. loss: 0.001054992511247595\n570/10000. loss: 0.0016742820541063945\n571/10000. loss: 0.002184485550969839\n572/10000. loss: 0.0017874498541156452\n573/10000. loss: 0.0021408433094620705\n574/10000. loss: 0.0013794703409075737\n575/10000. loss: 0.0011274789770444233\n576/10000. loss: 0.0011561845894902945\n577/10000. loss: 0.0022928748900691667\n578/10000. loss: 0.0022778501734137535\n579/10000. loss: 0.0011937703626851242\n580/10000. loss: 0.0033440223584572473\n581/10000. loss: 0.001878170296549797\n582/10000. loss: 0.0019887425005435944\n583/10000. loss: 0.0017156144604086876\n584/10000. loss: 0.0019942489452660084\n585/10000. loss: 0.001115532514328758\n586/10000. loss: 0.001745760440826416\n587/10000. loss: 0.0016012735043962796\n588/10000. loss: 0.0016027626891930897\n589/10000. loss: 0.0014444258995354176\n590/10000. loss: 0.0011748091783374548\n591/10000. loss: 0.0013209835936625798\n592/10000. loss: 0.0020387753223379454\n593/10000. loss: 0.001006936344007651\n594/10000. loss: 0.0012424790766090155\n595/10000. loss: 0.0010341086114446323\n596/10000. loss: 0.0011431040863196056\n597/10000. loss: 0.0009199910952399174\n598/10000. loss: 0.0017649633809924126\n599/10000. loss: 0.001196684471021096\n600/10000. loss: 0.0011566579341888428\n601/10000. loss: 0.0011831348141034443\n602/10000. loss: 0.0014059773335854213\n603/10000. loss: 0.001292875036597252\n604/10000. loss: 0.001521825169523557\n605/10000. loss: 0.0009133889495084683\n606/10000. loss: 0.0011783037334680557\n607/10000. loss: 0.0009306586192299923\n608/10000. loss: 0.0008009937591850758\n609/10000. loss: 0.001196497119963169\n610/10000. loss: 0.0007378526497632265\n611/10000. loss: 0.0007604834778855244\n612/10000. loss: 0.0009222840890288353\n613/10000. loss: 0.0017227202964325745\n614/10000. loss: 0.0014465499358872573\n615/10000. loss: 0.0008659789649148782\n616/10000. loss: 0.0008868536291023096\n617/10000. loss: 0.0012634132678310077\n618/10000. loss: 0.0008670284102360407\n619/10000. loss: 0.001021612746020158\n620/10000. loss: 0.0009938508737832308\n621/10000. loss: 0.0015344951922694843\n622/10000. loss: 0.0012882623511056106\n623/10000. loss: 0.001163244480267167\n624/10000. loss: 0.001292237313464284\n625/10000. loss: 0.0014426466077566147\n626/10000. loss: 0.0007882803523292145\n627/10000. loss: 0.0008594619575887918\n628/10000. loss: 0.0016057377991576989\n629/10000. loss: 0.0007752040401101112\n630/10000. loss: 0.0013510431163012981\n631/10000. loss: 0.001163690195729335\n632/10000. loss: 0.0012937259549895923\n633/10000. loss: 0.0010110050595055025\n634/10000. loss: 0.0011444004097332556\n635/10000. loss: 0.0008754781447350979\n636/10000. loss: 0.0010706436975548665\n637/10000. loss: 0.0011162512625257175\n638/10000. loss: 0.0009904907395442326\n639/10000. loss: 0.0012216650259991486\n640/10000. loss: 0.0007222668112566074\n641/10000. loss: 0.0007833410054445267\n642/10000. loss: 0.0008967613490919272\n643/10000. loss: 0.0008358875444779793\n644/10000. loss: 0.0010318254741529624\n645/10000. loss: 0.0009498302824795246\n646/10000. loss: 0.0009171892888844013\n647/10000. loss: 0.0008345659201343855\n648/10000. loss: 0.0011564469120154779\n649/10000. loss: 0.0010193038421372573\n650/10000. loss: 0.0009689095119635264\n651/10000. loss: 0.001064819594224294\n652/10000. loss: 0.0013389438390731812\n653/10000. loss: 0.0007794084182629982\n654/10000. loss: 0.0012656017982711394\n655/10000. loss: 0.0008903818670660257\n656/10000. loss: 0.0007700185912350813\n657/10000. loss: 0.0012549621363480885\n658/10000. loss: 0.0009517906388888756\n659/10000. loss: 0.0009914613328874111\n660/10000. loss: 0.000706506660208106\n661/10000. loss: 0.001278515284260114\n662/10000. loss: 0.0007436717860400677\n663/10000. loss: 0.0015622883414228756\n664/10000. loss: 0.0010338789628197749\n665/10000. loss: 0.0010497687229265769\n666/10000. loss: 0.0011070136291285355\n667/10000. loss: 0.0008529399055987597\n668/10000. loss: 0.0010039873110751312\n669/10000. loss: 0.0011811487687130768\n670/10000. loss: 0.0015150238759815693\n671/10000. loss: 0.0012509638133148353\n672/10000. loss: 0.0011493571413060029\n673/10000. loss: 0.0016124256265660126\n674/10000. loss: 0.0015384986375768979\n675/10000. loss: 0.0014186700185139973\n676/10000. loss: 0.0017290421140690644\n677/10000. loss: 0.0011723126905659835\n678/10000. loss: 0.0014869463630020618\n679/10000. loss: 0.0010007841823001702\n680/10000. loss: 0.0015933145768940449\n681/10000. loss: 0.0010327065829187632\n682/10000. loss: 0.001353889238089323\n683/10000. loss: 0.0013544425989190738\n684/10000. loss: 0.001613718302299579\n685/10000. loss: 0.0014402777887880802\n686/10000. loss: 0.0014727301895618439\n687/10000. loss: 0.0011748004083832104\n688/10000. loss: 0.0014113377158840497\n689/10000. loss: 0.0011190840353568394\n690/10000. loss: 0.0008227881044149399\n691/10000. loss: 0.001103185738126437\n692/10000. loss: 0.0010289275863518317\n693/10000. loss: 0.0013706229316691558\n694/10000. loss: 0.0010813544504344463\n695/10000. loss: 0.0010766243406881888\n696/10000. loss: 0.001332754734903574\n697/10000. loss: 0.0014695703672866027\n698/10000. loss: 0.001102620658154289\n699/10000. loss: 0.0008785165846347809\n700/10000. loss: 0.0007533878864099582\n701/10000. loss: 0.0009653021891911825\n702/10000. loss: 0.0012187456401685874\n703/10000. loss: 0.0009211273863911629\n704/10000. loss: 0.0011344643620153267\n705/10000. loss: 0.0012356343989570935\n706/10000. loss: 0.0010824644317229588\n707/10000. loss: 0.0012775171392907698\n708/10000. loss: 0.0007090657794227203\n709/10000. loss: 0.001595042645931244\n710/10000. loss: 0.0012744572789718707\n711/10000. loss: 0.0012079610799749692\n712/10000. loss: 0.0007905747431019942\n713/10000. loss: 0.0010327940496305625\n714/10000. loss: 0.0015687684838970501\n715/10000. loss: 0.0011686836369335651\n716/10000. loss: 0.0010815751738846302\n717/10000. loss: 0.001138766606648763\n718/10000. loss: 0.001471366888533036\n719/10000. loss: 0.0013687096846600373\n720/10000. loss: 0.0010520354844629765\n721/10000. loss: 0.001680439958969752\n722/10000. loss: 0.0007608801436920961\n723/10000. loss: 0.0010585327787945669\n724/10000. loss: 0.0010848807481427987\n725/10000. loss: 0.0009356249744693438\n726/10000. loss: 0.0007984418577204148\n727/10000. loss: 0.000731330830603838\n728/10000. loss: 0.0010564043962707121\n729/10000. loss: 0.0010279160148153703\n730/10000. loss: 0.0007424419745802879\n731/10000. loss: 0.0009482183959335089\n732/10000. loss: 0.0012810400997598965\n733/10000. loss: 0.0011336659081280231\n734/10000. loss: 0.00079571851529181\n735/10000. loss: 0.0007532041830321153\n736/10000. loss: 0.0007019340991973877\n737/10000. loss: 0.0007204127808411916\n738/10000. loss: 0.001517689786851406\n739/10000. loss: 0.0013999234264095624\n740/10000. loss: 0.0011994844923416774\n741/10000. loss: 0.0013897271516422431\n742/10000. loss: 0.0008271062591423591\n743/10000. loss: 0.0009060822582493225\n744/10000. loss: 0.001116798259317875\n745/10000. loss: 0.0008412258078654607\n746/10000. loss: 0.0015436064762373765\n747/10000. loss: 0.0009332293023665746\n748/10000. loss: 0.000727982105066379\n749/10000. loss: 0.000615113570044438\n750/10000. loss: 0.0010406003954509895\n751/10000. loss: 0.0008387874501446883\n752/10000. loss: 0.0007057761152585348\n753/10000. loss: 0.001122086929778258\n754/10000. loss: 0.0010862274405856927\n755/10000. loss: 0.0011724679886053007\n756/10000. loss: 0.0007496663214017948\n757/10000. loss: 0.0012274134593705337\n758/10000. loss: 0.000651847260693709\n759/10000. loss: 0.0012441505677998066\n760/10000. loss: 0.0009528730685512224\n761/10000. loss: 0.0007049546887477239\n762/10000. loss: 0.0007293061353266239\n763/10000. loss: 0.0010638490008811157\n764/10000. loss: 0.0010409172003467877\n765/10000. loss: 0.000920896806443731\n766/10000. loss: 0.0006708800792694092\n767/10000. loss: 0.0007690828448782364\n768/10000. loss: 0.0008813537036379179\n769/10000. loss: 0.0008672209611783425\n770/10000. loss: 0.0008644863652686278\n771/10000. loss: 0.0010259314440190792\n772/10000. loss: 0.0006699057606359323\n773/10000. loss: 0.0006913170218467712\n774/10000. loss: 0.001103327376767993\n775/10000. loss: 0.0009098521744211515\n776/10000. loss: 0.0011214592183629672\n777/10000. loss: 0.0012478836191197236\n778/10000. loss: 0.001313239336013794\n779/10000. loss: 0.0010557068356623252\n780/10000. loss: 0.0010611210018396378\n781/10000. loss: 0.0014155103514591854\n782/10000. loss: 0.0008733329207946857\n783/10000. loss: 0.0012009614147245884\n784/10000. loss: 0.0009544049389660358\n785/10000. loss: 0.0013030000651876132\n786/10000. loss: 0.0010822468126813571\n787/10000. loss: 0.0008226054875801007\n788/10000. loss: 0.0012092154162625472\n789/10000. loss: 0.0012373058125376701\n790/10000. loss: 0.000725005908558766\n791/10000. loss: 0.0007469007590164741\n792/10000. loss: 0.0007344088517129421\n793/10000. loss: 0.001199032257621487\n794/10000. loss: 0.0010806812594334285\n795/10000. loss: 0.0007134169961015383\n796/10000. loss: 0.0006661584290365378\n797/10000. loss: 0.0012941401607046525\n798/10000. loss: 0.0011469824239611626\n799/10000. loss: 0.001136778388172388\n800/10000. loss: 0.000896446251620849\n801/10000. loss: 0.0009960029274225235\n802/10000. loss: 0.0006963772854457299\n803/10000. loss: 0.0011410638689994812\n804/10000. loss: 0.0007107183337211609\n805/10000. loss: 0.0010051655893524487\n806/10000. loss: 0.0010939705340812604\n807/10000. loss: 0.0006810243551929792\n808/10000. loss: 0.0008892626501619816\n809/10000. loss: 0.0012515163980424404\n810/10000. loss: 0.0011622091600050528\n811/10000. loss: 0.0009316598686079184\n812/10000. loss: 0.0009302631951868534\n813/10000. loss: 0.0010296618565917015\n814/10000. loss: 0.0011130243850251038\n815/10000. loss: 0.0007395460270345211\n816/10000. loss: 0.0010334116717179616\n817/10000. loss: 0.0007120898614327112\n818/10000. loss: 0.0006513257976621389\n819/10000. loss: 0.0008376174761603276\n820/10000. loss: 0.0013634702190756798\n821/10000. loss: 0.0011882716789841652\n822/10000. loss: 0.0013869342704614003\n823/10000. loss: 0.0011504351471861203\n824/10000. loss: 0.001094409419844548\n825/10000. loss: 0.0007529574601600567\n826/10000. loss: 0.0006628893315792084\n827/10000. loss: 0.0008999500423669815\n828/10000. loss: 0.0009426751639693975\n829/10000. loss: 0.0010699174211670954\n830/10000. loss: 0.0006609821381668249\n831/10000. loss: 0.000998347532004118\n832/10000. loss: 0.00084148277528584\n833/10000. loss: 0.0010686983975271385\n834/10000. loss: 0.0006497519401212534\n835/10000. loss: 0.0007501681490490834\n836/10000. loss: 0.0007386427217473587\n837/10000. loss: 0.0011135757279892762\n838/10000. loss: 0.0009917157391707103\n839/10000. loss: 0.0010368066529432933\n840/10000. loss: 0.0013634286200006802\n841/10000. loss: 0.0009640627540647984\n842/10000. loss: 0.0010321013008554776\n843/10000. loss: 0.0008542196204264959\n844/10000. loss: 0.0009925926569849253\n845/10000. loss: 0.0009578632501264414\n846/10000. loss: 0.0013510682620108128\n847/10000. loss: 0.0013915070643027623\n848/10000. loss: 0.0012371149690200884\n849/10000. loss: 0.0014589478572209675\n850/10000. loss: 0.0006962654491265615\n851/10000. loss: 0.0011068303138017654\n852/10000. loss: 0.0009151749933759371\n853/10000. loss: 0.0008671021399398645\n854/10000. loss: 0.0015570345955590408\n855/10000. loss: 0.0011903670771668355\n856/10000. loss: 0.000998536745707194\n857/10000. loss: 0.0008458453230559826\n858/10000. loss: 0.0007340725666532913\n859/10000. loss: 0.001113510923460126\n860/10000. loss: 0.0011012229758004348\n861/10000. loss: 0.0009831392671912909\n862/10000. loss: 0.000710073004787167\n863/10000. loss: 0.000766785970578591\n864/10000. loss: 0.0007690345713247856\n865/10000. loss: 0.0008902715829511484\n866/10000. loss: 0.0008268458768725395\n867/10000. loss: 0.001013701471189658\n868/10000. loss: 0.0007090588721136252\n869/10000. loss: 0.0011142461250225704\n870/10000. loss: 0.0006850722711533308\n871/10000. loss: 0.0011154965807994206\n872/10000. loss: 0.0007004082823793093\n873/10000. loss: 0.0010724180998901527\n874/10000. loss: 0.0006621825353552898\n875/10000. loss: 0.0010093123031159241\n876/10000. loss: 0.0008488826764126619\n877/10000. loss: 0.0008334641655286154\n878/10000. loss: 0.0010589763211707275\n879/10000. loss: 0.0011515567700068157\n880/10000. loss: 0.0009241014098127683\n881/10000. loss: 0.0007183868438005447\n882/10000. loss: 0.0006531088147312403\n883/10000. loss: 0.0009430385349939266\n884/10000. loss: 0.0009396900422871113\n885/10000. loss: 0.0010657695432504017\n886/10000. loss: 0.0010189935564994812\n887/10000. loss: 0.0011977691513796647\n888/10000. loss: 0.0009558695989350477\n889/10000. loss: 0.0012809406034648418\n890/10000. loss: 0.0010926829030116398\n891/10000. loss: 0.0019039842300117016\n892/10000. loss: 0.0012857736243555944\n893/10000. loss: 0.0010713588756819565\n894/10000. loss: 0.0010562443640083075\n895/10000. loss: 0.0013007161517937977\n896/10000. loss: 0.001146759216984113\n897/10000. loss: 0.0012629026702294748\n898/10000. loss: 0.0012344086232284706\n899/10000. loss: 0.002042769609640042\n900/10000. loss: 0.002401590347290039\n901/10000. loss: 0.001744188057879607\n902/10000. loss: 0.001558183381954829\n903/10000. loss: 0.002414791223903497\n904/10000. loss: 0.002027482104798158\n905/10000. loss: 0.0026631044844786325\n906/10000. loss: 0.0013370304368436337\n907/10000. loss: 0.0014150887727737427\n908/10000. loss: 0.002270200445006291\n909/10000. loss: 0.002724994905292988\n910/10000. loss: 0.0022655779806276164\n911/10000. loss: 0.002285315034290155\n912/10000. loss: 0.0024647669245799384\n913/10000. loss: 0.0014471506389478843\n914/10000. loss: 0.002267514200260242\n915/10000. loss: 0.002820677434404691\n916/10000. loss: 0.002455874035755793\n917/10000. loss: 0.0024262008567651114\n918/10000. loss: 0.0026084966957569122\n919/10000. loss: 0.00206682737916708\n920/10000. loss: 0.002709982916712761\n921/10000. loss: 0.0027662826081116996\n922/10000. loss: 0.0033806941161553064\n923/10000. loss: 0.002511189008752505\n924/10000. loss: 0.00205352995544672\n925/10000. loss: 0.002454129047691822\n926/10000. loss: 0.0028263855104645095\n927/10000. loss: 0.0029741904387871423\n928/10000. loss: 0.0038370871916413307\n929/10000. loss: 0.0034941742196679115\n930/10000. loss: 0.0028142749021450677\n931/10000. loss: 0.0019694495325287185\n932/10000. loss: 0.0032270302375157676\n933/10000. loss: 0.003199638177951177\n934/10000. loss: 0.0027038843060533204\n935/10000. loss: 0.003086839492122332\n936/10000. loss: 0.0033210869878530502\n937/10000. loss: 0.003339232876896858\n938/10000. loss: 0.003263902540008227\n939/10000. loss: 0.004058142192661762\n940/10000. loss: 0.0031249293436606727\n941/10000. loss: 0.0022079674527049065\n942/10000. loss: 0.0023928314136962094\n943/10000. loss: 0.0018416144885122776\n944/10000. loss: 0.003010471041003863\n945/10000. loss: 0.0016132667660713196\n946/10000. loss: 0.0031403138612707457\n947/10000. loss: 0.002712842387457689\n948/10000. loss: 0.002154387223223845\n949/10000. loss: 0.0019647623412311077\n950/10000. loss: 0.00340752179423968\n951/10000. loss: 0.0022613598654667535\n952/10000. loss: 0.0027617579326033592\n953/10000. loss: 0.002522882384558519\n954/10000. loss: 0.0023198891431093216\n955/10000. loss: 0.0022785146720707417\n956/10000. loss: 0.0021578053322931132\n957/10000. loss: 0.002812842217584451\n958/10000. loss: 0.0014263334063192208\n959/10000. loss: 0.0013162163086235523\n960/10000. loss: 0.0019524435823162396\n961/10000. loss: 0.0014172530112167199\n962/10000. loss: 0.001499911459783713\n963/10000. loss: 0.0015537912646929424\n964/10000. loss: 0.0013107027237613995\n965/10000. loss: 0.001581259227047364\n966/10000. loss: 0.0016052362819512684\n967/10000. loss: 0.0016794954426586628\n968/10000. loss: 0.00164827440554897\n969/10000. loss: 0.0011967494307706754\n970/10000. loss: 0.0015020024341841538\n971/10000. loss: 0.0016397808988889058\n972/10000. loss: 0.001533213226745526\n973/10000. loss: 0.0008990616382410129\n974/10000. loss: 0.0013626657115916412\n975/10000. loss: 0.0018536679757138093\n976/10000. loss: 0.0015467782504856586\n977/10000. loss: 0.001342254380385081\n978/10000. loss: 0.0014445744454860687\n979/10000. loss: 0.001302502118051052\n980/10000. loss: 0.000777561217546463\n981/10000. loss: 0.0008054698506991068\n982/10000. loss: 0.0011332939223696787\n983/10000. loss: 0.0008058432334413131\n984/10000. loss: 0.0014936705119907856\n985/10000. loss: 0.0008984596158067385\n986/10000. loss: 0.0012161604439218838\n987/10000. loss: 0.001182319363579154\n988/10000. loss: 0.0011177370324730873\n989/10000. loss: 0.0012018132644395034\n990/10000. loss: 0.0008458836625019709\n991/10000. loss: 0.001038034213706851\n992/10000. loss: 0.0007310575650384029\n993/10000. loss: 0.001502409887810548\n994/10000. loss: 0.001538132627805074\n995/10000. loss: 0.0011225300841033459\n996/10000. loss: 0.0010084548654655616\n997/10000. loss: 0.0007666651314745346\n998/10000. loss: 0.0010076228839655716\n999/10000. loss: 0.0013095447793602943\n1000/10000. loss: 0.0009382712499548992\n1001/10000. loss: 0.0007437064001957575\n1002/10000. loss: 0.0011015857259432475\n1003/10000. loss: 0.0010099511127918959\n1004/10000. loss: 0.0012666276500870783\n1005/10000. loss: 0.0010721548460423946\n1006/10000. loss: 0.0009131296537816525\n1007/10000. loss: 0.001002433088918527\n1008/10000. loss: 0.001278747261191408\n1009/10000. loss: 0.0007642236693451802\n1010/10000. loss: 0.0007793067488819361\n1011/10000. loss: 0.0008741985075175762\n1012/10000. loss: 0.0010322417753438156\n1013/10000. loss: 0.0008834201532105604\n1014/10000. loss: 0.0008351909151921669\n1015/10000. loss: 0.0007506647768119971\n1016/10000. loss: 0.0010454166525353987\n1017/10000. loss: 0.0006802322653432687\n1018/10000. loss: 0.0007016840390861034\n1019/10000. loss: 0.0014199800789356232\n1020/10000. loss: 0.001345919445157051\n1021/10000. loss: 0.0012960822011033695\n1022/10000. loss: 0.0010364831735690434\n1023/10000. loss: 0.0008802007262905439\n1024/10000. loss: 0.00101687580657502\n1025/10000. loss: 0.0008320799097418785\n1026/10000. loss: 0.001054618585233887\n1027/10000. loss: 0.0008700063141683737\n1028/10000. loss: 0.0014594805737336476\n1029/10000. loss: 0.000694722713281711\n1030/10000. loss: 0.0008471477000663677\n1031/10000. loss: 0.0006735564675182104\n1032/10000. loss: 0.0010473895817995071\n1033/10000. loss: 0.0008813319727778435\n1034/10000. loss: 0.0010084187767157953\n1035/10000. loss: 0.0006914973103751739\n1036/10000. loss: 0.0005716986488550901\n1037/10000. loss: 0.0006644489088406166\n1038/10000. loss: 0.0005932893448819717\n1039/10000. loss: 0.0008691802310446898\n1040/10000. loss: 0.0009169937111437321\n1041/10000. loss: 0.0008559202154477438\n1042/10000. loss: 0.0005802885474016269\n1043/10000. loss: 0.0010471000180890162\n1044/10000. loss: 0.0014270152896642685\n1045/10000. loss: 0.0005809296077738205\n1046/10000. loss: 0.0007918556220829487\n1047/10000. loss: 0.000792675573999683\n1048/10000. loss: 0.0008462954623003801\n1049/10000. loss: 0.0008628383123626312\n1050/10000. loss: 0.0005777743256961306\n1051/10000. loss: 0.0009436630643904209\n1052/10000. loss: 0.0012205940050383408\n1053/10000. loss: 0.0008199665074547132\n1054/10000. loss: 0.0006046261017521223\n1055/10000. loss: 0.0006095658366878828\n1056/10000. loss: 0.0009215283207595348\n1057/10000. loss: 0.0006892033852636814\n1058/10000. loss: 0.0009335811094691356\n1059/10000. loss: 0.0007571599756677946\n1060/10000. loss: 0.0008437333938976129\n1061/10000. loss: 0.0009526953411599001\n1062/10000. loss: 0.0005400585165868202\n1063/10000. loss: 0.0009606854679683844\n1064/10000. loss: 0.0006282350514084101\n1065/10000. loss: 0.0010310492167870204\n1066/10000. loss: 0.0008922898365805546\n1067/10000. loss: 0.000954273467262586\n1068/10000. loss: 0.0008221800283839306\n1069/10000. loss: 0.0005672949676712354\n1070/10000. loss: 0.001067595633988579\n1071/10000. loss: 0.0008285189978778362\n1072/10000. loss: 0.0008975386153906584\n1073/10000. loss: 0.0006466197470823923\n1074/10000. loss: 0.0008279494165132443\n1075/10000. loss: 0.0007889422898491224\n1076/10000. loss: 0.0006185084348544478\n1077/10000. loss: 0.0012832448507348697\n1078/10000. loss: 0.0013123160849014919\n1079/10000. loss: 0.0006360103531430165\n1080/10000. loss: 0.0008840411125371853\n1081/10000. loss: 0.0006763976998627186\n1082/10000. loss: 0.0005590757355093956\n1083/10000. loss: 0.0006961394101381302\n1084/10000. loss: 0.0005543947142238418\n1085/10000. loss: 0.0009963922202587128\n1086/10000. loss: 0.0008146805533518394\n1087/10000. loss: 0.0006835584839185079\n1088/10000. loss: 0.0009228100534528494\n1089/10000. loss: 0.0008256489721437296\n1090/10000. loss: 0.0010278810126086075\n1091/10000. loss: 0.0011596417364974816\n1092/10000. loss: 0.0010049797128885984\n1093/10000. loss: 0.0006806187642117342\n1094/10000. loss: 0.0008342453899482886\n1095/10000. loss: 0.0005625313691173991\n1096/10000. loss: 0.0008047387624780337\n1097/10000. loss: 0.0009504319168627262\n1098/10000. loss: 0.0006305309555803736\n1099/10000. loss: 0.0005922178970649838\n1100/10000. loss: 0.0010485649884988864\n1101/10000. loss: 0.0006686076521873474\n1102/10000. loss: 0.0006173238313446442\n1103/10000. loss: 0.0007546793203800917\n1104/10000. loss: 0.0007798438891768456\n1105/10000. loss: 0.000987701816484332\n1106/10000. loss: 0.0005786638163651029\n1107/10000. loss: 0.000728108532105883\n1108/10000. loss: 0.0010508066043257713\n1109/10000. loss: 0.0009940162611504395\n1110/10000. loss: 0.0009065386839210987\n1111/10000. loss: 0.0011359748896211386\n1112/10000. loss: 0.0007711906606952349\n1113/10000. loss: 0.0006379621724287669\n1114/10000. loss: 0.000748098362237215\n1115/10000. loss: 0.0009654266759753227\n1116/10000. loss: 0.0009127329879750808\n1117/10000. loss: 0.0008478705616046985\n1118/10000. loss: 0.0006795548833906651\n1119/10000. loss: 0.000538336462341249\n1120/10000. loss: 0.0009285258129239082\n1121/10000. loss: 0.0005326518245662252\n1122/10000. loss: 0.0007057522113124529\n1123/10000. loss: 0.0010095970549931128\n1124/10000. loss: 0.0009097141834596792\n1125/10000. loss: 0.000757504099359115\n1126/10000. loss: 0.0005035276214281718\n1127/10000. loss: 0.0006292715358237425\n1128/10000. loss: 0.0009172225836664438\n1129/10000. loss: 0.0004717198899015784\n1130/10000. loss: 0.0010441153620680173\n1131/10000. loss: 0.000789102011670669\n1132/10000. loss: 0.0007735069375485182\n1133/10000. loss: 0.0007292095882197221\n1134/10000. loss: 0.0007926667264352242\n1135/10000. loss: 0.0009298294316977262\n1136/10000. loss: 0.0005709644950305423\n1137/10000. loss: 0.000575797283090651\n1138/10000. loss: 0.0010354200688501198\n1139/10000. loss: 0.0010587765524784725\n1140/10000. loss: 0.0008454367828865846\n1141/10000. loss: 0.0011756789560119312\n1142/10000. loss: 0.0008918653863171736\n1143/10000. loss: 0.001044188781330983\n1144/10000. loss: 0.000918978825211525\n1145/10000. loss: 0.0010219245838622253\n1146/10000. loss: 0.0006117148247237006\n1147/10000. loss: 0.0006636053634186586\n1148/10000. loss: 0.0005636306401963035\n1149/10000. loss: 0.0008575016787896553\n1150/10000. loss: 0.0007158075459301472\n1151/10000. loss: 0.0011174116904536884\n1152/10000. loss: 0.0011586561643828948\n1153/10000. loss: 0.0005061962098504106\n1154/10000. loss: 0.0006772427198787531\n1155/10000. loss: 0.000552037808423241\n1156/10000. loss: 0.0007707232919832071\n1157/10000. loss: 0.0005519726934532324\n1158/10000. loss: 0.0006914110078165928\n1159/10000. loss: 0.00058559017876784\n1160/10000. loss: 0.0008306185094018778\n1161/10000. loss: 0.0008954708464443684\n1162/10000. loss: 0.0010132680957516034\n1163/10000. loss: 0.0005147873889654875\n1164/10000. loss: 0.00072662935902675\n1165/10000. loss: 0.0008418946526944637\n1166/10000. loss: 0.000580940512008965\n1167/10000. loss: 0.0008090783376246691\n1168/10000. loss: 0.0007190430381645759\n1169/10000. loss: 0.00048671259234348935\n1170/10000. loss: 0.0010288663518925507\n1171/10000. loss: 0.0006711936245361964\n1172/10000. loss: 0.0005088032533725103\n1173/10000. loss: 0.0005264135543256998\n1174/10000. loss: 0.000937154982239008\n1175/10000. loss: 0.0006344718082497517\n1176/10000. loss: 0.0007834343705326319\n1177/10000. loss: 0.0004692124202847481\n1178/10000. loss: 0.0008206313941627741\n1179/10000. loss: 0.0007316571039458116\n1180/10000. loss: 0.0005357212309415141\n1181/10000. loss: 0.0007419111207127571\n1182/10000. loss: 0.0011135791428387165\n1183/10000. loss: 0.000515864153082172\n1184/10000. loss: 0.0007921805760512749\n1185/10000. loss: 0.0008839584576586882\n1186/10000. loss: 0.0008067210825781027\n1187/10000. loss: 0.0007456648163497448\n1188/10000. loss: 0.0007062433287501335\n1189/10000. loss: 0.0007649162628998359\n1190/10000. loss: 0.001021070871502161\n1191/10000. loss: 0.0008163037709891796\n1192/10000. loss: 0.0007672406112154325\n1193/10000. loss: 0.0006848759949207306\n1194/10000. loss: 0.0005955634017785391\n1195/10000. loss: 0.0012333701209475596\n1196/10000. loss: 0.0013355597232778866\n1197/10000. loss: 0.0004841665892551343\n1198/10000. loss: 0.0008995635434985161\n1199/10000. loss: 0.0006996607407927513\n1200/10000. loss: 0.0009097338964541753\n1201/10000. loss: 0.0010123023142417271\n1202/10000. loss: 0.0005822020660464963\n1203/10000. loss: 0.0005679447979976734\n1204/10000. loss: 0.0008936238009482622\n1205/10000. loss: 0.000789708225056529\n1206/10000. loss: 0.0011023723054677248\n1207/10000. loss: 0.0010613173556824524\n1208/10000. loss: 0.0008730808428178231\n1209/10000. loss: 0.0012293473506967227\n1210/10000. loss: 0.0007115496943394343\n1211/10000. loss: 0.001347240371008714\n1212/10000. loss: 0.0010383497768392165\n1213/10000. loss: 0.0006861986281971136\n1214/10000. loss: 0.0006779802497476339\n1215/10000. loss: 0.0014253056918581326\n1216/10000. loss: 0.0006023054787268242\n1217/10000. loss: 0.00068878677363197\n1218/10000. loss: 0.001001686944315831\n1219/10000. loss: 0.001005291473120451\n1220/10000. loss: 0.0006210632467021545\n1221/10000. loss: 0.0009549041278660297\n1222/10000. loss: 0.0008530714549124241\n1223/10000. loss: 0.0006127293066432079\n1224/10000. loss: 0.0006751518230885267\n1225/10000. loss: 0.0007016495801508427\n1226/10000. loss: 0.0006778423363963763\n1227/10000. loss: 0.0007801991887390614\n1228/10000. loss: 0.0010805570830901463\n1229/10000. loss: 0.0006328186330695947\n1230/10000. loss: 0.0007303863919029633\n1231/10000. loss: 0.0010475833745052416\n1232/10000. loss: 0.0011116570482651393\n1233/10000. loss: 0.0010105065690974395\n1234/10000. loss: 0.0009133926747987667\n1235/10000. loss: 0.0011565927416086197\n1236/10000. loss: 0.0007637756255765756\n1237/10000. loss: 0.0012360143785675366\n1238/10000. loss: 0.0013061710633337498\n1239/10000. loss: 0.0015871223683158557\n1240/10000. loss: 0.001462882695098718\n1241/10000. loss: 0.00082135076324145\n1242/10000. loss: 0.0015580362329880397\n1243/10000. loss: 0.0013532270677387714\n1244/10000. loss: 0.0015753939126928647\n1245/10000. loss: 0.0011298762013514836\n1246/10000. loss: 0.0015094482029477756\n1247/10000. loss: 0.0010147732682526112\n1248/10000. loss: 0.0009099649420628945\n1249/10000. loss: 0.0007970331547160944\n1250/10000. loss: 0.000833376698816816\n1251/10000. loss: 0.0011445178339878719\n1252/10000. loss: 0.001189605953792731\n1253/10000. loss: 0.00099923446153601\n1254/10000. loss: 0.0006927596405148506\n1255/10000. loss: 0.0010130535811185837\n1256/10000. loss: 0.0010458565472314756\n1257/10000. loss: 0.0006611102726310492\n1258/10000. loss: 0.0009461863276859125\n1259/10000. loss: 0.0009702055249363184\n1260/10000. loss: 0.0008780043572187424\n1261/10000. loss: 0.0007464016477266947\n1262/10000. loss: 0.0011978964321315289\n1263/10000. loss: 0.0007170333216587702\n1264/10000. loss: 0.0009418266514937083\n1265/10000. loss: 0.000807689968496561\n1266/10000. loss: 0.0009617316536605358\n1267/10000. loss: 0.0006440178646395603\n1268/10000. loss: 0.0006596008315682411\n1269/10000. loss: 0.0008611985637495915\n1270/10000. loss: 0.0009096119708071152\n1271/10000. loss: 0.0009348096015552679\n1272/10000. loss: 0.0012958050550272067\n1273/10000. loss: 0.000773953894774119\n1274/10000. loss: 0.0008744988590478897\n1275/10000. loss: 0.000826110364869237\n1276/10000. loss: 0.0008556459409495195\n1277/10000. loss: 0.0007216883823275566\n1278/10000. loss: 0.0008874175449212393\n1279/10000. loss: 0.0011642752215266228\n1280/10000. loss: 0.000681719354664286\n1281/10000. loss: 0.0007976608661313852\n1282/10000. loss: 0.001241672628869613\n1283/10000. loss: 0.0008776127360761166\n1284/10000. loss: 0.000809732669343551\n1285/10000. loss: 0.0010728253982961178\n1286/10000. loss: 0.0009988481178879738\n1287/10000. loss: 0.0005497564561665058\n1288/10000. loss: 0.001031944217781226\n1289/10000. loss: 0.0006936602294445038\n1290/10000. loss: 0.0009215189299235741\n1291/10000. loss: 0.000973459721232454\n1292/10000. loss: 0.001251151707644264\n1293/10000. loss: 0.0008606708919008573\n1294/10000. loss: 0.0009772854391485453\n1295/10000. loss: 0.0006405141903087497\n1296/10000. loss: 0.0011454927735030651\n1297/10000. loss: 0.0009396678457657496\n1298/10000. loss: 0.0007458798742542664\n1299/10000. loss: 0.001052070486669739\n1300/10000. loss: 0.0007770949353774389\n1301/10000. loss: 0.0007298480098446211\n1302/10000. loss: 0.0005854920794566473\n1303/10000. loss: 0.0007736127202709516\n1304/10000. loss: 0.0008305910353859266\n1305/10000. loss: 0.0007951539009809494\n1306/10000. loss: 0.0010087005017946165\n1307/10000. loss: 0.0010215668007731438\n1308/10000. loss: 0.0008866233595957359\n1309/10000. loss: 0.0010757970158010721\n1310/10000. loss: 0.000989519835760196\n1311/10000. loss: 0.0009954679602136214\n1312/10000. loss: 0.0008563464507460594\n1313/10000. loss: 0.0008406082633882761\n1314/10000. loss: 0.001152900590871771\n1315/10000. loss: 0.0013014900032430887\n1316/10000. loss: 0.0010613281435022752\n1317/10000. loss: 0.000812049334247907\n1318/10000. loss: 0.0012018847434471052\n1319/10000. loss: 0.001027432270348072\n1320/10000. loss: 0.0006841137850036224\n1321/10000. loss: 0.0008943841482202212\n1322/10000. loss: 0.0007115465899308523\n1323/10000. loss: 0.0010151177023847897\n1324/10000. loss: 0.0007314023872216543\n1325/10000. loss: 0.0009701968325922886\n1326/10000. loss: 0.0010001221671700478\n1327/10000. loss: 0.0006332140571127335\n1328/10000. loss: 0.0008963251020759344\n1329/10000. loss: 0.0009453198872506618\n1330/10000. loss: 0.0008231879522403082\n1331/10000. loss: 0.0005544848584880432\n1332/10000. loss: 0.0009074361684421698\n1333/10000. loss: 0.0010112229113777478\n1334/10000. loss: 0.0007630611459414164\n1335/10000. loss: 0.0005543774071459969\n1336/10000. loss: 0.0006056080649917325\n1337/10000. loss: 0.0009441045112907887\n1338/10000. loss: 0.0006998046301305294\n1339/10000. loss: 0.0008703280861179034\n1340/10000. loss: 0.0010094215006877978\n1341/10000. loss: 0.0005404708984618386\n1342/10000. loss: 0.0007944646446655194\n1343/10000. loss: 0.000581619911827147\n1344/10000. loss: 0.000781543863316377\n1345/10000. loss: 0.0008018087440480789\n1346/10000. loss: 0.0007560945426424345\n1347/10000. loss: 0.000816570594906807\n1348/10000. loss: 0.0008031482187410196\n1349/10000. loss: 0.000660687064131101\n1350/10000. loss: 0.0008329905879994234\n1351/10000. loss: 0.0008018425044914087\n1352/10000. loss: 0.0009426930919289589\n1353/10000. loss: 0.0008306521146247784\n1354/10000. loss: 0.0012866702551643054\n1355/10000. loss: 0.0007813865474114815\n1356/10000. loss: 0.0009476508324344953\n1357/10000. loss: 0.001504549290984869\n1358/10000. loss: 0.0011492001358419657\n1359/10000. loss: 0.001069035225858291\n1360/10000. loss: 0.0010135009263952572\n1361/10000. loss: 0.0013009928322086732\n1362/10000. loss: 0.001258459718277057\n1363/10000. loss: 0.001119726647933324\n1364/10000. loss: 0.0014113144328196843\n1365/10000. loss: 0.001448180681715409\n1366/10000. loss: 0.001338376197963953\n1367/10000. loss: 0.001455234984556834\n1368/10000. loss: 0.0010899129944543044\n1369/10000. loss: 0.0015398052831490834\n1370/10000. loss: 0.0015933516745766003\n1371/10000. loss: 0.001361946730564038\n1372/10000. loss: 0.0014744391664862633\n1373/10000. loss: 0.001375445630401373\n1374/10000. loss: 0.0009219284014155468\n1375/10000. loss: 0.0008433670736849308\n1376/10000. loss: 0.0011998345144093037\n1377/10000. loss: 0.0008575709847112497\n1378/10000. loss: 0.0012997474210957687\n1379/10000. loss: 0.0009956121599922578\n1380/10000. loss: 0.001172035001218319\n1381/10000. loss: 0.0011095303731660049\n1382/10000. loss: 0.0016631679609417915\n1383/10000. loss: 0.0006760486091176668\n1384/10000. loss: 0.0015204766144355137\n1385/10000. loss: 0.001419634868701299\n1386/10000. loss: 0.0014631644201775391\n1387/10000. loss: 0.0012215026654303074\n1388/10000. loss: 0.0015493097404638927\n1389/10000. loss: 0.0012818717708190281\n1390/10000. loss: 0.0011607541237026453\n1391/10000. loss: 0.0012573345253864925\n1392/10000. loss: 0.0019479583327968915\n1393/10000. loss: 0.0011398571853836377\n1394/10000. loss: 0.001216256758198142\n1395/10000. loss: 0.0011235765026261408\n1396/10000. loss: 0.0012900617439299822\n1397/10000. loss: 0.0009745640369753042\n1398/10000. loss: 0.0013998357268671195\n1399/10000. loss: 0.0011030676153798897\n1400/10000. loss: 0.001332309873153766\n1401/10000. loss: 0.0009289919398725033\n1402/10000. loss: 0.0006666156308104595\n1403/10000. loss: 0.0008210733843346437\n1404/10000. loss: 0.0006552710353086392\n1405/10000. loss: 0.0012331274338066578\n1406/10000. loss: 0.001087567846601208\n1407/10000. loss: 0.000859173092370232\n1408/10000. loss: 0.00110063833805422\n1409/10000. loss: 0.0008108386149009069\n1410/10000. loss: 0.0006544158483544985\n1411/10000. loss: 0.0007312096810589234\n1412/10000. loss: 0.0010163990470270317\n1413/10000. loss: 0.0005934792570769787\n1414/10000. loss: 0.0008706835409005483\n1415/10000. loss: 0.001199066328505675\n1416/10000. loss: 0.0009065473762651285\n1417/10000. loss: 0.00090990603591005\n1418/10000. loss: 0.0006577588307360808\n1419/10000. loss: 0.0009817984731247027\n1420/10000. loss: 0.0008143001856903235\n1421/10000. loss: 0.001212194561958313\n1422/10000. loss: 0.0009409958341469368\n1423/10000. loss: 0.0008817593722293774\n1424/10000. loss: 0.001007729209959507\n1425/10000. loss: 0.0008461784260968367\n1426/10000. loss: 0.0008437937746445338\n1427/10000. loss: 0.0008737208942572275\n1428/10000. loss: 0.0007126405059049526\n1429/10000. loss: 0.000927246098096172\n1430/10000. loss: 0.0011038055332998435\n1431/10000. loss: 0.0007703745892892281\n1432/10000. loss: 0.0009072034154087305\n1433/10000. loss: 0.000636604924996694\n1434/10000. loss: 0.0007517222935954729\n1435/10000. loss: 0.000857981542746226\n1436/10000. loss: 0.0009598820470273495\n1437/10000. loss: 0.0007020829555888971\n1438/10000. loss: 0.0006875175361831983\n1439/10000. loss: 0.0008674048197766145\n1440/10000. loss: 0.00047951098531484604\n1441/10000. loss: 0.0008030662623544534\n1442/10000. loss: 0.0008992554309467474\n1443/10000. loss: 0.0008497283949206272\n1444/10000. loss: 0.000968738691881299\n1445/10000. loss: 0.0008459568489342928\n1446/10000. loss: 0.0009636137789736191\n1447/10000. loss: 0.0005486217560246587\n1448/10000. loss: 0.0008858332100013891\n1449/10000. loss: 0.0007859633769840002\n1450/10000. loss: 0.0009830837758878868\n1451/10000. loss: 0.0008777346617231766\n1452/10000. loss: 0.0007878495380282402\n1453/10000. loss: 0.0010996080624560516\n1454/10000. loss: 0.0007234451671441396\n1455/10000. loss: 0.0006923467541734377\n1456/10000. loss: 0.0009169050802787145\n1457/10000. loss: 0.0006491344732542833\n1458/10000. loss: 0.0006798718435068926\n1459/10000. loss: 0.0005631238066901764\n1460/10000. loss: 0.001050337605799238\n1461/10000. loss: 0.0010487592468659084\n1462/10000. loss: 0.0011489244643598795\n1463/10000. loss: 0.0014387539898355801\n1464/10000. loss: 0.0006669051945209503\n1465/10000. loss: 0.0010434419382363558\n1466/10000. loss: 0.001015420847882827\n1467/10000. loss: 0.0009519299492239952\n1468/10000. loss: 0.0008802083320915699\n1469/10000. loss: 0.0007800799794495106\n1470/10000. loss: 0.0006342027336359024\n1471/10000. loss: 0.0007388472246627013\n1472/10000. loss: 0.0009766477936257918\n1473/10000. loss: 0.001004772571225961\n1474/10000. loss: 0.001088073942810297\n1475/10000. loss: 0.0010540150105953217\n1476/10000. loss: 0.0016603730618953705\n1477/10000. loss: 0.0006628520010660092\n1478/10000. loss: 0.0011632378833989303\n1479/10000. loss: 0.0008019307473053535\n1480/10000. loss: 0.0010500775339702766\n1481/10000. loss: 0.0012297766904036205\n1482/10000. loss: 0.000987195565054814\n1483/10000. loss: 0.0008267678786069155\n1484/10000. loss: 0.0012489000024894874\n1485/10000. loss: 0.0008260776133586963\n1486/10000. loss: 0.0006433515421425303\n1487/10000. loss: 0.0009480046574026346\n1488/10000. loss: 0.001202336357285579\n1489/10000. loss: 0.0008363809126118819\n1490/10000. loss: 0.0008261518863340219\n1491/10000. loss: 0.0009790816499541204\n1492/10000. loss: 0.0009423003842433294\n1493/10000. loss: 0.0008499551719675461\n1494/10000. loss: 0.0009387037716805935\n1495/10000. loss: 0.0009692676831036806\n1496/10000. loss: 0.000852604474251469\n1497/10000. loss: 0.0009752856567502022\n1498/10000. loss: 0.0012192366023858388\n1499/10000. loss: 0.0011428707124044497\n1500/10000. loss: 0.0009072400474299988\n1501/10000. loss: 0.0006185867823660374\n1502/10000. loss: 0.0009632030657182137\n1503/10000. loss: 0.000820852272833387\n1504/10000. loss: 0.0009252976936598619\n1505/10000. loss: 0.0008873601133624712\n1506/10000. loss: 0.0011233182934423287\n1507/10000. loss: 0.000750393761942784\n1508/10000. loss: 0.0010272586562981207\n1509/10000. loss: 0.0007266670775910219\n1510/10000. loss: 0.0009565929261346658\n1511/10000. loss: 0.0007423927697042624\n1512/10000. loss: 0.0011802089090148609\n1513/10000. loss: 0.0008712383763243755\n1514/10000. loss: 0.0011331684266527493\n1515/10000. loss: 0.0005228253624712428\n1516/10000. loss: 0.0012322345282882452\n1517/10000. loss: 0.0008912064755956332\n1518/10000. loss: 0.0008473582565784454\n1519/10000. loss: 0.0011166627518832684\n1520/10000. loss: 0.0012026389595121145\n1521/10000. loss: 0.0007267810093859831\n1522/10000. loss: 0.0007297095532218615\n1523/10000. loss: 0.0013088107419510682\n1524/10000. loss: 0.0008502323180437088\n1525/10000. loss: 0.0012103430926799774\n1526/10000. loss: 0.0012683042635520299\n1527/10000. loss: 0.0007864529422173897\n1528/10000. loss: 0.001992313346515099\n1529/10000. loss: 0.0010932930745184422\n1530/10000. loss: 0.0012404965236783028\n1531/10000. loss: 0.0015087869639197986\n1532/10000. loss: 0.0009678429923951626\n1533/10000. loss: 0.0015631926556428273\n1534/10000. loss: 0.0010620987353225548\n1535/10000. loss: 0.0012330257644255955\n1536/10000. loss: 0.002024364657700062\n1537/10000. loss: 0.0010248368295530479\n1538/10000. loss: 0.002300417826821407\n1539/10000. loss: 0.001667261899759372\n1540/10000. loss: 0.00110049305173258\n1541/10000. loss: 0.0015010670758783817\n1542/10000. loss: 0.001362727799763282\n1543/10000. loss: 0.0011624897985408704\n1544/10000. loss: 0.0012043213937431574\n1545/10000. loss: 0.001994913754363855\n1546/10000. loss: 0.001790929740915696\n1547/10000. loss: 0.0016721471523245175\n1548/10000. loss: 0.0010411683470010757\n1549/10000. loss: 0.0009125911941130956\n1550/10000. loss: 0.001984446464727322\n1551/10000. loss: 0.001012023848791917\n1552/10000. loss: 0.0012248487522204716\n1553/10000. loss: 0.0013788724318146706\n1554/10000. loss: 0.0007650047385444244\n1555/10000. loss: 0.000979671875635783\n1556/10000. loss: 0.0010374061142404873\n1557/10000. loss: 0.001172407219807307\n1558/10000. loss: 0.000970358494669199\n1559/10000. loss: 0.0010836923029273748\n1560/10000. loss: 0.0007195296542098125\n1561/10000. loss: 0.0011233972230305274\n1562/10000. loss: 0.0007133789670964082\n1563/10000. loss: 0.0010863204176227252\n1564/10000. loss: 0.000633798500833412\n1565/10000. loss: 0.001076305905977885\n1566/10000. loss: 0.0009789603451887767\n1567/10000. loss: 0.0006814231940855583\n1568/10000. loss: 0.0007432803201178709\n1569/10000. loss: 0.0005601282852391402\n1570/10000. loss: 0.0010509970597922802\n1571/10000. loss: 0.000871188472956419\n1572/10000. loss: 0.0006216098554432392\n1573/10000. loss: 0.0007530401926487684\n1574/10000. loss: 0.000764498021453619\n1575/10000. loss: 0.0008497933546702067\n1576/10000. loss: 0.000847154917816321\n1577/10000. loss: 0.0005435465524593989\n1578/10000. loss: 0.0005613271690284213\n1579/10000. loss: 0.0008193943649530411\n1580/10000. loss: 0.000490365355896453\n1581/10000. loss: 0.0005682541135077676\n1582/10000. loss: 0.0007647136226296425\n1583/10000. loss: 0.000524998603699108\n1584/10000. loss: 0.0005556699664642414\n1585/10000. loss: 0.0007995980946967999\n1586/10000. loss: 0.000513662351295352\n1587/10000. loss: 0.0006316234357655048\n1588/10000. loss: 0.0004928521035859982\n1589/10000. loss: 0.00063107597331206\n1590/10000. loss: 0.0007073871480921904\n1591/10000. loss: 0.0006914257537573576\n1592/10000. loss: 0.0005924647363523642\n1593/10000. loss: 0.000693304929882288\n1594/10000. loss: 0.0007844827293107907\n1595/10000. loss: 0.00044429391467322904\n1596/10000. loss: 0.0005135110501820842\n1597/10000. loss: 0.0005766248796135187\n1598/10000. loss: 0.0005193661587933699\n1599/10000. loss: 0.0006884032239516576\n1600/10000. loss: 0.0009665406929949919\n1601/10000. loss: 0.0006575748945275942\n1602/10000. loss: 0.0009738883624474207\n1603/10000. loss: 0.0006816376311083635\n1604/10000. loss: 0.0007926063456883033\n1605/10000. loss: 0.0007365723140537739\n1606/10000. loss: 0.0010621725426365931\n1607/10000. loss: 0.0005428282699237267\n1608/10000. loss: 0.0005637968424707651\n1609/10000. loss: 0.0009889306190113227\n1610/10000. loss: 0.0009311660348127285\n1611/10000. loss: 0.0007207357169439396\n1612/10000. loss: 0.000527472235262394\n1613/10000. loss: 0.0008875133935362101\n1614/10000. loss: 0.0009065011205772558\n1615/10000. loss: 0.0007280687956760327\n1616/10000. loss: 0.0005608704717208942\n1617/10000. loss: 0.0008624426554888487\n1618/10000. loss: 0.0007962331486244997\n1619/10000. loss: 0.0006175498322894176\n1620/10000. loss: 0.0007199665221075217\n1621/10000. loss: 0.0010036146268248558\n1622/10000. loss: 0.0005182123277336359\n1623/10000. loss: 0.0008534938873102268\n1624/10000. loss: 0.0009927861392498016\n1625/10000. loss: 0.0009287178205947081\n1626/10000. loss: 0.0007824741769582033\n1627/10000. loss: 0.0007751921657472849\n1628/10000. loss: 0.0007135296085228523\n1629/10000. loss: 0.0009414814412593842\n1630/10000. loss: 0.0008315970189869404\n1631/10000. loss: 0.0005090580477068821\n1632/10000. loss: 0.0007458791757623354\n1633/10000. loss: 0.0013340686758359273\n1634/10000. loss: 0.0008530540702243646\n1635/10000. loss: 0.0007654337678104639\n1636/10000. loss: 0.0006010609989364942\n1637/10000. loss: 0.0005863115657120943\n1638/10000. loss: 0.001060276214654247\n1639/10000. loss: 0.0010744125271836917\n1640/10000. loss: 0.0009862839554746945\n1641/10000. loss: 0.0008289283917595943\n1642/10000. loss: 0.0008471597296496233\n1643/10000. loss: 0.0008496093408515056\n1644/10000. loss: 0.0008214562355230252\n1645/10000. loss: 0.0006250295943270127\n1646/10000. loss: 0.0009856410324573517\n1647/10000. loss: 0.0010052483218411605\n1648/10000. loss: 0.0007632989436388016\n1649/10000. loss: 0.0007482892833650112\n1650/10000. loss: 0.000840153389920791\n1651/10000. loss: 0.0007623727433383465\n1652/10000. loss: 0.0008451766334474087\n1653/10000. loss: 0.0006997372644642988\n1654/10000. loss: 0.0009521141182631254\n1655/10000. loss: 0.0006918845853457848\n1656/10000. loss: 0.0011997038188079994\n1657/10000. loss: 0.0007874518632888794\n1658/10000. loss: 0.0008404111334433159\n1659/10000. loss: 0.0010108303589125474\n1660/10000. loss: 0.0012006943579763174\n1661/10000. loss: 0.0010526844610770543\n1662/10000. loss: 0.0012895791636159022\n1663/10000. loss: 0.0011281038168817759\n1664/10000. loss: 0.001822172353665034\n1665/10000. loss: 0.0010285164850453536\n1666/10000. loss: 0.0008825061377137899\n1667/10000. loss: 0.0007564874831587076\n1668/10000. loss: 0.0013058266292015712\n1669/10000. loss: 0.0007982008003940185\n1670/10000. loss: 0.0011952387479444344\n1671/10000. loss: 0.0015003141015768051\n1672/10000. loss: 0.000820936169475317\n1673/10000. loss: 0.0010299246447781722\n1674/10000. loss: 0.0010852542084952195\n1675/10000. loss: 0.0010134355785946052\n1676/10000. loss: 0.0009289953547219435\n1677/10000. loss: 0.0009166393429040909\n1678/10000. loss: 0.0009082495234906673\n1679/10000. loss: 0.0008468820403019587\n1680/10000. loss: 0.0008320444418738285\n1681/10000. loss: 0.0007848657357196013\n1682/10000. loss: 0.0011028296624620755\n1683/10000. loss: 0.000726397925366958\n1684/10000. loss: 0.0005273433635011315\n1685/10000. loss: 0.0008041605663796266\n1686/10000. loss: 0.0004920516706382235\n1687/10000. loss: 0.0004655094041178624\n1688/10000. loss: 0.000631688085074226\n1689/10000. loss: 0.0005201149033382535\n1690/10000. loss: 0.0006081287283450365\n1691/10000. loss: 0.0008068324532359838\n1692/10000. loss: 0.0006815927724043528\n1693/10000. loss: 0.0009877447349329789\n1694/10000. loss: 0.0006444312554473678\n1695/10000. loss: 0.0006535024537394444\n1696/10000. loss: 0.0010108364125092824\n1697/10000. loss: 0.0007515940815210342\n1698/10000. loss: 0.0006842856916288534\n1699/10000. loss: 0.0007857431968053182\n1700/10000. loss: 0.0006127280260746678\n1701/10000. loss: 0.0008216130081564188\n1702/10000. loss: 0.0008596304493645827\n1703/10000. loss: 0.0008202493190765381\n1704/10000. loss: 0.000541545256661872\n1705/10000. loss: 0.0008162010150651137\n1706/10000. loss: 0.0008439410788317522\n1707/10000. loss: 0.000540447731812795\n1708/10000. loss: 0.0005643948291738828\n1709/10000. loss: 0.0011108348456521828\n1710/10000. loss: 0.0008454231234888235\n1711/10000. loss: 0.000930788383508722\n1712/10000. loss: 0.0011011365180214245\n1713/10000. loss: 0.0010012430138885975\n1714/10000. loss: 0.000982549972832203\n1715/10000. loss: 0.0008382350982477268\n1716/10000. loss: 0.0008583657909184694\n1717/10000. loss: 0.0009540635316322247\n1718/10000. loss: 0.0009722732162723938\n1719/10000. loss: 0.0006818460921446482\n1720/10000. loss: 0.0006215136963874102\n1721/10000. loss: 0.0006538755260407925\n1722/10000. loss: 0.0005488917231559753\n1723/10000. loss: 0.0008252267725765705\n1724/10000. loss: 0.0008581675744305054\n1725/10000. loss: 0.0009806666833659012\n1726/10000. loss: 0.0010173437961687644\n1727/10000. loss: 0.0007832216409345468\n1728/10000. loss: 0.001011899517228206\n1729/10000. loss: 0.0006038098751256863\n1730/10000. loss: 0.0008217928310235342\n1731/10000. loss: 0.0008378901208440462\n1732/10000. loss: 0.000563665060326457\n1733/10000. loss: 0.0004963458050042391\n1734/10000. loss: 0.000770378935461243\n1735/10000. loss: 0.000476314725043873\n1736/10000. loss: 0.0006228812271729112\n1737/10000. loss: 0.0008379475524028143\n1738/10000. loss: 0.0006906700630982717\n1739/10000. loss: 0.0008662727195769548\n1740/10000. loss: 0.0008451420969019333\n1741/10000. loss: 0.0007254338512818018\n1742/10000. loss: 0.0006115467598040899\n1743/10000. loss: 0.0007082358933985233\n1744/10000. loss: 0.0009227113332599401\n1745/10000. loss: 0.0009792052830259006\n1746/10000. loss: 0.0009872089140117168\n1747/10000. loss: 0.0007915111103405555\n1748/10000. loss: 0.0007499211933463812\n1749/10000. loss: 0.0009433524683117867\n1750/10000. loss: 0.0008824251126497984\n1751/10000. loss: 0.0009595568602283796\n1752/10000. loss: 0.0009617134928703308\n1753/10000. loss: 0.0017020649587114651\n1754/10000. loss: 0.0006905103412767252\n1755/10000. loss: 0.001108865796898802\n1756/10000. loss: 0.0008994905898968378\n1757/10000. loss: 0.0011144871823489666\n1758/10000. loss: 0.001185617409646511\n1759/10000. loss: 0.0008252399663130442\n1760/10000. loss: 0.0009680466415981451\n1761/10000. loss: 0.0007979422807693481\n1762/10000. loss: 0.0010500021744519472\n1763/10000. loss: 0.0012121043788890045\n1764/10000. loss: 0.001147353866448005\n1765/10000. loss: 0.0012751591857522726\n1766/10000. loss: 0.0009472763631492853\n1767/10000. loss: 0.0010274560190737247\n1768/10000. loss: 0.0011138958701243002\n1769/10000. loss: 0.001385323703289032\n1770/10000. loss: 0.001053201189885537\n1771/10000. loss: 0.0006521986797451973\n1772/10000. loss: 0.0013475753366947174\n1773/10000. loss: 0.0009631146676838398\n1774/10000. loss: 0.0008695160504430532\n1775/10000. loss: 0.001201887692635258\n1776/10000. loss: 0.0010873669137557347\n1777/10000. loss: 0.001044635350505511\n1778/10000. loss: 0.00125163064027826\n1779/10000. loss: 0.0005992670388271412\n1780/10000. loss: 0.000774990456799666\n1781/10000. loss: 0.0007700632947186629\n1782/10000. loss: 0.001073744148015976\n1783/10000. loss: 0.0008499966158221165\n1784/10000. loss: 0.0007706099810699621\n1785/10000. loss: 0.0008464610824982325\n1786/10000. loss: 0.0007161917164921761\n1787/10000. loss: 0.0004717849660664797\n1788/10000. loss: 0.0006721578538417816\n1789/10000. loss: 0.0006206427545597156\n1790/10000. loss: 0.00048694278423984844\n1791/10000. loss: 0.0007043201476335526\n1792/10000. loss: 0.0006197992867479721\n1793/10000. loss: 0.0005505488564570745\n1794/10000. loss: 0.0007490315474569798\n1795/10000. loss: 0.0005508228593195478\n1796/10000. loss: 0.0007494670959810416\n1797/10000. loss: 0.0003908673922220866\n1798/10000. loss: 0.0005834285402670503\n1799/10000. loss: 0.0006755911745131016\n1800/10000. loss: 0.0006247664568945765\n1801/10000. loss: 0.00047492833497623604\n1802/10000. loss: 0.0007919122775395712\n1803/10000. loss: 0.0005475421591351429\n1804/10000. loss: 0.00048433834065993625\n1805/10000. loss: 0.0008752281622340282\n1806/10000. loss: 0.0006043583465119203\n1807/10000. loss: 0.0007006924909849962\n1808/10000. loss: 0.0006043595882753531\n1809/10000. loss: 0.0005799318508555492\n1810/10000. loss: 0.0006389152258634567\n1811/10000. loss: 0.0005030203998709718\n1812/10000. loss: 0.0006998491783936819\n1813/10000. loss: 0.0006439910115053257\n1814/10000. loss: 0.0007170888129621744\n1815/10000. loss: 0.0006149680896972617\n1816/10000. loss: 0.000887915181616942\n1817/10000. loss: 0.0006953349026540915\n1818/10000. loss: 0.0005947661120444536\n1819/10000. loss: 0.000585460647319754\n1820/10000. loss: 0.0004384277466063698\n1821/10000. loss: 0.0007320489579190811\n1822/10000. loss: 0.0003891679225489497\n1823/10000. loss: 0.0005589483383422097\n1824/10000. loss: 0.0008178567513823509\n1825/10000. loss: 0.0007124952971935272\n1826/10000. loss: 0.0006285374208043019\n1827/10000. loss: 0.0006837942637503147\n1828/10000. loss: 0.0004538372935106357\n1829/10000. loss: 0.000733744353055954\n1830/10000. loss: 0.0006523507957657179\n1831/10000. loss: 0.0005225942780574163\n1832/10000. loss: 0.000801882396141688\n1833/10000. loss: 0.0008941865526139736\n1834/10000. loss: 0.0005022546586891016\n1835/10000. loss: 0.0006935256533324718\n1836/10000. loss: 0.000776924037684997\n1837/10000. loss: 0.0011213870408634345\n1838/10000. loss: 0.001166400033980608\n1839/10000. loss: 0.0011168099008500576\n1840/10000. loss: 0.0013978139807780583\n1841/10000. loss: 0.0016912791567544143\n1842/10000. loss: 0.0012045088224112988\n1843/10000. loss: 0.0019927301133672395\n1844/10000. loss: 0.001968946928779284\n1845/10000. loss: 0.0026737969989577928\n1846/10000. loss: 0.0020901855702201524\n1847/10000. loss: 0.0024378842984636626\n1848/10000. loss: 0.0022964890425403914\n1849/10000. loss: 0.0027567679062485695\n1850/10000. loss: 0.0018967160334189732\n1851/10000. loss: 0.002589244395494461\n1852/10000. loss: 0.0022198321918646493\n1853/10000. loss: 0.004886403679847717\n1854/10000. loss: 0.0032852167884508767\n1855/10000. loss: 0.003568477307756742\n1856/10000. loss: 0.0019471334914366405\n1857/10000. loss: 0.0042574026932319\n1858/10000. loss: 0.0037098477284113565\n1859/10000. loss: 0.005627749487757683\n1860/10000. loss: 0.003962029392520587\n1861/10000. loss: 0.003070713331302007\n1862/10000. loss: 0.002993525005877018\n1863/10000. loss: 0.0024665341091652713\n1864/10000. loss: 0.00398127796749274\n1865/10000. loss: 0.002615782432258129\n1866/10000. loss: 0.0019543777840832868\n1867/10000. loss: 0.0029459670186042786\n1868/10000. loss: 0.001920985213170449\n1869/10000. loss: 0.0025252578780055046\n1870/10000. loss: 0.0031644701957702637\n1871/10000. loss: 0.003147571347653866\n1872/10000. loss: 0.0009209899387011925\n1873/10000. loss: 0.003956368503471215\n1874/10000. loss: 0.0014646452230711777\n1875/10000. loss: 0.0020519538472096124\n1876/10000. loss: 0.001564509856204192\n1877/10000. loss: 0.003275500610470772\n1878/10000. loss: 0.0014312372853358586\n1879/10000. loss: 0.0018217408408721287\n1880/10000. loss: 0.0034191819528738656\n1881/10000. loss: 0.0021853315023084483\n1882/10000. loss: 0.0014867903664708138\n1883/10000. loss: 0.002177793998271227\n1884/10000. loss: 0.0010145321333160002\n1885/10000. loss: 0.002984104057153066\n1886/10000. loss: 0.0008306676366676887\n1887/10000. loss: 0.002002923438946406\n1888/10000. loss: 0.0014405843491355579\n1889/10000. loss: 0.001490384650727113\n1890/10000. loss: 0.0019738453750809035\n1891/10000. loss: 0.0013188842373589675\n1892/10000. loss: 0.001780882788201173\n1893/10000. loss: 0.002323268291850885\n1894/10000. loss: 0.001009929149101178\n1895/10000. loss: 0.0015623876824975014\n1896/10000. loss: 0.0011819556045035522\n1897/10000. loss: 0.0013391111666957538\n1898/10000. loss: 0.0013576298952102661\n1899/10000. loss: 0.0008461504088093837\n1900/10000. loss: 0.0012112832628190517\n1901/10000. loss: 0.001318288656572501\n1902/10000. loss: 0.0011388286172101896\n1903/10000. loss: 0.0009920171772440274\n1904/10000. loss: 0.000863305681074659\n1905/10000. loss: 0.0013656864563624065\n1906/10000. loss: 0.0005571461127450069\n1907/10000. loss: 0.000563224001477162\n1908/10000. loss: 0.000625317256587247\n1909/10000. loss: 0.000590093278636535\n1910/10000. loss: 0.0005346031005804738\n1911/10000. loss: 0.000788034638389945\n1912/10000. loss: 0.0008256652702887853\n1913/10000. loss: 0.0010926949325948954\n1914/10000. loss: 0.0007611731222520272\n1915/10000. loss: 0.0008163206124057373\n1916/10000. loss: 0.0007220411983629068\n1917/10000. loss: 0.0004972237705563506\n1918/10000. loss: 0.00039433167936901253\n1919/10000. loss: 0.0005032907938584685\n1920/10000. loss: 0.0007348171590516964\n1921/10000. loss: 0.0006097735992322365\n1922/10000. loss: 0.00045899185352027416\n1923/10000. loss: 0.0005826013317952553\n1924/10000. loss: 0.0008577781263738871\n1925/10000. loss: 0.0006497585369894902\n1926/10000. loss: 0.00044538868435968954\n1927/10000. loss: 0.000857859073827664\n1928/10000. loss: 0.0006099046828846136\n1929/10000. loss: 0.000787489814683795\n1930/10000. loss: 0.0009173985260228316\n1931/10000. loss: 0.0007268374320119619\n1932/10000. loss: 0.0006478856860970458\n1933/10000. loss: 0.000732342324530085\n1934/10000. loss: 0.0007108518232901891\n1935/10000. loss: 0.0007343994608769814\n1936/10000. loss: 0.0004741491284221411\n1937/10000. loss: 0.0009558376235266527\n1938/10000. loss: 0.0005804200191050768\n1939/10000. loss: 0.000504025723785162\n1940/10000. loss: 0.0007593377182881037\n1941/10000. loss: 0.0006338047484556834\n1942/10000. loss: 0.0004902887934197983\n1943/10000. loss: 0.0007496234029531479\n1944/10000. loss: 0.0008701916473607222\n1945/10000. loss: 0.0013319108014305432\n1946/10000. loss: 0.001194689500456055\n1947/10000. loss: 0.0007727335517605146\n1948/10000. loss: 0.001123211346566677\n1949/10000. loss: 0.0014561149291694164\n1950/10000. loss: 0.0013357941061258316\n1951/10000. loss: 0.0012209087920685608\n1952/10000. loss: 0.0017871870659291744\n1953/10000. loss: 0.0016037003758052986\n1954/10000. loss: 0.0017962045967578888\n1955/10000. loss: 0.00196809321641922\n1956/10000. loss: 0.0012243237967292468\n1957/10000. loss: 0.0015456564724445343\n1958/10000. loss: 0.0015381999934713046\n1959/10000. loss: 0.001348123885691166\n1960/10000. loss: 0.0012857296193639438\n1961/10000. loss: 0.0017653259759147961\n1962/10000. loss: 0.0011098656492928665\n1963/10000. loss: 0.0011749060358852148\n1964/10000. loss: 0.0012141931025932233\n1965/10000. loss: 0.0007832891618212064\n1966/10000. loss: 0.0007754174682001272\n1967/10000. loss: 0.0011528381922592719\n1968/10000. loss: 0.0012283246808995802\n1969/10000. loss: 0.0012128058200081189\n1970/10000. loss: 0.0016415738500654697\n1971/10000. loss: 0.0014944834013779957\n1972/10000. loss: 0.0015243932915230591\n1973/10000. loss: 0.0012529326292375724\n1974/10000. loss: 0.0013400791212916374\n1975/10000. loss: 0.00159030066182216\n1976/10000. loss: 0.0012927451947083075\n1977/10000. loss: 0.0011119211558252573\n1978/10000. loss: 0.0008926374527315298\n1979/10000. loss: 0.0012187076111634572\n1980/10000. loss: 0.0009056444590290388\n1981/10000. loss: 0.000835108570754528\n1982/10000. loss: 0.0011280061056216557\n1983/10000. loss: 0.0012687849036107461\n1984/10000. loss: 0.0014801753374437492\n1985/10000. loss: 0.0009484759842356046\n1986/10000. loss: 0.0011698782133559387\n1987/10000. loss: 0.0008009173131237427\n1988/10000. loss: 0.001352575607597828\n1989/10000. loss: 0.0013277814723551273\n1990/10000. loss: 0.0009644266683608294\n1991/10000. loss: 0.0010664381552487612\n1992/10000. loss: 0.0012723551287005346\n1993/10000. loss: 0.0010441504418849945\n1994/10000. loss: 0.0010677760777374108\n1995/10000. loss: 0.0010778950527310371\n1996/10000. loss: 0.0010129425209015608\n1997/10000. loss: 0.0011184186053772767\n1998/10000. loss: 0.0005941293202340603\n1999/10000. loss: 0.000853301336367925\n2000/10000. loss: 0.0008993972248087326\n2001/10000. loss: 0.0010292250663042068\n2002/10000. loss: 0.0007226463252057632\n2003/10000. loss: 0.000574796344153583\n2004/10000. loss: 0.0009726833862562975\n2005/10000. loss: 0.0007881637041767439\n2006/10000. loss: 0.0005343375572313865\n2007/10000. loss: 0.0009244030807167292\n2008/10000. loss: 0.0007739200567205747\n2009/10000. loss: 0.0007623963368435701\n2010/10000. loss: 0.0005414586824675401\n2011/10000. loss: 0.0007204689706365267\n2012/10000. loss: 0.000513191451318562\n2013/10000. loss: 0.0007748839755853018\n2014/10000. loss: 0.0006830457908411821\n2015/10000. loss: 0.0006888469991584619\n2016/10000. loss: 0.0004930043360218406\n2017/10000. loss: 0.0006302421679720283\n2018/10000. loss: 0.00042380877615263063\n2019/10000. loss: 0.000691990756119291\n2020/10000. loss: 0.0004642390413209796\n2021/10000. loss: 0.0005908245220780373\n2022/10000. loss: 0.0005684227605039874\n2023/10000. loss: 0.0005666966705272595\n2024/10000. loss: 0.00042595221505810815\n2025/10000. loss: 0.00043501487622658413\n2026/10000. loss: 0.000659677820901076\n2027/10000. loss: 0.0006494917130718628\n2028/10000. loss: 0.0005919133157779773\n2029/10000. loss: 0.00043393217492848635\n2030/10000. loss: 0.00041892608472456533\n2031/10000. loss: 0.00040751575337102014\n2032/10000. loss: 0.00045326123169312876\n2033/10000. loss: 0.0004495036943505208\n2034/10000. loss: 0.0005670280661433935\n2035/10000. loss: 0.00040784943848848343\n2036/10000. loss: 0.0006876251815507809\n2037/10000. loss: 0.0006737857280919949\n2038/10000. loss: 0.00047892886990060407\n2039/10000. loss: 0.0005415078873435656\n2040/10000. loss: 0.0005648416311790546\n2041/10000. loss: 0.0006589153005431095\n2042/10000. loss: 0.0006130707915872335\n2043/10000. loss: 0.0004336803685873747\n2044/10000. loss: 0.0004556251224130392\n2045/10000. loss: 0.00043567964651932317\n2046/10000. loss: 0.0006821005760381619\n2047/10000. loss: 0.000820056302472949\n2048/10000. loss: 0.0005363471573218703\n2049/10000. loss: 0.0005976633013536533\n2050/10000. loss: 0.0007116203972448906\n2051/10000. loss: 0.0006616193180282911\n2052/10000. loss: 0.0004688296855116884\n2053/10000. loss: 0.0006438246928155422\n2054/10000. loss: 0.0007629917624096075\n2055/10000. loss: 0.00040150759741663933\n2056/10000. loss: 0.00037307416399319965\n2057/10000. loss: 0.00048252327057222527\n2058/10000. loss: 0.001149371654416124\n2059/10000. loss: 0.0007063124018410841\n2060/10000. loss: 0.0006315373272324601\n2061/10000. loss: 0.0006369552963102857\n2062/10000. loss: 0.0006211938646932443\n2063/10000. loss: 0.000683656350399057\n2064/10000. loss: 0.00045016908552497625\n2065/10000. loss: 0.0008156356246521076\n2066/10000. loss: 0.0007071887763837973\n2067/10000. loss: 0.00048350108166535694\n2068/10000. loss: 0.00046642924038072425\n2069/10000. loss: 0.0008622980676591396\n2070/10000. loss: 0.0007382575422525406\n2071/10000. loss: 0.0009721750393509865\n2072/10000. loss: 0.0006053901355092725\n2073/10000. loss: 0.000661865808069706\n2074/10000. loss: 0.0009276968582222859\n2075/10000. loss: 0.0008583220187574625\n2076/10000. loss: 0.0008517235983163118\n2077/10000. loss: 0.0009166189314176639\n2078/10000. loss: 0.0005642447698240479\n2079/10000. loss: 0.0009946311668803294\n2080/10000. loss: 0.0009845173917710781\n2081/10000. loss: 0.0012558494539310534\n2082/10000. loss: 0.0009220139278719822\n2083/10000. loss: 0.0008647572249174118\n2084/10000. loss: 0.0012120947552224\n2085/10000. loss: 0.001032697968184948\n2086/10000. loss: 0.0007161852748443683\n2087/10000. loss: 0.0009693495618800322\n2088/10000. loss: 0.001390875472376744\n2089/10000. loss: 0.0008786926046013832\n2090/10000. loss: 0.0008369753292451302\n2091/10000. loss: 0.0009521905643244585\n2092/10000. loss: 0.0006878006582458814\n2093/10000. loss: 0.001583777057627837\n2094/10000. loss: 0.0007439488545060158\n2095/10000. loss: 0.0011672119920452435\n2096/10000. loss: 0.0008226568655421337\n2097/10000. loss: 0.000945497847472628\n2098/10000. loss: 0.0010521189930538337\n2099/10000. loss: 0.001104260018716256\n2100/10000. loss: 0.001169744102905194\n2101/10000. loss: 0.0008862426814933618\n2102/10000. loss: 0.0009936906086901824\n2103/10000. loss: 0.0006750853111346563\n2104/10000. loss: 0.0007598018273711205\n2105/10000. loss: 0.0013616890646517277\n2106/10000. loss: 0.0008913044972966114\n2107/10000. loss: 0.0009284412177900473\n2108/10000. loss: 0.0006544069231798252\n2109/10000. loss: 0.0007354302797466516\n2110/10000. loss: 0.0005251707043498755\n2111/10000. loss: 0.000776216775799791\n2112/10000. loss: 0.0008467829320579767\n2113/10000. loss: 0.0006664303752283255\n2114/10000. loss: 0.0008149133839954933\n2115/10000. loss: 0.0005785004856685797\n2116/10000. loss: 0.0009515282387534777\n2117/10000. loss: 0.0005172172095626593\n2118/10000. loss: 0.0008139694885661205\n2119/10000. loss: 0.0007288060151040554\n2120/10000. loss: 0.0006134983850643039\n2121/10000. loss: 0.0005557923577725887\n2122/10000. loss: 0.0005180245110144218\n2123/10000. loss: 0.0004862489877268672\n2124/10000. loss: 0.0006922983253995577\n2125/10000. loss: 0.0007073658828934034\n2126/10000. loss: 0.0007777352972577015\n2127/10000. loss: 0.00039896062420060235\n2128/10000. loss: 0.00047484308015555143\n2129/10000. loss: 0.0005366869736462831\n2130/10000. loss: 0.0005490621939922372\n2131/10000. loss: 0.0005366234884907802\n2132/10000. loss: 0.000420111115090549\n2133/10000. loss: 0.0003672651946544647\n2134/10000. loss: 0.0006043758864204088\n2135/10000. loss: 0.0006724779959768057\n2136/10000. loss: 0.0006658080189178387\n2137/10000. loss: 0.0007362300530076027\n2138/10000. loss: 0.00047068325026581687\n2139/10000. loss: 0.0004908182503034672\n2140/10000. loss: 0.0007374685568114122\n2141/10000. loss: 0.0004079292993992567\n2142/10000. loss: 0.0005055759490157167\n2143/10000. loss: 0.0006358934721599022\n2144/10000. loss: 0.0005344646827628216\n2145/10000. loss: 0.0005583086749538779\n2146/10000. loss: 0.0004029685320953528\n2147/10000. loss: 0.0006806477904319763\n2148/10000. loss: 0.00038205556726704043\n2149/10000. loss: 0.0005740435250724355\n2150/10000. loss: 0.0005560856079682708\n2151/10000. loss: 0.0006556952527413765\n2152/10000. loss: 0.0006343214772641659\n2153/10000. loss: 0.0007259747168670098\n2154/10000. loss: 0.0006486785520489017\n2155/10000. loss: 0.0005662035352240006\n2156/10000. loss: 0.00056785197618107\n2157/10000. loss: 0.0006356129888445139\n2158/10000. loss: 0.00046058449273308116\n2159/10000. loss: 0.000661635926614205\n2160/10000. loss: 0.000734519058217605\n2161/10000. loss: 0.0008257037649552027\n2162/10000. loss: 0.0005201286791513363\n2163/10000. loss: 0.0005085010003919402\n2164/10000. loss: 0.0006557328160852194\n2165/10000. loss: 0.0005226443366458019\n2166/10000. loss: 0.0006973034081359705\n2167/10000. loss: 0.0006721824562797943\n2168/10000. loss: 0.00044081592932343483\n2169/10000. loss: 0.0005670962079117695\n2170/10000. loss: 0.0007459383147458235\n2171/10000. loss: 0.0011716440009574096\n2172/10000. loss: 0.0006005970450739065\n2173/10000. loss: 0.00048580742441117764\n2174/10000. loss: 0.0006170115666463971\n2175/10000. loss: 0.0007292193671067556\n2176/10000. loss: 0.0006526481204976639\n2177/10000. loss: 0.0006231265142560005\n2178/10000. loss: 0.0007802022931476434\n2179/10000. loss: 0.0005295024408648411\n2180/10000. loss: 0.0008511785417795181\n2181/10000. loss: 0.0008908757784714302\n2182/10000. loss: 0.0017811104965706666\n2183/10000. loss: 0.0010775839909911156\n2184/10000. loss: 0.0010371770864973466\n2185/10000. loss: 0.0008826428093016148\n2186/10000. loss: 0.0008926752489060163\n2187/10000. loss: 0.0011278286886711915\n2188/10000. loss: 0.000788580005367597\n2189/10000. loss: 0.0009211609916140636\n2190/10000. loss: 0.0008030550864835581\n2191/10000. loss: 0.0009628282859921455\n2192/10000. loss: 0.0006337645851696531\n2193/10000. loss: 0.0010250476965059836\n2194/10000. loss: 0.0008870614692568779\n2195/10000. loss: 0.0007744692265987396\n2196/10000. loss: 0.0007287313540776571\n2197/10000. loss: 0.0005609577832122644\n2198/10000. loss: 0.0008337101899087429\n2199/10000. loss: 0.000506668584421277\n2200/10000. loss: 0.00047532162473847467\n2201/10000. loss: 0.0006608698361863693\n2202/10000. loss: 0.0009292379642526308\n2203/10000. loss: 0.0009311207880576452\n2204/10000. loss: 0.0007686327832440535\n2205/10000. loss: 0.0004325838914761941\n2206/10000. loss: 0.0009422679431736469\n2207/10000. loss: 0.0005583722765247027\n2208/10000. loss: 0.0006661431398242712\n2209/10000. loss: 0.000620492889235417\n2210/10000. loss: 0.0006734064469734827\n2211/10000. loss: 0.0005372068844735622\n2212/10000. loss: 0.0007095950034757456\n2213/10000. loss: 0.0007127307665844759\n2214/10000. loss: 0.0007567202361921469\n2215/10000. loss: 0.0008175622982283434\n2216/10000. loss: 0.0005715780425816774\n2217/10000. loss: 0.0006822647216419379\n2218/10000. loss: 0.0005713478894904256\n2219/10000. loss: 0.0008093488092223803\n2220/10000. loss: 0.0007769227959215641\n2221/10000. loss: 0.00047539896331727505\n2222/10000. loss: 0.0009289271353433529\n2223/10000. loss: 0.0007578656077384949\n2224/10000. loss: 0.0005367280682548881\n2225/10000. loss: 0.0005865689599886537\n2226/10000. loss: 0.0006096985889598727\n2227/10000. loss: 0.0005529146486272415\n2228/10000. loss: 0.0007770551213373741\n2229/10000. loss: 0.0006303256377577782\n2230/10000. loss: 0.0005505015530313054\n2231/10000. loss: 0.0006854900469382604\n2232/10000. loss: 0.000979326122129957\n2233/10000. loss: 0.0010889143062134583\n2234/10000. loss: 0.000720596561829249\n2235/10000. loss: 0.0004910617911567291\n2236/10000. loss: 0.0007376242429018021\n2237/10000. loss: 0.0005108184413984418\n2238/10000. loss: 0.0009409294774134954\n2239/10000. loss: 0.00046707588868836564\n2240/10000. loss: 0.00046891594926516217\n2241/10000. loss: 0.00041888429162402946\n2242/10000. loss: 0.00042533529146264\n2243/10000. loss: 0.0007937146971623102\n2244/10000. loss: 0.000436968596962591\n2245/10000. loss: 0.0008386386713633934\n2246/10000. loss: 0.0004817829467356205\n2247/10000. loss: 0.0006013199842224518\n2248/10000. loss: 0.0006092166683326165\n2249/10000. loss: 0.0006418936730672916\n2250/10000. loss: 0.0005783813151841363\n2251/10000. loss: 0.0005610402828703324\n2252/10000. loss: 0.0006204016972333193\n2253/10000. loss: 0.00037147845917691785\n2254/10000. loss: 0.000702819088473916\n2255/10000. loss: 0.0005983294686302543\n2256/10000. loss: 0.0007811688507596651\n2257/10000. loss: 0.0007291947646687428\n2258/10000. loss: 0.0004551306677361329\n2259/10000. loss: 0.0006907963349173466\n2260/10000. loss: 0.0007538010831922293\n2261/10000. loss: 0.0005777704839905103\n2262/10000. loss: 0.0009884972435732682\n2263/10000. loss: 0.000521564157679677\n2264/10000. loss: 0.0007316382446636757\n2265/10000. loss: 0.0005543986723447839\n2266/10000. loss: 0.0006466763637339076\n2267/10000. loss: 0.0007709832862019539\n2268/10000. loss: 0.0009288632621367773\n2269/10000. loss: 0.000739505048841238\n2270/10000. loss: 0.0009284534025937319\n2271/10000. loss: 0.0011113887497534354\n2272/10000. loss: 0.0011369353160262108\n2273/10000. loss: 0.0009988310436407726\n2274/10000. loss: 0.00127407427256306\n2275/10000. loss: 0.0011395427864044905\n2276/10000. loss: 0.0011775832002361615\n2277/10000. loss: 0.00132315078129371\n2278/10000. loss: 0.0015353248454630375\n2279/10000. loss: 0.001081410174568494\n2280/10000. loss: 0.0012682500916222732\n2281/10000. loss: 0.0015024932411809762\n2282/10000. loss: 0.0008901966114838918\n2283/10000. loss: 0.001090517733246088\n2284/10000. loss: 0.0017034436265627544\n2285/10000. loss: 0.001214518832663695\n2286/10000. loss: 0.0015223704588909943\n2287/10000. loss: 0.001144272896150748\n2288/10000. loss: 0.0013787765055894852\n2289/10000. loss: 0.0013206377625465393\n2290/10000. loss: 0.0013622439776857693\n2291/10000. loss: 0.0017146444879472256\n2292/10000. loss: 0.0010474738664925098\n2293/10000. loss: 0.0008387196188171705\n2294/10000. loss: 0.000877312229325374\n2295/10000. loss: 0.0011083162389695644\n2296/10000. loss: 0.0006745898475249609\n2297/10000. loss: 0.0009717932747056087\n2298/10000. loss: 0.00059671300308158\n2299/10000. loss: 0.0007617202742646138\n2300/10000. loss: 0.0007251889910548925\n2301/10000. loss: 0.000590045975210766\n2302/10000. loss: 0.0005335969229539236\n2303/10000. loss: 0.0007212907851984104\n2304/10000. loss: 0.0008638429765899976\n2305/10000. loss: 0.0006385863913844029\n2306/10000. loss: 0.0006093757304673394\n2307/10000. loss: 0.000649351510219276\n2308/10000. loss: 0.00041503490259250003\n2309/10000. loss: 0.0007566655986011028\n2310/10000. loss: 0.0006979873093465964\n2311/10000. loss: 0.0008627166971564293\n2312/10000. loss: 0.0009050468603769938\n2313/10000. loss: 0.0009241290390491486\n2314/10000. loss: 0.0006241579540073872\n2315/10000. loss: 0.0013561715992788474\n2316/10000. loss: 0.0015062489546835423\n2317/10000. loss: 0.0011883920524269342\n2318/10000. loss: 0.0012532342225313187\n2319/10000. loss: 0.0010132977428535621\n2320/10000. loss: 0.001324605041493972\n2321/10000. loss: 0.0015184061291317146\n2322/10000. loss: 0.0007656848368545374\n2323/10000. loss: 0.0015805546815196674\n2324/10000. loss: 0.0012042639621843894\n2325/10000. loss: 0.0012309527955949306\n2326/10000. loss: 0.0008788059931248426\n2327/10000. loss: 0.0011745290830731392\n2328/10000. loss: 0.001046231792618831\n2329/10000. loss: 0.0015142705912391345\n2330/10000. loss: 0.0016488227993249893\n2331/10000. loss: 0.0023152129724621773\n2332/10000. loss: 0.0015552394712964694\n2333/10000. loss: 0.001811949536204338\n2334/10000. loss: 0.0017236250763138135\n2335/10000. loss: 0.0015196882498761017\n2336/10000. loss: 0.0018531270325183868\n2337/10000. loss: 0.0011313677144547303\n2338/10000. loss: 0.0015272432938218117\n2339/10000. loss: 0.0012560657535990079\n2340/10000. loss: 0.0012816324985275667\n2341/10000. loss: 0.0011923981364816427\n2342/10000. loss: 0.001128341614579161\n2343/10000. loss: 0.0012848332989960909\n2344/10000. loss: 0.0016703565294543903\n2345/10000. loss: 0.001213253941386938\n2346/10000. loss: 0.001011884305626154\n2347/10000. loss: 0.0007138399717708429\n2348/10000. loss: 0.0009379625941316286\n2349/10000. loss: 0.0008514531267186006\n2350/10000. loss: 0.0010754365163544815\n2351/10000. loss: 0.000981278174246351\n2352/10000. loss: 0.0006476662044102947\n2353/10000. loss: 0.0008959342570354542\n2354/10000. loss: 0.0005449651507660747\n2355/10000. loss: 0.0004899952715883652\n2356/10000. loss: 0.0004992280155420303\n2357/10000. loss: 0.0005945820594206452\n2358/10000. loss: 0.0006625169577697912\n2359/10000. loss: 0.0006034205046792825\n2360/10000. loss: 0.0006925615016371012\n2361/10000. loss: 0.0009126327931880951\n2362/10000. loss: 0.0008515859954059124\n2363/10000. loss: 0.0005065985023975372\n2364/10000. loss: 0.0005847321978459755\n2365/10000. loss: 0.0005551246382916967\n2366/10000. loss: 0.0007067445355157057\n2367/10000. loss: 0.0004507938089470069\n2368/10000. loss: 0.0007307621805618206\n2369/10000. loss: 0.0007659100616971651\n2370/10000. loss: 0.0010393221552173297\n2371/10000. loss: 0.0007154491419593493\n2372/10000. loss: 0.0008963168753931919\n2373/10000. loss: 0.0006946693174540997\n2374/10000. loss: 0.0005013343955700597\n2375/10000. loss: 0.0009507607513417801\n2376/10000. loss: 0.0008032443777968487\n2377/10000. loss: 0.0006541040105124315\n2378/10000. loss: 0.0008751373582830032\n2379/10000. loss: 0.0007463924121111631\n2380/10000. loss: 0.0009098677740742763\n2381/10000. loss: 0.0005900495452806354\n2382/10000. loss: 0.0005373410725345215\n2383/10000. loss: 0.0006903348645816246\n2384/10000. loss: 0.0007630807037154833\n2385/10000. loss: 0.0004951560404151678\n2386/10000. loss: 0.0005328400681416193\n2387/10000. loss: 0.0006162615027278662\n2388/10000. loss: 0.0006768199770400921\n2389/10000. loss: 0.000504946568980813\n2390/10000. loss: 0.0007004969132443269\n2391/10000. loss: 0.000597339045877258\n2392/10000. loss: 0.000731464009732008\n2393/10000. loss: 0.00042025318058828515\n2394/10000. loss: 0.0006851486396044493\n2395/10000. loss: 0.0005814847536385059\n2396/10000. loss: 0.0006302533826480309\n2397/10000. loss: 0.000839451172699531\n2398/10000. loss: 0.0009367625849942366\n2399/10000. loss: 0.0009205762762576342\n2400/10000. loss: 0.0006317362034072479\n2401/10000. loss: 0.0004411247791722417\n2402/10000. loss: 0.0007292854910095533\n2403/10000. loss: 0.00043501212106396753\n2404/10000. loss: 0.000681789048636953\n2405/10000. loss: 0.0006195385164270798\n2406/10000. loss: 0.0004640331802268823\n2407/10000. loss: 0.0006667664274573326\n2408/10000. loss: 0.0004915999403844277\n2409/10000. loss: 0.0008348192398746809\n2410/10000. loss: 0.0007292166507492462\n2411/10000. loss: 0.0005495828421165546\n2412/10000. loss: 0.0007360164696971575\n2413/10000. loss: 0.0005166437476873398\n2414/10000. loss: 0.0005464861945559581\n2415/10000. loss: 0.0005822821209828059\n2416/10000. loss: 0.0007662465795874596\n2417/10000. loss: 0.0007224065096427997\n2418/10000. loss: 0.0004850642678017418\n2419/10000. loss: 0.0006834239854166905\n2420/10000. loss: 0.0008355366686979929\n2421/10000. loss: 0.0007501382691164812\n2422/10000. loss: 0.0007299322169274092\n2423/10000. loss: 0.000993456148232023\n2424/10000. loss: 0.0007082742328445116\n2425/10000. loss: 0.0007814858884861072\n2426/10000. loss: 0.0006782697358479103\n2427/10000. loss: 0.000674636879314979\n2428/10000. loss: 0.0005154525861144066\n2429/10000. loss: 0.0009170073705414931\n2430/10000. loss: 0.0006332687335088849\n2431/10000. loss: 0.0005915191334982713\n2432/10000. loss: 0.0008837122780581316\n2433/10000. loss: 0.000897691740343968\n2434/10000. loss: 0.001091813047726949\n2435/10000. loss: 0.0009029921299467484\n2436/10000. loss: 0.0009651080084343752\n2437/10000. loss: 0.0008723780823250612\n2438/10000. loss: 0.0006946784754594167\n2439/10000. loss: 0.0009794185558954875\n2440/10000. loss: 0.0009852851120134194\n2441/10000. loss: 0.0009910969529300928\n2442/10000. loss: 0.00106561246017615\n2443/10000. loss: 0.0009842823880414169\n2444/10000. loss: 0.0009410170217355093\n2445/10000. loss: 0.0005874281438688437\n2446/10000. loss: 0.0011436173226684332\n2447/10000. loss: 0.0008517100941389799\n2448/10000. loss: 0.0008508459820101658\n2449/10000. loss: 0.0006934101693332195\n2450/10000. loss: 0.001035647854829828\n2451/10000. loss: 0.0006450437164554993\n2452/10000. loss: 0.0006238415371626616\n2453/10000. loss: 0.0005047812592238188\n2454/10000. loss: 0.0006640106439590454\n2455/10000. loss: 0.0008621628706653913\n2456/10000. loss: 0.0005759946070611477\n2457/10000. loss: 0.0009409671183675528\n2458/10000. loss: 0.000589944616270562\n2459/10000. loss: 0.0006127504166215658\n2460/10000. loss: 0.0008817417547106743\n2461/10000. loss: 0.0005580793755749861\n2462/10000. loss: 0.00046810414642095566\n2463/10000. loss: 0.0005019340120876828\n2464/10000. loss: 0.0006532427699615558\n2465/10000. loss: 0.00044444610830396414\n2466/10000. loss: 0.000691176780189077\n2467/10000. loss: 0.00042740934683630866\n2468/10000. loss: 0.0006058962705234686\n2469/10000. loss: 0.00040356023237109184\n2470/10000. loss: 0.0004914057596276203\n2471/10000. loss: 0.0005114254308864474\n2472/10000. loss: 0.00040999415796250105\n2473/10000. loss: 0.000514098055039843\n2474/10000. loss: 0.00038206348350892466\n2475/10000. loss: 0.0006269811419770122\n2476/10000. loss: 0.00038322356219093007\n2477/10000. loss: 0.000566092940668265\n2478/10000. loss: 0.0005703879287466407\n2479/10000. loss: 0.0005145169949779908\n2480/10000. loss: 0.000435587833635509\n2481/10000. loss: 0.0004042360621194045\n2482/10000. loss: 0.0006269065973659357\n2483/10000. loss: 0.00041460664942860603\n2484/10000. loss: 0.0005290814830611149\n2485/10000. loss: 0.00034843206716080505\n2486/10000. loss: 0.0006174189814676841\n2487/10000. loss: 0.0004612087116887172\n2488/10000. loss: 0.0003870203314969937\n2489/10000. loss: 0.00035630667116492987\n2490/10000. loss: 0.00033917716549088556\n2491/10000. loss: 0.00036953703965991735\n2492/10000. loss: 0.00033286035371323425\n2493/10000. loss: 0.0004807447548955679\n2494/10000. loss: 0.0005007416087513169\n2495/10000. loss: 0.00048132155401011306\n2496/10000. loss: 0.0005847392603754997\n2497/10000. loss: 0.0005984095623716712\n2498/10000. loss: 0.00047758598035822314\n2499/10000. loss: 0.0004796102487792571\n2500/10000. loss: 0.00040713546331971884\n2501/10000. loss: 0.0004359372736265262\n2502/10000. loss: 0.000465411227196455\n2503/10000. loss: 0.0008313550303379694\n2504/10000. loss: 0.0004977213684469461\n2505/10000. loss: 0.0006088094475368658\n2506/10000. loss: 0.000380665451909105\n2507/10000. loss: 0.0005735998274758458\n2508/10000. loss: 0.0007955019827932119\n2509/10000. loss: 0.0005747608374804258\n2510/10000. loss: 0.0004283336456865072\n2511/10000. loss: 0.0005227984317267934\n2512/10000. loss: 0.0006416659646977981\n2513/10000. loss: 0.0005450206032643715\n2514/10000. loss: 0.0006913289738198122\n2515/10000. loss: 0.0006313679041340947\n2516/10000. loss: 0.0007063328133275112\n2517/10000. loss: 0.0005160433162624637\n2518/10000. loss: 0.0005621690070256591\n2519/10000. loss: 0.0008026007562875748\n2520/10000. loss: 0.0005667653555671374\n2521/10000. loss: 0.0005720792105421424\n2522/10000. loss: 0.00045488491499175626\n2523/10000. loss: 0.0006345127476379275\n2524/10000. loss: 0.000517426097455124\n2525/10000. loss: 0.0005913105172415575\n2526/10000. loss: 0.0004781484603881836\n2527/10000. loss: 0.00043294023877630633\n2528/10000. loss: 0.0008144055803616842\n2529/10000. loss: 0.000545969232916832\n2530/10000. loss: 0.000378206605091691\n2531/10000. loss: 0.0005173524065564076\n2532/10000. loss: 0.0005313855363056064\n2533/10000. loss: 0.0008136366959661245\n2534/10000. loss: 0.00039301603101193905\n2535/10000. loss: 0.0005618830521901449\n2536/10000. loss: 0.0006180008252461752\n2537/10000. loss: 0.0007353493322928747\n2538/10000. loss: 0.0003984242600078384\n2539/10000. loss: 0.0004142624093219638\n2540/10000. loss: 0.0006732571249206861\n2541/10000. loss: 0.0008116367583473524\n2542/10000. loss: 0.0005896312262242039\n2543/10000. loss: 0.0004910378096004327\n2544/10000. loss: 0.0006139935770382484\n2545/10000. loss: 0.00036400552683820325\n2546/10000. loss: 0.0004267058102414012\n2547/10000. loss: 0.0003905211730549733\n2548/10000. loss: 0.0004864446818828583\n2549/10000. loss: 0.0005346461354444424\n2550/10000. loss: 0.0005285150449102124\n2551/10000. loss: 0.00034961069468408823\n2552/10000. loss: 0.00035861670039594173\n2553/10000. loss: 0.00034251995384693146\n2554/10000. loss: 0.0005848292882243792\n2555/10000. loss: 0.00035199569538235664\n2556/10000. loss: 0.0005089232387642065\n2557/10000. loss: 0.0006175790913403034\n2558/10000. loss: 0.00035453801198552054\n2559/10000. loss: 0.0004458657543485363\n2560/10000. loss: 0.00039139964307347935\n2561/10000. loss: 0.00044197387372454006\n2562/10000. loss: 0.0006569089212765297\n2563/10000. loss: 0.00047296262346208096\n2564/10000. loss: 0.0007891835023959478\n2565/10000. loss: 0.0005836165122066935\n2566/10000. loss: 0.0005250232061371207\n2567/10000. loss: 0.0005034496231625477\n2568/10000. loss: 0.0003977710148319602\n2569/10000. loss: 0.0005508287188907465\n2570/10000. loss: 0.0004005752271041274\n2571/10000. loss: 0.0006160359674443802\n2572/10000. loss: 0.000913974829018116\n2573/10000. loss: 0.0008138801592091719\n2574/10000. loss: 0.0010959018642703693\n2575/10000. loss: 0.0010479451157152653\n2576/10000. loss: 0.0009958473965525627\n2577/10000. loss: 0.0010309226345270872\n2578/10000. loss: 0.0019224191394944985\n2579/10000. loss: 0.0016159545630216599\n2580/10000. loss: 0.0016818239043156307\n2581/10000. loss: 0.0018925340846180916\n2582/10000. loss: 0.002145198949923118\n2583/10000. loss: 0.0016015991568565369\n2584/10000. loss: 0.00237819692119956\n2585/10000. loss: 0.0015995061645905178\n2586/10000. loss: 0.0012209002549449603\n2587/10000. loss: 0.0018560293441017468\n2588/10000. loss: 0.0014870893210172653\n2589/10000. loss: 0.0016039128725727398\n2590/10000. loss: 0.0013249260373413563\n2591/10000. loss: 0.0019526478524009387\n2592/10000. loss: 0.0017125485464930534\n2593/10000. loss: 0.0007891419809311628\n2594/10000. loss: 0.0011470925528556108\n2595/10000. loss: 0.0014969940918187301\n2596/10000. loss: 0.0009942653123289347\n2597/10000. loss: 0.0010235737233112256\n2598/10000. loss: 0.001311068267871936\n2599/10000. loss: 0.0010806573554873466\n2600/10000. loss: 0.0006891942272583643\n2601/10000. loss: 0.0012377092304329078\n2602/10000. loss: 0.0009443671442568302\n2603/10000. loss: 0.000602867299069961\n2604/10000. loss: 0.0008632914784053961\n2605/10000. loss: 0.0007230075231442848\n2606/10000. loss: 0.0008721513828883568\n2607/10000. loss: 0.0009586618592341741\n2608/10000. loss: 0.0007318491892268261\n2609/10000. loss: 0.0006534846810003122\n2610/10000. loss: 0.0008043155539780855\n2611/10000. loss: 0.00042430458900829154\n2612/10000. loss: 0.001055365117887656\n2613/10000. loss: 0.0006725528898338476\n2614/10000. loss: 0.0007618017649898926\n2615/10000. loss: 0.0004410669207572937\n2616/10000. loss: 0.0007356344722211361\n2617/10000. loss: 0.0007645175792276859\n2618/10000. loss: 0.0007206518979122242\n2619/10000. loss: 0.0006616875374068817\n2620/10000. loss: 0.0007725687076648077\n2621/10000. loss: 0.0006482565077021718\n2622/10000. loss: 0.0006596144909660021\n2623/10000. loss: 0.0003779575539131959\n2624/10000. loss: 0.0006582013641794523\n2625/10000. loss: 0.0005360788200050592\n2626/10000. loss: 0.0003993265563622117\n2627/10000. loss: 0.0007343267401059469\n2628/10000. loss: 0.0005354929016903043\n2629/10000. loss: 0.0003905919147655368\n2630/10000. loss: 0.0005421448343743881\n2631/10000. loss: 0.0004911799139032761\n2632/10000. loss: 0.0005542420937369267\n2633/10000. loss: 0.0005967502171794573\n2634/10000. loss: 0.0003709011944010854\n2635/10000. loss: 0.0004979055374860764\n2636/10000. loss: 0.0007109826741119226\n2637/10000. loss: 0.0004128910368308425\n2638/10000. loss: 0.0007721235354741415\n2639/10000. loss: 0.0005244207180415591\n2640/10000. loss: 0.0006647299354275068\n2641/10000. loss: 0.000658712505052487\n2642/10000. loss: 0.0005855385291700562\n2643/10000. loss: 0.000624528193535904\n2644/10000. loss: 0.0006163640258212885\n2645/10000. loss: 0.0006536459550261497\n2646/10000. loss: 0.0006820034856597582\n2647/10000. loss: 0.0006071087749054035\n2648/10000. loss: 0.0007618033948043982\n2649/10000. loss: 0.0007467029305795828\n2650/10000. loss: 0.0008092517964541912\n2651/10000. loss: 0.000593893618012468\n2652/10000. loss: 0.0010576279213031132\n2653/10000. loss: 0.0012758105682830017\n2654/10000. loss: 0.0010106642730534077\n2655/10000. loss: 0.0012711674595872562\n2656/10000. loss: 0.000925703284641107\n2657/10000. loss: 0.0007164418542136749\n2658/10000. loss: 0.0013801315799355507\n2659/10000. loss: 0.0009327547159045935\n2660/10000. loss: 0.001148736880471309\n2661/10000. loss: 0.0006626650380591551\n2662/10000. loss: 0.0006102904832611481\n2663/10000. loss: 0.0007508418057113886\n2664/10000. loss: 0.001108801846082012\n2665/10000. loss: 0.001005161941672365\n2666/10000. loss: 0.0005845850488791863\n2667/10000. loss: 0.0005581203537682692\n2668/10000. loss: 0.0006505211349576712\n2669/10000. loss: 0.000914490781724453\n2670/10000. loss: 0.000616653123870492\n2671/10000. loss: 0.0005349612717206279\n2672/10000. loss: 0.0007785237394273281\n2673/10000. loss: 0.0006782428051034609\n2674/10000. loss: 0.00044878863263875246\n2675/10000. loss: 0.0004638441993544499\n2676/10000. loss: 0.0007187378748009602\n2677/10000. loss: 0.0005340120599915584\n2678/10000. loss: 0.0007558586852004131\n2679/10000. loss: 0.0007187728770077229\n2680/10000. loss: 0.0006651845760643482\n2681/10000. loss: 0.000484160923709472\n2682/10000. loss: 0.0005232048376152912\n2683/10000. loss: 0.0010577511663238208\n2684/10000. loss: 0.0006743543005237976\n2685/10000. loss: 0.0007803709401438633\n2686/10000. loss: 0.0006535237965484461\n2687/10000. loss: 0.0007784860984732708\n2688/10000. loss: 0.0006491764603803555\n2689/10000. loss: 0.0006461974699050188\n2690/10000. loss: 0.0007629707300414642\n2691/10000. loss: 0.0007573654875159264\n2692/10000. loss: 0.00044695047351221245\n2693/10000. loss: 0.0006522391146669785\n2694/10000. loss: 0.0006531399364272753\n2695/10000. loss: 0.0006102234280357758\n2696/10000. loss: 0.0006256090321888527\n2697/10000. loss: 0.0008105603822817405\n2698/10000. loss: 0.0006467982505758604\n2699/10000. loss: 0.0008874657408644756\n2700/10000. loss: 0.0006615328602492809\n2701/10000. loss: 0.0008918091965218385\n2702/10000. loss: 0.001496768556535244\n2703/10000. loss: 0.0010821990047891934\n2704/10000. loss: 0.0015995434174935024\n2705/10000. loss: 0.0012291868527730305\n2706/10000. loss: 0.0012054528730611007\n2707/10000. loss: 0.0009635578220089277\n2708/10000. loss: 0.0010766141737500827\n2709/10000. loss: 0.0011907946318387985\n2710/10000. loss: 0.0013696419385572274\n2711/10000. loss: 0.0008483524434268475\n2712/10000. loss: 0.0010540061630308628\n2713/10000. loss: 0.000744806524987022\n2714/10000. loss: 0.0006954584581156572\n2715/10000. loss: 0.0007847272014866272\n2716/10000. loss: 0.0009132753281543652\n2717/10000. loss: 0.001054683079322179\n2718/10000. loss: 0.0008420767262578011\n2719/10000. loss: 0.0007611056789755821\n2720/10000. loss: 0.0007438145888348421\n2721/10000. loss: 0.0006993965556224188\n2722/10000. loss: 0.00073886647199591\n2723/10000. loss: 0.0008652654166022936\n2724/10000. loss: 0.0008403336784491936\n2725/10000. loss: 0.0006311197066679597\n2726/10000. loss: 0.000690908869728446\n2727/10000. loss: 0.0005136392234514157\n2728/10000. loss: 0.00041554806133111316\n2729/10000. loss: 0.0004925192333757877\n2730/10000. loss: 0.0003651537699624896\n2731/10000. loss: 0.00045790174044668674\n2732/10000. loss: 0.0003554834208140771\n2733/10000. loss: 0.0006866746892531713\n2734/10000. loss: 0.0005972976408277949\n2735/10000. loss: 0.0006791899601618449\n2736/10000. loss: 0.00038233834008375805\n2737/10000. loss: 0.00044911416868368786\n2738/10000. loss: 0.0006390652464081844\n2739/10000. loss: 0.0006079824330906073\n2740/10000. loss: 0.0008458172281583151\n2741/10000. loss: 0.0005939860905831059\n2742/10000. loss: 0.0008732386243840059\n2743/10000. loss: 0.0015836916863918304\n2744/10000. loss: 0.0012946752831339836\n2745/10000. loss: 0.002178429625928402\n2746/10000. loss: 0.0012087938375771046\n2747/10000. loss: 0.0013128956779837608\n2748/10000. loss: 0.0025498544176419577\n2749/10000. loss: 0.002281320902208487\n2750/10000. loss: 0.003821627547343572\n2751/10000. loss: 0.0016510958472887676\n2752/10000. loss: 0.0026148324832320213\n2753/10000. loss: 0.0021015064169963202\n2754/10000. loss: 0.0032114780818422637\n2755/10000. loss: 0.0027201902121305466\n2756/10000. loss: 0.001971643573294083\n2757/10000. loss: 0.0018025552853941917\n2758/10000. loss: 0.0019429891059796016\n2759/10000. loss: 0.0017834353881577651\n2760/10000. loss: 0.0016238826016585033\n2761/10000. loss: 0.000818661026035746\n2762/10000. loss: 0.00107578095048666\n2763/10000. loss: 0.001558190832535426\n2764/10000. loss: 0.0007134008531769117\n2765/10000. loss: 0.0010766723038007815\n2766/10000. loss: 0.0012931481469422579\n2767/10000. loss: 0.0008680425429095825\n2768/10000. loss: 0.000982998947923382\n2769/10000. loss: 0.0011712233535945415\n2770/10000. loss: 0.0009195863579710325\n2771/10000. loss: 0.0010106673774619896\n2772/10000. loss: 0.0007427883489678303\n2773/10000. loss: 0.0009638511886199316\n2774/10000. loss: 0.0007537941758831342\n2775/10000. loss: 0.000695432381083568\n2776/10000. loss: 0.0007361150346696377\n2777/10000. loss: 0.0006578261964023113\n2778/10000. loss: 0.0005185953341424465\n2779/10000. loss: 0.0007391710144778093\n2780/10000. loss: 0.0006010612317671379\n2781/10000. loss: 0.0008901233474413554\n2782/10000. loss: 0.0008570431576420864\n2783/10000. loss: 0.0010911600353817146\n2784/10000. loss: 0.000898396751532952\n2785/10000. loss: 0.0013984969506661098\n2786/10000. loss: 0.001676291072120269\n2787/10000. loss: 0.0014446931891143322\n2788/10000. loss: 0.0016084082114199798\n2789/10000. loss: 0.0015970336583753426\n2790/10000. loss: 0.0014898621787627537\n2791/10000. loss: 0.0013841496159633\n2792/10000. loss: 0.0011386337379614513\n2793/10000. loss: 0.0012405199619630973\n2794/10000. loss: 0.0010930844582617283\n2795/10000. loss: 0.0008314762574930986\n2796/10000. loss: 0.0013115281860033672\n2797/10000. loss: 0.0008514618190626303\n2798/10000. loss: 0.0006287203480799993\n2799/10000. loss: 0.0007398754047850767\n2800/10000. loss: 0.0007719709537923336\n2801/10000. loss: 0.0006699988152831793\n2802/10000. loss: 0.0005079800418267647\n2803/10000. loss: 0.0005672841022411982\n2804/10000. loss: 0.0007030735723674297\n2805/10000. loss: 0.00042678900839140016\n2806/10000. loss: 0.0008176942355930805\n2807/10000. loss: 0.0005121314898133278\n2808/10000. loss: 0.0006734627143790325\n2809/10000. loss: 0.0006007623936360081\n2810/10000. loss: 0.000887998224546512\n2811/10000. loss: 0.0005218049045652151\n2812/10000. loss: 0.0005138174165040255\n2813/10000. loss: 0.0006634663635243973\n2814/10000. loss: 0.0006273305043578148\n2815/10000. loss: 0.0005566816544160247\n2816/10000. loss: 0.0004863354843109846\n2817/10000. loss: 0.0004437232855707407\n2818/10000. loss: 0.0006219888649260005\n2819/10000. loss: 0.0009286353985468546\n2820/10000. loss: 0.0009397272951900959\n2821/10000. loss: 0.001050422511373957\n2822/10000. loss: 0.0005493453160549203\n2823/10000. loss: 0.0009941225871443748\n2824/10000. loss: 0.0007981122471392155\n2825/10000. loss: 0.0009505719256897768\n2826/10000. loss: 0.0008877701281259457\n2827/10000. loss: 0.0013819169253110886\n2828/10000. loss: 0.0013020435192932684\n2829/10000. loss: 0.001093644027908643\n2830/10000. loss: 0.0008242593612521887\n2831/10000. loss: 0.0014208282033602397\n2832/10000. loss: 0.0014466112479567528\n2833/10000. loss: 0.0010560163451979558\n2834/10000. loss: 0.001647042731444041\n2835/10000. loss: 0.00130050303414464\n2836/10000. loss: 0.0009030324096481005\n2837/10000. loss: 0.0012507160815099876\n2838/10000. loss: 0.0010489022048811119\n2839/10000. loss: 0.0014360272325575352\n2840/10000. loss: 0.0011216962399582069\n2841/10000. loss: 0.0013468852266669273\n2842/10000. loss: 0.0011357169908781846\n2843/10000. loss: 0.0009973074775189161\n2844/10000. loss: 0.001649715006351471\n2845/10000. loss: 0.0016550238554676373\n2846/10000. loss: 0.00118586840108037\n2847/10000. loss: 0.001677941686163346\n2848/10000. loss: 0.0011320173895607393\n2849/10000. loss: 0.0012259486441810925\n2850/10000. loss: 0.0012529673210034769\n2851/10000. loss: 0.001217885718991359\n2852/10000. loss: 0.0012763917135695617\n2853/10000. loss: 0.001076893803353111\n2854/10000. loss: 0.0010904520750045776\n2855/10000. loss: 0.0009158661123365164\n2856/10000. loss: 0.001023102318868041\n2857/10000. loss: 0.0008673383854329586\n2858/10000. loss: 0.000782013793165485\n2859/10000. loss: 0.001060033372292916\n2860/10000. loss: 0.0010054619051516056\n2861/10000. loss: 0.0005672643892467022\n2862/10000. loss: 0.0011027168948203325\n2863/10000. loss: 0.0008747850855191549\n2864/10000. loss: 0.001252726496507724\n2865/10000. loss: 0.0011403387567649286\n2866/10000. loss: 0.0007766198832541704\n2867/10000. loss: 0.0005335309154664477\n2868/10000. loss: 0.0006772482302039862\n2869/10000. loss: 0.000722743726025025\n2870/10000. loss: 0.0008571435076495012\n2871/10000. loss: 0.0004511749915157755\n2872/10000. loss: 0.0005937076639384031\n2873/10000. loss: 0.0006811672355979681\n2874/10000. loss: 0.000683580602829655\n2875/10000. loss: 0.00043236146060129005\n2876/10000. loss: 0.000415107817389071\n2877/10000. loss: 0.0006142274166146914\n2878/10000. loss: 0.000604476702089111\n2879/10000. loss: 0.0004470179943988721\n2880/10000. loss: 0.0004336320562288165\n2881/10000. loss: 0.0004769958322867751\n2882/10000. loss: 0.0006379803720240792\n2883/10000. loss: 0.00035052459376553696\n2884/10000. loss: 0.0006926302642871937\n2885/10000. loss: 0.00036354685047020513\n2886/10000. loss: 0.0004766915614406268\n2887/10000. loss: 0.0004196712203944723\n2888/10000. loss: 0.00033673946745693684\n2889/10000. loss: 0.0004992480777824918\n2890/10000. loss: 0.0004892748935769001\n2891/10000. loss: 0.0002826808486133814\n2892/10000. loss: 0.0005461434678484997\n2893/10000. loss: 0.0003236307529732585\n2894/10000. loss: 0.0006409071308250228\n2895/10000. loss: 0.00035937662081172067\n2896/10000. loss: 0.000670853303745389\n2897/10000. loss: 0.00029514054767787457\n2898/10000. loss: 0.0006723636761307716\n2899/10000. loss: 0.0003701574169099331\n2900/10000. loss: 0.00033517930811891955\n2901/10000. loss: 0.0005283172164733211\n2902/10000. loss: 0.00041051243897527456\n2903/10000. loss: 0.00034066922186563414\n2904/10000. loss: 0.0006695838334659735\n2905/10000. loss: 0.0006280819264551004\n2906/10000. loss: 0.00045201811008155346\n2907/10000. loss: 0.00044729017342130345\n2908/10000. loss: 0.0005983815838893255\n2909/10000. loss: 0.0005737190755705038\n2910/10000. loss: 0.0005018620286136866\n2911/10000. loss: 0.0004462234986325105\n2912/10000. loss: 0.0004537922019759814\n2913/10000. loss: 0.0005144924313450853\n2914/10000. loss: 0.0004405038586507241\n2915/10000. loss: 0.0005134115926921368\n2916/10000. loss: 0.0005300599926461776\n2917/10000. loss: 0.0003474556530515353\n2918/10000. loss: 0.0005580368839825193\n2919/10000. loss: 0.0003492582666998108\n2920/10000. loss: 0.00035691373826315004\n2921/10000. loss: 0.0004946814151480794\n2922/10000. loss: 0.0003688265181457003\n2923/10000. loss: 0.00044052380447586376\n2924/10000. loss: 0.0004431960017730792\n2925/10000. loss: 0.00036570724720756215\n2926/10000. loss: 0.0004533214960247278\n2927/10000. loss: 0.00040148078308751184\n2928/10000. loss: 0.0006387365671495596\n2929/10000. loss: 0.0003740347844238083\n2930/10000. loss: 0.0005992845787356297\n2931/10000. loss: 0.0006781659709910551\n2932/10000. loss: 0.000358275487087667\n2933/10000. loss: 0.000610126725708445\n2934/10000. loss: 0.0006295601294065515\n2935/10000. loss: 0.000672149316718181\n2936/10000. loss: 0.0007128122573097547\n2937/10000. loss: 0.0007174942487229904\n2938/10000. loss: 0.0007558845294018587\n2939/10000. loss: 0.0007904150988906622\n2940/10000. loss: 0.0008963570774843296\n2941/10000. loss: 0.001112579678495725\n2942/10000. loss: 0.0012359350609282653\n2943/10000. loss: 0.000986057488868634\n2944/10000. loss: 0.0019765604908267656\n2945/10000. loss: 0.00242452509701252\n2946/10000. loss: 0.002187915767232577\n2947/10000. loss: 0.004036931941906611\n2948/10000. loss: 0.003375910222530365\n2949/10000. loss: 0.0027284277603030205\n2950/10000. loss: 0.0036830871055523553\n2951/10000. loss: 0.003082015241185824\n2952/10000. loss: 0.0023102867417037487\n2953/10000. loss: 0.0027981450160344443\n2954/10000. loss: 0.0032496576507886252\n2955/10000. loss: 0.0023907835905750594\n2956/10000. loss: 0.0016476611296335857\n2957/10000. loss: 0.0018262251590689023\n2958/10000. loss: 0.0009079258888959885\n2959/10000. loss: 0.0014505060389637947\n2960/10000. loss: 0.0016403133049607277\n2961/10000. loss: 0.0015421914868056774\n2962/10000. loss: 0.0009052573392788569\n2963/10000. loss: 0.000772267347201705\n2964/10000. loss: 0.0014226604253053665\n2965/10000. loss: 0.0009197765029966831\n2966/10000. loss: 0.0012315756175667048\n2967/10000. loss: 0.0010364822422464688\n2968/10000. loss: 0.0007832784516115984\n2969/10000. loss: 0.0008867058592538039\n2970/10000. loss: 0.000964911732201775\n2971/10000. loss: 0.0010960674068580072\n2972/10000. loss: 0.0008269610504309336\n2973/10000. loss: 0.000649942783638835\n2974/10000. loss: 0.0006394078178952137\n2975/10000. loss: 0.0007736238185316324\n2976/10000. loss: 0.0007173064320037762\n2977/10000. loss: 0.0005938092557092508\n2978/10000. loss: 0.0005402465661366781\n2979/10000. loss: 0.00040815487348785\n2980/10000. loss: 0.0004379325934375326\n2981/10000. loss: 0.0007401642700036367\n2982/10000. loss: 0.0004709604351470868\n2983/10000. loss: 0.0006213472224771976\n2984/10000. loss: 0.00035845080856233835\n2985/10000. loss: 0.00043195039809991914\n2986/10000. loss: 0.0005230498500168324\n2987/10000. loss: 0.0006016306579113007\n2988/10000. loss: 0.0004243190245081981\n2989/10000. loss: 0.0005187136121094227\n2990/10000. loss: 0.0004854979148755471\n2991/10000. loss: 0.00041741908838351566\n2992/10000. loss: 0.0007587408957382044\n2993/10000. loss: 0.0004916831385344267\n2994/10000. loss: 0.0003589144131789605\n2995/10000. loss: 0.00031924445647746325\n2996/10000. loss: 0.0006937387709816297\n2997/10000. loss: 0.00032894328857461613\n2998/10000. loss: 0.0005647627792010704\n2999/10000. loss: 0.0004685478052124381\n3000/10000. loss: 0.00042716953127334517\n3001/10000. loss: 0.0003496416223545869\n3002/10000. loss: 0.0006280165786544482\n3003/10000. loss: 0.0005383063107728958\n3004/10000. loss: 0.00034077721647918224\n3005/10000. loss: 0.0006579600740224123\n3006/10000. loss: 0.0004407025796050827\n3007/10000. loss: 0.0004358483711257577\n3008/10000. loss: 0.00048079947009682655\n3009/10000. loss: 0.0004542372577513258\n3010/10000. loss: 0.0005181825254112482\n3011/10000. loss: 0.000543953695644935\n3012/10000. loss: 0.00033317071696122486\n3013/10000. loss: 0.0006085417699068785\n3014/10000. loss: 0.0005380545432368914\n3015/10000. loss: 0.0003589602808157603\n3016/10000. loss: 0.00046805703702072304\n3017/10000. loss: 0.0006767603723953167\n3018/10000. loss: 0.000390696261699001\n3019/10000. loss: 0.000605485945319136\n3020/10000. loss: 0.0005565180520837506\n3021/10000. loss: 0.00045315657431880635\n3022/10000. loss: 0.00032445408093432587\n3023/10000. loss: 0.00034467507308969897\n3024/10000. loss: 0.0005819416837766767\n3025/10000. loss: 0.00035433308221399784\n3026/10000. loss: 0.0005284537716458241\n3027/10000. loss: 0.0005551446617270509\n3028/10000. loss: 0.0003296514041721821\n3029/10000. loss: 0.00037119459981719655\n3030/10000. loss: 0.0004491427292426427\n3031/10000. loss: 0.0006273276327798764\n3032/10000. loss: 0.0003646857027585308\n3033/10000. loss: 0.0003091603284701705\n3034/10000. loss: 0.0003379311723013719\n3035/10000. loss: 0.0004491768777370453\n3036/10000. loss: 0.0004609785197923581\n3037/10000. loss: 0.0003564482710013787\n3038/10000. loss: 0.00040105726414670545\n3039/10000. loss: 0.0006405596310893694\n3040/10000. loss: 0.0003342649433761835\n3041/10000. loss: 0.00037543712339053553\n3042/10000. loss: 0.00045449985191226006\n3043/10000. loss: 0.0005773321414987246\n3044/10000. loss: 0.00041223041868458193\n3045/10000. loss: 0.0005121404925982157\n3046/10000. loss: 0.0004790444703151782\n3047/10000. loss: 0.0003276893791432182\n3048/10000. loss: 0.0004141691606491804\n3049/10000. loss: 0.0004458165494725108\n3050/10000. loss: 0.0005976456838349501\n3051/10000. loss: 0.00044452639607091743\n3052/10000. loss: 0.0005209378432482481\n3053/10000. loss: 0.0005380620714277029\n3054/10000. loss: 0.0005361551108459631\n3055/10000. loss: 0.0003702969600756963\n3056/10000. loss: 0.0005872843321412802\n3057/10000. loss: 0.0005515454104170203\n3058/10000. loss: 0.0005600378305340806\n3059/10000. loss: 0.0005759565780560175\n3060/10000. loss: 0.0007285509879390398\n3061/10000. loss: 0.000839421913648645\n3062/10000. loss: 0.0009618850114444891\n3063/10000. loss: 0.0009332935636242231\n3064/10000. loss: 0.0010735941274712484\n3065/10000. loss: 0.0010981899686157703\n3066/10000. loss: 0.0013192311550180118\n3067/10000. loss: 0.001859446366628011\n3068/10000. loss: 0.004252451782425244\n3069/10000. loss: 0.0029123717298110328\n3070/10000. loss: 0.0015595111375053723\n3071/10000. loss: 0.003058680333197117\n3072/10000. loss: 0.00751689945658048\n3073/10000. loss: 0.0038550527145465216\n3074/10000. loss: 0.007132863005002339\n3075/10000. loss: 0.006542171041170756\n3076/10000. loss: 0.005253046130140622\n3077/10000. loss: 0.006045779834191005\n3078/10000. loss: 0.006273384516437848\n3079/10000. loss: 0.005554795265197754\n3080/10000. loss: 0.004012036758164565\n3081/10000. loss: 0.0037018867830435434\n3082/10000. loss: 0.002627173438668251\n3083/10000. loss: 0.0020939799336095652\n3084/10000. loss: 0.0024256535495320954\n3085/10000. loss: 0.002382038626819849\n3086/10000. loss: 0.001971871592104435\n3087/10000. loss: 0.0016785521681110065\n3088/10000. loss: 0.0012476606449733179\n3089/10000. loss: 0.0011154734529554844\n3090/10000. loss: 0.0011275198000172775\n3091/10000. loss: 0.001604878343641758\n3092/10000. loss: 0.0013394087242583435\n3093/10000. loss: 0.0011529583328713973\n3094/10000. loss: 0.001512799256791671\n3095/10000. loss: 0.0010954516474157572\n3096/10000. loss: 0.0012636508326977491\n3097/10000. loss: 0.0010916837491095066\n3098/10000. loss: 0.0007734323541323344\n3099/10000. loss: 0.0006613382138311863\n3100/10000. loss: 0.0007508180569857359\n3101/10000. loss: 0.0007865655546387037\n3102/10000. loss: 0.0007280511781573296\n3103/10000. loss: 0.0006889865423242251\n3104/10000. loss: 0.000414177270916601\n3105/10000. loss: 0.0006603910587728024\n3106/10000. loss: 0.00045512445891896885\n3107/10000. loss: 0.0004414548942198356\n3108/10000. loss: 0.0004969748357931773\n3109/10000. loss: 0.0007076220742116371\n3110/10000. loss: 0.00046013617732872564\n3111/10000. loss: 0.000567687830577294\n3112/10000. loss: 0.0005492270380879442\n3113/10000. loss: 0.0006991025681296984\n3114/10000. loss: 0.0005773794061193863\n3115/10000. loss: 0.00043199929253508645\n3116/10000. loss: 0.0005421869767208894\n3117/10000. loss: 0.0006272345005224148\n3118/10000. loss: 0.00043420109432190657\n3119/10000. loss: 0.0004295728091771404\n3120/10000. loss: 0.0006396073537568251\n3121/10000. loss: 0.00042878413417687017\n3122/10000. loss: 0.0006121807576467594\n3123/10000. loss: 0.00043295502352217835\n3124/10000. loss: 0.000462791610819598\n3125/10000. loss: 0.0005415775425111254\n3126/10000. loss: 0.0004656415743132432\n3127/10000. loss: 0.0006749369204044342\n3128/10000. loss: 0.0004740285997589429\n3129/10000. loss: 0.0004350259356821577\n3130/10000. loss: 0.0005156801004583637\n3131/10000. loss: 0.0004176554890970389\n3132/10000. loss: 0.00036100259361167747\n3133/10000. loss: 0.00041314233870555955\n3134/10000. loss: 0.00040562838936845463\n3135/10000. loss: 0.00047115546961625415\n3136/10000. loss: 0.0005157874741901954\n3137/10000. loss: 0.0007692635990679264\n3138/10000. loss: 0.0003526032281418641\n3139/10000. loss: 0.0005491964984685183\n3140/10000. loss: 0.00032621963570515317\n3141/10000. loss: 0.0005197541322559118\n3142/10000. loss: 0.00043856888078153133\n3143/10000. loss: 0.0003168086016861101\n3144/10000. loss: 0.0003950160462409258\n3145/10000. loss: 0.00031651688429216546\n3146/10000. loss: 0.0004431129976486166\n3147/10000. loss: 0.0003674769541248679\n3148/10000. loss: 0.0004960945419346293\n3149/10000. loss: 0.0006955275312066078\n3150/10000. loss: 0.0003098895346435408\n3151/10000. loss: 0.0004710485227406025\n3152/10000. loss: 0.0005559892548869053\n3153/10000. loss: 0.00043454021215438843\n3154/10000. loss: 0.0005111630307510495\n3155/10000. loss: 0.0003269740069905917\n3156/10000. loss: 0.0004392430807153384\n3157/10000. loss: 0.0006256163275490204\n3158/10000. loss: 0.00033389396655062836\n3159/10000. loss: 0.000563628583525618\n3160/10000. loss: 0.0005419421941041946\n3161/10000. loss: 0.000507438788190484\n3162/10000. loss: 0.0005772218573838472\n3163/10000. loss: 0.0005087497799346844\n3164/10000. loss: 0.0005551399663090706\n3165/10000. loss: 0.0004392109500865142\n3166/10000. loss: 0.0006620960775762796\n3167/10000. loss: 0.0004507958268125852\n3168/10000. loss: 0.0005040682153776288\n3169/10000. loss: 0.0005474089023967584\n3170/10000. loss: 0.0005894151593868931\n3171/10000. loss: 0.0004430572735145688\n3172/10000. loss: 0.0006316829239949584\n3173/10000. loss: 0.0005042787330845991\n3174/10000. loss: 0.0005229490343481302\n3175/10000. loss: 0.0003893208922818303\n3176/10000. loss: 0.00048594312587132055\n3177/10000. loss: 0.0003849949377278487\n3178/10000. loss: 0.00044885731767863035\n3179/10000. loss: 0.00034105606998006505\n3180/10000. loss: 0.000447435537353158\n3181/10000. loss: 0.00047177262604236603\n3182/10000. loss: 0.00032129996300985414\n3183/10000. loss: 0.0004133978703369697\n3184/10000. loss: 0.0003298761633535226\n3185/10000. loss: 0.0004199279937893152\n3186/10000. loss: 0.0004309345968067646\n3187/10000. loss: 0.0004934032137195269\n3188/10000. loss: 0.0004571587002525727\n3189/10000. loss: 0.0003287194607158502\n3190/10000. loss: 0.0005055897248287996\n3191/10000. loss: 0.00044848397374153137\n3192/10000. loss: 0.00039404948862890404\n3193/10000. loss: 0.0004161090279618899\n3194/10000. loss: 0.0005168279555315772\n3195/10000. loss: 0.0004025607680281003\n3196/10000. loss: 0.00034380482975393534\n3197/10000. loss: 0.0003893437484900157\n3198/10000. loss: 0.0005743970395997167\n3199/10000. loss: 0.0004141350121547778\n3200/10000. loss: 0.0003788076573982835\n3201/10000. loss: 0.00040717814893772203\n3202/10000. loss: 0.00031733747649316985\n3203/10000. loss: 0.00040698501591881114\n3204/10000. loss: 0.0004895056287447611\n3205/10000. loss: 0.0003337043259913723\n3206/10000. loss: 0.00039272864038745564\n3207/10000. loss: 0.0005023465491831303\n3208/10000. loss: 0.0005927220142136017\n3209/10000. loss: 0.00032989059885342914\n3210/10000. loss: 0.000306446027631561\n3211/10000. loss: 0.0004338830476626754\n3212/10000. loss: 0.00047501723747700453\n3213/10000. loss: 0.0006384362156192461\n3214/10000. loss: 0.000405952800065279\n3215/10000. loss: 0.0005315660964697599\n3216/10000. loss: 0.0002997041253062586\n3217/10000. loss: 0.0002850018596897523\n3218/10000. loss: 0.0002737604857732852\n3219/10000. loss: 0.0004832940176129341\n3220/10000. loss: 0.0005870001235355934\n3221/10000. loss: 0.0004719930002465844\n3222/10000. loss: 0.0003098380984738469\n3223/10000. loss: 0.000538382368783156\n3224/10000. loss: 0.0005394043400883675\n3225/10000. loss: 0.0006528453280528387\n3226/10000. loss: 0.0005123143394788107\n3227/10000. loss: 0.0007783458568155766\n3228/10000. loss: 0.0005605508728573719\n3229/10000. loss: 0.000333588570356369\n3230/10000. loss: 0.0004414098026851813\n3231/10000. loss: 0.00034819267845402163\n3232/10000. loss: 0.0006307928512493769\n3233/10000. loss: 0.00044007420850296813\n3234/10000. loss: 0.0004833228498076399\n3235/10000. loss: 0.0003790719589839379\n3236/10000. loss: 0.0005802915741999944\n3237/10000. loss: 0.000320822698995471\n3238/10000. loss: 0.00037353454778591794\n3239/10000. loss: 0.0005547418259084225\n3240/10000. loss: 0.0004224070192625125\n3241/10000. loss: 0.0005734141062324246\n3242/10000. loss: 0.000493337904723982\n3243/10000. loss: 0.000510736679037412\n3244/10000. loss: 0.000450719924022754\n3245/10000. loss: 0.0005937283082554737\n3246/10000. loss: 0.0006441283815850815\n3247/10000. loss: 0.0005138398846611381\n3248/10000. loss: 0.0006012637168169022\n3249/10000. loss: 0.00040604119809965294\n3250/10000. loss: 0.0008084777897844712\n3251/10000. loss: 0.0006376661670704683\n3252/10000. loss: 0.0005648080647612611\n3253/10000. loss: 0.00047253720307101804\n3254/10000. loss: 0.0006703776307404041\n3255/10000. loss: 0.0005411109110961357\n3256/10000. loss: 0.0005341497405121723\n3257/10000. loss: 0.0006205784933020672\n3258/10000. loss: 0.0003932241428022583\n3259/10000. loss: 0.0003707448098187645\n3260/10000. loss: 0.000668348278850317\n3261/10000. loss: 0.000516511577491959\n3262/10000. loss: 0.0006214946818848451\n3263/10000. loss: 0.0007910708275934061\n3264/10000. loss: 0.0004597151031096776\n3265/10000. loss: 0.0005937636985133091\n3266/10000. loss: 0.00041691819205880165\n3267/10000. loss: 0.00046768734076370794\n3268/10000. loss: 0.0006296159699559212\n3269/10000. loss: 0.0006126205359275142\n3270/10000. loss: 0.0004924040598173937\n3271/10000. loss: 0.0003723927851145466\n3272/10000. loss: 0.000553936775152882\n3273/10000. loss: 0.00041089676475773257\n3274/10000. loss: 0.0005693869510044655\n3275/10000. loss: 0.0003487979993224144\n3276/10000. loss: 0.0003936207698037227\n3277/10000. loss: 0.0003291652149831255\n3278/10000. loss: 0.0006429880935077866\n3279/10000. loss: 0.0004972965301324924\n3280/10000. loss: 0.0006053824909031391\n3281/10000. loss: 0.0005305659336348375\n3282/10000. loss: 0.0005297161017855009\n3283/10000. loss: 0.0006688958965241909\n3284/10000. loss: 0.0004908561240881681\n3285/10000. loss: 0.0003350697613010804\n3286/10000. loss: 0.00048448956416298944\n3287/10000. loss: 0.0005007351282984018\n3288/10000. loss: 0.0005434913715968529\n3289/10000. loss: 0.000609979343911012\n3290/10000. loss: 0.00031310141397019226\n3291/10000. loss: 0.00046452623791992664\n3292/10000. loss: 0.0005207719126095375\n3293/10000. loss: 0.000597043273349603\n3294/10000. loss: 0.0005864289899667104\n3295/10000. loss: 0.0003531165032957991\n3296/10000. loss: 0.0005009438609704375\n3297/10000. loss: 0.0005338240492468079\n3298/10000. loss: 0.00044101391298075515\n3299/10000. loss: 0.00033070030622184277\n3300/10000. loss: 0.0004934016615152359\n3301/10000. loss: 0.0005253037282576164\n3302/10000. loss: 0.0004407213224718968\n3303/10000. loss: 0.0006743733150263628\n3304/10000. loss: 0.00041755835991352797\n3305/10000. loss: 0.00032278464641422033\n3306/10000. loss: 0.00035153057736655075\n3307/10000. loss: 0.00042643118649721146\n3308/10000. loss: 0.0004907997014621893\n3309/10000. loss: 0.0005409706694384416\n3310/10000. loss: 0.0005993257509544492\n3311/10000. loss: 0.0005476399480054776\n3312/10000. loss: 0.0005021352165689071\n3313/10000. loss: 0.00042321377744277317\n3314/10000. loss: 0.0005617965168009201\n3315/10000. loss: 0.0005872085457667708\n3316/10000. loss: 0.0006326754810288548\n3317/10000. loss: 0.00039754357809821766\n3318/10000. loss: 0.000742162267367045\n3319/10000. loss: 0.0007425270353754362\n3320/10000. loss: 0.0006112009286880493\n3321/10000. loss: 0.0005182455061003566\n3322/10000. loss: 0.0006387494116400679\n3323/10000. loss: 0.0007146922095368305\n3324/10000. loss: 0.00037131311061481637\n3325/10000. loss: 0.00047502949989090365\n3326/10000. loss: 0.0005511864243696133\n3327/10000. loss: 0.000401109845067064\n3328/10000. loss: 0.0007470023507873217\n3329/10000. loss: 0.0005547467153519392\n3330/10000. loss: 0.000677044503390789\n3331/10000. loss: 0.0004428151684502761\n3332/10000. loss: 0.0005762891378253698\n3333/10000. loss: 0.0003664641020198663\n3334/10000. loss: 0.00036199286114424467\n3335/10000. loss: 0.0004086424208556612\n3336/10000. loss: 0.00067277648486197\n3337/10000. loss: 0.0005749837340166172\n3338/10000. loss: 0.0006830600711206595\n3339/10000. loss: 0.0007054023444652557\n3340/10000. loss: 0.0005655240577956041\n3341/10000. loss: 0.00041205273009836674\n3342/10000. loss: 0.0008275506552308798\n3343/10000. loss: 0.0007278413977473974\n3344/10000. loss: 0.00086578579309086\n3345/10000. loss: 0.0009634893697996935\n3346/10000. loss: 0.0007698205299675465\n3347/10000. loss: 0.000659811000029246\n3348/10000. loss: 0.0006897001682470242\n3349/10000. loss: 0.0006744170095771551\n3350/10000. loss: 0.0006015043084820112\n3351/10000. loss: 0.0007557996238271395\n3352/10000. loss: 0.0006014884759982427\n3353/10000. loss: 0.0008729791734367609\n3354/10000. loss: 0.0007307416914651791\n3355/10000. loss: 0.0008786767721176147\n3356/10000. loss: 0.0006277774227783084\n3357/10000. loss: 0.001070479319120447\n3358/10000. loss: 0.0006424907284478346\n3359/10000. loss: 0.0007040625593314568\n3360/10000. loss: 0.0007473056515057882\n3361/10000. loss: 0.000649480459590753\n3362/10000. loss: 0.0006360109740247329\n3363/10000. loss: 0.0006548361852765083\n3364/10000. loss: 0.000507430755533278\n3365/10000. loss: 0.00048513244837522507\n3366/10000. loss: 0.0006467473382751147\n3367/10000. loss: 0.0004498645042379697\n3368/10000. loss: 0.0004763250860075156\n3369/10000. loss: 0.0005383489963908991\n3370/10000. loss: 0.0004548939565817515\n3371/10000. loss: 0.0005081117463608583\n3372/10000. loss: 0.0006554694070170323\n3373/10000. loss: 0.0005138168732325236\n3374/10000. loss: 0.0002944376125621299\n3375/10000. loss: 0.0005058591874937216\n3376/10000. loss: 0.00044113028949747485\n3377/10000. loss: 0.0003197712746138374\n3378/10000. loss: 0.0004062241253753503\n3379/10000. loss: 0.0003299982442210118\n3380/10000. loss: 0.0005392367796351513\n3381/10000. loss: 0.00032522880549853045\n3382/10000. loss: 0.0005743166742225488\n3383/10000. loss: 0.0003947937705864509\n3384/10000. loss: 0.0004302427793542544\n3385/10000. loss: 0.0005314087805648645\n3386/10000. loss: 0.000575962980898718\n3387/10000. loss: 0.0005675212402517597\n3388/10000. loss: 0.0006707770129044851\n3389/10000. loss: 0.0004301259759813547\n3390/10000. loss: 0.0006929110580434402\n3391/10000. loss: 0.000475324767952164\n3392/10000. loss: 0.0003871713997796178\n3393/10000. loss: 0.00048174639232456684\n3394/10000. loss: 0.00044427210620294016\n3395/10000. loss: 0.0003734846444179614\n3396/10000. loss: 0.0005136883507172266\n3397/10000. loss: 0.0002887114533223212\n3398/10000. loss: 0.00044678054594745237\n3399/10000. loss: 0.00036208308301866055\n3400/10000. loss: 0.00041156836474935216\n3401/10000. loss: 0.00029179989360272884\n3402/10000. loss: 0.0004899192911883196\n3403/10000. loss: 0.0004007481038570404\n3404/10000. loss: 0.00030069398538519937\n3405/10000. loss: 0.0004202369212483366\n3406/10000. loss: 0.00027313818767045933\n3407/10000. loss: 0.00043006955335537594\n3408/10000. loss: 0.00030994795573254425\n3409/10000. loss: 0.0004390263929963112\n3410/10000. loss: 0.00040213100146502256\n3411/10000. loss: 0.000398595390530924\n3412/10000. loss: 0.00037481232235829037\n3413/10000. loss: 0.0003208944690413773\n3414/10000. loss: 0.0005117745604366064\n3415/10000. loss: 0.00031874656754856306\n3416/10000. loss: 0.0004060703795403242\n3417/10000. loss: 0.0005064996269841989\n3418/10000. loss: 0.00039012070434788865\n3419/10000. loss: 0.00031213031616061926\n3420/10000. loss: 0.00039836418970177573\n3421/10000. loss: 0.000464737454118828\n3422/10000. loss: 0.0004265316141148408\n3423/10000. loss: 0.00035368348471820354\n3424/10000. loss: 0.0002829674437331657\n3425/10000. loss: 0.00038883741945028305\n3426/10000. loss: 0.000438014161773026\n3427/10000. loss: 0.0003458186984062195\n3428/10000. loss: 0.000509008183144033\n3429/10000. loss: 0.00026923239541550476\n3430/10000. loss: 0.0004701250387976567\n3431/10000. loss: 0.0004015484591946006\n3432/10000. loss: 0.00043024316740532714\n3433/10000. loss: 0.0005171726612995068\n3434/10000. loss: 0.00035647380476196605\n3435/10000. loss: 0.0003213776120295127\n3436/10000. loss: 0.0006667516039063534\n3437/10000. loss: 0.0007711136713624001\n3438/10000. loss: 0.00036496649651477736\n3439/10000. loss: 0.0007649392355233431\n3440/10000. loss: 0.0014553672323624294\n3441/10000. loss: 0.001837249690045913\n3442/10000. loss: 0.001507103443145752\n3443/10000. loss: 0.0025389070312182107\n3444/10000. loss: 0.0026079720507065454\n3445/10000. loss: 0.0030497449139753976\n3446/10000. loss: 0.003571245508889357\n3447/10000. loss: 0.004732480583091577\n3448/10000. loss: 0.005024531856179237\n3449/10000. loss: 0.004876987387736638\n3450/10000. loss: 0.00509389986594518\n3451/10000. loss: 0.0030401935800909996\n3452/10000. loss: 0.004119862491885821\n3453/10000. loss: 0.0021651160592834153\n3454/10000. loss: 0.0014876641022662322\n3455/10000. loss: 0.0023089988778034845\n3456/10000. loss: 0.003045010690887769\n3457/10000. loss: 0.0019919400413831076\n3458/10000. loss: 0.0018219323828816414\n3459/10000. loss: 0.0018865404029687245\n3460/10000. loss: 0.0020935569579402604\n3461/10000. loss: 0.0017832277032236259\n3462/10000. loss: 0.001401428443690141\n3463/10000. loss: 0.001119704373801748\n3464/10000. loss: 0.0007324561011046171\n3465/10000. loss: 0.0007111420854926109\n3466/10000. loss: 0.0007490306937446197\n3467/10000. loss: 0.0008706047665327787\n3468/10000. loss: 0.0008984293478230635\n3469/10000. loss: 0.0009001725508521\n3470/10000. loss: 0.00100941591275235\n3471/10000. loss: 0.0009168756660073996\n3472/10000. loss: 0.0009949412196874619\n3473/10000. loss: 0.0008173695920656124\n3474/10000. loss: 0.0006998892252643903\n3475/10000. loss: 0.0007584770210087299\n3476/10000. loss: 0.0007098858865598837\n3477/10000. loss: 0.0004952086601406336\n3478/10000. loss: 0.00041587103623896837\n3479/10000. loss: 0.0005268095216403405\n3480/10000. loss: 0.0006504539245118698\n3481/10000. loss: 0.0003234604567599793\n3482/10000. loss: 0.000521407462656498\n3483/10000. loss: 0.0005424727375308672\n3484/10000. loss: 0.0005786188800508777\n3485/10000. loss: 0.0007378991382817427\n3486/10000. loss: 0.0005612447081754605\n3487/10000. loss: 0.0005821466523533066\n3488/10000. loss: 0.0008703609928488731\n3489/10000. loss: 0.0006577752064913511\n3490/10000. loss: 0.000786196750899156\n3491/10000. loss: 0.0005764250721161565\n3492/10000. loss: 0.0005108579061925411\n3493/10000. loss: 0.000755366093168656\n3494/10000. loss: 0.0009405545424669981\n3495/10000. loss: 0.0006073537903527418\n3496/10000. loss: 0.0007303178620835146\n3497/10000. loss: 0.0008266716419408718\n3498/10000. loss: 0.0005969259267052015\n3499/10000. loss: 0.0005237148919453224\n3500/10000. loss: 0.0005346538964658976\n3501/10000. loss: 0.0006387553876265883\n3502/10000. loss: 0.00047240111355980236\n3503/10000. loss: 0.0004245118470862508\n3504/10000. loss: 0.0004549067622671525\n3505/10000. loss: 0.0005151438914860288\n3506/10000. loss: 0.0004935732576996088\n3507/10000. loss: 0.0004188469999159376\n3508/10000. loss: 0.0005245067877694964\n3509/10000. loss: 0.0006290250457823277\n3510/10000. loss: 0.000493502321963509\n3511/10000. loss: 0.0004196402927239736\n3512/10000. loss: 0.0005126665346324444\n3513/10000. loss: 0.00041656335815787315\n3514/10000. loss: 0.000464865006506443\n3515/10000. loss: 0.0005514305084943771\n3516/10000. loss: 0.00048721255734562874\n3517/10000. loss: 0.00032950867898762226\n3518/10000. loss: 0.0003266342294712861\n3519/10000. loss: 0.0003058319174063702\n3520/10000. loss: 0.00033393362537026405\n3521/10000. loss: 0.000509131078918775\n3522/10000. loss: 0.00044636680589367944\n3523/10000. loss: 0.0005319297779351473\n3524/10000. loss: 0.00034560907321671647\n3525/10000. loss: 0.0005079921878253421\n3526/10000. loss: 0.00031047880960007507\n3527/10000. loss: 0.0004209458129480481\n3528/10000. loss: 0.00041377202918132144\n3529/10000. loss: 0.0005212747491896152\n3530/10000. loss: 0.0005765441649903854\n3531/10000. loss: 0.00031177839264273643\n3532/10000. loss: 0.00034791692936172086\n3533/10000. loss: 0.00031478318851441145\n3534/10000. loss: 0.0004078704708566268\n3535/10000. loss: 0.00030810948616514605\n3536/10000. loss: 0.00043425068724900484\n3537/10000. loss: 0.00028527213726192713\n3538/10000. loss: 0.00031637448895101744\n3539/10000. loss: 0.0004988329795499643\n3540/10000. loss: 0.0003785123893370231\n3541/10000. loss: 0.0005024056881666183\n3542/10000. loss: 0.00048506702296435833\n3543/10000. loss: 0.0006113864947110415\n3544/10000. loss: 0.00026915040022383135\n3545/10000. loss: 0.0002640004192168514\n3546/10000. loss: 0.0003521257701019446\n3547/10000. loss: 0.0005437759682536125\n3548/10000. loss: 0.0004566581143687169\n3549/10000. loss: 0.0004243201110512018\n3550/10000. loss: 0.000279522268101573\n3551/10000. loss: 0.0004585655794168512\n3552/10000. loss: 0.0002998279912086825\n3553/10000. loss: 0.00033042180196692544\n3554/10000. loss: 0.000490642113921543\n3555/10000. loss: 0.0006279309357826909\n3556/10000. loss: 0.0004142071896543105\n3557/10000. loss: 0.00046830981348951656\n3558/10000. loss: 0.0005875454129030307\n3559/10000. loss: 0.0005409084260463715\n3560/10000. loss: 0.0005624094434703389\n3561/10000. loss: 0.00032462594875444967\n3562/10000. loss: 0.0003792049052814643\n3563/10000. loss: 0.0005477847686658303\n3564/10000. loss: 0.000548584774757425\n3565/10000. loss: 0.0003458948340266943\n3566/10000. loss: 0.0005749770595381657\n3567/10000. loss: 0.0003367609654863675\n3568/10000. loss: 0.00047289781893293065\n3569/10000. loss: 0.0004618747237448891\n3570/10000. loss: 0.00046464428305625916\n3571/10000. loss: 0.0004046813119202852\n3572/10000. loss: 0.00033804471604526043\n3573/10000. loss: 0.0004875774805744489\n3574/10000. loss: 0.000565089751034975\n3575/10000. loss: 0.0004045576012382905\n3576/10000. loss: 0.0006157391083737215\n3577/10000. loss: 0.0003839889153217276\n3578/10000. loss: 0.00038972427137196064\n3579/10000. loss: 0.0004781521080682675\n3580/10000. loss: 0.0005269846490894755\n3581/10000. loss: 0.0002599244083588322\n3582/10000. loss: 0.00033049875249465305\n3583/10000. loss: 0.000334586133249104\n3584/10000. loss: 0.0005550251031915346\n3585/10000. loss: 0.0004751207306981087\n3586/10000. loss: 0.0004075874264041583\n3587/10000. loss: 0.0004772906346867482\n3588/10000. loss: 0.0004879578482359648\n3589/10000. loss: 0.00029822248810281354\n3590/10000. loss: 0.0002924912647965054\n3591/10000. loss: 0.0005571073076377312\n3592/10000. loss: 0.0003290286598106225\n3593/10000. loss: 0.00042790376270810765\n3594/10000. loss: 0.0004711281896258394\n3595/10000. loss: 0.000341782346367836\n3596/10000. loss: 0.0003392184929301341\n3597/10000. loss: 0.00047905626706779003\n3598/10000. loss: 0.0003170455650736888\n3599/10000. loss: 0.0003293203966071208\n3600/10000. loss: 0.0004642509932940205\n3601/10000. loss: 0.00047923453773061436\n3602/10000. loss: 0.000333149335347116\n3603/10000. loss: 0.000528299754175047\n3604/10000. loss: 0.0004753101384267211\n3605/10000. loss: 0.0004995485845332345\n3606/10000. loss: 0.0003588015291218956\n3607/10000. loss: 0.0006394986994564533\n3608/10000. loss: 0.0006168342661112547\n3609/10000. loss: 0.0009366879239678383\n3610/10000. loss: 0.0006399831424156824\n3611/10000. loss: 0.0008015042791763941\n3612/10000. loss: 0.0011679714856048424\n3613/10000. loss: 0.0014917518322666485\n3614/10000. loss: 0.001956363053371509\n3615/10000. loss: 0.0021286408106486\n3616/10000. loss: 0.0029781749472022057\n3617/10000. loss: 0.002832124630610148\n3618/10000. loss: 0.0012563841106990974\n3619/10000. loss: 0.0030920496210455894\n3620/10000. loss: 0.0021866345778107643\n3621/10000. loss: 0.002255777052293221\n3622/10000. loss: 0.0016207477698723476\n3623/10000. loss: 0.00220171253507336\n3624/10000. loss: 0.0012545986101031303\n3625/10000. loss: 0.001692581611375014\n3626/10000. loss: 0.001190406425545613\n3627/10000. loss: 0.001318270651002725\n3628/10000. loss: 0.0011302786879241467\n3629/10000. loss: 0.0009734709747135639\n3630/10000. loss: 0.0010526568318406742\n3631/10000. loss: 0.0011846187214056652\n3632/10000. loss: 0.001088822027668357\n3633/10000. loss: 0.0007795326722164949\n3634/10000. loss: 0.0008918289095163345\n3635/10000. loss: 0.0005767557304352522\n3636/10000. loss: 0.0004987090360373259\n3637/10000. loss: 0.0008108046992371479\n3638/10000. loss: 0.0006035736296325922\n3639/10000. loss: 0.0007593765233953794\n3640/10000. loss: 0.0007824740993479887\n3641/10000. loss: 0.0007224861377229294\n3642/10000. loss: 0.0008058589107046524\n3643/10000. loss: 0.0006576554539302985\n3644/10000. loss: 0.0005652696127071977\n3645/10000. loss: 0.0006152320032318433\n3646/10000. loss: 0.0006242637755349278\n3647/10000. loss: 0.00048245303332805634\n3648/10000. loss: 0.0004091671435162425\n3649/10000. loss: 0.0006717280484735966\n3650/10000. loss: 0.0005089369369670749\n3651/10000. loss: 0.00036057468969374895\n3652/10000. loss: 0.00039859899940590066\n3653/10000. loss: 0.0005691980477422476\n3654/10000. loss: 0.00045250907229880494\n3655/10000. loss: 0.00035037705674767494\n3656/10000. loss: 0.0003853099575887124\n3657/10000. loss: 0.0003513807508473595\n3658/10000. loss: 0.0007024786124626795\n3659/10000. loss: 0.0005573151477922996\n3660/10000. loss: 0.00035344444525738555\n3661/10000. loss: 0.0004118389915674925\n3662/10000. loss: 0.0003375538702433308\n3663/10000. loss: 0.0005150739258776108\n3664/10000. loss: 0.0006505554386725029\n3665/10000. loss: 0.0006179258149738113\n3666/10000. loss: 0.0004846282924214999\n3667/10000. loss: 0.0003350681314865748\n3668/10000. loss: 0.0004981277355303367\n3669/10000. loss: 0.0005357969009007016\n3670/10000. loss: 0.0004132482378433148\n3671/10000. loss: 0.0004639908050497373\n3672/10000. loss: 0.0003249043559965988\n3673/10000. loss: 0.00043397063078979653\n3674/10000. loss: 0.0005347081848109762\n3675/10000. loss: 0.0003036736937550207\n3676/10000. loss: 0.0005803471819187204\n3677/10000. loss: 0.0006220946088433266\n3678/10000. loss: 0.0002887280813107888\n3679/10000. loss: 0.0003053063410334289\n3680/10000. loss: 0.00040314757886032265\n3681/10000. loss: 0.00039918324910104275\n3682/10000. loss: 0.00041959139828880626\n3683/10000. loss: 0.0003749524476006627\n3684/10000. loss: 0.0003916197456419468\n3685/10000. loss: 0.000364727689884603\n3686/10000. loss: 0.0003271196425581972\n3687/10000. loss: 0.00042356861134370166\n3688/10000. loss: 0.0004824113566428423\n3689/10000. loss: 0.00043969644078363973\n3690/10000. loss: 0.0005021744873374701\n3691/10000. loss: 0.0004207779420539737\n3692/10000. loss: 0.00029886665288358927\n3693/10000. loss: 0.00033974942440787953\n3694/10000. loss: 0.00047312785560886067\n3695/10000. loss: 0.00028453530588497716\n3696/10000. loss: 0.00031094941853856045\n3697/10000. loss: 0.0003243856675301989\n3698/10000. loss: 0.0002962942235171795\n3699/10000. loss: 0.00028413332377870876\n3700/10000. loss: 0.0002781742950901389\n3701/10000. loss: 0.0003611657302826643\n3702/10000. loss: 0.0005643313440183798\n3703/10000. loss: 0.0003288286194826166\n3704/10000. loss: 0.0003018274049585064\n3705/10000. loss: 0.0002996069185125331\n3706/10000. loss: 0.0005540311491737763\n3707/10000. loss: 0.000530014435450236\n3708/10000. loss: 0.000485782278701663\n3709/10000. loss: 0.0004146466962993145\n3710/10000. loss: 0.0005801903704802195\n3711/10000. loss: 0.00038965993250409764\n3712/10000. loss: 0.0005599448923021555\n3713/10000. loss: 0.00038660472879807156\n3714/10000. loss: 0.0007098804538448652\n3715/10000. loss: 0.00045564611597607535\n3716/10000. loss: 0.00035904782513777417\n3717/10000. loss: 0.00036943843588232994\n3718/10000. loss: 0.000484062513957421\n3719/10000. loss: 0.0004097783239558339\n3720/10000. loss: 0.0003253367419044177\n3721/10000. loss: 0.000420698585609595\n3722/10000. loss: 0.00028505350928753614\n3723/10000. loss: 0.0005608752835541964\n3724/10000. loss: 0.00033550899631033343\n3725/10000. loss: 0.0004730599466711283\n3726/10000. loss: 0.00047302090873320896\n3727/10000. loss: 0.0004811022663488984\n3728/10000. loss: 0.0003940712970991929\n3729/10000. loss: 0.00038761079000929993\n3730/10000. loss: 0.0004891934028516213\n3731/10000. loss: 0.0002995178025836746\n3732/10000. loss: 0.0002827176552576323\n3733/10000. loss: 0.00035640554657826823\n3734/10000. loss: 0.0005100208412234982\n3735/10000. loss: 0.00043583308191349107\n3736/10000. loss: 0.0004877669271081686\n3737/10000. loss: 0.00044246964777509373\n3738/10000. loss: 0.0004607667215168476\n3739/10000. loss: 0.0006269008542100588\n3740/10000. loss: 0.0003396743753304084\n3741/10000. loss: 0.00040394595513741177\n3742/10000. loss: 0.00041782316596557695\n3743/10000. loss: 0.0005519821618994077\n3744/10000. loss: 0.0006365506754567226\n3745/10000. loss: 0.0005350850988179445\n3746/10000. loss: 0.0005152734229341149\n3747/10000. loss: 0.000535317036944131\n3748/10000. loss: 0.0003727363267292579\n3749/10000. loss: 0.0007308862016846737\n3750/10000. loss: 0.0004829787261163195\n3751/10000. loss: 0.00047494253764549893\n3752/10000. loss: 0.0006826600680748621\n3753/10000. loss: 0.0004191584885120392\n3754/10000. loss: 0.000673922710120678\n3755/10000. loss: 0.0009034674925108751\n3756/10000. loss: 0.0010798271590222914\n3757/10000. loss: 0.0007193750546624263\n3758/10000. loss: 0.001352783137311538\n3759/10000. loss: 0.0009724561435480913\n3760/10000. loss: 0.0013299142010509968\n3761/10000. loss: 0.0011468551432092984\n3762/10000. loss: 0.0013097502912084262\n3763/10000. loss: 0.0010919299287100632\n3764/10000. loss: 0.0009511110838502645\n3765/10000. loss: 0.0013519374964137871\n3766/10000. loss: 0.0009043041306237379\n3767/10000. loss: 0.0010167257860302925\n3768/10000. loss: 0.0008622008220603069\n3769/10000. loss: 0.0006836980270842711\n3770/10000. loss: 0.0011027591147770484\n3771/10000. loss: 0.0006476764101535082\n3772/10000. loss: 0.0006167443546776971\n3773/10000. loss: 0.000878477313866218\n3774/10000. loss: 0.0009581128445764383\n3775/10000. loss: 0.0006267193627233306\n3776/10000. loss: 0.000774116488173604\n3777/10000. loss: 0.0006134463474154472\n3778/10000. loss: 0.0006555796135216951\n3779/10000. loss: 0.0007116192330916723\n3780/10000. loss: 0.0005716830880070726\n3781/10000. loss: 0.0004579521094759305\n3782/10000. loss: 0.000588075335447987\n3783/10000. loss: 0.00043760395298401516\n3784/10000. loss: 0.0006202229609092077\n3785/10000. loss: 0.0003874542114014427\n3786/10000. loss: 0.0006221085010717312\n3787/10000. loss: 0.0005172502327089509\n3788/10000. loss: 0.0004569875697294871\n3789/10000. loss: 0.000581262051127851\n3790/10000. loss: 0.00040280733567972976\n3791/10000. loss: 0.0006048704963177443\n3792/10000. loss: 0.00040183755724380415\n3793/10000. loss: 0.0004921200064321359\n3794/10000. loss: 0.0003399467095732689\n3795/10000. loss: 0.0006565943670769533\n3796/10000. loss: 0.0005654057022184134\n3797/10000. loss: 0.0005370902363210917\n3798/10000. loss: 0.00036464860507597524\n3799/10000. loss: 0.0005303564636657635\n3800/10000. loss: 0.0005142396160711845\n3801/10000. loss: 0.00039354199543595314\n3802/10000. loss: 0.0003314315884684523\n3803/10000. loss: 0.0003792975718776385\n3804/10000. loss: 0.0002802082259828846\n3805/10000. loss: 0.00040277924078206223\n3806/10000. loss: 0.00027825834695249796\n3807/10000. loss: 0.0002619463096683224\n3808/10000. loss: 0.00038570080262919265\n3809/10000. loss: 0.0003120967885479331\n3810/10000. loss: 0.00028858077712357044\n3811/10000. loss: 0.0002641026706745227\n3812/10000. loss: 0.00044084157949934405\n3813/10000. loss: 0.0004252790240570903\n3814/10000. loss: 0.0005526806538303693\n3815/10000. loss: 0.00034729969532539445\n3816/10000. loss: 0.0004421401924143235\n3817/10000. loss: 0.00027487547292063635\n3818/10000. loss: 0.00044408537602672976\n3819/10000. loss: 0.00042514239127437275\n3820/10000. loss: 0.00043361967739959556\n3821/10000. loss: 0.0003835206540922324\n3822/10000. loss: 0.0004361203173175454\n3823/10000. loss: 0.00041382193254927796\n3824/10000. loss: 0.00047392118722200394\n3825/10000. loss: 0.00027533967901642126\n3826/10000. loss: 0.0004319441116725405\n3827/10000. loss: 0.00031043950002640486\n3828/10000. loss: 0.00043814706926544506\n3829/10000. loss: 0.0004748001229017973\n3830/10000. loss: 0.0003344772073129813\n3831/10000. loss: 0.00042131892405450344\n3832/10000. loss: 0.0003347796155139804\n3833/10000. loss: 0.0004947840934619308\n3834/10000. loss: 0.000399164001767834\n3835/10000. loss: 0.00029096752405166626\n3836/10000. loss: 0.0004336549124370019\n3837/10000. loss: 0.0003607522618646423\n3838/10000. loss: 0.00044852402061223984\n3839/10000. loss: 0.00025812685877705616\n3840/10000. loss: 0.0005335287423804402\n3841/10000. loss: 0.0004525598681842287\n3842/10000. loss: 0.0004579797387123108\n3843/10000. loss: 0.0004996375646442175\n3844/10000. loss: 0.0005920635691533486\n3845/10000. loss: 0.0004889988728488485\n3846/10000. loss: 0.0005413539474830031\n3847/10000. loss: 0.0014856602065265179\n3848/10000. loss: 0.0005310042373215159\n3849/10000. loss: 0.001414484033981959\n3850/10000. loss: 0.0019276700913906097\n3851/10000. loss: 0.0014520871142546337\n3852/10000. loss: 0.0013283311078945796\n3853/10000. loss: 0.0015734750777482986\n3854/10000. loss: 0.0018330393359065056\n3855/10000. loss: 0.0012474344111979008\n3856/10000. loss: 0.002073512257387241\n3857/10000. loss: 0.0021621044725179672\n3858/10000. loss: 0.0019813448501129947\n3859/10000. loss: 0.0018746598313252132\n3860/10000. loss: 0.0019856269160906472\n3861/10000. loss: 0.001379974652081728\n3862/10000. loss: 0.0019225506111979485\n3863/10000. loss: 0.001528997750331958\n3864/10000. loss: 0.0009088651277124882\n3865/10000. loss: 0.0010970402508974075\n3866/10000. loss: 0.0017175738078852494\n3867/10000. loss: 0.0008727253880351782\n3868/10000. loss: 0.0007737802031139532\n3869/10000. loss: 0.00045772800998141366\n3870/10000. loss: 0.0004317760467529297\n3871/10000. loss: 0.000470790973243614\n3872/10000. loss: 0.0004903411803146204\n3873/10000. loss: 0.0005171208952864011\n3874/10000. loss: 0.0006206689092020193\n3875/10000. loss: 0.0005128102687497934\n3876/10000. loss: 0.000506856944411993\n3877/10000. loss: 0.0004105563275516033\n3878/10000. loss: 0.0005051275948062539\n3879/10000. loss: 0.0005537700684120258\n3880/10000. loss: 0.000621649669483304\n3881/10000. loss: 0.0004922625763962666\n3882/10000. loss: 0.0006668847830345234\n3883/10000. loss: 0.0005773938416192929\n3884/10000. loss: 0.0005634749929110209\n3885/10000. loss: 0.0005167134416600069\n3886/10000. loss: 0.0005382565626253685\n3887/10000. loss: 0.0006901877156148354\n3888/10000. loss: 0.0003702046039203803\n3889/10000. loss: 0.000331172797208031\n3890/10000. loss: 0.000437974464148283\n3891/10000. loss: 0.0004362412728369236\n3892/10000. loss: 0.0005744116303200523\n3893/10000. loss: 0.0004035158781334758\n3894/10000. loss: 0.00032360289090623456\n3895/10000. loss: 0.0002938673909132679\n3896/10000. loss: 0.0002980582454862694\n3897/10000. loss: 0.00035421647286663455\n3898/10000. loss: 0.00038630197135110694\n3899/10000. loss: 0.0002904026381050547\n3900/10000. loss: 0.00030246219830587506\n3901/10000. loss: 0.0005198140473415455\n3902/10000. loss: 0.0004376999956245224\n3903/10000. loss: 0.00037582296257217723\n3904/10000. loss: 0.0003292692902808388\n3905/10000. loss: 0.0003294031290958325\n3906/10000. loss: 0.00045300348817060393\n3907/10000. loss: 0.00037457840517163277\n3908/10000. loss: 0.000295005738735199\n3909/10000. loss: 0.00035111869995792705\n3910/10000. loss: 0.0003631753691782554\n3911/10000. loss: 0.0003917706587041418\n3912/10000. loss: 0.00042109296191483736\n3913/10000. loss: 0.0005375407248114547\n3914/10000. loss: 0.000429417472332716\n3915/10000. loss: 0.00041345879435539246\n3916/10000. loss: 0.00044852351614584524\n3917/10000. loss: 0.0003712392256905635\n3918/10000. loss: 0.0002789529971778393\n3919/10000. loss: 0.00046018076439698535\n3920/10000. loss: 0.0002890702065390845\n3921/10000. loss: 0.00030380239089330036\n3922/10000. loss: 0.0009888785425573587\n3923/10000. loss: 0.0004383970517665148\n3924/10000. loss: 0.00035035858551661175\n3925/10000. loss: 0.0003272262401878834\n3926/10000. loss: 0.0003045525712271531\n3927/10000. loss: 0.00034085148945450783\n3928/10000. loss: 0.00045625326068451005\n3929/10000. loss: 0.0003067399569166203\n3930/10000. loss: 0.0002693198621273041\n3931/10000. loss: 0.00043593847658485174\n3932/10000. loss: 0.0003506878080467383\n3933/10000. loss: 0.0003416225081309676\n3934/10000. loss: 0.0005199279015262922\n3935/10000. loss: 0.00027764291735365987\n3936/10000. loss: 0.00036733058126022417\n3937/10000. loss: 0.0003191763535141945\n3938/10000. loss: 0.0004577499736721317\n3939/10000. loss: 0.000459773155550162\n3940/10000. loss: 0.0003476600783566634\n3941/10000. loss: 0.0003466487784559528\n3942/10000. loss: 0.0004571051492045323\n3943/10000. loss: 0.0003197278322962423\n3944/10000. loss: 0.000431641082589825\n3945/10000. loss: 0.0003610218021397789\n3946/10000. loss: 0.00037596859813978273\n3947/10000. loss: 0.0004804017177472512\n3948/10000. loss: 0.00038699495295683545\n3949/10000. loss: 0.00028858079652612406\n3950/10000. loss: 0.0003100595010134081\n3951/10000. loss: 0.0004029685320953528\n3952/10000. loss: 0.0004318456631153822\n3953/10000. loss: 0.00045776343904435635\n3954/10000. loss: 0.0006169631766776243\n3955/10000. loss: 0.0005450276657938957\n3956/10000. loss: 0.0004616673104465008\n3957/10000. loss: 0.00031663574433575076\n3958/10000. loss: 0.00043825642205774784\n3959/10000. loss: 0.0006138380461682876\n3960/10000. loss: 0.00040950820160408813\n3961/10000. loss: 0.00045997191530962783\n3962/10000. loss: 0.0005605998449027538\n3963/10000. loss: 0.0007638156724472841\n3964/10000. loss: 0.0005683590425178409\n3965/10000. loss: 0.0009048706851899624\n3966/10000. loss: 0.0012487849841515224\n3967/10000. loss: 0.0008074945459763209\n3968/10000. loss: 0.000984412928422292\n3969/10000. loss: 0.0017216258371869724\n3970/10000. loss: 0.0020409459248185158\n3971/10000. loss: 0.0024623608527084193\n3972/10000. loss: 0.00250883586704731\n3973/10000. loss: 0.0023050600041945777\n3974/10000. loss: 0.002132576579848925\n3975/10000. loss: 0.00196661613881588\n3976/10000. loss: 0.0020906906574964523\n3977/10000. loss: 0.0019425659750898678\n3978/10000. loss: 0.002284691669046879\n3979/10000. loss: 0.0012319519494970639\n3980/10000. loss: 0.0009392128946880499\n3981/10000. loss: 0.001991447682181994\n3982/10000. loss: 0.0012851835538943608\n3983/10000. loss: 0.0016044772540529568\n3984/10000. loss: 0.0012865327298641205\n3985/10000. loss: 0.0008563500984261433\n3986/10000. loss: 0.0010162409550199907\n3987/10000. loss: 0.001095312802741925\n3988/10000. loss: 0.0009990727218488853\n3989/10000. loss: 0.0008816181216388941\n3990/10000. loss: 0.0008498195093125105\n3991/10000. loss: 0.0006051899399608374\n3992/10000. loss: 0.0004142005927860737\n3993/10000. loss: 0.0008293624656895796\n3994/10000. loss: 0.0008178579155355692\n3995/10000. loss: 0.0007475367747247219\n3996/10000. loss: 0.0005733619521682461\n3997/10000. loss: 0.0007679646369069815\n3998/10000. loss: 0.0007833894342184067\n3999/10000. loss: 0.000571280368603766\n4000/10000. loss: 0.0006366142382224401\n4001/10000. loss: 0.0006279986506948868\n4002/10000. loss: 0.0005114733551939329\n4003/10000. loss: 0.00047780720827480155\n4004/10000. loss: 0.0003995477066685756\n4005/10000. loss: 0.0004079574719071388\n4006/10000. loss: 0.0005354102468118072\n4007/10000. loss: 0.0003521739272400737\n4008/10000. loss: 0.0004114379019786914\n4009/10000. loss: 0.00040391723935802776\n4010/10000. loss: 0.00034055672585964203\n4011/10000. loss: 0.0005005750572308898\n4012/10000. loss: 0.0002827735152095556\n4013/10000. loss: 0.0003636677283793688\n4014/10000. loss: 0.0003268887521699071\n4015/10000. loss: 0.000535513274371624\n4016/10000. loss: 0.0004935547088583311\n4017/10000. loss: 0.0005208572838455439\n4018/10000. loss: 0.0005075817074005803\n4019/10000. loss: 0.00045315657431880635\n4020/10000. loss: 0.0004812179443736871\n4021/10000. loss: 0.0004122363170608878\n4022/10000. loss: 0.00034469785168766975\n4023/10000. loss: 0.0005391143107165893\n4024/10000. loss: 0.00033982043775419396\n4025/10000. loss: 0.00029677472775802016\n4026/10000. loss: 0.000459324490899841\n4027/10000. loss: 0.0004957551524663965\n4028/10000. loss: 0.00038041314110159874\n4029/10000. loss: 0.0002947759348899126\n4030/10000. loss: 0.0002989789160589377\n4031/10000. loss: 0.0005637979678188761\n4032/10000. loss: 0.00037240853998810053\n4033/10000. loss: 0.00043407427923132974\n4034/10000. loss: 0.00042951332094768685\n4035/10000. loss: 0.00045410737705727416\n4036/10000. loss: 0.00036489909204343957\n4037/10000. loss: 0.00038952256242434186\n4038/10000. loss: 0.00045034960688402254\n4039/10000. loss: 0.0004630193579941988\n4040/10000. loss: 0.00030366422530884546\n4041/10000. loss: 0.00046063947957009077\n4042/10000. loss: 0.00029340857872739434\n4043/10000. loss: 0.00048277627987166244\n4044/10000. loss: 0.0004807313283284505\n4045/10000. loss: 0.00045974296517670155\n4046/10000. loss: 0.0002971159992739558\n4047/10000. loss: 0.0003913279700403412\n4048/10000. loss: 0.0004200648982077837\n4049/10000. loss: 0.00043357737983266514\n4050/10000. loss: 0.00035924076413114864\n4051/10000. loss: 0.0003748995950445533\n4052/10000. loss: 0.0004825121723115444\n4053/10000. loss: 0.00033001252450048923\n4054/10000. loss: 0.0003480231777454416\n4055/10000. loss: 0.0003389860891426603\n4056/10000. loss: 0.00026279938174411654\n4057/10000. loss: 0.0005161525526394447\n4058/10000. loss: 0.0003057217691093683\n4059/10000. loss: 0.00035234664877255756\n4060/10000. loss: 0.00035800303642948467\n4061/10000. loss: 0.000409606300915281\n4062/10000. loss: 0.0004298643519481023\n4063/10000. loss: 0.00026571940785894793\n4064/10000. loss: 0.00037808444661398727\n4065/10000. loss: 0.0002813499498491486\n4066/10000. loss: 0.0004785988324632247\n4067/10000. loss: 0.00028273680557807285\n4068/10000. loss: 0.0003776857629418373\n4069/10000. loss: 0.00044386919277409714\n4070/10000. loss: 0.000365740736015141\n4071/10000. loss: 0.0005151292231554786\n4072/10000. loss: 0.0003240725491195917\n4073/10000. loss: 0.0004495329534014066\n4074/10000. loss: 0.0002752905711531639\n4075/10000. loss: 0.00047057824364552897\n4076/10000. loss: 0.000490664504468441\n4077/10000. loss: 0.00040300912223756313\n4078/10000. loss: 0.00039025159397472936\n4079/10000. loss: 0.00045539656033118564\n4080/10000. loss: 0.00033220998011529446\n4081/10000. loss: 0.0005509655845041076\n4082/10000. loss: 0.00034322581874827546\n4083/10000. loss: 0.0004895709765454134\n4084/10000. loss: 0.0005152498294288913\n4085/10000. loss: 0.0006472116413836678\n4086/10000. loss: 0.0006107761679838101\n4087/10000. loss: 0.000706306037803491\n4088/10000. loss: 0.000658125306169192\n4089/10000. loss: 0.0007458574449022611\n4090/10000. loss: 0.0008306738454848528\n4091/10000. loss: 0.0009612347930669785\n4092/10000. loss: 0.0010214541107416153\n4093/10000. loss: 0.0008768129628151655\n4094/10000. loss: 0.001291401218622923\n4095/10000. loss: 0.0007584378278503815\n4096/10000. loss: 0.0018896072482069333\n4097/10000. loss: 0.001102898425112168\n4098/10000. loss: 0.0012766245442132156\n4099/10000. loss: 0.0021728910505771637\n4100/10000. loss: 0.003291879780590534\n4101/10000. loss: 0.0009018414032955965\n4102/10000. loss: 0.0013375038591523964\n4103/10000. loss: 0.0012144528639813263\n4104/10000. loss: 0.0008290585440893968\n4105/10000. loss: 0.0006815027445554733\n4106/10000. loss: 0.0005543624671796957\n4107/10000. loss: 0.0010395366698503494\n4108/10000. loss: 0.0007048491388559341\n4109/10000. loss: 0.0007207375019788742\n4110/10000. loss: 0.000556171095619599\n4111/10000. loss: 0.0008285647879044215\n4112/10000. loss: 0.0006619219978650411\n4113/10000. loss: 0.0006490443677951893\n4114/10000. loss: 0.0004810433990011613\n4115/10000. loss: 0.0004595603483418624\n4116/10000. loss: 0.0005713808350265026\n4117/10000. loss: 0.000579703482799232\n4118/10000. loss: 0.0005316774671276411\n4119/10000. loss: 0.0005797749618068337\n4120/10000. loss: 0.0005097031050051252\n4121/10000. loss: 0.000565798138268292\n4122/10000. loss: 0.00047886363851527375\n4123/10000. loss: 0.0005072461208328605\n4124/10000. loss: 0.00044141259665290516\n4125/10000. loss: 0.0004028615852197011\n4126/10000. loss: 0.0004313486473013957\n4127/10000. loss: 0.0003230537986382842\n4128/10000. loss: 0.00041716021951287985\n4129/10000. loss: 0.00040620320942252874\n4130/10000. loss: 0.0005439004938428601\n4131/10000. loss: 0.0004564432504897316\n4132/10000. loss: 0.0005365286876137058\n4133/10000. loss: 0.00042762502562254667\n4134/10000. loss: 0.000385417602956295\n4135/10000. loss: 0.0005118335441996654\n4136/10000. loss: 0.000603529391810298\n4137/10000. loss: 0.00039220772062738735\n4138/10000. loss: 0.00045170898859699565\n4139/10000. loss: 0.00038634395847717923\n4140/10000. loss: 0.00038320183133085567\n4141/10000. loss: 0.0004067909127722184\n4142/10000. loss: 0.0003012413702284296\n4143/10000. loss: 0.00028166105039417744\n4144/10000. loss: 0.0002676779016231497\n4145/10000. loss: 0.0002704792811224858\n4146/10000. loss: 0.0005033844305823246\n4147/10000. loss: 0.0004483700031414628\n4148/10000. loss: 0.0004491851820300023\n4149/10000. loss: 0.000429219954336683\n4150/10000. loss: 0.00028568665341784555\n4151/10000. loss: 0.0005119996300588051\n4152/10000. loss: 0.000340308955249687\n4153/10000. loss: 0.00025971113548924524\n4154/10000. loss: 0.0005042017437517643\n4155/10000. loss: 0.00033380265813320875\n4156/10000. loss: 0.00041953427717089653\n4157/10000. loss: 0.0003148573062693079\n4158/10000. loss: 0.00033631637537231046\n4159/10000. loss: 0.0004744878193984429\n4160/10000. loss: 0.0002604302523347239\n4161/10000. loss: 0.0002202376490458846\n4162/10000. loss: 0.0003799501961718003\n4163/10000. loss: 0.00024704926181584597\n4164/10000. loss: 0.0003099648553567628\n4165/10000. loss: 0.00032440510888894397\n4166/10000. loss: 0.00025252351770177484\n4167/10000. loss: 0.0003525728825479746\n4168/10000. loss: 0.00032500375527888536\n4169/10000. loss: 0.0002746672835201025\n4170/10000. loss: 0.0003878128870079915\n4171/10000. loss: 0.00044197123497724533\n4172/10000. loss: 0.000403617974370718\n4173/10000. loss: 0.0004250347459067901\n4174/10000. loss: 0.0003373953513801098\n4175/10000. loss: 0.0005118925279627243\n4176/10000. loss: 0.0003641864750534296\n4177/10000. loss: 0.00032458788094421226\n4178/10000. loss: 0.00030338003610571224\n4179/10000. loss: 0.0003203360247425735\n4180/10000. loss: 0.0005771104091157516\n4181/10000. loss: 0.0002863421104848385\n4182/10000. loss: 0.0004570059633503358\n4183/10000. loss: 0.0005862294929102063\n4184/10000. loss: 0.0004955777355159322\n4185/10000. loss: 0.0007764040492475033\n4186/10000. loss: 0.0010146024481703837\n4187/10000. loss: 0.0008212129275004069\n4188/10000. loss: 0.0006560709637900194\n4189/10000. loss: 0.001111609861254692\n4190/10000. loss: 0.0010408434706429641\n4191/10000. loss: 0.001354852846513192\n4192/10000. loss: 0.0016114201086262863\n4193/10000. loss: 0.0017089969478547573\n4194/10000. loss: 0.001108319265767932\n4195/10000. loss: 0.0013706531996528308\n4196/10000. loss: 0.001270999200642109\n4197/10000. loss: 0.001102705408508579\n4198/10000. loss: 0.0007450821188588937\n4199/10000. loss: 0.0010623175961275895\n4200/10000. loss: 0.0011328435502946377\n4201/10000. loss: 0.0014596986584365368\n4202/10000. loss: 0.00048017269000411034\n4203/10000. loss: 0.0006350486073642969\n4204/10000. loss: 0.0005488336707154909\n4205/10000. loss: 0.0008324162724117438\n4206/10000. loss: 0.0006074000072355071\n4207/10000. loss: 0.00041162633957962197\n4208/10000. loss: 0.0004023573516557614\n4209/10000. loss: 0.0004979485335449377\n4210/10000. loss: 0.0005775958610077699\n4211/10000. loss: 0.0009178648858020703\n4212/10000. loss: 0.00040606708110620576\n4213/10000. loss: 0.0005046415608376265\n4214/10000. loss: 0.0005915688040355841\n4215/10000. loss: 0.0005738061154261231\n4216/10000. loss: 0.0003925286776696642\n4217/10000. loss: 0.0004916319157928228\n4218/10000. loss: 0.0004258553187052409\n4219/10000. loss: 0.0004242016390586893\n4220/10000. loss: 0.00029663640695313614\n4221/10000. loss: 0.0004473707328240077\n4222/10000. loss: 0.0003805192730699976\n4223/10000. loss: 0.00030427412517989677\n4224/10000. loss: 0.0005246287134165565\n4225/10000. loss: 0.00038961767374227446\n4226/10000. loss: 0.0007129260338842869\n4227/10000. loss: 0.00047437241300940514\n4228/10000. loss: 0.0005232268789162239\n4229/10000. loss: 0.0003558796209593614\n4230/10000. loss: 0.00043968499327699345\n4231/10000. loss: 0.0004718715014557044\n4232/10000. loss: 0.0004969004075974226\n4233/10000. loss: 0.0002902340687190493\n4234/10000. loss: 0.00026300254588325817\n4235/10000. loss: 0.00036379299126565456\n4236/10000. loss: 0.00028105433254192275\n4237/10000. loss: 0.0005005747855951389\n4238/10000. loss: 0.0004695858806371689\n4239/10000. loss: 0.0003292728215456009\n4240/10000. loss: 0.00034742076726009447\n4241/10000. loss: 0.0005311554608245691\n4242/10000. loss: 0.00023764530972888073\n4243/10000. loss: 0.0004204104964931806\n4244/10000. loss: 0.000338367884978652\n4245/10000. loss: 0.0003226038340168695\n4246/10000. loss: 0.0004013835374886791\n4247/10000. loss: 0.00029317660179610056\n4248/10000. loss: 0.00034400859537223977\n4249/10000. loss: 0.0003797749135022362\n4250/10000. loss: 0.00031923184481759864\n4251/10000. loss: 0.0004901681871463855\n4252/10000. loss: 0.00034774358694752056\n4253/10000. loss: 0.0005197484667102495\n4254/10000. loss: 0.00033117306884378195\n4255/10000. loss: 0.00035803578794002533\n4256/10000. loss: 0.0003256401202330987\n4257/10000. loss: 0.0004570404998958111\n4258/10000. loss: 0.0004907050946106514\n4259/10000. loss: 0.00028835262249534327\n4260/10000. loss: 0.00028111952512214583\n4261/10000. loss: 0.00026639719726517797\n4262/10000. loss: 0.00040192909849186737\n4263/10000. loss: 0.0002830104009869198\n4264/10000. loss: 0.00035725142030666274\n4265/10000. loss: 0.00030588139391814667\n4266/10000. loss: 0.0003005813729638855\n4267/10000. loss: 0.0004843637580052018\n4268/10000. loss: 0.00026187108596786857\n4269/10000. loss: 0.00034260125054667395\n4270/10000. loss: 0.00026418416139980155\n4271/10000. loss: 0.00042477925308048725\n4272/10000. loss: 0.00027452494638661545\n4273/10000. loss: 0.0003940376142660777\n4274/10000. loss: 0.0003976391162723303\n4275/10000. loss: 0.0004938977460066477\n4276/10000. loss: 0.0005500577390193939\n4277/10000. loss: 0.0005259315560882291\n4278/10000. loss: 0.0005695946359386047\n4279/10000. loss: 0.0007915738193939129\n4280/10000. loss: 0.0004703007483234008\n4281/10000. loss: 0.0008352008492996296\n4282/10000. loss: 0.0020863182532290616\n4283/10000. loss: 0.0016130444904168446\n4284/10000. loss: 0.0014010622786978881\n4285/10000. loss: 0.0011906976190706093\n4286/10000. loss: 0.002427582008143266\n4287/10000. loss: 0.002435084277143081\n4288/10000. loss: 0.0017676472974320252\n4289/10000. loss: 0.0017295473565657933\n4290/10000. loss: 0.0025475891306996346\n4291/10000. loss: 0.002148375070343415\n4292/10000. loss: 0.0018685953691601753\n4293/10000. loss: 0.0012880428694188595\n4294/10000. loss: 0.0011776751683404048\n4295/10000. loss: 0.0017948004727562268\n4296/10000. loss: 0.0012293930631130934\n4297/10000. loss: 0.0009176225091020266\n4298/10000. loss: 0.000807661097496748\n4299/10000. loss: 0.0007544211888064941\n4300/10000. loss: 0.0003861059279491504\n4301/10000. loss: 0.0008337704154352347\n4302/10000. loss: 0.0006106945996483167\n4303/10000. loss: 0.0006065765240540107\n4304/10000. loss: 0.0005899882332111398\n4305/10000. loss: 0.0006584740864733855\n4306/10000. loss: 0.0005038660019636154\n4307/10000. loss: 0.0005109080423911413\n4308/10000. loss: 0.00037764959658185643\n4309/10000. loss: 0.00041913101449608803\n4310/10000. loss: 0.00045788664525995654\n4311/10000. loss: 0.0003820544807240367\n4312/10000. loss: 0.00043681333772838116\n4313/10000. loss: 0.00043518437693516415\n4314/10000. loss: 0.0005564372598504027\n4315/10000. loss: 0.00043966295197606087\n4316/10000. loss: 0.0004303195746615529\n4317/10000. loss: 0.0004501756823932131\n4318/10000. loss: 0.0005201965104788542\n4319/10000. loss: 0.0004709452235450347\n4320/10000. loss: 0.000548328428218762\n4321/10000. loss: 0.00034848309587687254\n4322/10000. loss: 0.0003672998088101546\n4323/10000. loss: 0.0003536465422560771\n4324/10000. loss: 0.0004561230695496003\n4325/10000. loss: 0.0005767281788090864\n4326/10000. loss: 0.0003719209150100748\n4327/10000. loss: 0.0004014494673659404\n4328/10000. loss: 0.00039246841333806515\n4329/10000. loss: 0.0004577438812702894\n4330/10000. loss: 0.0003584959389020999\n4331/10000. loss: 0.0003136306380232175\n4332/10000. loss: 0.00045307865366339684\n4333/10000. loss: 0.000327436796699961\n4334/10000. loss: 0.00032607380611201126\n4335/10000. loss: 0.0004286711337044835\n4336/10000. loss: 0.0003394947076837222\n4337/10000. loss: 0.0002852109222051998\n4338/10000. loss: 0.0002510913764126599\n4339/10000. loss: 0.0004124909173697233\n4340/10000. loss: 0.0004959778937821587\n4341/10000. loss: 0.0004017944059645136\n4342/10000. loss: 0.0004680210258811712\n4343/10000. loss: 0.00032565432290236157\n4344/10000. loss: 0.00032878465329607326\n4345/10000. loss: 0.0004010536940768361\n4346/10000. loss: 0.00045927229803055525\n4347/10000. loss: 0.00040736918648084003\n4348/10000. loss: 0.0002707579600003858\n4349/10000. loss: 0.0004048994742333889\n4350/10000. loss: 0.00041973233843843144\n4351/10000. loss: 0.00025195562435934943\n4352/10000. loss: 0.00030875904485583305\n4353/10000. loss: 0.0004080943763256073\n4354/10000. loss: 0.00046491432779779035\n4355/10000. loss: 0.00028720366147657234\n4356/10000. loss: 0.0004124159070973595\n4357/10000. loss: 0.00037078986254831153\n4358/10000. loss: 0.00039476214442402124\n4359/10000. loss: 0.00038574961945414543\n4360/10000. loss: 0.0005339424436291059\n4361/10000. loss: 0.00043890920157233876\n4362/10000. loss: 0.00033156387507915497\n4363/10000. loss: 0.0002593829412944615\n4364/10000. loss: 0.0003248957800678909\n4365/10000. loss: 0.0002866243206275006\n4366/10000. loss: 0.00039253934907416504\n4367/10000. loss: 0.0003502418597539266\n4368/10000. loss: 0.00029081238123277825\n4369/10000. loss: 0.00038874149322509766\n4370/10000. loss: 0.0002939357267071803\n4371/10000. loss: 0.0003142297306718926\n4372/10000. loss: 0.00032121455296874046\n4373/10000. loss: 0.0003385917516425252\n4374/10000. loss: 0.0002744309992219011\n4375/10000. loss: 0.0002530723965416352\n4376/10000. loss: 0.0004708346289892991\n4377/10000. loss: 0.00027851849639167386\n4378/10000. loss: 0.00044618026974300545\n4379/10000. loss: 0.0004322342186545332\n4380/10000. loss: 0.0004366671588892738\n4381/10000. loss: 0.0002756453662489851\n4382/10000. loss: 0.0003975794728224476\n4383/10000. loss: 0.0004041568997005622\n4384/10000. loss: 0.00033216058121373254\n4385/10000. loss: 0.00025978940539062023\n4386/10000. loss: 0.0003825386520475149\n4387/10000. loss: 0.00019188943163802227\n4388/10000. loss: 0.00031324118996659916\n4389/10000. loss: 0.0003047465191533168\n4390/10000. loss: 0.0004455241917942961\n4391/10000. loss: 0.00029582066539054114\n4392/10000. loss: 0.00033686434229214984\n4393/10000. loss: 0.0003700058829660217\n4394/10000. loss: 0.000507585626716415\n4395/10000. loss: 0.0006111681771775087\n4396/10000. loss: 0.00030680142420654494\n4397/10000. loss: 0.0006763979326933622\n4398/10000. loss: 0.0005736548531179627\n4399/10000. loss: 0.0009638645375768343\n4400/10000. loss: 0.0013975131635864575\n4401/10000. loss: 0.0020590401254594326\n4402/10000. loss: 0.0012857597321271896\n4403/10000. loss: 0.002940350522597631\n4404/10000. loss: 0.0026793675497174263\n4405/10000. loss: 0.0024720363629360995\n4406/10000. loss: 0.004278292879462242\n4407/10000. loss: 0.002897240842382113\n4408/10000. loss: 0.00561505804459254\n4409/10000. loss: 0.00285097553084294\n4410/10000. loss: 0.002695972720781962\n4411/10000. loss: 0.002391902885089318\n4412/10000. loss: 0.001997309271246195\n4413/10000. loss: 0.0021466556936502457\n4414/10000. loss: 0.0015573686299224694\n4415/10000. loss: 0.001888105645775795\n4416/10000. loss: 0.00140823470428586\n4417/10000. loss: 0.0015516048297286034\n4418/10000. loss: 0.001207785913720727\n4419/10000. loss: 0.001214179831246535\n4420/10000. loss: 0.0009843500641485055\n4421/10000. loss: 0.0005971038481220603\n4422/10000. loss: 0.0007171325851231813\n4423/10000. loss: 0.0005428136792033911\n4424/10000. loss: 0.0006522089242935181\n4425/10000. loss: 0.0007353364489972591\n4426/10000. loss: 0.0009954643125335376\n4427/10000. loss: 0.0007003193410734335\n4428/10000. loss: 0.0008283613715320826\n4429/10000. loss: 0.0007540440807739893\n4430/10000. loss: 0.0006321780383586884\n4431/10000. loss: 0.0006370228560020527\n4432/10000. loss: 0.000488962202022473\n4433/10000. loss: 0.0006042375850180784\n4434/10000. loss: 0.00045838626101613045\n4435/10000. loss: 0.000456932505282263\n4436/10000. loss: 0.0004885493932912747\n4437/10000. loss: 0.00041189002028356\n4438/10000. loss: 0.0004936013525972763\n4439/10000. loss: 0.0007413740580280622\n4440/10000. loss: 0.00033458974212408066\n4441/10000. loss: 0.0005217901586244503\n4442/10000. loss: 0.0003820353886112571\n4443/10000. loss: 0.000265319113774846\n4444/10000. loss: 0.00029743420115361613\n4445/10000. loss: 0.0005743417423218489\n4446/10000. loss: 0.000516061515857776\n4447/10000. loss: 0.0003252161550335586\n4448/10000. loss: 0.00047927928001930314\n4449/10000. loss: 0.00033099751453846693\n4450/10000. loss: 0.0005269310204312205\n4451/10000. loss: 0.00035777556089063484\n4452/10000. loss: 0.0004624206339940429\n4453/10000. loss: 0.00033707839126388234\n4454/10000. loss: 0.00034425178697953623\n4455/10000. loss: 0.00033193132063994807\n4456/10000. loss: 0.0002895625851427515\n4457/10000. loss: 0.00035770652660479146\n4458/10000. loss: 0.00046432654683788616\n4459/10000. loss: 0.00037068889165918034\n4460/10000. loss: 0.00037251490478714305\n4461/10000. loss: 0.0003469182411208749\n4462/10000. loss: 0.000568962306715548\n4463/10000. loss: 0.0004162186135848363\n4464/10000. loss: 0.00030208483804017305\n4465/10000. loss: 0.0004426312322417895\n4466/10000. loss: 0.00042360058675209683\n4467/10000. loss: 0.0002890940910826127\n4468/10000. loss: 0.0003441295896967252\n4469/10000. loss: 0.00035445337804655236\n4470/10000. loss: 0.00033310509752482176\n4471/10000. loss: 0.0002529071255897482\n4472/10000. loss: 0.0004155741383632024\n4473/10000. loss: 0.000344921446715792\n4474/10000. loss: 0.00040771331017216045\n4475/10000. loss: 0.0005384492687880993\n4476/10000. loss: 0.0004099914027998845\n4477/10000. loss: 0.0003400923063357671\n4478/10000. loss: 0.0004645644997557004\n4479/10000. loss: 0.0003159827707956235\n4480/10000. loss: 0.0004604979573438565\n4481/10000. loss: 0.00048133102245628834\n4482/10000. loss: 0.0002614982076920569\n4483/10000. loss: 0.0005161444035669168\n4484/10000. loss: 0.00025015204058339197\n4485/10000. loss: 0.0005012591524670521\n4486/10000. loss: 0.00038724986370652914\n4487/10000. loss: 0.0003164667868986726\n4488/10000. loss: 0.0006157769821584225\n4489/10000. loss: 0.00034138928943624097\n4490/10000. loss: 0.0005166528280824423\n4491/10000. loss: 0.0003608835395425558\n4492/10000. loss: 0.00027631196038176614\n4493/10000. loss: 0.0003843854259078701\n4494/10000. loss: 0.00039573883016904193\n4495/10000. loss: 0.00047743467924495536\n4496/10000. loss: 0.00027410998397196334\n4497/10000. loss: 0.0003412287915125489\n4498/10000. loss: 0.00037039835782100755\n4499/10000. loss: 0.0002719239564612508\n4500/10000. loss: 0.00025172570409874123\n4501/10000. loss: 0.00023381390686457357\n4502/10000. loss: 0.00025505851954221725\n4503/10000. loss: 0.00048365688417106867\n4504/10000. loss: 0.00028816833704089123\n4505/10000. loss: 0.00039445531244079274\n4506/10000. loss: 0.00027027446776628494\n4507/10000. loss: 0.00032324332278221846\n4508/10000. loss: 0.0002598344775227209\n4509/10000. loss: 0.00027691064557681483\n4510/10000. loss: 0.0002963928272947669\n4511/10000. loss: 0.00032221642322838306\n4512/10000. loss: 0.0002552705506483714\n4513/10000. loss: 0.00029565409446756047\n4514/10000. loss: 0.00025551482879867155\n4515/10000. loss: 0.0004143485178550084\n4516/10000. loss: 0.0004540301936989029\n4517/10000. loss: 0.0003065952332690358\n4518/10000. loss: 0.0002662703045643866\n4519/10000. loss: 0.00024090800434350967\n4520/10000. loss: 0.0003454852073142926\n4521/10000. loss: 0.00037161501434942085\n4522/10000. loss: 0.0004120029043406248\n4523/10000. loss: 0.0003938676090911031\n4524/10000. loss: 0.0003890144483496745\n4525/10000. loss: 0.0003591240771735708\n4526/10000. loss: 0.00032680054816106957\n4527/10000. loss: 0.0002500881867793699\n4528/10000. loss: 0.00027421757113188505\n4529/10000. loss: 0.0005031062755733728\n4530/10000. loss: 0.00040239332399020594\n4531/10000. loss: 0.0003013356084314485\n4532/10000. loss: 0.00024175748694688082\n4533/10000. loss: 0.00030411997189124423\n4534/10000. loss: 0.00042273441795259714\n4535/10000. loss: 0.00046153638201455277\n4536/10000. loss: 0.00041321106255054474\n4537/10000. loss: 0.0002575322675208251\n4538/10000. loss: 0.0002936361512790124\n4539/10000. loss: 0.0003129120450466871\n4540/10000. loss: 0.0002271527579675118\n4541/10000. loss: 0.0002794372267089784\n4542/10000. loss: 0.0003523706303288539\n4543/10000. loss: 0.000525445289288958\n4544/10000. loss: 0.00043060847868522006\n4545/10000. loss: 0.0003271264722570777\n4546/10000. loss: 0.0003978208793948094\n4547/10000. loss: 0.00043088783665249747\n4548/10000. loss: 0.000608458649367094\n4549/10000. loss: 0.00045877912392218906\n4550/10000. loss: 0.00030784370998541516\n4551/10000. loss: 0.0005163540287564198\n4552/10000. loss: 0.0006672504047552744\n4553/10000. loss: 0.0005707543653746446\n4554/10000. loss: 0.0004658023438726862\n4555/10000. loss: 0.0008960272340724865\n4556/10000. loss: 0.0010072668083012104\n4557/10000. loss: 0.001092735289906462\n4558/10000. loss: 0.0011004675179719925\n4559/10000. loss: 0.0009334557689726353\n4560/10000. loss: 0.0012942079920321703\n4561/10000. loss: 0.0011059828102588654\n4562/10000. loss: 0.0016497462056577206\n4563/10000. loss: 0.0012171033304184675\n4564/10000. loss: 0.0008845408447086811\n4565/10000. loss: 0.0011995097156614065\n4566/10000. loss: 0.0011617598744730155\n4567/10000. loss: 0.0007802981417626143\n4568/10000. loss: 0.0009532716746131579\n4569/10000. loss: 0.0008977100563546022\n4570/10000. loss: 0.0009318330946067969\n4571/10000. loss: 0.0009001699897150198\n4572/10000. loss: 0.0005520180954287449\n4573/10000. loss: 0.0005294910321633021\n4574/10000. loss: 0.0004048748717953761\n4575/10000. loss: 0.0006038220599293709\n4576/10000. loss: 0.0006546006382753452\n4577/10000. loss: 0.0003421635289366047\n4578/10000. loss: 0.000525639004384478\n4579/10000. loss: 0.00033948795559505623\n4580/10000. loss: 0.00035460673583050567\n4581/10000. loss: 0.00036931049544364214\n4582/10000. loss: 0.0004345151052499811\n4583/10000. loss: 0.000370414888796707\n4584/10000. loss: 0.00039450637996196747\n4585/10000. loss: 0.0003243470564484596\n4586/10000. loss: 0.0003602541983127594\n4587/10000. loss: 0.0004546344668293993\n4588/10000. loss: 0.000529094870823125\n4589/10000. loss: 0.0005485585425049067\n4590/10000. loss: 0.00043105279716352624\n4591/10000. loss: 0.0004917818199222287\n4592/10000. loss: 0.0005185238551348448\n4593/10000. loss: 0.00044275967714687187\n4594/10000. loss: 0.0005375004839152098\n4595/10000. loss: 0.0004259547373900811\n4596/10000. loss: 0.00044929337066908676\n4597/10000. loss: 0.0004271285142749548\n4598/10000. loss: 0.0003693287338440617\n4599/10000. loss: 0.00041035248432308435\n4600/10000. loss: 0.00039337521108488244\n4601/10000. loss: 0.00035503716208040714\n4602/10000. loss: 0.00038857727001110714\n4603/10000. loss: 0.0003347208645815651\n4604/10000. loss: 0.00041113949070374173\n4605/10000. loss: 0.00033982024372865755\n4606/10000. loss: 0.0004422339067483942\n4607/10000. loss: 0.0002769787291375299\n4608/10000. loss: 0.0005918468038241068\n4609/10000. loss: 0.0002526244691883524\n4610/10000. loss: 0.00026469764998182654\n4611/10000. loss: 0.00027292576851323247\n4612/10000. loss: 0.0003289705685650309\n4613/10000. loss: 0.000384889116200308\n4614/10000. loss: 0.00039631372783333063\n4615/10000. loss: 0.00033737307724853355\n4616/10000. loss: 0.0003633858480801185\n4617/10000. loss: 0.00026407015199462575\n4618/10000. loss: 0.00026343815261498094\n4619/10000. loss: 0.0002598752810930212\n4620/10000. loss: 0.0003332240351786216\n4621/10000. loss: 0.00042332347948104143\n4622/10000. loss: 0.00029335326204697293\n4623/10000. loss: 0.00039611927544077236\n4624/10000. loss: 0.000482499444236358\n4625/10000. loss: 0.0003604512894526124\n4626/10000. loss: 0.00044913093249003094\n4627/10000. loss: 0.0002557527623139322\n4628/10000. loss: 0.0004634583213677009\n4629/10000. loss: 0.0006023632983366648\n4630/10000. loss: 0.0004052928028007348\n4631/10000. loss: 0.0002877262692588071\n4632/10000. loss: 0.0002735953506392737\n4633/10000. loss: 0.00023522746050730348\n4634/10000. loss: 0.0003731773079683383\n4635/10000. loss: 0.0003766341833397746\n4636/10000. loss: 0.00037879877102871734\n4637/10000. loss: 0.0002884683587277929\n4638/10000. loss: 0.0003051986374581854\n4639/10000. loss: 0.00041986846675475437\n4640/10000. loss: 0.0003660661168396473\n4641/10000. loss: 0.0002685252499456207\n4642/10000. loss: 0.00032111672529329854\n4643/10000. loss: 0.0003929345402866602\n4644/10000. loss: 0.0004141267854720354\n4645/10000. loss: 0.0004394888722648223\n4646/10000. loss: 0.0002919487693967919\n4647/10000. loss: 0.0003312228946015239\n4648/10000. loss: 0.000238895610285302\n4649/10000. loss: 0.0004322874204566081\n4650/10000. loss: 0.0004071272366369764\n4651/10000. loss: 0.00034098509543885786\n4652/10000. loss: 0.0003542771252493064\n4653/10000. loss: 0.00030356701851511997\n4654/10000. loss: 0.00030185918634136516\n4655/10000. loss: 0.00029521359829232097\n4656/10000. loss: 0.0004344662108148138\n4657/10000. loss: 0.0005216974144180616\n4658/10000. loss: 0.00039020021601269644\n4659/10000. loss: 0.0005685638170689344\n4660/10000. loss: 0.00038552094095697004\n4661/10000. loss: 0.0006831448214749495\n4662/10000. loss: 0.0012994000377754371\n4663/10000. loss: 0.0006296364590525627\n4664/10000. loss: 0.0027228382726510367\n4665/10000. loss: 0.002126647780338923\n4666/10000. loss: 0.0032209555308024087\n4667/10000. loss: 0.002727604160706202\n4668/10000. loss: 0.0016836868599057198\n4669/10000. loss: 0.002969016321003437\n4670/10000. loss: 0.0011413992227365573\n4671/10000. loss: 0.0018680981981257598\n4672/10000. loss: 0.002146637998521328\n4673/10000. loss: 0.0017856502284606297\n4674/10000. loss: 0.0019893180578947067\n4675/10000. loss: 0.0013593841964999835\n4676/10000. loss: 0.0011777248388777177\n4677/10000. loss: 0.0013537810494502385\n4678/10000. loss: 0.0016483149180809658\n4679/10000. loss: 0.001049073413014412\n4680/10000. loss: 0.00148064736276865\n4681/10000. loss: 0.0012520930419365566\n4682/10000. loss: 0.000914829084649682\n4683/10000. loss: 0.0007795124935607115\n4684/10000. loss: 0.0007195073024680217\n4685/10000. loss: 0.000791782590871056\n4686/10000. loss: 0.0005287180344263712\n4687/10000. loss: 0.000607451229977111\n4688/10000. loss: 0.0005501771423344811\n4689/10000. loss: 0.0005978616730620464\n4690/10000. loss: 0.0005789901285121838\n4691/10000. loss: 0.0004592235976209243\n4692/10000. loss: 0.0006637523571650187\n4693/10000. loss: 0.0006306817134221395\n4694/10000. loss: 0.00046258656463275355\n4695/10000. loss: 0.00045814551413059235\n4696/10000. loss: 0.0004506956708307068\n4697/10000. loss: 0.00035683248036851484\n4698/10000. loss: 0.00032118931024645764\n4699/10000. loss: 0.000332582703170677\n4700/10000. loss: 0.0004913355223834515\n4701/10000. loss: 0.000325273082125932\n4702/10000. loss: 0.000382625808318456\n4703/10000. loss: 0.0004302241140976548\n4704/10000. loss: 0.00028464900484929484\n4705/10000. loss: 0.0003030887455679476\n4706/10000. loss: 0.0003298299852758646\n4707/10000. loss: 0.0006276893739899\n4708/10000. loss: 0.0003119351846786837\n4709/10000. loss: 0.00047259808828433353\n4710/10000. loss: 0.0003962862150122722\n4711/10000. loss: 0.0002852230876063307\n4712/10000. loss: 0.0004514534957706928\n4713/10000. loss: 0.0003248447513518234\n4714/10000. loss: 0.0003797166670362155\n4715/10000. loss: 0.00041661737486720085\n4716/10000. loss: 0.0003146092058159411\n4717/10000. loss: 0.00042237372448047\n4718/10000. loss: 0.00042209789777795476\n4719/10000. loss: 0.00026960587517047924\n4720/10000. loss: 0.00041283678729087114\n4721/10000. loss: 0.0004123247927054763\n4722/10000. loss: 0.0005347709326694409\n4723/10000. loss: 0.0004777483797321717\n4724/10000. loss: 0.000288551712098221\n4725/10000. loss: 0.00026324589271098375\n4726/10000. loss: 0.0002646217471919954\n4727/10000. loss: 0.00032610058163603145\n4728/10000. loss: 0.00031768082408234477\n4729/10000. loss: 0.00048476011337091524\n4730/10000. loss: 0.0004325847451885541\n4731/10000. loss: 0.0004007273431246479\n4732/10000. loss: 0.00029247205626840395\n4733/10000. loss: 0.0002644471436118086\n4734/10000. loss: 0.00024176656734198332\n4735/10000. loss: 0.0003314835097019871\n4736/10000. loss: 0.0002506104065105319\n4737/10000. loss: 0.0002769312316862245\n4738/10000. loss: 0.00025510735576972365\n4739/10000. loss: 0.000294775003567338\n4740/10000. loss: 0.00029716974434753257\n4741/10000. loss: 0.0003167116083204746\n4742/10000. loss: 0.0003379432794948419\n4743/10000. loss: 0.0003828657791018486\n4744/10000. loss: 0.00028920026185611886\n4745/10000. loss: 0.0004322691820561886\n4746/10000. loss: 0.0003979602673401435\n4747/10000. loss: 0.0002432413942491015\n4748/10000. loss: 0.0002551879539775352\n4749/10000. loss: 0.0002516797200466196\n4750/10000. loss: 0.00030922592850402\n4751/10000. loss: 0.0002892017364501953\n4752/10000. loss: 0.0003530017177884777\n4753/10000. loss: 0.000282040040474385\n4754/10000. loss: 0.00021146676347901425\n4755/10000. loss: 0.00028148233347261947\n4756/10000. loss: 0.00022954212424034873\n4757/10000. loss: 0.00041634938679635525\n4758/10000. loss: 0.00037815053171167773\n4759/10000. loss: 0.00044716956714789074\n4760/10000. loss: 0.0002650613508497675\n4761/10000. loss: 0.00030822209858645994\n4762/10000. loss: 0.00029373927585159737\n4763/10000. loss: 0.00044966857725133497\n4764/10000. loss: 0.0004677871086945136\n4765/10000. loss: 0.0002634033638363083\n4766/10000. loss: 0.0003033074705551068\n4767/10000. loss: 0.00036127733377118904\n4768/10000. loss: 0.00045599051130314666\n4769/10000. loss: 0.0003610887409498294\n4770/10000. loss: 0.000347873584056894\n4771/10000. loss: 0.0002738079832245906\n4772/10000. loss: 0.00027820873462284607\n4773/10000. loss: 0.00025140505749732256\n4774/10000. loss: 0.0003368222775558631\n4775/10000. loss: 0.00027197308372706175\n4776/10000. loss: 0.00035452454661329585\n4777/10000. loss: 0.00025224809845288593\n4778/10000. loss: 0.0003854154298702876\n4779/10000. loss: 0.0002738469047471881\n4780/10000. loss: 0.0002549190927917759\n4781/10000. loss: 0.0002259208122268319\n4782/10000. loss: 0.00024343324669947228\n4783/10000. loss: 0.0002519437499965231\n4784/10000. loss: 0.0004419178779547413\n4785/10000. loss: 0.0002765599638223648\n4786/10000. loss: 0.00027481633393714827\n4787/10000. loss: 0.00036325732556482154\n4788/10000. loss: 0.00022688705939799547\n4789/10000. loss: 0.00022321844395870963\n4790/10000. loss: 0.00029593131815393764\n4791/10000. loss: 0.00028179438474277657\n4792/10000. loss: 0.00024248958410074314\n4793/10000. loss: 0.00034060659042249125\n4794/10000. loss: 0.0006201779857898752\n4795/10000. loss: 0.00034613492122540873\n4796/10000. loss: 0.0003355403120319049\n4797/10000. loss: 0.0004366911016404629\n4798/10000. loss: 0.00029799351856733364\n4799/10000. loss: 0.0002558465348556638\n4800/10000. loss: 0.0002379187111121913\n4801/10000. loss: 0.0003014332226788004\n4802/10000. loss: 0.00048297686347117025\n4803/10000. loss: 0.00036846078000962734\n4804/10000. loss: 0.0003481458794946472\n4805/10000. loss: 0.00045084689433375996\n4806/10000. loss: 0.0003283795279761155\n4807/10000. loss: 0.00030387806085248786\n4808/10000. loss: 0.00027258698052416247\n4809/10000. loss: 0.0003788030395905177\n4810/10000. loss: 0.00029218371491879225\n4811/10000. loss: 0.0003645824423680703\n4812/10000. loss: 0.00040207124159981805\n4813/10000. loss: 0.0004956741274024049\n4814/10000. loss: 0.00034840327377120656\n4815/10000. loss: 0.00048490543849766254\n4816/10000. loss: 0.0008739407639950514\n4817/10000. loss: 0.0010679892729967833\n4818/10000. loss: 0.0008913450874388218\n4819/10000. loss: 0.0014498654442528884\n4820/10000. loss: 0.0008735436325271925\n4821/10000. loss: 0.0020054147268335023\n4822/10000. loss: 0.0013786077809830506\n4823/10000. loss: 0.0018522217869758606\n4824/10000. loss: 0.0013285245125492413\n4825/10000. loss: 0.001588505848000447\n4826/10000. loss: 0.002178476812938849\n4827/10000. loss: 0.00188984132061402\n4828/10000. loss: 0.0019562182327111564\n4829/10000. loss: 0.001767276165386041\n4830/10000. loss: 0.0017430405132472515\n4831/10000. loss: 0.0013822875916957855\n4832/10000. loss: 0.0007267002171526352\n4833/10000. loss: 0.0011508410486082237\n4834/10000. loss: 0.001244679403801759\n4835/10000. loss: 0.00044245234069724876\n4836/10000. loss: 0.0006117433464775482\n4837/10000. loss: 0.0008285025445123514\n4838/10000. loss: 0.0007501294215520223\n4839/10000. loss: 0.0005474241139988104\n4840/10000. loss: 0.0005840785646190246\n4841/10000. loss: 0.0006067246043433746\n4842/10000. loss: 0.0006373721019675335\n4843/10000. loss: 0.0005107819257924954\n4844/10000. loss: 0.00047794686785588664\n4845/10000. loss: 0.0005390862158189217\n4846/10000. loss: 0.0004618677000204722\n4847/10000. loss: 0.0004912522078181306\n4848/10000. loss: 0.0004145080844561259\n4849/10000. loss: 0.000392434885725379\n4850/10000. loss: 0.00035868026316165924\n4851/10000. loss: 0.00029766889444241923\n4852/10000. loss: 0.0004917026963084936\n4853/10000. loss: 0.0003307056613266468\n4854/10000. loss: 0.0004983303369954228\n4855/10000. loss: 0.0004345094785094261\n4856/10000. loss: 0.00037822201071927947\n4857/10000. loss: 0.00042317272163927555\n4858/10000. loss: 0.0004096524401878317\n4859/10000. loss: 0.0005029375897720456\n4860/10000. loss: 0.0004327603771040837\n4861/10000. loss: 0.0003956099196026723\n4862/10000. loss: 0.00032729352824389935\n4863/10000. loss: 0.00033328453234086436\n4864/10000. loss: 0.0003361701965332031\n4865/10000. loss: 0.0002680474426597357\n4866/10000. loss: 0.00027877532799417776\n4867/10000. loss: 0.0003056831192225218\n4868/10000. loss: 0.00030025622497002286\n4869/10000. loss: 0.00024865645294388133\n4870/10000. loss: 0.000434335321187973\n4871/10000. loss: 0.00037300330586731434\n4872/10000. loss: 0.0002725244654963414\n4873/10000. loss: 0.00031495821895077825\n4874/10000. loss: 0.00042279049133261043\n4875/10000. loss: 0.0003817407414317131\n4876/10000. loss: 0.000499569073629876\n4877/10000. loss: 0.0003224750010607143\n4878/10000. loss: 0.00031527547010531026\n4879/10000. loss: 0.00048724030299733084\n4880/10000. loss: 0.0002770441157432894\n4881/10000. loss: 0.0002193947439081967\n4882/10000. loss: 0.00034913156802455586\n4883/10000. loss: 0.00037789678511520225\n4884/10000. loss: 0.0002668515274611612\n4885/10000. loss: 0.00027208162161211175\n4886/10000. loss: 0.00023281362761432925\n4887/10000. loss: 0.00030074264698972303\n4888/10000. loss: 0.0002868598676286638\n4889/10000. loss: 0.00029672142894317705\n4890/10000. loss: 0.00026070326566696167\n4891/10000. loss: 0.00038238056004047394\n4892/10000. loss: 0.00038144493009895086\n4893/10000. loss: 0.0003036020401244362\n4894/10000. loss: 0.0003630415303632617\n4895/10000. loss: 0.00020487924727300802\n4896/10000. loss: 0.0003919457085430622\n4897/10000. loss: 0.00031511309013391536\n4898/10000. loss: 0.00023507381168504557\n4899/10000. loss: 0.0003820282096664111\n4900/10000. loss: 0.00030804276078318554\n4901/10000. loss: 0.0004139827797189355\n4902/10000. loss: 0.00026266420415292185\n4903/10000. loss: 0.00027812904833505553\n4904/10000. loss: 0.00042321587291856605\n4905/10000. loss: 0.00041458515139917534\n4906/10000. loss: 0.00035440622984121245\n4907/10000. loss: 0.0003864610722909371\n4908/10000. loss: 0.00025384964343781274\n4909/10000. loss: 0.0003846765806277593\n4910/10000. loss: 0.00042171737489600975\n4911/10000. loss: 0.00023421323082099357\n4912/10000. loss: 0.00031773575271169346\n4913/10000. loss: 0.00030669471016153693\n4914/10000. loss: 0.0003235215942064921\n4915/10000. loss: 0.00038606316472093266\n4916/10000. loss: 0.0002594148584951957\n4917/10000. loss: 0.0003931936031828324\n4918/10000. loss: 0.0003918157114336888\n4919/10000. loss: 0.000314806355163455\n4920/10000. loss: 0.0002372542512603104\n4921/10000. loss: 0.0002687303737426798\n4922/10000. loss: 0.0002940026655172308\n4923/10000. loss: 0.00026168945866326493\n4924/10000. loss: 0.00022359200132389864\n4925/10000. loss: 0.00027475015182668966\n4926/10000. loss: 0.00027230502261469763\n4927/10000. loss: 0.0003544225279862682\n4928/10000. loss: 0.00034734210930764675\n4929/10000. loss: 0.00022478452107558647\n4930/10000. loss: 0.00030241801869124174\n4931/10000. loss: 0.0003570876627539595\n4932/10000. loss: 0.0003610054263845086\n4933/10000. loss: 0.00025495083536952734\n4934/10000. loss: 0.0002841448488955696\n4935/10000. loss: 0.0003990872452656428\n4936/10000. loss: 0.00033073779195547104\n4937/10000. loss: 0.00021721236407756805\n4938/10000. loss: 0.0003066789358854294\n4939/10000. loss: 0.0002891644835472107\n4940/10000. loss: 0.0003390849257508914\n4941/10000. loss: 0.0003106882407640417\n4942/10000. loss: 0.00031299039255827665\n4943/10000. loss: 0.00027084948184589547\n4944/10000. loss: 0.00029882503440603614\n4945/10000. loss: 0.00035113942188521224\n4946/10000. loss: 0.00036702537909150124\n4947/10000. loss: 0.00026215494532758993\n4948/10000. loss: 0.00026298826560378075\n4949/10000. loss: 0.000383701641112566\n4950/10000. loss: 0.0003919146644572417\n4951/10000. loss: 0.00023631827207282186\n4952/10000. loss: 0.00045740992451707524\n4953/10000. loss: 0.0003566356996695201\n4954/10000. loss: 0.00022977701155468822\n4955/10000. loss: 0.0002876539947465062\n4956/10000. loss: 0.00019790387401978174\n4957/10000. loss: 0.0003045642127593358\n4958/10000. loss: 0.00026699041094010073\n4959/10000. loss: 0.0003213476932918032\n4960/10000. loss: 0.0002859055530279875\n4961/10000. loss: 0.0003932448647295435\n4962/10000. loss: 0.0003162948026632269\n4963/10000. loss: 0.00024706524952004355\n4964/10000. loss: 0.00023146206513047218\n4965/10000. loss: 0.00024190442248558006\n4966/10000. loss: 0.0004164399579167366\n4967/10000. loss: 0.00030183788233747083\n4968/10000. loss: 0.0003634841414168477\n4969/10000. loss: 0.00033305174050231773\n4970/10000. loss: 0.00024803246681888896\n4971/10000. loss: 0.00037310194845000905\n4972/10000. loss: 0.0002491573492685954\n4973/10000. loss: 0.0004391266265884042\n4974/10000. loss: 0.0002647757452602188\n4975/10000. loss: 0.0004644059808924794\n4976/10000. loss: 0.00034291332121938467\n4977/10000. loss: 0.0004815417341887951\n4978/10000. loss: 0.0004653555030624072\n4979/10000. loss: 0.0006348446477204561\n4980/10000. loss: 0.000757971623291572\n4981/10000. loss: 0.0009343018755316734\n4982/10000. loss: 0.0012078801325211923\n4983/10000. loss: 0.0024651912972331047\n4984/10000. loss: 0.002818754563728968\n4985/10000. loss: 0.002048961818218231\n4986/10000. loss: 0.0034037018194794655\n4987/10000. loss: 0.0018187786142031352\n4988/10000. loss: 0.006145484124620755\n4989/10000. loss: 0.009111234297355017\n4990/10000. loss: 0.007926490157842636\n4991/10000. loss: 0.006054213270545006\n4992/10000. loss: 0.0051442719995975494\n4993/10000. loss: 0.002999681979417801\n4994/10000. loss: 0.0017630488922198613\n4995/10000. loss: 0.0017874638239542644\n4996/10000. loss: 0.0011753848909089963\n4997/10000. loss: 0.0016008568927645683\n4998/10000. loss: 0.0016202710879345734\n4999/10000. loss: 0.0019514026741186778\n5000/10000. loss: 0.0019268634108205636\n5001/10000. loss: 0.0014273825411995251\n5002/10000. loss: 0.0007945074855039517\n5003/10000. loss: 0.0005911609623581171\n5004/10000. loss: 0.0010298423003405333\n5005/10000. loss: 0.0011868342602004607\n5006/10000. loss: 0.0010447204113006592\n5007/10000. loss: 0.0012203583028167486\n5008/10000. loss: 0.0015501235611736774\n5009/10000. loss: 0.0010631806217133999\n5010/10000. loss: 0.000799254048615694\n5011/10000. loss: 0.0006210351518044869\n5012/10000. loss: 0.00046132543745140236\n5013/10000. loss: 0.000552831799723208\n5014/10000. loss: 0.0007445299221823612\n5015/10000. loss: 0.0006582324082652727\n5016/10000. loss: 0.0006739948876202106\n5017/10000. loss: 0.0007309522479772568\n5018/10000. loss: 0.00047042981411019963\n5019/10000. loss: 0.0005868722607071201\n5020/10000. loss: 0.0006542378881325325\n5021/10000. loss: 0.0004794620132694642\n5022/10000. loss: 0.0004656002080688874\n5023/10000. loss: 0.0006013287541766962\n5024/10000. loss: 0.0006350287003442645\n5025/10000. loss: 0.00038256604845325154\n5026/10000. loss: 0.0005134072853252292\n5027/10000. loss: 0.00040825378770629567\n5028/10000. loss: 0.000297459385668238\n5029/10000. loss: 0.00048039923422038555\n5030/10000. loss: 0.0005003466503694654\n5031/10000. loss: 0.0004651531344279647\n5032/10000. loss: 0.0005188739160075784\n5033/10000. loss: 0.00047921397102375823\n5034/10000. loss: 0.0004804901157816251\n5035/10000. loss: 0.0004341355136906107\n5036/10000. loss: 0.00034764544883122045\n5037/10000. loss: 0.0004898547194898129\n5038/10000. loss: 0.0005127176797638336\n5039/10000. loss: 0.0006020818836987019\n5040/10000. loss: 0.000493364369807144\n5041/10000. loss: 0.0005395128779734174\n5042/10000. loss: 0.00032826350070536137\n5043/10000. loss: 0.0005150019812087218\n5044/10000. loss: 0.0005170946630338827\n5045/10000. loss: 0.00028846711696436006\n5046/10000. loss: 0.00027100507092351717\n5047/10000. loss: 0.00033698099044462043\n5048/10000. loss: 0.00035625459471096593\n5049/10000. loss: 0.00033153694433470565\n5050/10000. loss: 0.0002930233410249154\n5051/10000. loss: 0.00047466251999139786\n5052/10000. loss: 0.00029570880966881913\n5053/10000. loss: 0.0004006827560563882\n5054/10000. loss: 0.00032164928658554953\n5055/10000. loss: 0.00036526126010964316\n5056/10000. loss: 0.00027675476546088856\n5057/10000. loss: 0.00036048260517418385\n5058/10000. loss: 0.00034766364842653275\n5059/10000. loss: 0.0002843738184310496\n5060/10000. loss: 0.0002547729527577758\n5061/10000. loss: 0.00028021843172609806\n5062/10000. loss: 0.0004934313474223018\n5063/10000. loss: 0.0002794628186772267\n5064/10000. loss: 0.000424872695778807\n5065/10000. loss: 0.0005141445435583591\n5066/10000. loss: 0.00037335685919970274\n5067/10000. loss: 0.00034473165093610686\n5068/10000. loss: 0.00034783365360150736\n5069/10000. loss: 0.0003761936289568742\n5070/10000. loss: 0.0002767155723025401\n5071/10000. loss: 0.00031411383921901387\n5072/10000. loss: 0.00025380647275596857\n5073/10000. loss: 0.0003422932156051199\n5074/10000. loss: 0.0003912680937598149\n5075/10000. loss: 0.00032064570890118677\n5076/10000. loss: 0.00025509381278728444\n5077/10000. loss: 0.0005148851778358221\n5078/10000. loss: 0.00035360098506013554\n5079/10000. loss: 0.0003023058719312151\n5080/10000. loss: 0.00035010188973198336\n5081/10000. loss: 0.0003457206379001339\n5082/10000. loss: 0.00041459434820959967\n5083/10000. loss: 0.0002480072046940525\n5084/10000. loss: 0.00027231222096209723\n5085/10000. loss: 0.000438997366776069\n5086/10000. loss: 0.0004692791650692622\n5087/10000. loss: 0.0003045954314681391\n5088/10000. loss: 0.0004795818046356241\n5089/10000. loss: 0.00033872760832309723\n5090/10000. loss: 0.0004914384723330537\n5091/10000. loss: 0.00047852044614652794\n5092/10000. loss: 0.0008378264804681143\n5093/10000. loss: 0.0003077764025268455\n5094/10000. loss: 0.0002914682263508439\n5095/10000. loss: 0.00042708714803059894\n5096/10000. loss: 0.0002922395554681619\n5097/10000. loss: 0.0005169415380805731\n5098/10000. loss: 0.00033534085378050804\n5099/10000. loss: 0.00026905946045493084\n5100/10000. loss: 0.0004065598671634992\n5101/10000. loss: 0.00038754551981886226\n5102/10000. loss: 0.0003411495126783848\n5103/10000. loss: 0.0002709589898586273\n5104/10000. loss: 0.00048505708885689575\n5105/10000. loss: 0.0003395703000326951\n5106/10000. loss: 0.00024557098125418025\n5107/10000. loss: 0.0003950671913723151\n5108/10000. loss: 0.0003146775028047462\n5109/10000. loss: 0.0003881348141779502\n5110/10000. loss: 0.0003235334297642112\n5111/10000. loss: 0.000284852731662492\n5112/10000. loss: 0.0002453236180978517\n5113/10000. loss: 0.0003225117106921971\n5114/10000. loss: 0.0003487707969422142\n5115/10000. loss: 0.0003769976319745183\n5116/10000. loss: 0.0004106983154391249\n5117/10000. loss: 0.00045489434463282424\n5118/10000. loss: 0.00030146845771620673\n5119/10000. loss: 0.00027719840484981734\n5120/10000. loss: 0.000296779558993876\n5121/10000. loss: 0.00023929301338891187\n5122/10000. loss: 0.0003581060639893015\n5123/10000. loss: 0.0004240424605086446\n5124/10000. loss: 0.00032293301774188876\n5125/10000. loss: 0.00029762131938089925\n5126/10000. loss: 0.0003143542368585865\n5127/10000. loss: 0.00028313614893704653\n5128/10000. loss: 0.00025588227435946465\n5129/10000. loss: 0.00026833302884673077\n5130/10000. loss: 0.0002645777227977912\n5131/10000. loss: 0.00033756446403761703\n5132/10000. loss: 0.0003719078376889229\n5133/10000. loss: 0.00023973461550970873\n5134/10000. loss: 0.000334413954988122\n5135/10000. loss: 0.0004192998943229516\n5136/10000. loss: 0.0002593526927133401\n5137/10000. loss: 0.00025011108179266256\n5138/10000. loss: 0.00023175093034903208\n5139/10000. loss: 0.00032937084324657917\n5140/10000. loss: 0.0002526236736836533\n5141/10000. loss: 0.0003111778642050922\n5142/10000. loss: 0.0002924579700144629\n5143/10000. loss: 0.00036924821324646473\n5144/10000. loss: 0.00024145729063699642\n5145/10000. loss: 0.00025884931286176044\n5146/10000. loss: 0.0005806592913965384\n5147/10000. loss: 0.00035600488384564716\n5148/10000. loss: 0.00027212521915013593\n5149/10000. loss: 0.0005165229085832834\n5150/10000. loss: 0.0002725356607697904\n5151/10000. loss: 0.00040545384399592876\n5152/10000. loss: 0.0003736203070729971\n5153/10000. loss: 0.00037751505927493173\n5154/10000. loss: 0.0003858314206202825\n5155/10000. loss: 0.0003352131849775712\n5156/10000. loss: 0.0002777241364431878\n5157/10000. loss: 0.0003607507872705658\n5158/10000. loss: 0.0004434071791668733\n5159/10000. loss: 0.0003547228795165817\n5160/10000. loss: 0.00023173408893247446\n5161/10000. loss: 0.00023056782083585858\n5162/10000. loss: 0.0003101370530202985\n5163/10000. loss: 0.0004055133710304896\n5164/10000. loss: 0.0002887304872274399\n5165/10000. loss: 0.000254539637050281\n5166/10000. loss: 0.0003692536459614833\n5167/10000. loss: 0.00027424481231719255\n5168/10000. loss: 0.0002963246079161763\n5169/10000. loss: 0.00022198702208697796\n5170/10000. loss: 0.0002913657420625289\n5171/10000. loss: 0.00020267323513204852\n5172/10000. loss: 0.00035687935693810385\n5173/10000. loss: 0.00021938985446467996\n5174/10000. loss: 0.00028724485309794545\n5175/10000. loss: 0.0002407108355934421\n5176/10000. loss: 0.0004024492421497901\n5177/10000. loss: 0.00041341436250756186\n5178/10000. loss: 0.00022718523784230152\n5179/10000. loss: 0.0002508399193175137\n5180/10000. loss: 0.00027322908863425255\n5181/10000. loss: 0.00034811611597736675\n5182/10000. loss: 0.0002636506299798687\n5183/10000. loss: 0.00028964046699305374\n5184/10000. loss: 0.0004399063376088937\n5185/10000. loss: 0.00027864820246274274\n5186/10000. loss: 0.0002747187584949036\n5187/10000. loss: 0.000539289826216797\n5188/10000. loss: 0.00046438327990472317\n5189/10000. loss: 0.00035248879188050825\n5190/10000. loss: 0.0003946001719062527\n5191/10000. loss: 0.00028821952097738784\n5192/10000. loss: 0.00022785126930102706\n5193/10000. loss: 0.00023167939313376942\n5194/10000. loss: 0.0003476267835746209\n5195/10000. loss: 0.00022474499807382622\n5196/10000. loss: 0.00025288791706164676\n5197/10000. loss: 0.0002871314063668251\n5198/10000. loss: 0.00037593274222066003\n5199/10000. loss: 0.00021065210845942298\n5200/10000. loss: 0.00027367424142236513\n5201/10000. loss: 0.00037434852371613186\n5202/10000. loss: 0.00035081105306744576\n5203/10000. loss: 0.00020758887209619084\n5204/10000. loss: 0.00037322208906213444\n5205/10000. loss: 0.0002962848520837724\n5206/10000. loss: 0.00029796922657017905\n5207/10000. loss: 0.00032135141858210164\n5208/10000. loss: 0.00032856858645876247\n5209/10000. loss: 0.00034209884082277614\n5210/10000. loss: 0.00029408198315650225\n5211/10000. loss: 0.00027692220949878293\n5212/10000. loss: 0.00031274677409480017\n5213/10000. loss: 0.000256804710564514\n5214/10000. loss: 0.00026053341571241617\n5215/10000. loss: 0.0004204648236433665\n5216/10000. loss: 0.0003089381692310174\n5217/10000. loss: 0.0003840560869624217\n5218/10000. loss: 0.0002756679702239732\n5219/10000. loss: 0.00024476660958801705\n5220/10000. loss: 0.00034349287549654645\n5221/10000. loss: 0.00029289487671727937\n5222/10000. loss: 0.0003291563286135594\n5223/10000. loss: 0.0003732257367422183\n5224/10000. loss: 0.0004506797607367237\n5225/10000. loss: 0.00036174590544154245\n5226/10000. loss: 0.000498431579520305\n5227/10000. loss: 0.0005652594457690915\n5228/10000. loss: 0.0005176448418448368\n5229/10000. loss: 0.0007010740227997303\n5230/10000. loss: 0.0006506384427969655\n5231/10000. loss: 0.001562963395069043\n5232/10000. loss: 0.0010763274816175301\n5233/10000. loss: 0.0017394668733080227\n5234/10000. loss: 0.0014735587562123935\n5235/10000. loss: 0.0016107203749318917\n5236/10000. loss: 0.0012943653855472803\n5237/10000. loss: 0.001439354692896207\n5238/10000. loss: 0.0012630828811476629\n5239/10000. loss: 0.0012831790372729301\n5240/10000. loss: 0.0011600932727257411\n5241/10000. loss: 0.0012105318407217662\n5242/10000. loss: 0.0009902624879032373\n5243/10000. loss: 0.0008950574944416682\n5244/10000. loss: 0.0007182907623549303\n5245/10000. loss: 0.0005215289226422707\n5246/10000. loss: 0.0004540647690494855\n5247/10000. loss: 0.0005246219225227833\n5248/10000. loss: 0.0004478262271732092\n5249/10000. loss: 0.0004392530924330155\n5250/10000. loss: 0.0003770398519312342\n5251/10000. loss: 0.0003992456089084347\n5252/10000. loss: 0.0003932479303330183\n5253/10000. loss: 0.0005482578029235204\n5254/10000. loss: 0.00039640022441744804\n5255/10000. loss: 0.00036584818735718727\n5256/10000. loss: 0.0005695118258396784\n5257/10000. loss: 0.0005087251774966717\n5258/10000. loss: 0.0006210101613154014\n5259/10000. loss: 0.0003309420232350628\n5260/10000. loss: 0.0003149878078450759\n5261/10000. loss: 0.0003343020798638463\n5262/10000. loss: 0.0002988699513177077\n5263/10000. loss: 0.0004378278584529956\n5264/10000. loss: 0.0003622284857556224\n5265/10000. loss: 0.00024633501501133043\n5266/10000. loss: 0.00043544452637434006\n5267/10000. loss: 0.00026552365549529594\n5268/10000. loss: 0.0002584791897485654\n5269/10000. loss: 0.00041763562088211376\n5270/10000. loss: 0.0002964394516311586\n5271/10000. loss: 0.0002429874730296433\n5272/10000. loss: 0.00038747854220370453\n5273/10000. loss: 0.00044902639153103036\n5274/10000. loss: 0.0003832561584810416\n5275/10000. loss: 0.0003706393763422966\n5276/10000. loss: 0.0005328014958649874\n5277/10000. loss: 0.0004462773213163018\n5278/10000. loss: 0.00031078742661823827\n5279/10000. loss: 0.00037409675618012744\n5280/10000. loss: 0.00032184711502244073\n5281/10000. loss: 0.0004107433681686719\n5282/10000. loss: 0.00032275455305352807\n5283/10000. loss: 0.0003011665927867095\n5284/10000. loss: 0.0003996805753558874\n5285/10000. loss: 0.0002290105912834406\n5286/10000. loss: 0.00022447248920798302\n5287/10000. loss: 0.0003376854971672098\n5288/10000. loss: 0.00037227986225237447\n5289/10000. loss: 0.00027054625873764354\n5290/10000. loss: 0.0003693730492765705\n5291/10000. loss: 0.0003321185164774458\n5292/10000. loss: 0.0002948274292672674\n5293/10000. loss: 0.0002071090469447275\n5294/10000. loss: 0.00027139401451374095\n5295/10000. loss: 0.0003611311161269744\n5296/10000. loss: 0.0003120808784539501\n5297/10000. loss: 0.00039382255636155605\n5298/10000. loss: 0.0002588850911706686\n5299/10000. loss: 0.0002888421295210719\n5300/10000. loss: 0.00035712406194458407\n5301/10000. loss: 0.00025854783598333597\n5302/10000. loss: 0.0003657380196576317\n5303/10000. loss: 0.0002276379382237792\n5304/10000. loss: 0.0003485390916466713\n5305/10000. loss: 0.0002572741359472275\n5306/10000. loss: 0.0004622383664051692\n5307/10000. loss: 0.000241531563612322\n5308/10000. loss: 0.00024132810843487582\n5309/10000. loss: 0.00025236187502741814\n5310/10000. loss: 0.00022295107676958045\n5311/10000. loss: 0.00039702172701557476\n5312/10000. loss: 0.00040119261636088294\n5313/10000. loss: 0.0003374510755141576\n5314/10000. loss: 0.00027545207800964516\n5315/10000. loss: 0.00022123825813954076\n5316/10000. loss: 0.00039852239812413853\n5317/10000. loss: 0.00020966837958743176\n5318/10000. loss: 0.000387316569685936\n5319/10000. loss: 0.0003078015288338065\n5320/10000. loss: 0.00025251628054926795\n5321/10000. loss: 0.0003830937979122003\n5322/10000. loss: 0.00022291005977119008\n5323/10000. loss: 0.00033279388056447107\n5324/10000. loss: 0.0003088028752245009\n5325/10000. loss: 0.0003523821554457148\n5326/10000. loss: 0.0002996175123068194\n5327/10000. loss: 0.00033634835078070563\n5328/10000. loss: 0.0003239032036314408\n5329/10000. loss: 0.00022921509419878325\n5330/10000. loss: 0.00023070540434370437\n5331/10000. loss: 0.00027369150969510275\n5332/10000. loss: 0.0002979059354402125\n5333/10000. loss: 0.0003138820951183637\n5334/10000. loss: 0.0002888360759243369\n5335/10000. loss: 0.0004028525048245986\n5336/10000. loss: 0.00032642961014062166\n5337/10000. loss: 0.00031724195772161085\n5338/10000. loss: 0.0002777691697701812\n5339/10000. loss: 0.00032091581185037893\n5340/10000. loss: 0.00033963751047849655\n5341/10000. loss: 0.00028744762918601435\n5342/10000. loss: 0.00025831857540955144\n5343/10000. loss: 0.00026219662201280397\n5344/10000. loss: 0.0002871988496432702\n5345/10000. loss: 0.0003146586629251639\n5346/10000. loss: 0.00019667105516418815\n5347/10000. loss: 0.00024003745056688786\n5348/10000. loss: 0.0003623971715569496\n5349/10000. loss: 0.00019491921799878278\n5350/10000. loss: 0.00025372952222824097\n5351/10000. loss: 0.00023352079248676697\n5352/10000. loss: 0.00024869201782469946\n5353/10000. loss: 0.00021277383590737978\n5354/10000. loss: 0.0002021079029267033\n5355/10000. loss: 0.00019747835661595067\n5356/10000. loss: 0.0003632044730087121\n5357/10000. loss: 0.00020658071540916959\n5358/10000. loss: 0.00022778037237003446\n5359/10000. loss: 0.0003358001510302226\n5360/10000. loss: 0.00030673797785614926\n5361/10000. loss: 0.00038102602896591026\n5362/10000. loss: 0.00026202830485999584\n5363/10000. loss: 0.0002501128668275972\n5364/10000. loss: 0.0002876791792611281\n5365/10000. loss: 0.0004562627679357926\n5366/10000. loss: 0.00031053109948212904\n5367/10000. loss: 0.0005654554503659407\n5368/10000. loss: 0.000648214171330134\n5369/10000. loss: 0.00037522835191339254\n5370/10000. loss: 0.0012592715211212635\n5371/10000. loss: 0.0006510211775700251\n5372/10000. loss: 0.0007358717266470194\n5373/10000. loss: 0.001493242879708608\n5374/10000. loss: 0.0028677061200141907\n5375/10000. loss: 0.0018087057396769524\n5376/10000. loss: 0.0033861640840768814\n5377/10000. loss: 0.0035245887314279876\n5378/10000. loss: 0.002176467174043258\n5379/10000. loss: 0.0031711204598347345\n5380/10000. loss: 0.002065946658452352\n5381/10000. loss: 0.001181885755310456\n5382/10000. loss: 0.0008043434160451094\n5383/10000. loss: 0.0008502217630545298\n5384/10000. loss: 0.0008765371361126503\n5385/10000. loss: 0.001172558559725682\n5386/10000. loss: 0.0010744696483016014\n5387/10000. loss: 0.0010608471930027008\n5388/10000. loss: 0.001165551133453846\n5389/10000. loss: 0.0006203156275053819\n5390/10000. loss: 0.0006504409248009324\n5391/10000. loss: 0.00040355635186036426\n5392/10000. loss: 0.0004140659002587199\n5393/10000. loss: 0.0005349515316387018\n5394/10000. loss: 0.0007440287154167891\n5395/10000. loss: 0.0007236403568337361\n5396/10000. loss: 0.0005941752654810747\n5397/10000. loss: 0.0005923310139526924\n5398/10000. loss: 0.00045483435193697613\n5399/10000. loss: 0.0003142242591517667\n5400/10000. loss: 0.0005066981539130211\n5401/10000. loss: 0.00038262953360875446\n5402/10000. loss: 0.0004579065910850962\n5403/10000. loss: 0.0006097054186587533\n5404/10000. loss: 0.000557575219621261\n5405/10000. loss: 0.0005126065419365963\n5406/10000. loss: 0.0005118285771459341\n5407/10000. loss: 0.00045452189321319264\n5408/10000. loss: 0.0003203724045306444\n5409/10000. loss: 0.0002670448545056085\n5410/10000. loss: 0.00042477366514503956\n5411/10000. loss: 0.0003043260464134316\n5412/10000. loss: 0.00040103499001512927\n5413/10000. loss: 0.0003692254734536012\n5414/10000. loss: 0.0004449512731904785\n5415/10000. loss: 0.0004853555001318455\n5416/10000. loss: 0.0003136591015694042\n5417/10000. loss: 0.0002701640672360857\n5418/10000. loss: 0.00040234788320958614\n5419/10000. loss: 0.00040723072985808056\n5420/10000. loss: 0.0003449218347668648\n5421/10000. loss: 0.00043023711380859214\n5422/10000. loss: 0.0003477058295781414\n5423/10000. loss: 0.00043669416724393767\n5424/10000. loss: 0.00028962583746761084\n5425/10000. loss: 0.0004919440252706409\n5426/10000. loss: 0.0003451225347816944\n5427/10000. loss: 0.0003946940414607525\n5428/10000. loss: 0.00041736817608277005\n5429/10000. loss: 0.00027325879394387204\n5430/10000. loss: 0.00047745462507009506\n5431/10000. loss: 0.00039406058688958484\n5432/10000. loss: 0.0002521731269856294\n5433/10000. loss: 0.0003873356617987156\n5434/10000. loss: 0.0003094455460086465\n5435/10000. loss: 0.0004733833872402708\n5436/10000. loss: 0.0003690406447276473\n5437/10000. loss: 0.00042585508587459725\n5438/10000. loss: 0.0003151520698641737\n5439/10000. loss: 0.0004745327169075608\n5440/10000. loss: 0.00025127418727303546\n5441/10000. loss: 0.0003977002343162894\n5442/10000. loss: 0.0002579275945511957\n5443/10000. loss: 0.0002641896329199274\n5444/10000. loss: 0.0002428595907986164\n5445/10000. loss: 0.0002885763921464483\n5446/10000. loss: 0.0002829148822153608\n5447/10000. loss: 0.0002663214496957759\n5448/10000. loss: 0.0004178187033782403\n5449/10000. loss: 0.00025250075850635767\n5450/10000. loss: 0.0006533832444498936\n5451/10000. loss: 0.0002767296585564812\n5452/10000. loss: 0.00041383702773600817\n5453/10000. loss: 0.0004367487272247672\n5454/10000. loss: 0.00021056425369655093\n5455/10000. loss: 0.0003002348627584676\n5456/10000. loss: 0.00026547868037596345\n5457/10000. loss: 0.00040559199017783004\n5458/10000. loss: 0.0003448993278046449\n5459/10000. loss: 0.0002253253866607944\n5460/10000. loss: 0.0003770275895173351\n5461/10000. loss: 0.00043923450478663045\n5462/10000. loss: 0.00043478491716086864\n5463/10000. loss: 0.000305261656952401\n5464/10000. loss: 0.0004425977822393179\n5465/10000. loss: 0.0003436827488864462\n5466/10000. loss: 0.00027482966349149746\n5467/10000. loss: 0.0002750113296012084\n5468/10000. loss: 0.00043754628859460354\n5469/10000. loss: 0.00024784230239068467\n5470/10000. loss: 0.00025914710325499374\n5471/10000. loss: 0.00029337790329009295\n5472/10000. loss: 0.00033354243108381826\n5473/10000. loss: 0.0004068951044852535\n5474/10000. loss: 0.0002823350951075554\n5475/10000. loss: 0.00034497429927190143\n5476/10000. loss: 0.0003404459760834773\n5477/10000. loss: 0.00037656833107272786\n5478/10000. loss: 0.00035599009909977514\n5479/10000. loss: 0.000337798148393631\n5480/10000. loss: 0.00037735017637411755\n5481/10000. loss: 0.0002259592292830348\n5482/10000. loss: 0.000375781402302285\n5483/10000. loss: 0.0003258670525004466\n5484/10000. loss: 0.0002496110779854159\n5485/10000. loss: 0.00026980154992391664\n5486/10000. loss: 0.00020248842580864826\n5487/10000. loss: 0.00022632624798764786\n5488/10000. loss: 0.0003429900001113613\n5489/10000. loss: 0.0003691720776259899\n5490/10000. loss: 0.0002813396858982742\n5491/10000. loss: 0.00029590997534493607\n5492/10000. loss: 0.0003413773374632001\n5493/10000. loss: 0.0003284758422523737\n5494/10000. loss: 0.00029492861358448863\n5495/10000. loss: 0.000290268372433881\n5496/10000. loss: 0.00022864291289200386\n5497/10000. loss: 0.0002840591866212587\n5498/10000. loss: 0.00043191803464045125\n5499/10000. loss: 0.000531437573954463\n5500/10000. loss: 0.0003596460058664282\n5501/10000. loss: 0.00030253819810847443\n5502/10000. loss: 0.0002882269521554311\n5503/10000. loss: 0.00019664874222750464\n5504/10000. loss: 0.00021949115519722304\n5505/10000. loss: 0.00027915980899706483\n5506/10000. loss: 0.0002136571565642953\n5507/10000. loss: 0.00035248149652034044\n5508/10000. loss: 0.0002661812080380817\n5509/10000. loss: 0.0002427372382953763\n5510/10000. loss: 0.0004146801463017861\n5511/10000. loss: 0.00029964120282481116\n5512/10000. loss: 0.00038795227495332557\n5513/10000. loss: 0.0004064335177342097\n5514/10000. loss: 0.0002483629311124484\n5515/10000. loss: 0.0002792190061882138\n5516/10000. loss: 0.00036596083858360845\n5517/10000. loss: 0.00035246329692502815\n5518/10000. loss: 0.00038302297859142226\n5519/10000. loss: 0.00032095483038574457\n5520/10000. loss: 0.0003936550347134471\n5521/10000. loss: 0.0003876828122884035\n5522/10000. loss: 0.00031369532613704604\n5523/10000. loss: 0.0005959509871900082\n5524/10000. loss: 0.0004635967779904604\n5525/10000. loss: 0.000619435915723443\n5526/10000. loss: 0.0004829512909054756\n5527/10000. loss: 0.0006119218499710163\n5528/10000. loss: 0.0004894110218932232\n5529/10000. loss: 0.0006465286326905092\n5530/10000. loss: 0.0006090238457545638\n5531/10000. loss: 0.0007144640355060498\n5532/10000. loss: 0.000662059678385655\n5533/10000. loss: 0.0005997682067876061\n5534/10000. loss: 0.000615120322133104\n5535/10000. loss: 0.00039765890687704086\n5536/10000. loss: 0.0004926297115162015\n5537/10000. loss: 0.000255144783295691\n5538/10000. loss: 0.00030110155542691547\n5539/10000. loss: 0.0004290224363406499\n5540/10000. loss: 0.00028686415559301776\n5541/10000. loss: 0.0003654203998545806\n5542/10000. loss: 0.0003047256808107098\n5543/10000. loss: 0.00037525113051136333\n5544/10000. loss: 0.00025370569589237374\n5545/10000. loss: 0.00035461624308178824\n5546/10000. loss: 0.0003815947954232494\n5547/10000. loss: 0.0002554304664954543\n5548/10000. loss: 0.0002492566127330065\n5549/10000. loss: 0.0004240860774492224\n5550/10000. loss: 0.00027147723206629354\n5551/10000. loss: 0.00024231705659379563\n5552/10000. loss: 0.00038778805173933506\n5553/10000. loss: 0.0002574930355573694\n5554/10000. loss: 0.0004108496941626072\n5555/10000. loss: 0.00029189535416662693\n5556/10000. loss: 0.0003893450290585558\n5557/10000. loss: 0.00026514796384920675\n5558/10000. loss: 0.00044021177260826033\n5559/10000. loss: 0.0004030768759548664\n5560/10000. loss: 0.00021376363777865967\n5561/10000. loss: 0.0002818664846320947\n5562/10000. loss: 0.00042433202421913546\n5563/10000. loss: 0.00022363812119389573\n5564/10000. loss: 0.0003820090399434169\n5565/10000. loss: 0.00036687908383707207\n5566/10000. loss: 0.0003556861774995923\n5567/10000. loss: 0.0002343268133699894\n5568/10000. loss: 0.00022348115453496575\n5569/10000. loss: 0.00022406026255339384\n5570/10000. loss: 0.0002671550416077177\n5571/10000. loss: 0.00030211937458564836\n5572/10000. loss: 0.00026831659488379955\n5573/10000. loss: 0.0003120706727107366\n5574/10000. loss: 0.00030320089232797426\n5575/10000. loss: 0.0002617131103761494\n5576/10000. loss: 0.00041332336453100044\n5577/10000. loss: 0.0001999032295619448\n5578/10000. loss: 0.00032547143443177146\n5579/10000. loss: 0.00039512186776846647\n5580/10000. loss: 0.0002568843774497509\n5581/10000. loss: 0.00019669224275276065\n5582/10000. loss: 0.0003575435063491265\n5583/10000. loss: 0.00021369827057545385\n5584/10000. loss: 0.00019772405115266642\n5585/10000. loss: 0.00024694895061353844\n5586/10000. loss: 0.0003022099457060297\n5587/10000. loss: 0.00037371056775252026\n5588/10000. loss: 0.00040775145559261244\n5589/10000. loss: 0.0003617775704090794\n5590/10000. loss: 0.00021843990543857217\n5591/10000. loss: 0.0003014798276126385\n5592/10000. loss: 0.0004218871084352334\n5593/10000. loss: 0.00042994172933200997\n5594/10000. loss: 0.000335159944370389\n5595/10000. loss: 0.0004729783395305276\n5596/10000. loss: 0.0004825668875128031\n5597/10000. loss: 0.0005248229329784712\n5598/10000. loss: 0.00039541011210530996\n5599/10000. loss: 0.0004850845628728469\n5600/10000. loss: 0.0006343773178135356\n5601/10000. loss: 0.0005355271666000286\n5602/10000. loss: 0.0007963267465432485\n5603/10000. loss: 0.0007069098452727\n5604/10000. loss: 0.001322082243859768\n5605/10000. loss: 0.0005620569766809543\n5606/10000. loss: 0.0011732003185898066\n5607/10000. loss: 0.001160273017982642\n5608/10000. loss: 0.0015890300273895264\n5609/10000. loss: 0.00098087793836991\n5610/10000. loss: 0.0019592344760894775\n5611/10000. loss: 0.0017060586251318455\n5612/10000. loss: 0.0018044128082692623\n5613/10000. loss: 0.0023451953505476317\n5614/10000. loss: 0.0014128067220250766\n5615/10000. loss: 0.0018358798697590828\n5616/10000. loss: 0.0013462072238326073\n5617/10000. loss: 0.001028815284371376\n5618/10000. loss: 0.0008840002119541168\n5619/10000. loss: 0.0005858850587780277\n5620/10000. loss: 0.00033171459411581356\n5621/10000. loss: 0.0004626536586632331\n5622/10000. loss: 0.0005464266675213972\n5623/10000. loss: 0.0005103656246016423\n5624/10000. loss: 0.000435127061791718\n5625/10000. loss: 0.0005857707001268864\n5626/10000. loss: 0.0006021206888059775\n5627/10000. loss: 0.000492495057793955\n5628/10000. loss: 0.0005221936541299025\n5629/10000. loss: 0.000531164308389028\n5630/10000. loss: 0.0003627489398544033\n5631/10000. loss: 0.000401758976901571\n5632/10000. loss: 0.0004524306083718936\n5633/10000. loss: 0.0003248436648088197\n5634/10000. loss: 0.0003650953294709325\n5635/10000. loss: 0.00040806690230965614\n5636/10000. loss: 0.00029174213220054906\n5637/10000. loss: 0.0004528588615357876\n5638/10000. loss: 0.0003177836964217325\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车 增加LSTM\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\nimport warnings\n# 忽略特定类型的警告\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n                ('l1',nn.Linear(input_size,256)),\n                ('relu1',nn.ReLU()),\n                ('l2',nn.Linear(256,256)),\n                ('relu2',nn.ReLU()),\n                ('l3',nn.Linear(256,1))\n            ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n# LSTM模型\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        self.net1=nn.Sequential(OrderedDict([\n            ('lstm',nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout))\n        ]))\n        self.net2=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(hidden_size,hidden_size)),\n            ('relu',nn.ReLU()),\n            ('l2',nn.Linear(hidden_size,1))\n        ]))\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.lstm_layers = lstm_layers\n        self.dropout = dropout\n#         # LSTM编码器，接受输入序列并输出隐藏状态\n#         self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n#         # 线性层，将LSTM的输出映射到1维\n#         self.linear1 = nn.Linear(hidden_size, hidden_size)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.linear2 = nn.Linear(hidden_size, 1)\n        # 初始化线性层的权重和偏置\n#         nn.init.normal_(self.linear1.weight, 0, 0.02)\n#         nn.init.constant_(self.linear1.bias, 0.0)\n#         nn.init.normal_(self.linear2.weight, 0, 0.02)\n#         nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    def forward(self, x, weights):\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        lstm_encoder = nn.LSTM(self.input_size, self.hidden_size, self.lstm_layers, batch_first=False, dropout=self.dropout)\n        # 将已知的权重替换为模型的权重\n        lstm_encoder.weight_ih_l0.data = weights[0]\n        lstm_encoder.weight_hh_l0.data = weights[1]\n        lstm_encoder.bias_ih_l0.data = weights[2]\n        lstm_encoder.bias_hh_l0.data = weights[3]\n#         # 线性层，将LSTM的输出映射到1维\n#         self.linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.linear2 = nn.Linear(self.hidden_size, 1)\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = lstm_encoder(x)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out = F.linear(h_n, weights[4], weights[5])\n        x = F.relu(out)\n        out = F.linear(h_n, weights[6], weights[7])\n        return out\n\n\nclass lstm_test(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_test, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n#         nn.init.normal_(self.linear1.weight, 0, 0.02)\n#         nn.init.constant_(self.linear1.bias, 0.0)\n#         nn.init.normal_(self.linear2.weight, 0, 0.02)\n#         nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len)\n        return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = int(self.k/2)\n        self.models = []\n\n    def inner_loop(self, task):\n        learner = lstm_test(input_size = 3, lstm_layers = 1).to(device)\n        state_dict = copy.deepcopy(self.net.state_dict())\n        learner.load_state_dict(state_dict)\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = dict((name, param) for (name, param) in learner.named_parameters())  # 复制当前网络参数作为临时参数\n#         weightz = copy.deepcopy(temp_weights)\n        dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程\n        loss = inner_train(dataloader1, learner, temp_weights, self.criterion) / self.batch_size\n        g = torch.autograd.grad(loss, learner.parameters(), create_graph=True)\n\n        g = tuple(g[i] if i != 0 else torch.zeros(g[0].shape, requires_grad=True) for i in range(len(g)))\n        \n#         temp_weights = dict((name, param - self.alpha * g) for ((name, param), g) in zip(temp_weights.items(), g))\n        \n        for ((name, param), g_tuple) in zip(temp_weights.items(), g):\n            # 对梯度元组中的每个张量进行设备移动，并计算新的参数值\n            new_param = param.to(device) - self.alpha * g_tuple.to(device)\n            temp_weights[name] = new_param\n        # 测试weight有没有变化\n#         for (name, param) in temp_weights.items():\n#             print(temp_weights[name] == weightz[name])    \n        learner.load_state_dict(temp_weights)\n        \n        dataloader2 = DataLoader(\n                dataset_loader2,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        metaloss = inner_train(dataloader2, learner, temp_weights, self.criterion) / self.batch_size\n        return metaloss ,learner\n\n    def outer_loop(self, num_epochs):  # epoch 500\n        total_loss = 0\n        for epoch in range(1, num_epochs + 1):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()  # 从任务分布中采样一个元任务\n            grads = []\n            for i in tasks:\n                task = DataTask(i)\n                metaloss, learner = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                g = torch.autograd.grad(metaloss, learner.parameters(), create_graph=True)\n\n                #zero the gradients\n                g = tuple(g[i] for i in range(len(g)))\n\n                meta_grads = {name:g for ((name, _), g) in zip(learner.named_parameters(), g)}\n\n                #pass the meta-task gradients to the \n                grads.append(meta_grads)\n                metaloss_sum += metaloss  # mete_loss求和\n            dataset1,dataset2 = task.sample_data(size=20)  # 从任务中采样数据集 D\n            dataloader = DataLoader(\n                    dataset1,\n                    batch_size=10,\n                    shuffle=True,\n                    num_workers=1,\n                    drop_last=False)\n            loss = inner_train(dataloader, self.net, grads, self.criterion) / 10\n            loss.backward(retain_graph=True)\n\n            # Unpack the gradients from dictionary of meta-gradients\n            gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n\n            hooks = []\n            for(k,v) in self.net.named_parameters():\n                def get_closure():\n                    key = k\n                    def replace_grad(grad):\n                        return gradients[key]\n                    return replace_grad\n                hooks.append(v.register_hook(get_closure()))\n            torch.optim.Adam(self.net.parameters(), lr=self.beta).zero_grad()\n            loss.backward()\n\n            torch.optim.Adam(self.net.parameters(), lr=self.beta).step()\n\n            for h in hooks:\n                h.remove()\n\n            self.models.append(copy.deepcopy(self.net.state_dict()))\n\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    net = lstm_test(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n# print(list(net.parameters()))\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=300)\n\n\nmaml.outer_loop(num_epochs=5000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n# print(list(net.parameters()))\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn_model(input_size = his_horizon*3)\nelif model_type == 'lstm':\n    dummy_net = lstm_test(input_size = 3, lstm_layers = 1)\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=5\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.Adam\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n        opt.zero_grad()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n        opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T00:46:07.258848Z","iopub.execute_input":"2024-03-31T00:46:07.259664Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda:0\n1/5000. loss: 0.001435397503276666\n2/5000. loss: 0.0011475549545139074\n3/5000. loss: 0.00138047244399786\n4/5000. loss: 0.0013754662747184436\n5/5000. loss: 0.002124756264189879\n6/5000. loss: 0.002204833241800467\n7/5000. loss: 0.002136906608939171\n8/5000. loss: 0.0021589193493127823\n9/5000. loss: 0.001954034281273683\n10/5000. loss: 0.0018329080194234848\n11/5000. loss: 0.0020344884445269904\n12/5000. loss: 0.002271635147432486\n13/5000. loss: 0.001988455497970184\n14/5000. loss: 0.002149212329337994\n15/5000. loss: 0.0012424499727785587\n16/5000. loss: 0.0024165508026878038\n17/5000. loss: 0.0012527085685481627\n18/5000. loss: 0.0013146384929617245\n19/5000. loss: 0.002215956337749958\n20/5000. loss: 0.0017124880105257034\n21/5000. loss: 0.0015735048800706863\n22/5000. loss: 0.001778071125348409\n23/5000. loss: 0.0012910581814746063\n24/5000. loss: 0.0015506517762939136\n25/5000. loss: 0.001510181153813998\n26/5000. loss: 0.0016583927596608798\n27/5000. loss: 0.0020579624300201735\n28/5000. loss: 0.00151081383228302\n29/5000. loss: 0.0014616744592785835\n30/5000. loss: 0.0008891841862350702\n31/5000. loss: 0.0013686465099453926\n32/5000. loss: 0.001432351612796386\n33/5000. loss: 0.0019302054618795712\n34/5000. loss: 0.0007578710404535135\n35/5000. loss: 0.001695140264928341\n36/5000. loss: 0.0015036522721250851\n37/5000. loss: 0.0011485995880017679\n38/5000. loss: 0.0011870671684543292\n39/5000. loss: 0.0020684621607263884\n40/5000. loss: 0.002146949991583824\n41/5000. loss: 0.0017846332242091496\n42/5000. loss: 0.0015289631361762683\n43/5000. loss: 0.0012108038645237684\n44/5000. loss: 0.0012493463388333719\n45/5000. loss: 0.0010595975909382105\n46/5000. loss: 0.0014260027868052323\n47/5000. loss: 0.0018608076497912407\n48/5000. loss: 0.0017010831894973915\n49/5000. loss: 0.0017982575421531994\n50/5000. loss: 0.0011512764419118564\n51/5000. loss: 0.002237008884549141\n52/5000. loss: 0.0017998353578150272\n53/5000. loss: 0.0019682361744344234\n54/5000. loss: 0.0007068435661494732\n55/5000. loss: 0.0015616392095883687\n56/5000. loss: 0.0010605931747704744\n57/5000. loss: 0.0007309893456598123\n58/5000. loss: 0.0014155738366146882\n59/5000. loss: 0.0013907095417380333\n60/5000. loss: 0.0011431243425856035\n61/5000. loss: 0.0007869009083757798\n62/5000. loss: 0.002052716755618652\n63/5000. loss: 0.0008813493574659029\n64/5000. loss: 0.0011374771129339933\n65/5000. loss: 0.002098296924183766\n66/5000. loss: 0.0015058231850465138\n67/5000. loss: 0.001973648245135943\n68/5000. loss: 0.0008626645430922508\n69/5000. loss: 0.0013704315448800723\n70/5000. loss: 0.0008183961423734824\n71/5000. loss: 0.0014485685775677364\n72/5000. loss: 0.0014019898759822051\n73/5000. loss: 0.001991329714655876\n74/5000. loss: 0.0020866532189150653\n75/5000. loss: 0.001448382002611955\n76/5000. loss: 0.0014568553306162357\n77/5000. loss: 0.0015584599847594898\n78/5000. loss: 0.0010695566112796466\n79/5000. loss: 0.0016210381872951984\n80/5000. loss: 0.001921716146171093\n81/5000. loss: 0.0021736910566687584\n82/5000. loss: 0.000825835857540369\n83/5000. loss: 0.0008385463152080774\n84/5000. loss: 0.0016624474277098973\n85/5000. loss: 0.0018573820901413758\n86/5000. loss: 0.0018164664506912231\n87/5000. loss: 0.0008381918693582217\n88/5000. loss: 0.0010590610715250175\n89/5000. loss: 0.001581253328671058\n90/5000. loss: 0.0017562288170059521\n91/5000. loss: 0.001776234246790409\n92/5000. loss: 0.002092310848335425\n93/5000. loss: 0.0016956847781936328\n94/5000. loss: 0.0011201199765006702\n95/5000. loss: 0.001019572140648961\n96/5000. loss: 0.0014811063495775063\n97/5000. loss: 0.002094703105588754\n98/5000. loss: 0.0011145117071767647\n99/5000. loss: 0.002238948829472065\n100/5000. loss: 0.001341133068005244\n101/5000. loss: 0.0013833694780866306\n102/5000. loss: 0.0014080793286363285\n103/5000. loss: 0.0017846423822144668\n104/5000. loss: 0.001116510946303606\n105/5000. loss: 0.0022305333986878395\n106/5000. loss: 0.0008688884942481915\n107/5000. loss: 0.0020082108676433563\n108/5000. loss: 0.0011710300265500944\n109/5000. loss: 0.00159828985730807\n110/5000. loss: 0.0018745133032401402\n111/5000. loss: 0.0010999211420615513\n112/5000. loss: 0.0022324112554391227\n113/5000. loss: 0.0007965038530528545\n114/5000. loss: 0.0019305208697915077\n115/5000. loss: 0.0009719206330676874\n116/5000. loss: 0.001567314223696788\n117/5000. loss: 0.0011094561778008938\n118/5000. loss: 0.0022543038552006087\n119/5000. loss: 0.0021338158597548804\n120/5000. loss: 0.0013474527125557263\n121/5000. loss: 0.001368298816184203\n122/5000. loss: 0.0013580598557988803\n123/5000. loss: 0.0017573692214985688\n124/5000. loss: 0.0012769273792703946\n125/5000. loss: 0.0013507744297385216\n126/5000. loss: 0.0013069737081726391\n127/5000. loss: 0.00209438723201553\n128/5000. loss: 0.00078915199264884\n129/5000. loss: 0.0014104767081638177\n130/5000. loss: 0.0012303524029751618\n131/5000. loss: 0.0011594418125847976\n132/5000. loss: 0.0018293432270487149\n133/5000. loss: 0.0020195900773008666\n134/5000. loss: 0.001657121970007817\n135/5000. loss: 0.0014419762107233207\n136/5000. loss: 0.0020260331220924854\n137/5000. loss: 0.0016477457247674465\n138/5000. loss: 0.0010008242291708787\n139/5000. loss: 0.0012002494186162949\n140/5000. loss: 0.0007189168439557155\n141/5000. loss: 0.0019973510255416236\n142/5000. loss: 0.0014009270817041397\n143/5000. loss: 0.0013104965910315514\n144/5000. loss: 0.001279769620547692\n145/5000. loss: 0.0016298874591787655\n146/5000. loss: 0.0009473960381001234\n147/5000. loss: 0.002007594952980677\n148/5000. loss: 0.0013691784503559272\n149/5000. loss: 0.0021885656751692295\n150/5000. loss: 0.0012688807522257168\n151/5000. loss: 0.0019044339035948117\n152/5000. loss: 0.0013738293200731277\n153/5000. loss: 0.0015436058553556602\n154/5000. loss: 0.0018560569733381271\n155/5000. loss: 0.002074537022660176\n156/5000. loss: 0.0012079422983030479\n157/5000. loss: 0.001306180376559496\n158/5000. loss: 0.0012684798178573449\n159/5000. loss: 0.0014252960681915283\n160/5000. loss: 0.0014041235360006492\n161/5000. loss: 0.0013709412887692451\n162/5000. loss: 0.0017461736376086872\n163/5000. loss: 0.0020774475609262786\n164/5000. loss: 0.0014916295185685158\n165/5000. loss: 0.0021363707880179086\n166/5000. loss: 0.0010122041373203199\n167/5000. loss: 0.0020578651068111262\n168/5000. loss: 0.001157197558010618\n169/5000. loss: 0.0015010667654375236\n170/5000. loss: 0.0021089004973570504\n171/5000. loss: 0.0015780267616113026\n172/5000. loss: 0.0020734270413716636\n173/5000. loss: 0.0013242258379856746\n174/5000. loss: 0.0014488110318779945\n175/5000. loss: 0.0017231542927523453\n176/5000. loss: 0.0021849777549505234\n177/5000. loss: 0.0013561975210905075\n178/5000. loss: 0.0018057130898038547\n179/5000. loss: 0.0021226045986016593\n180/5000. loss: 0.0013563999285300572\n181/5000. loss: 0.0008473320243259271\n182/5000. loss: 0.0019519465665022533\n183/5000. loss: 0.0013642939738929272\n184/5000. loss: 0.0012898984520385663\n185/5000. loss: 0.002139465572933356\n186/5000. loss: 0.0013248842830459278\n187/5000. loss: 0.002143841547270616\n188/5000. loss: 0.0007557825495799383\n189/5000. loss: 0.001396024910112222\n190/5000. loss: 0.0014396728947758675\n191/5000. loss: 0.0007341591020425161\n192/5000. loss: 0.002185581251978874\n193/5000. loss: 0.0011840644292533398\n194/5000. loss: 0.0020905261238416037\n195/5000. loss: 0.0013034967705607414\n196/5000. loss: 0.0010952276643365622\n197/5000. loss: 0.001409259159117937\n198/5000. loss: 0.0016648873376349609\n199/5000. loss: 0.0013516855736573536\n200/5000. loss: 0.0014615982460478942\n201/5000. loss: 0.0011626843673487504\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车  version2   内循环使用adam\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        # 根据索引划分数据集\n        data_indices =shuffled_indices[:data_size] # 测试 (1881)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_indices = shuffled_indices[:data_ratio]\n    return data[data_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# 定义模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(input_size,256)),\n            ('relu1',nn.ReLU()),\n            ('l2',nn.Linear(256,256)),\n            ('relu2',nn.ReLU()),\n            ('l3',nn.Linear(256,1))\n        ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def argforward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net.argforward(x, temp_weights).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set = split_train(meta_data, size)\n#         print(data_set[0].shape)\n        dataset_loader = ImitationCarFolData(meta_data_set, max_len = max_len)\n        return dataset_loader\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 元数据集的数量\n        self.n = n  # 每次获取数据集的数量\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = self.k\n\n    def inner_loop(self, task):\n        # 内循环更新参数，用于计算元学习损失\n#         temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        temp_weights = self.weights\n        inner_optim = torch.optim.Adam(temp_weights, self.alpha)\n        dataset_loader = task.sample_data(size=self.k)  # 从任务中采样数据集 D\n        dataloader = DataLoader(\n                dataset_loader,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程  第一步\n        loss = inner_train(dataloader, self.net, temp_weights, self.criterion) / self.k\n        # 优化过程\n        inner_optim.zero_grad()\n        grads = torch.autograd.grad(loss, temp_weights)  # 计算损失对参数的梯度\n        for w, g in zip(temp_weights, grads):\n            w.grad = g\n        inner_optim.step()\n#         temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # 临时参数更新 梯度下降\n        # 第二步\n        metaloss = inner_train(dataloader, self.net, temp_weights, self.criterion) / self.k\n        return metaloss\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        total_loss = 0\n        for epoch in tqdm(range(1, num_epochs + 1)):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()  # 从任务分布中采样一个元任务\n            for i in tasks:\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights)  # 计算元学习损失对参数的梯度\n            # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'nn'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.0001,beta=0.0001,tasks=data_tasks,k=150)\n\n\nmaml.outer_loop(num_epochs=5000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn.Sequential(OrderedDict([\n        ('l1', nn.Linear(his_horizon*3,256)),\n        ('relu1', nn.ReLU()),\n        ('l2', nn.Linear(256,256)),\n        ('relu2', nn.ReLU()),\n        ('l3', nn.Linear(256,1))\n    ]))\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=10\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.SGD\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n        opt.zero_grad()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n        opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T01:16:16.519658Z","iopub.execute_input":"2024-02-25T01:16:16.519947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 原跟车训练测试过程\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom collections import OrderedDict\nimport os\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\nimport learn2learn as l2l\nfrom pyts.image import GramianAngularField\nfrom pyts.image import MarkovTransitionField\nfrom PIL import Image\nimport warnings\n# 忽略特定类型的警告\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/to_img_log\")\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# car_following_data = HighD_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # HighD(12541, 4, 375)\n\n# 保存日志\ndef get_logger(filename, verbosity=1, name=None):\n    # 设置不同verbosity对应的日志级别\n    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n    # 设置日志输出格式\n    formatter = logging.Formatter(\n        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level_dict[verbosity])\n    # 创建文件处理器，将日志写入文件\n    fh = logging.FileHandler(filename, \"w\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # 创建控制台处理器，将日志输出到控制台\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n# 日志保存路径\n# logger = get_logger('/kaggle/working/to_img_main.log')\n\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n    # np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts  # (max_len - 1)\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\nclass CNN(nn.Module):\n    def __init__(self, output_size, in_channels=3, hid_dim=60, embedding_size=None):\n        super(CNN, self).__init__()\n\n        if embedding_size is None:\n            embedding_size = hid_dim\n\n        self.encoder = l2l.vision.models.ConvBase(channels=in_channels, hidden=hid_dim, max_pool=True, layers=3,\n                                                  max_pool_factor=1)\n#         self.encoder = torch.nn.Sequential(\n#             nn.Conv2d(3,8,3,1,1),\n#             nn.BatchNorm2d(8),\n#             nn.ReLU(),\n#             nn.Dropout(0.3)\n#         )\n\n        self.features = torch.nn.Sequential(\n            self.encoder,\n            nn.Dropout(0.2),\n            nn.Flatten(),\n        )\n        self.classifier = torch.nn.Linear(\n            embedding_size,\n            output_size,\n            bias=True,\n        )\n#         self.classifier.weight.data.normal_()\n#         self.classifier.bias.data.mul_(0.0)\n\n        self.hidden_size = hid_dim\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# Train\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = split_train(car_following_data,0.8,0.1)\ntrain_data = SPMD1_train\ntest_data = SPMD1_test\nval_data = SPMD1_val\n# train_data = NGSIM_train\n# test_data = NGSIM_test\n# val_data = NGSIM_val\nprint(train_data.shape, test_data.shape, val_data.shape)\ndataset = 'SPMD1'\nmodel_type = 'cnn'\nbatch_size = 32\ntotal_epochs = 10\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=10,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\n# lr = 0.001\nmtf = MarkovTransitionField(image_size=his_horizon)\ngasf = GramianAngularField(method='summation')\ngadf = GramianAngularField(method='difference')\n\n# 定义保存文件的文件夹路径\nsave_folder = '/kaggle/input/model-state'\nsave = f'img_{model_type}_{dataset}.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n# 定义文件保存路径\nsave_path = os.path.join(save_folder, save)\n\n# 根据名称定义模型\nif model_type == 'cnn':\n    model = CNN(output_size = 1).to(device)\nmodel_state = list(model.parameters())\n# print(model_state)\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr, weight_decay=3e-4)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\nprint(\"----\")\n# 训练过程\nfor epoch in tqdm(range(total_epochs)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    model.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        if model_type == 'cnn':\n            space = x_data[0].clone().detach() # (bitch_size,149)\n            sv = x_data[1].clone().detach()\n            relSpeed = x_data[2].clone().detach()\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,bs,3)，y->(149,bs)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,bs)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,bs)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, bs, 3)\n            if model_type == 'cnn':\n                a = space[:, frame-his_horizon:frame] # (bs, 10)\n                b = sv[:, frame-his_horizon:frame]\n                c = relSpeed[:, frame-his_horizon:frame]\n                # 进行转换\n                # space_change = torch.tensor(mtf.transform(a))\n                # sv_change = torch.tensor(mtf.transform(b))\n                # relS_change = torch.tensor(mtf.transform(c))\n                # space_change = torch.tensor(gasf.transform(a))\n                # sv_change = torch.tensor(gasf.transform(b))\n                # relS_change = torch.tensor(gasf.transform(c))\n                space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n                sv_change = torch.tensor(gadf.transform(b))\n                relS_change = torch.tensor(gadf.transform(c))\n                x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = model(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        model_optim.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    model.eval()\n    error_list = []\n    for i, item in enumerate(validation_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        if model_type == 'cnn':\n            space = x_data[0].clone().detach() # (bitch_size,149)\n            sv = x_data[1].clone().detach()\n            relSpeed = x_data[2].clone().detach()\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,bs,3)，y->(149,bs)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'cnn':\n                a = space[:, frame-his_horizon:frame] # (bs, 10)\n                b = sv[:, frame-his_horizon:frame]\n                c = relSpeed[:, frame-his_horizon:frame]\n                # 进行转换\n                # space_change = torch.tensor(mtf.transform(a))\n                # sv_change = torch.tensor(mtf.transform(b))\n                # relS_change = torch.tensor(mtf.transform(c))\n                # space_change = torch.tensor(gasf.transform(a))\n                # sv_change = torch.tensor(gasf.transform(b))\n                # relS_change = torch.tensor(gasf.transform(c))\n                space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n                sv_change = torch.tensor(gadf.transform(b))\n                relS_change = torch.tensor(gadf.transform(c))\n                x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            acc_pre = model(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n        model_optim.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    val_error = mean_validation_error\n    if best_validation_loss is None or best_validation_loss > val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = val_error\n        # save the best model\n        with open(save, 'wb') as f:\n            torch.save(model, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\nprint(\"Epoch:{0}| Best Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\n# plt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\n# plt.savefig(os.path.join(plt_path, 'img_train_val_loss.png'))\nplt.show()\nplt.close()\n\n\n# 用该数据集训练得到的模型去对其他数据集进行测试\n# Ts = 0.04\n# max_len = 375\n# # car_following_data = NGSIM_data\n# # print(car_following_data.shape)\n# # train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# test_data = HighD_test\n# # 创建测试集 DataLoader\n# test_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\n# test_loader = DataLoader(\n#         test_dataset,\n#         batch_size=batch_size,\n#         shuffle=False,\n#         num_workers=1,\n#         drop_last=True)\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    if model_type == 'cnn':\n        space = x_data[0].clone().detach() # (bitch_size,149)\n        sv = x_data[1].clone().detach()\n        relSpeed = x_data[2].clone().detach()\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    # x->(149,bs,3)，y->(149,bs)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n    # print(T,B,d)  # 149  20  3\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n        if model_type == 'cnn':\n            a = space[:, frame-his_horizon:frame] # (bs, 10)\n            b = sv[:, frame-his_horizon:frame]\n            c = relSpeed[:, frame-his_horizon:frame]\n            # 进行转换\n            # space_change = torch.tensor(mtf.transform(a))\n            # sv_change = torch.tensor(mtf.transform(b))\n            # relS_change = torch.tensor(mtf.transform(c))\n            # space_change = torch.tensor(gasf.transform(a))\n            # sv_change = torch.tensor(gasf.transform(b))\n            # relS_change = torch.tensor(gasf.transform(c))\n            space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n            sv_change = torch.tensor(gadf.transform(b))\n            relS_change = torch.tensor(gadf.transform(c))\n            x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n# plt.savefig(os.path.join(plt_path, 'img_Spacing.png'))\nplt.show()\nplt.close()\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n# plt.savefig(os.path.join(plt_path, 'img_Speed.png'))\nplt.show()\nplt.close()\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.04\n# max_len = 375\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\nmin_error = 10000\nmin_index = 0\nmax_error = 0\nmax_index = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    if model_type == 'cnn':\n        space = x_data[0].clone().detach() # (bitch_size,149)\n        sv = x_data[1].clone().detach()\n        relSpeed = x_data[2].clone().detach()\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    # x->(149,bs,3)，y->(149,bs)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n    # print(T,B,d)  # 149  20  3\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n        if model_type == 'cnn':\n            a = space[:, frame-his_horizon:frame] # (bs, 10)\n            b = sv[:, frame-his_horizon:frame]\n            c = relSpeed[:, frame-his_horizon:frame]\n            # 进行转换\n            # space_change = torch.tensor(mtf.transform(a))\n            # sv_change = torch.tensor(mtf.transform(b))\n            # relS_change = torch.tensor(mtf.transform(c))\n            # space_change = torch.tensor(gasf.transform(a))\n            # sv_change = torch.tensor(gasf.transform(b))\n            # relS_change = torch.tensor(gasf.transform(c))\n            space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n            sv_change = torch.tensor(gadf.transform(b))\n            relS_change = torch.tensor(gadf.transform(c))\n            x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n    # for index, (space_obs, space_pre) in enumerate(zip(spacing_obs[:, 0].cpu().detach().numpy(), spacing_pre[:, 0].cpu().detach().numpy())):\n    #     writer.add_scalars('NGSIM:{}/Spacing'.format(count), {'GT': space_obs, 'prediction': space_pre}, global_step=index)\n    # for index, (speed_obs, speed_pre) in enumerate(zip(x_data_orig[:, 0, 1].cpu().detach().numpy(), x_data[:, 0, 1].cpu().detach().numpy())):\n    #     writer.add_scalars('NGSIM:{}/Speed'.format(count), {'GT': speed_obs, 'prediction': speed_pre}, global_step=index)\n    # writer.close()\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n    if error > max_error:\n        max_error = error\n        max_index = count\n    if error < min_error:\n        min_error = error\n        min_index = count\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"index（{}）min_error：{}, index（{}）max_error：{}\".format(min_index, min_error, max_index, max_error))\n# logger.info(\"index（{}）min_error：{}, index（{}）max_error：{}\".format(min_index, min_error, max_index, max_error))\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\n# logger.info(\"count={}，col={}，rate={:.5f}%\".format(count, col, col/count*100))\n# logger.info(\"jerk={:.5f}，miniumu_ttc={:.5f}\".format(np.mean(jerk_set), np.mean(minimum_ttc_set)))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n# logger.info(\"mean_test_error：{:.5f}\".format(mean_test_error))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:33:39.638721Z","iopub.execute_input":"2024-04-25T13:33:39.638995Z","iopub.status.idle":"2024-04-25T13:33:56.102746Z","shell.execute_reply.started":"2024-04-25T13:33:39.638970Z","shell.execute_reply":"2024-04-25T13:33:56.101533Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlearn2learn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01ml2l\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GramianAngularField\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkovTransitionField\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'learn2learn'"],"ename":"ModuleNotFoundError","evalue":"No module named 'learn2learn'","output_type":"error"}]},{"cell_type":"code","source":"import numbers\nfrom copy import copy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport random\n\n\ndef extract_top_level_dict(current_dict):\n    \"\"\"\n    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n    :param value: Param value\n    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n    :return: A dictionary graph of the params already added to the graph.\n    \"\"\"\n    output_dict = dict()\n    for key in current_dict.keys():\n        name = key.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"block_dict.\", \"\")\n        name = name.replace(\"module-\", \"\")\n        top_level = name.split(\".\")[0]\n        sub_level = \".\".join(name.split(\".\")[1:])\n\n        if top_level not in output_dict:\n            if sub_level == \"\":\n                output_dict[top_level] = current_dict[key]\n            else:\n                output_dict[top_level] = {sub_level: current_dict[key]}\n        else:\n            new_item = {key: value for key, value in output_dict[top_level].items()}\n            new_item[sub_level] = current_dict[key]\n            output_dict[top_level] = new_item\n\n    #print(current_dict.keys(), output_dict.keys())\n    return output_dict\n\n\nclass MetaLinearLayer(nn.Module):\n    def __init__(self, input_shape, num_filters, use_bias):\n        \"\"\"\n        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n        learning setting.\n        :param input_shape: The shape of the input data, in the form (b, f)\n        :param num_filters: Number of output filters\n        :param use_bias: Whether to use biases or not.\n        \"\"\"\n        super(MetaLinearLayer, self).__init__()\n        b, c = input_shape\n\n        self.use_bias = use_bias\n        self.weights = nn.Parameter(torch.ones(num_filters, c))\n        nn.init.xavier_uniform_(self.weights)\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.zeros(num_filters))\n\n    def forward(self, x, params=None):\n        \"\"\"\n        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n        Otherwise passed params will be used to execute the function.\n        :param x: Input data batch, in the form (b, f)\n        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n        Otherwise the external are used.\n        :return: The result of the linear function.\n        \"\"\"\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n            if self.use_bias:\n                (weight, bias) = params[\"weights\"], params[\"bias\"]\n            else:\n                (weight) = params[\"weights\"]\n                bias = None\n        else:\n            pass\n            #print('no inner loop params', self)\n\n            if self.use_bias:\n                weight, bias = self.weights, self.bias\n            else:\n                weight = self.weights\n                bias = None\n        # print(x.shape)\n        out = F.linear(input=x, weight=weight, bias=bias)\n        return out\n\n\nclass MetaStepLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        super(MetaStepLossNetwork, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        out = x\n\n        self.linear1 = MetaLinearLayer(input_shape=self.input_shape,\n                                                    num_filters=self.input_dim, use_bias=True)\n\n        self.linear2 = MetaLinearLayer(input_shape=(1, self.input_dim),\n                                                    num_filters=1, use_bias=True)\n\n\n        out = self.linear1(out)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n    def forward(self, x, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n\n        linear1_params = None\n        linear2_params = None\n\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n\n            linear1_params = params['linear1']\n            linear2_params = params['linear2']\n\n        out = x\n        \n        out = self.linear1(out, linear1_params)\n        out = F.relu_(out)\n        out = self.linear2(out, linear2_params)\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass MetaLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        \"\"\"\n        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n        used at inference time. Enables inner loop optimization readily.\n        :param im_shape: The input image batch shape.\n        :param num_output_classes: The number of output classes of the network.\n        :param args: A named tuple containing the system's hyperparameters.\n        :param device: The device to run this on.\n        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n        be enabled. \n        \"\"\"\n        super(MetaLossNetwork, self).__init__()\n        \n        self.device = device\n        # self.args = args\n        self.notspi = notspi\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        self.layer_dict = nn.ModuleDict()\n        # ф\n        for i in range(self.num_steps): \n            self.layer_dict['step{}'.format(i)] = MetaStepLossNetwork(self.input_dim, notspi=self.notspi, device=self.device)\n\n            out = self.layer_dict['step{}'.format(i)](x)\n\n    def forward(self, x, num_step, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n        param_dict = dict()\n\n        if params is not None: \n            # params = {key: value[0] for key, value in params.items()}\n            param_dict = extract_top_level_dict(current_dict=params)\n            \n        for name, param in self.layer_dict.named_parameters():\n            path_bits = name.split(\".\")\n            layer_name = path_bits[0]\n            if layer_name not in param_dict:\n                param_dict[layer_name] = None\n\n            \n        out = x\n        \n        out = self.layer_dict['step{}'.format(num_step)](out, param_dict['step{}'.format(num_step)])\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass StepLossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(StepLossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n        output_dim = num_loss_net_layers * 2 * 2 # 2 for weight and bias, another 2 for multiplier and offset\n\n        self.linear1 = nn.Linear(input_dim, input_dim)\n        self.activation = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(input_dim, output_dim)\n\n        self.multiplier_bias = nn.Parameter(torch.zeros(output_dim // 2))\n        self.offset_bias = nn.Parameter(torch.zeros(output_dim // 2))\n\n    def forward(self, task_state, num_step, loss_params):\n        # ψ\n        out = self.linear1(task_state)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n        generated_multiplier, generated_offset = torch.chunk(out, chunks=2, dim=-1)\n\n        i = 0\n        updated_loss_weights = dict()\n        for key, val in loss_params.items():\n            if 'step{}'.format(num_step) in key:\n                updated_loss_weights[key] = (1 + self.multiplier_bias[i] * generated_multiplier[i]) * val + \\\n                                             self.offset_bias[i] * generated_offset[i]\n                i+=1\n\n        return updated_loss_weights\n\n\nclass LossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(LossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.loss_adapter = nn.ModuleList()\n        for i in range(self.num_steps): \n            self.loss_adapter.append(StepLossAdapter(input_dim, num_loss_net_layers, notspi=notspi, device=device))\n\n    def forward(self, task_state, num_step, loss_params):\n        return self.loss_adapter[num_step](task_state, num_step, loss_params)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nfrom torch import _weight_norm\nimport numpy as np\nfrom math import sqrt\nimport random\n\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim_in = input_dim\n        self.dim_k = input_dim//2\n        self.dim_v = input_dim//2\n        self.num_heads = num_heads\n        self.linear_q = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_k = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_v = nn.Linear(input_dim, input_dim, bias=True)\n        self._norm_fact = 1 / sqrt((input_dim//2) // num_heads)\n\n    def forward(self, x, param=None):\n        batch, n, dim_in = x.shape\n        assert dim_in == self.dim_in\n\n        nh = self.num_heads\n        dk = self.dim_k // nh  # dim_k of each head\n        dv = self.dim_v // nh  # dim_v of each head\n        if param == None:\n            q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            v = self.linear_v(x).reshape(batch, n, nh, dv*2).transpose(1, 2)  # (batch, nh, n, dv)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1]).reshape(batch, n, nh, dk).transpose(1, 2)\n            k = F.linear(x, weight=param[2], bias=param[3]).reshape(batch, n, nh, dk).transpose(1, 2)\n            v = F.linear(x, weight=param[4], bias=param[5]).reshape(batch, n, nh, dv*2).transpose(1, 2)\n        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact  # batch, nh, n, n\n        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n\n        att = torch.matmul(dist, v)  # batch, nh, n, dv\n        out = att.transpose(1, 2).reshape(batch, n, self.dim_in)  # batch, n, dim_v\n        \n        # out = att.reshape(att.shape[0], -1)\n\n        return out\n    \n\nclass conv1d(nn.Module):\n    def _init_(self):\n        super(conv1d, self).__init__()\n    \n    def forward(self, x, weight, bias, stride, padding, dilation):\n        return F.conv1d(x, weight, bias=bias, stride=stride, padding=padding, dilation=dilation)\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm1 = nn.BatchNorm1d(n_outputs)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        # self.relu1 = nn.Tanh()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm2 = nn.BatchNorm1d(n_outputs)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        # self.relu2 = nn.Tanh()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.batchnorm1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.batchnorm2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        # self.relu = nn.Tanh()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        self.kernel_size = kernel_size\n        self.dropout = dropout\n        self.num_inputs = num_inputs\n        self.num_channels = num_channels\n        layers = []\n        # self.vars_bn = nn.ParameterList()\n        num_levels = len(num_channels)\n        # num_levels = num_channels\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n        self.token = nn.Parameter(torch.ones(1, num_channels[-1]))\n        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=10, nhead= 5,\\\n        #                     dim_feedforward=10, batch_first=True)\n        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n        # self.features = nn.LSTM(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.features = nn.GRU(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.attention = TemporalAttention(num_channels[-1])\n        self.attention = MultiHeadSelfAttention(num_channels[-1])\n        self.mlp = nn.Sequential(\n            nn.Linear(num_channels[-1], num_channels[-1]),\n            # nn.LeakyReLU(negative_slope=0.01),\n            nn.ReLU(), \n            nn.Linear(num_channels[-1], 1)\n        )\n        # self.features = nn.Linear(num_channels[-1], num_channels[-1])\n        # self.decoder = nn.Linear(num_channels[-1], 1)\n\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=num_channels[i-1], nhead= 4,\\\n        #                     dim_feedforward=num_channels[i-1], batch_first=False)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        # self.tanh = nn.Tanh()\n\n    def forward(self, x, weights=None):\n        device =  x[0].device\n        if weights == None:\n            # transformer output:\n            # x = self.transformer_encoder(x) + x\n            \n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n            # output = self.attention(output)\n            # linear output:\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n            # h_n = self.features(output)\n            \n            # LSTM output:\n            # enc_x, (h_n, c_n) = self.features(output)\n            \n            # GRU output:\n            # enc_x, h_n = self.features(output)\n\n            # if len(h_n.shape) == 3:\n            #     h_n = h_n[-1] # (32,16)\n            \n            # 通过线性层和激活函数得到最终输出\n            # out = self.decoder(F.relu(h_n))\n            # out = self.decoder(F.leaky_relu(h_n, negative_slope=0.01))\n            out = self.mlp(output)\n        else:\n            # TemporalBlock0\n            # 实际权重 = 模数 * 方向向量\n            ks = self.kernel_size\n            weight1 = _weight_norm(weights[3], weights[2], 0)\n            conv1d1 = weight_norm(nn.Conv1d(self.num_inputs, self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d1._conv_forward(x, weight=weight1, bias=weights[1])\n            # output1 = F.conv1d(x, weight=weights[1], bias=weights[2], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean1 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp1(output1), running_mean=running_mean1, running_var=running_var1, weight=weights[4], bias=weights[5])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            weight2 = _weight_norm(weights[8], weights[7], 0)\n            conv1d2 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d2._conv_forward(output1, weight=weight2, bias=weights[6])\n            # output1 = F.conv1d(output1, weight=weights[5], bias=weights[6], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean2 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp2(output1), running_mean=running_mean2, running_var=running_var2, weight=weights[9], bias=weights[10])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            res1 = F.conv1d(x, weights[11], bias=weights[12], stride=1)\n            # output1 = F.tanh(output1 + res1)\n            output1 = F.relu(output1 + res1)\n\n            # TemporalBlock1\n            weight3 = _weight_norm(weights[15], weights[14], 0)\n            conv1d3 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d3._conv_forward(output1, weight=weight3, bias=weights[13])\n            # output2 = F.conv1d(output1, weight=weights[11], bias=weights[12], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean3 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp1(output2), running_mean=running_mean3, running_var=running_var3, weight=weights[16], bias=weights[17])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            weight4 = _weight_norm(weights[20], weights[19], 0)\n            conv1d4 = weight_norm(nn.Conv1d(self.num_channels[1], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d4._conv_forward(output2, weight=weight4, bias=weights[18])\n            # output2 = F.conv1d(output2, weight=weights[15], bias=weights[16], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean4 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var4 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp2(output2), running_mean=running_mean4, running_var=running_var4, weight=weights[21], bias=weights[22])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            res2 = F.conv1d(output1, weights[23], bias=weights[24], stride=1)\n            # output2 = F.tanh(output2 + res2)\n            output = F.relu(output2 + res2)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[25:31])\n            output = output[:, -1, :]\n\n            # output = output.transpose(1, 2)\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n\n            # out = F.leaky_relu(F.linear(output, weight=weights[31] , bias=weights[32]), negative_slope=0.01)\n            out = F.relu(F.linear(output, weight=weights[31] , bias=weights[32]))\n            out = F.linear(out, weight=weights[33], bias=weights[34])\n\n        return out\n    \n\n# MAML-跟车  version1\n# %matplotlib inline\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport copy\nimport random\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\n# from net.TCN import TemporalConvNet\n# from net.TCN_wn_bn import TemporalConvNet\n# from net.TCN_layer import TemporalConvNet\n# from net.TCN import MultiHeadSelfAttention\n# from net.cbrd import cbrd\n# from net.meta_neural_network_architectures import MetaLossNetwork, LossAdapter\n\n# 设置随机种子\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/MAML_alfa_log\")\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\n# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\ngpu = 0\ndevice = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu')\nprint(device)\n\n\n# # 保存日志\n# def get_logger(filename, verbosity=1, name=None):\n#     # 设置不同verbosity对应的日志级别\n#     level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n#     # 设置日志输出格式\n#     formatter = logging.Formatter(\n#         \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n#     )\n#     logger = logging.getLogger(name)\n#     logger.setLevel(level_dict[verbosity])\n#     # 创建文件处理器，将日志写入文件\n#     fh = logging.FileHandler(filename, \"w\")\n#     fh.setFormatter(formatter)\n#     logger.addHandler(fh)\n#     # 创建控制台处理器，将日志输出到控制台\n#     sh = logging.StreamHandler()\n#     sh.setFormatter(formatter)\n#     logger.addHandler(sh)\n#     return logger\n# # 日志保存路径\n# logger = get_logger('/home/ubuntu/fedavg/FollowNet-car/log/MAML_alfa_oooollllldddd.log')\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len, Ts = 0.1):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n        self.Ts = Ts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / self.Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(\n                nn.Linear(input_size,256),\n                nn.ReLU(),\n                nn.Linear(256,256),\n                nn.ReLU(),\n                nn.Linear(256,1)\n            )\n        self.input_size = input_size\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights=None):\n        if weights == None:\n            x = self.net(x)\n        else:\n            x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n            x=F.relu(x)\n            x=F.linear(x,weights[2],weights[3])\n            x=F.relu(x)\n            x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 计算ttc\ndef calculate_safety(ttc):\n    minimum_ttc = min(ttc)\n    return minimum_ttc\n# 多数据同时验证\ndef model_evaluate(model,his_horizon,testdata):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    model.eval()\n\n    jerk_set = torch.empty(0).to(device)\n    error_set = torch.empty(0).to(device)\n    minimum_ttc_set = torch.empty(0).to(device)\n\n    criterion = nn.MSELoss()\n    count = 0\n    col = 0\n    for i, item in enumerate(testdata):#每个样本挨个出来\n        \n        x_data, y_data = item['inputs'], item['label']\n        x_data = torch.stack(x_data)#3x149\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        count += B\n        x_data_orig = x_data.clone().detach()#提前克隆的副本\n        \n        col_list = torch.full((B,), -1).to(device)\n        acc_batch = torch.empty(B,0).to(device)\n        ttc_batch = torch.empty(B,0).to(device)\n\n        \n        for frame in range(his_horizon, T):#对32个样本检测\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]   # (20, 3, 10)\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            #x = x.transpose(0,1).reshape(B, -1)\n            acc_pre = model(x).squeeze()\n            if acc_pre.dim() == 0:\n                acc_pre = acc_pre.unsqueeze(0)\n            acc_batch = torch.cat((acc_batch, acc_pre.unsqueeze(1)), dim=1)#记录所有acc     \n            if model_type == 'cnn1d' or model_type == 'tcn':\n                if frame < T-1:\n                    # 根据当前速度和加速度计算下一时间速度\n                    sv_spd_ = x_data[:, 1, frame] + acc_pre*Ts # (32)\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n                    # 计算下一时间速度的相对速度\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n                    # 该时刻真实相对速度\n                    delta_v = x_data[:, -1, frame] # (32)\n                    # 通过两车车距加上相对位移得到下一时间段车距 ？\n                    spacing_ = x_data[:, 0, frame] + Ts*(delta_v + delta_v_)/2 # (32)\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    # update 根据计算得到的值，更新下一时间的值\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[:, :, frame + 1] = next_frame_data\n            else:\n                if frame < T-1:#开环训练，不断依照预测值更新下一个时间节点的数据\n                    sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts#新的自车速度\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)#保证速度不小于等于0\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_#vt临时计算的\n                    delta_v = x_data[frame, :, -1]#v0原有的\n                    spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2#新的距离\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[frame + 1] = next_frame_data\n            if spacing_.dim() == 0:\n                ttc_batch_ = (-spacing_ / delta_v_).unsqueeze(0)\n            else:\n                ttc_batch_ = (-spacing_ / delta_v_)\n                \n            ttc_batch = torch.cat((ttc_batch, ttc_batch_.unsqueeze(1)), dim=1)\n        for i in range(B):#对32个样本统一处理\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                spacing_obs = x_data_orig[i,0,...]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[i, 0, :col_list[i]]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[i, 0, :]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            else:\n                spacing_obs = x_data_orig[...,i,0]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[:col_list[i],i, 0]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[:,i, 0]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            acc_batch_ = acc_batch_.cpu().detach().numpy()\n            jerk_single = np.mean(np.abs(np.diff(acc_batch_)/Ts))\n            TTC_single = [x for x in ttc_batch_ if x >= 0]#除去其中TTC小于0的\n            if len(TTC_single) > 0:\n                minimum_ttc_single = calculate_safety(TTC_single)#该样本的最小TTC\n                minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)      \n            jerk_set= torch.cat((jerk_set,torch.tensor(jerk_single).unsqueeze(0).to(device)), dim=0)\n            error_set= torch.cat((error_set,torch.tensor(error_single).unsqueeze(0).to(device)), dim=0)\n        col = col + torch.sum(col_list != -1).item()\n    if len(minimum_ttc_set) == 0:\n        ttc = 0\n    else:\n        ttc = sum(minimum_ttc_set)/len(minimum_ttc_set)\n    error = sum(error_set)/len(error_set)\n    return error,col,count,sum(jerk_set)/len(jerk_set),ttc#还需修改（以MSE为基准）\n\n\ndef finetuning():\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    # Train\n    Ts = 0.1\n    max_len = 150\n    og_net = maml.net\n    # 创建一个与原始网络结构相同的虚拟网络\n    if model_type == 'nn':\n        dummy_net = nn_model(input_size = his_horizon*3)\n    elif model_type == 'tcn':\n        dummy_net = TemporalConvNet(num_inputs=3, num_channels=(16,32))\n#     elif model_type == 'cnn1d':\n#         dummy_net = CNN1D()\n        # dummy_net = cbrd(num_inputs=3, num_output=1)\n    dummy_net=dummy_net.to(device)\n\n    # 多gpu训练\n    # if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n    #     dummy_net = torch.nn.DataParallel(dummy_net)  # 自动选择gpu\n    # dummy_net.to(device)\n\n    # 加载原始网络的权重\n    # weight_path = torch.load(state_path)\n    # dummy_net.load_state_dict(weight_path.state_dict())\n    dummy_net.load_state_dict(og_net.state_dict())\n    # 进行迭代，每次更新虚拟网络的参数\n    num_shots=5\n    lr = 0.01\n    loss_fn=nn.MSELoss()\n    optim=torch.optim.Adam\n    opt=optim(dummy_net.parameters(),lr=lr, weight_decay=3e-4)\n\n\n    # 数据集划分\n    def split_data(data,data_ratio):\n        # random.seed(SEED)\n        # np.random.seed(SEED)\n        # torch.manual_seed(SEED)\n        if data_ratio > 0 and data_ratio <= 1:\n            # 如果小于等于1，根据输入百分比计算获取数据集的数量\n            data_size=int(len(data)*data_ratio)\n        else:\n            # 如果大于1，则data_ratio为获取数据集中的数量\n            data_size = data_ratio\n        return data[:data_size]\n    # np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n    # 获取数据集的数量\n    K=40\n    dataset_train = split_data(train_data, K)\n    dataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len, Ts=Ts)\n    train_loader = DataLoader(\n                dataset_loader_train,\n                batch_size=10,\n                shuffle=True,\n                num_workers=1,\n                drop_last=True)\n    dataset_test = test_data\n    dataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len, Ts=Ts)\n    test_loader = DataLoader(\n                dataset_loader_test,\n                batch_size=64,\n                shuffle=False,\n                num_workers=1,\n                drop_last=False)\n    # dataset_val = NGSIM_val\n    # dataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len, Ts=Ts)\n    # val_loader = DataLoader(\n    #             dataset_loader_val,\n    #             batch_size=10,\n    #             shuffle=True,\n    #             num_workers=1,\n    #             drop_last=True)\n\n\n    # 初始化变量\n    train_loss_his = [] # 训练损失\n    test_error_his = [] # 测试误差\n    best_train_loss = None # 最佳训练损失\n\n    # 训练过程\n    best_error = 10000\n    for epoch in range(num_shots):\n        train_losses = [] # 记录每个epoch的训练损失\n        validation_losses = [] # 记录每个epoch的验证损失\n        jerk_val = 0\n        dummy_net.train()\n        # 遍历数据集\n        for i, item in enumerate(train_loader):\n            # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n            # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n            x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n            # Put T into the first dimension, B, T, d -> T, B, d\n            # 将x_data中3个(32,374)连接，转换成(3,32,374)\n            x_data = torch.stack(x_data)\n            # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                # x->(bs,3,149)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n                B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            else:\n                # x->(149,bs,3)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n                T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            # print(T,B,d)  # 149  20  3\n                \n            y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n            y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n            # 从历史数据时间步开始遍历\n            for frame in range(his_horizon, T):\n                if model_type == 'cnn1d' or model_type == 'tcn':\n                    x= x_data[:, :, frame-his_horizon:frame]\n                else:\n                    x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n                if model_type == 'nn':\n                    x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n                # 根据his_horizon个数据预测加速度\n                acc_pre = dummy_net(x).squeeze() # (32)\n                y_pre[frame - his_horizon] = acc_pre\n            #计算损失并进行反传及优化\n            loss = loss_fn(y_pre, y_label)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n            opt.step()\n\n            train_losses.append(loss.item())\n        # 计算本轮平均损失\n        train_loss = np.mean(train_losses)\n\n        train_loss_his.append(train_loss)\n        print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n#         logger.info(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n        mean_spacing_error,col,count,jerk,miniumu_ttc = model_evaluate(dummy_net,his_horizon,test_loader)\n        print(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n#         logger.info(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n        if mean_spacing_error < best_error:\n            best_error = mean_spacing_error\n            # save the best model\n#             with open(save_path, 'wb') as f:\n#                 torch.save(dummy_net, f)\n    return best_error\n\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion, metal=None):\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n            \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x, temp_weights).squeeze() # (32)\n            # acc_pre = net.module(x, temp_weights).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        if metal == None:\n            return loss\n        else:\n            return loss, y_pre\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n            if adapt_tasks:\n                task_net = HighD_net\n            # print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Lyft_net\n            # print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD1_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD1_net\n            # print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = NGSIM_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD2_net\n            # print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Waymo_net\n            # print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len, Ts = Ts)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len, Ts = Ts)\n        if adapt_tasks:\n            return dataset_loader1, dataset_loader2, task_net\n        else:\n            return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self, epoch):\n        # 从1到5中随机选择3个不重复的数\n        # 创建一个新的随机数生成器实例\n        rng = np.random.RandomState(epoch)\n        data_choice = rng.choice(self.data_range, size=self.n, replace=False) + 1\n        # data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\ndef attenuate_init(attenuator, panning, task_embeddings, names_weights_copy):\n        ## Attenuate\n        updated_names_weights_copy = list()\n        \n        # 生成衰减参数和偏移参数\n        if attenuator == None:\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(key + beta[i])\n                i+=1\n        elif panning == None:\n            gamma = attenuator(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key)\n                i+=1\n        else:\n            gamma = attenuator(task_embeddings)\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key + beta[i])\n                i+=1\n            \n        return updated_names_weights_copy\n\n\ndef get_inner_loop_parameter_dict(params):\n    \"\"\"\n    Returns a dictionary with the parameters to use for inner loop updates.\n    :param params: A dictionary of the network's parameters.\n    :return: A dictionary of the parameters to use for the inner loop optimization process.\n    \"\"\"\n    param_dict = dict()\n    for name, param in params:\n        if param.requires_grad:\n            param_dict[name] = param.to(device)\n\n    return param_dict\n\ndef get_per_step_loss_importance_vector(inner_step, current_epoch):\n        \"\"\"\n        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n        loss towards the optimization loss.\n        :return: A tensor to be used to compute the weighted average of the loss, useful for\n        the MSL (Multi Step Loss) mechanism.\n        \"\"\"\n        multi_step_loss_num_epochs = 20\n        loss_weights = np.ones(shape=(inner_step)) * (\n                1.0 / inner_step)\n        decay_rate = 1.0 / inner_step / multi_step_loss_num_epochs\n        min_value_for_non_final_losses = 0.03 / inner_step\n        for i in range(len(loss_weights) - 1):\n            curr_value = np.maximum(loss_weights[i] - (current_epoch * decay_rate), min_value_for_non_final_losses)\n            loss_weights[i] = curr_value\n\n        curr_value = np.minimum(\n            loss_weights[-1] + (current_epoch * (inner_step - 1) * decay_rate),\n            1.0 - ((inner_step - 1) * min_value_for_non_final_losses))\n        loss_weights[-1] = curr_value\n        loss_weights = torch.Tensor(loss_weights).to(device)\n        return loss_weights\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k, weight_decay, inner_step):  # (net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=150)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.inner_step = inner_step  # 内部循环更新的步数\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.meta_test = []\n        self.finetuning_error = []\n        self.plot_every = 10  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.first_add = 0  # 用于添加优化器优化的参数\n        self.batch_size = int(self.k/2)\n        # 设置权重衰减因子\n        self.weight_decay = weight_decay\n        self.num_layers = len(self.weights)\n        params = [{'params': self.weights}]\n        if MeTAL:\n            base_learner_num_layers = len(self.weights)\n            support_meta_loss_num_dim = base_learner_num_layers + 2\n            support_adapter_num_dim = base_learner_num_layers + 1\n\n            self.meta_loss = MetaLossNetwork(support_meta_loss_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_loss_adapter = LossAdapter(support_adapter_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_loss_adapter.parameters()}\n            params.append(new_params)\n\n            if semi_supervised:\n                query_num_dim = base_learner_num_layers + 1\n                self.meta_query_loss = MetaLossNetwork(query_num_dim, notspi=self.inner_step, device=device).to(device=device)\n                self.meta_query_loss_adapter = LossAdapter(query_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n                new_params = {'params': self.meta_query_loss.parameters()}\n                params.append(new_params)\n                new_params = {'params': self.meta_query_loss_adapter.parameters()}\n                params.append(new_params)\n\n        if attenuate:  #L2F\n            self.attenuator = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Sigmoid()\n            ).to(device=device)\n            new_params = {'params': self.attenuator.parameters()}\n            params.append(new_params)\n        if tapt:  #bias\n            self.panning = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Tanh()\n            ).to(device=device)\n            new_params = {'params': self.panning.parameters()}\n            params.append(new_params)\n        if alfa:  # ALFA\n            input_dim = self.num_layers*2\n            self.regularizer = nn.Sequential(\n                nn.Linear(input_dim, input_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(input_dim, input_dim)\n            ).to(device=device)\n            new_params = {'params': self.regularizer.parameters()}\n            params.append(new_params)\n        self.meta_optimiser = torch.optim.Adam(params,lr=self.beta, weight_decay=3e-4)\n\n    def inner_loop(self, task):\n        taskloss = []\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        if adapt_tasks:\n            dataset_loader1, dataset_loader2, task_net = task.sample_data(size=self.k)\n            task_optimiser = torch.optim.Adam([{'params':task_net.parameters()}],lr=0.001, weight_decay=3e-4)\n        else:\n            dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        for step in range(self.inner_step):\n            if MeTAL:\n                support_task_state = []\n                support_loss, support_preds = inner_train(dataloader1, self.net, temp_weights, self.criterion, metal=1)\n                support_loss /= self.batch_size\n                support_task_state.append(support_loss)\n\n                for v in temp_weights:\n                    support_task_state.append(v.mean())\n\n                support_task_state = torch.stack(support_task_state)\n                adapt_support_task_state = (support_task_state - support_task_state.mean())/(support_task_state.std() + 1e-12)                                                 \n\n                updated_meta_loss_weights = self.meta_loss_adapter(adapt_support_task_state, step, self.names_loss_weights_copy)\n\n                # support_y = torch.zeros(support_preds.shape).to(device)\n                # support_y[torch.arange(support_y.size(0)), y] = 1\n                support_task_state = torch.cat((\n                    support_task_state.view(1, -1).expand(support_preds.size(1), -1),\n                    support_preds.transpose(0, 1).mean(dim=1, keepdim=True)\n                    # support_y\n                ), -1)\n\n                support_task_state = (support_task_state - support_task_state.mean()) / (support_task_state.std() + 1e-12)\n                meta_support_loss = self.meta_loss(support_task_state, step, params=updated_meta_loss_weights).mean().squeeze()\n                if semi_supervised:\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    _, query_preds = inner_train(dataloader2, self.net, temp_weights, self.criterion, metal=1)\n                    query_task_state = []\n                    for v in temp_weights:\n                        query_task_state.append(v.mean())\n                    # out_prob = F.log_softmax(query_preds)\n                    # instance_entropy = torch.sum(torch.exp(out_prob) * out_prob, dim=-1)\n                    query_task_state = torch.stack(query_task_state)\n                    query_task_state = torch.cat((\n                                query_task_state.view(1, -1).expand(query_preds.size(1), -1), \n                                query_preds.transpose(0, 1).mean(dim=1, keepdim=True)\n                                # instance_entropy.view(-1, 1)\n                    ), -1)\n\n                    query_task_state = (query_task_state - query_task_state.mean())/(query_task_state.std() + 1e-12)\n                    updated_meta_query_loss_weights = self.meta_query_loss_adapter(query_task_state.mean(0), step, self.names_query_loss_weights_copy)\n\n                    meta_query_loss = self.meta_query_loss(query_task_state, step, params=updated_meta_query_loss_weights).mean().squeeze()\n\n                    loss = support_loss + meta_support_loss + meta_query_loss\n                else:\n                    loss = support_loss + meta_support_loss\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            else:\n                # 训练过程\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            if step == 0:\n                if adapt_tasks:     # every tasks\n                    grads = tuple(grads[i] for i in range(len(grads)))\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    temp_weights = attenuate_init(task_net, None, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    new_loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                    grads = torch.autograd.grad(new_loss, temp_weights, create_graph=True, retain_graph=True)\n\n                if attenuate or tapt:    #L2F\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    if attenuate and tapt:\n                        temp_weights = attenuate_init(self.attenuator, self.panning, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    elif attenuate and not tapt:\n                        temp_weights = attenuate_init(self.attenuator, None, layerwise_mean_grads, temp_weights)\n                    elif not attenuate and tapt:\n                        temp_weights = attenuate_init(None, self.panning, layerwise_mean_grads, temp_weights)\n                    new_loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                    grads = torch.autograd.grad(new_loss, temp_weights, create_graph=True, retain_graph=True)\n\n            if alfa:    #ALFA\n                # 用于存储权重的平均值和梯度的平均值\n                per_step_task_embedding = []\n                for v in temp_weights:\n                    per_step_task_embedding.append(v.mean())\n            \n                for i in range(len(grads)):\n                    per_step_task_embedding.append(grads[i].mean())\n\n                per_step_task_embedding = torch.stack(per_step_task_embedding)\n\n                generated_params = self.regularizer(per_step_task_embedding)\n\n                generated_alpha, generated_beta = torch.split(generated_params, split_size_or_sections=self.num_layers)\n                generated_alpha_params = []\n                generated_beta_params = []\n                g = 0\n                for key in temp_weights:\n                    generated_alpha_params.append(generated_alpha[g])\n                    generated_beta_params.append(generated_beta[g])\n                    g+=1\n                # 初始化ALFA自适应参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    if random_init:\n                        self.names_beta_dict_per_param = nn.ParameterDict()\n                    self.names_alpha_dict = nn.ParameterDict()\n                    self.names_beta_dict = nn.ParameterDict()\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    for idx, param in enumerate(temp_weights):\n\n                        if random_init:\n                        # per-param weight decay for random init\n                            self.names_beta_dict_per_param[str(idx)] = nn.Parameter(\n                                data=torch.ones(param.shape) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_learning_rates)\n\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step),\n                                requires_grad=use_learnable_beta)\n                            \n                            self.names_beta_dict_per_param.to(device)\n                        else:\n                            # per-step per-layer meta-learnable weight decay bias term (for more stable training and better performance by 2~3%)\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_beta)\n                        \n                        # per-step per-layer meta-learnable learning rate bias term (for more stable training and better performance by 2~3%)\n                        self.names_alpha_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                        \n                        self.names_beta_dict.to(device)\n                        self.names_alpha_dict.to(device)\n                    # 将额外的参数添加到优化器中\n                    if random_init:\n                        self.meta_optimiser.add_param_group({'params': self.names_beta_dict_per_param.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_beta_dict.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_alpha_dict.parameters()})\n            else:\n                # 使用可学习的参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    self.names_learning_rates_dict = nn.ParameterDict()\n                    for idx, param in enumerate(temp_weights):\n                        self.names_learning_rates_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                    self.names_learning_rates_dict.to(device)\n                    if use_learnable_alpha:\n                        self.meta_optimiser.add_param_group({'params': self.names_learning_rates_dict.parameters()})\n                        \n                    if inner_update == \"L2\" and self.weight_decay != None:\n                        self.names_weight_decay_dict = nn.ParameterDict()\n                        for idx, param in enumerate(temp_weights):\n                            self.names_weight_decay_dict[str(idx)] = nn.Parameter(\n                                    data=torch.ones(self.inner_step) * init_weight_decay,\n                                    requires_grad=use_learnable_beta)\n                        self.names_weight_decay_dict.to(device)\n                        if use_learnable_beta:\n                            self.meta_optimiser.add_param_group({'params': self.names_weight_decay_dict.parameters()})\n\n            if inner_update == \"L2\" and self.weight_decay != None:\n                # temp_weights = [w - self.alpha * (g + (self.weight_decay * w)) for w, g in zip(temp_weights, grads)]\n                # temp_weights = [((1 - self.alpha * self.weight_decay) * w) - (self.alpha * g) for w, g in zip(temp_weights, grads)]\n                temp_weights = [((1 - self.names_learning_rates_dict[str(key)][step] * self.names_weight_decay_dict[str(key)][step]) * w) - (self.names_learning_rates_dict[str(key)][step] * g) for key, (w, g) in enumerate(zip(temp_weights, grads))]\n            elif inner_update == \"ALFA\" and alfa:\n                if random_init:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step] * self.names_beta_dict_per_param[str(key)]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n                else:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n            else:\n                temp_weights = [w - self.names_learning_rates_dict[str(key)][step] * g for key, (w, g) in enumerate(zip(temp_weights, grads))]  # 临时参数更新 梯度下降\n\n            if use_multi_step_loss_optimization and self.epoch < 20:\n                dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                taskloss.append(self.per_step_loss_importance_vectors[step] * metaloss)\n            else:\n                if step == (self.inner_step - 1):\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                    taskloss.append(metaloss)\n        task_losses = torch.sum(torch.stack(taskloss))\n        if adapt_tasks:\n            task_optimiser.zero_grad()\n            metaloss.backward(retain_graph=True)\n            task_optimiser.step()\n        return task_losses\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        best_loss = 10000\n        best_index = 0\n        for epoch in range(1, num_epochs + 1):\n            self.epoch = epoch\n            if MeTAL:\n                self.names_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_loss.named_parameters())\n                if semi_supervised:\n                    self.names_query_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_query_loss.named_parameters())\n            total_loss = 0\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task(epoch)\n            for i in tasks:\n                if use_multi_step_loss_optimization:\n                    self.per_step_loss_importance_vectors = get_per_step_loss_importance_vector(self.inner_step, epoch)\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            # metaloss_sum.backward(retain_graph=True)\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights, retain_graph=True)  # 计算元学习损失对参数的梯度\n            # # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n\n            if MeTAL:\n                meta_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss.parameters(), meta_loss_grads):\n                    w.grad = g\n\n                meta_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss_adapter.parameters(), meta_loss_adapter_grads):\n                    w.grad = g\n\n                if semi_supervised:\n                    meta_query_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss.parameters(), meta_query_loss_grads):\n                        w.grad = g\n\n                    meta_query_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss_adapter.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss_adapter.parameters(), meta_query_loss_adapter_grads):\n                        w.grad = g\n\n            if not alfa and use_learnable_alpha:\n                learning_rates_grads = torch.autograd.grad(metaloss_sum, list(self.names_learning_rates_dict.parameters()), retain_graph=True)\n                for w, g in zip(self.names_learning_rates_dict.parameters(), learning_rates_grads):\n                    w.grad = g\n                if inner_update == \"L2\" and self.weight_decay != None and use_learnable_beta:\n                    weight_decay_grads = torch.autograd.grad(metaloss_sum, list(self.names_weight_decay_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_weight_decay_dict.parameters(), weight_decay_grads):\n                        w.grad = g\n\n            if attenuate:\n                attenuate_grads = torch.autograd.grad(metaloss_sum, list(self.attenuator.parameters()), retain_graph=True)\n                for w, g in zip(self.attenuator.parameters(), attenuate_grads):\n                    w.grad = g\n            if tapt:\n                tapt_grads = torch.autograd.grad(metaloss_sum, list(self.panning.parameters()), retain_graph=True)\n                for w, g in zip(self.panning.parameters(), tapt_grads):\n                    w.grad = g\n            if alfa:\n                alfa_grads = torch.autograd.grad(metaloss_sum, list(self.regularizer.parameters()), retain_graph=True)\n                for w, g in zip(self.regularizer.parameters(), alfa_grads):\n                    w.grad = g\n                if use_learnable_beta:\n                    names_beta_grads = torch.autograd.grad(metaloss_sum, list(self.names_beta_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict.parameters(), names_beta_grads):\n                        w.grad = g\n                if use_learnable_alpha:\n                    names_alpha_grads = torch.autograd.grad(metaloss_sum, self.names_alpha_dict.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_alpha_dict.parameters(), names_alpha_grads):\n                        w.grad = g\n                if random_init:\n                    random_init_grads = torch.autograd.grad(metaloss_sum, self.names_beta_dict_per_param.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict_per_param.parameters(), random_init_grads):\n                        w.grad = g\n            \n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            mserror,col,count,jerk,ttc = model_evaluate(self.net,his_horizon,meta_loader)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                print(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n#                 logger.info(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n#                 logger.info(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n                if mserror < best_loss:\n                    best_loss = mserror\n                    best_index = epoch\n                    # with open(file_path, 'wb') as f:\n                    #     torch.save(self.net, f)\n                self.meta_losses.append(total_loss / self.print_every)\n                self.meta_test.append(mserror.cpu().detach().numpy() / self.print_every)\n            self.finetuning_error.append(finetuning())\n            print(\"   \")\n            # fine_error = finetuning()\n            # self.finetuning_error.append(fine_error)\n            # if epoch % self.plot_every == 0:\n            #     epoch_path = f\"/home/ubuntu/fedavg/FollowNet-car/MAML_state/maml_l2f_test/MAML_nn_a0.01_b0.0001_{epoch}E.pt\" # 元初始化参数\n            #     # weight_change = torch.load(file_path)\n            #     with open(epoch_path, 'wb') as f:\n            #         torch.save(self.net, f)\n        print(\"(\", best_index, \")\",\"best_loss:\", best_loss)\n\ndataset = 'SPMD2'\nmodel_type = 'tcn'\nif dataset == 'SPMD1':\n    train_data = SPMD1_train\n    test_data = SPMD1_test\nelif dataset == 'SPMD2':\n    train_data = SPMD2_train\n    test_data = SPMD2_test\nelif dataset == 'Waymo':\n    train_data = Waymo_train\n    test_data = Waymo_test\nelif dataset == 'NGSIM':\n    train_data = NGSIM_train\n    test_data = NGSIM_test\nelif dataset == 'Lyft':\n    train_data = Lyft_train\n    test_data = Lyft_test\nelif dataset == 'HighD':\n    train_data = HighD_train\n    test_data = HighD_test\n# 定义保存文件的文件夹路径\n# file_path = \"/home/ubuntu/fedavg/FollowNet-car/MAML_state/MAML_alfa_nn_a0.01_b0.0001_k300_E50.pt\" # 元初始化参数\n# save_folder = '/home/ubuntu/fedavg/FollowNet-car/state'\n# save = f'MAML_ALFA_{model_type}_{dataset}_oooollllldddd.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\n# if not os.path.exists(save_folder):\n#     os.makedirs(save_folder)\n# 定义文件保存路径\n# save_path = os.path.join(save_folder, save)\nmeta_test = test_data\nmeta_loader_test = ImitationCarFolData(meta_test, max_len = max_len, Ts=Ts)\nmeta_loader = DataLoader(\n            meta_loader_test,\n            batch_size=96,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n# 模型初始化\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'tcn':\n    net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\n# elif model_type == 'cnn1d':\n#     net = CNN1D().to(device)\n    # net = cbrd(num_inputs=3, num_output=1).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n\n# model_state = list(net.parameters())\n# print(model_state)\n\n# 多gpu训练\n# if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n#     net = torch.nn.DataParallel(net)  # 自动选择gpu\n# net.to(device)\n\n# 内部循环更新策略\nMeTAL = False   # 任务自适应loss函数\nsemi_supervised = False  # 使用查询集 作为半监督\nadapt_tasks = False  # 权重初始化 每个任务用一个\nattenuate = False  # 权重初始化 所有任务共用 权重衰减\ntapt = False  # 权重初始化 所有任务共用 权重偏差\nalfa = False        # 自适应学习优化超参数\nrandom_init = False\nuse_learnable_learning_rates = random_init\nuse_learnable_beta = False  # 使优化参数beta可学习\nuse_learnable_alpha = False  # 使优化参数alpha可学习\nuse_multi_step_loss_optimization = False  # 内部循环多步时，使用最后一步的Loss，还是使用多步的Loss。\ninner_update = 'ALFA'\nif adapt_tasks:\n    num_layers = len(list(net.parameters()))\n    HighD_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Lyft_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD1_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD2_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Waymo_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.01,tasks=data_tasks,k=200,weight_decay=0.0001,inner_step=1)\n\n\nmaml.outer_loop(num_epochs=500)\nplt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(maml.meta_losses)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_Loss.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.meta_test)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_mserror.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.finetuning_error)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_finetuning_mserror.png'))\nplt.show()\nplt.close()\n# # 将数据保存到文件中\nnp.save('/kaggle/working/kaggle_maml_meta_losses.npy', np.array(maml.meta_losses))\nnp.save('/kaggle/working/kaggle_maml_meta_test.npy', np.array(maml.meta_test))\nnp.save('/kaggle/working/kaggle_maml_finetuning_error.npy', np.array(maml.finetuning_error))\n\n\nprint('----------------------')\n# logger.info(\"--------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:21:27.314557Z","iopub.execute_input":"2024-06-03T01:21:27.314897Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_26/3337634293.py:867: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)\n","output_type":"stream"},{"name":"stdout","text":"1/500. loss: 0.0030535062154134116\n1/500. mserror: 102.7873306274414  col: 1087  count: 3637  jerk: 0.00438933540135622  ttc: 36.295753479003906\nEpoch: 1| Train Loss: 0.1696991\nmean_spacing_error：97.53593，col=304，count=3637，rate=8.35854%，jerk=0.00040，miniumu_ttc=134.02794\nEpoch: 2| Train Loss: 0.1518969\nmean_spacing_error：96.88409，col=340，count=3637，rate=9.34836%，jerk=0.00327，miniumu_ttc=232.09081\nEpoch: 3| Train Loss: 0.1512302\nmean_spacing_error：95.04176，col=303，count=3637，rate=8.33104%，jerk=0.00330，miniumu_ttc=105.82082\nEpoch: 4| Train Loss: 0.1518357\nmean_spacing_error：84.95623，col=362，count=3637，rate=9.95326%，jerk=0.01029，miniumu_ttc=86.44263\nEpoch: 5| Train Loss: 0.1476017\nmean_spacing_error：63.69027，col=243，count=3637，rate=6.68133%，jerk=0.01057，miniumu_ttc=110.82883\n   \n2/500. loss: 0.003802728528777758\n2/500. mserror: 76.08035278320312  col: 477  count: 3637  jerk: 0.0006114662974141538  ttc: 81.97216033935547\nEpoch: 1| Train Loss: 0.1650520\nmean_spacing_error：93.45181，col=313，count=3637，rate=8.60599%，jerk=0.00046，miniumu_ttc=152.57030\nEpoch: 2| Train Loss: 0.1517792\nmean_spacing_error：91.36572，col=308，count=3637，rate=8.46852%，jerk=0.00107，miniumu_ttc=104.26459\nEpoch: 3| Train Loss: 0.1476234\nmean_spacing_error：87.66953，col=168，count=3637，rate=4.61919%，jerk=0.00845，miniumu_ttc=576.83191\nEpoch: 4| Train Loss: 0.1369270\nmean_spacing_error：87.86489，col=72，count=3637，rate=1.97965%，jerk=0.02108，miniumu_ttc=115.93440\nEpoch: 5| Train Loss: 0.1279370\nmean_spacing_error：75.88918，col=132，count=3637，rate=3.62936%，jerk=0.01982，miniumu_ttc=80.55441\n   \n3/500. loss: 0.005185063928365707\n3/500. mserror: 82.82811737060547  col: 398  count: 3637  jerk: 9.706171113066375e-05  ttc: 127.0958023071289\nEpoch: 1| Train Loss: 0.1705745\nmean_spacing_error：81.79623，col=406，count=3637，rate=11.16305%，jerk=0.00042，miniumu_ttc=94.42758\nEpoch: 2| Train Loss: 0.1526331\nmean_spacing_error：92.14896，col=302，count=3637，rate=8.30355%，jerk=0.00116，miniumu_ttc=89.82133\nEpoch: 3| Train Loss: 0.1476943\nmean_spacing_error：77.60785，col=116，count=3637，rate=3.18944%，jerk=0.01398，miniumu_ttc=75.12600\nEpoch: 4| Train Loss: 0.1378521\nmean_spacing_error：74.30209，col=190，count=3637，rate=5.22409%，jerk=0.01623，miniumu_ttc=111.29977\nEpoch: 5| Train Loss: 0.1331098\nmean_spacing_error：64.53221，col=300，count=3637，rate=8.24856%，jerk=0.02192，miniumu_ttc=373.03033\n   \n4/500. loss: 0.004086076902846496\n4/500. mserror: 88.57457733154297  col: 358  count: 3637  jerk: 6.075452984077856e-05  ttc: 159.3597869873047\nEpoch: 1| Train Loss: 0.1729297\nmean_spacing_error：86.34361，col=371，count=3637，rate=10.20071%，jerk=0.00059，miniumu_ttc=127.34771\nEpoch: 2| Train Loss: 0.1527433\nmean_spacing_error：85.73493，col=343，count=3637，rate=9.43085%，jerk=0.00126，miniumu_ttc=89.70692\nEpoch: 3| Train Loss: 0.1496352\nmean_spacing_error：74.79703，col=285，count=3637，rate=7.83613%，jerk=0.00507，miniumu_ttc=123.81218\nEpoch: 4| Train Loss: 0.1413666\nmean_spacing_error：54.81345，col=53，count=3637，rate=1.45724%，jerk=0.03282，miniumu_ttc=187.42487\nEpoch: 5| Train Loss: 0.1274566\nmean_spacing_error：55.57704，col=403，count=3637，rate=11.08056%，jerk=0.02796，miniumu_ttc=49.42076\n   \n5/500. loss: 0.0029972984145085015\n5/500. mserror: 94.32530975341797  col: 321  count: 3637  jerk: 7.883879879955202e-05  ttc: 103.52875518798828\nEpoch: 1| Train Loss: 0.1713433\nmean_spacing_error：85.68188，col=372，count=3637，rate=10.22821%，jerk=0.00040，miniumu_ttc=80.21905\nEpoch: 2| Train Loss: 0.1531347\nmean_spacing_error：92.23515，col=337，count=3637，rate=9.26588%，jerk=0.00058，miniumu_ttc=116.24204\nEpoch: 3| Train Loss: 0.1512677\nmean_spacing_error：96.28854，col=297，count=3637，rate=8.16607%，jerk=0.00089，miniumu_ttc=225.43843\nEpoch: 4| Train Loss: 0.1497704\nmean_spacing_error：68.79221，col=318，count=3637，rate=8.74347%，jerk=0.00465，miniumu_ttc=240.32031\nEpoch: 5| Train Loss: 0.1442489\nmean_spacing_error：50.91695，col=389，count=3637，rate=10.69563%，jerk=0.01498，miniumu_ttc=103.29002\n   \n6/500. loss: 0.004524751255909602\n6/500. mserror: 95.28851318359375  col: 316  count: 3637  jerk: 4.476325193536468e-05  ttc: 123.28595733642578\nEpoch: 1| Train Loss: 0.1721206\nmean_spacing_error：87.82501，col=352，count=3637，rate=9.67831%，jerk=0.00066，miniumu_ttc=101.26574\nEpoch: 2| Train Loss: 0.1524787\nmean_spacing_error：90.67455，col=324，count=3637，rate=8.90844%，jerk=0.00068，miniumu_ttc=85.78259\nEpoch: 3| Train Loss: 0.1502633\nmean_spacing_error：85.09480，col=292，count=3637，rate=8.02859%，jerk=0.00294，miniumu_ttc=243.45312\nEpoch: 4| Train Loss: 0.1456226\nmean_spacing_error：55.08740，col=265，count=3637，rate=7.28622%，jerk=0.01733，miniumu_ttc=108.61851\nEpoch: 5| Train Loss: 0.1386368\nmean_spacing_error：51.01949，col=117，count=3637，rate=3.21694%，jerk=0.04091，miniumu_ttc=47.65339\n   \n7/500. loss: 0.0035893029222885766\n7/500. mserror: 93.2220687866211  col: 331  count: 3637  jerk: 0.00017305636720266193  ttc: 143.7620086669922\nEpoch: 1| Train Loss: 0.1699223\nmean_spacing_error：85.32935，col=336，count=3637，rate=9.23838%，jerk=0.00136，miniumu_ttc=93.61397\nEpoch: 2| Train Loss: 0.1523237\nmean_spacing_error：90.86434，col=318，count=3637，rate=8.74347%，jerk=0.00067，miniumu_ttc=103.93442\nEpoch: 3| Train Loss: 0.1504669\nmean_spacing_error：87.48333，col=295，count=3637，rate=8.11108%，jerk=0.00272，miniumu_ttc=110.98349\nEpoch: 4| Train Loss: 0.1462063\nmean_spacing_error：50.29774，col=206，count=3637，rate=5.66401%，jerk=0.02065，miniumu_ttc=82.77686\nEpoch: 5| Train Loss: 0.1352085\nmean_spacing_error：44.13921，col=33，count=3637，rate=0.90734%，jerk=0.06117，miniumu_ttc=55.93787\n   \n8/500. loss: 0.0034927095596988997\n8/500. mserror: 89.49050903320312  col: 352  count: 3637  jerk: 0.00027506871265359223  ttc: 97.63641357421875\nEpoch: 1| Train Loss: 0.1668874\nmean_spacing_error：92.82344，col=308，count=3637，rate=8.46852%，jerk=0.00108，miniumu_ttc=133.68672\nEpoch: 2| Train Loss: 0.1522390\nmean_spacing_error：87.03185，col=347，count=3637，rate=9.54083%，jerk=0.00062，miniumu_ttc=106.62947\nEpoch: 3| Train Loss: 0.1503811\nmean_spacing_error：92.48135，col=287，count=3637，rate=7.89112%，jerk=0.00229，miniumu_ttc=179.79294\nEpoch: 4| Train Loss: 0.1464633\nmean_spacing_error：60.11114，col=308，count=3637，rate=8.46852%，jerk=0.01446，miniumu_ttc=89.56214\nEpoch: 5| Train Loss: 0.1352791\nmean_spacing_error：57.04828，col=179，count=3637，rate=4.92164%，jerk=0.03384，miniumu_ttc=96.27278\n   \n9/500. loss: 0.003940950768689315\n9/500. mserror: 88.09210968017578  col: 364  count: 3637  jerk: 0.00029283235198818147  ttc: 170.2154083251953\nEpoch: 1| Train Loss: 0.1621754\nmean_spacing_error：97.10872，col=292，count=3637，rate=8.02859%，jerk=0.00126，miniumu_ttc=142.23074\nEpoch: 2| Train Loss: 0.1519148\nmean_spacing_error：88.87791，col=357，count=3637，rate=9.81578%，jerk=0.00014，miniumu_ttc=179.70201\nEpoch: 3| Train Loss: 0.1512247\nmean_spacing_error：100.23952，col=294，count=3637，rate=8.08359%，jerk=0.00051，miniumu_ttc=151.41209\nEpoch: 4| Train Loss: 0.1509273\nmean_spacing_error：87.04676，col=316，count=3637，rate=8.68848%，jerk=0.00274，miniumu_ttc=107.19322\nEpoch: 5| Train Loss: 0.1497427\nmean_spacing_error：71.95567，col=343，count=3637，rate=9.43085%，jerk=0.00664，miniumu_ttc=159.28714\n   \n10/500. loss: 0.0036664381623268127\n10/500. mserror: 86.38362884521484  col: 372  count: 3637  jerk: 0.0004402940394356847  ttc: 73.56668090820312\nEpoch: 1| Train Loss: 0.1548301\nmean_spacing_error：94.40276，col=323，count=3637，rate=8.88095%，jerk=0.00005，miniumu_ttc=106.08024\nEpoch: 2| Train Loss: 0.1516276\nmean_spacing_error：91.91949，col=339，count=3637，rate=9.32087%，jerk=0.00010，miniumu_ttc=92.81734\nEpoch: 3| Train Loss: 0.1512521\nmean_spacing_error：99.83772，col=298，count=3637，rate=8.19357%，jerk=0.00010，miniumu_ttc=117.57861\nEpoch: 4| Train Loss: 0.1513827\nmean_spacing_error：97.63049，col=299，count=3637，rate=8.22106%，jerk=0.00059，miniumu_ttc=129.62941\nEpoch: 5| Train Loss: 0.1512971\nmean_spacing_error：89.67513，col=315，count=3637，rate=8.66098%，jerk=0.00100，miniumu_ttc=95.34422\n   \n11/500. loss: 0.0024369433522224426\n11/500. mserror: 82.99295043945312  col: 398  count: 3637  jerk: 0.0006378381513059139  ttc: 124.02587127685547\nEpoch: 1| Train Loss: 0.1535901\nmean_spacing_error：95.17117，col=311，count=3637，rate=8.55100%，jerk=0.00014，miniumu_ttc=251.75540\nEpoch: 2| Train Loss: 0.1516783\nmean_spacing_error：80.56559，col=340，count=3637，rate=9.34836%，jerk=0.00200，miniumu_ttc=104.21780\nEpoch: 3| Train Loss: 0.1497986\nmean_spacing_error：77.26541，col=342，count=3637，rate=9.40335%，jerk=0.00341，miniumu_ttc=124.80921\nEpoch: 4| Train Loss: 0.1466421\nmean_spacing_error：48.86593，col=389，count=3637，rate=10.69563%，jerk=0.02234，miniumu_ttc=39.32040\nEpoch: 5| Train Loss: 0.1421344\nmean_spacing_error：31.04092，col=175，count=3637，rate=4.81166%，jerk=0.03516，miniumu_ttc=76.38074\n   \n12/500. loss: 0.0022939788177609444\n12/500. mserror: 80.53028869628906  col: 424  count: 3637  jerk: 0.0005686376825906336  ttc: 78.040771484375\nEpoch: 1| Train Loss: 0.1537041\nmean_spacing_error：97.27780，col=306，count=3637，rate=8.41353%，jerk=0.00010，miniumu_ttc=105.24493\nEpoch: 2| Train Loss: 0.1520759\nmean_spacing_error：85.48347，col=348，count=3637，rate=9.56833%，jerk=0.00088，miniumu_ttc=133.76175\nEpoch: 3| Train Loss: 0.1502032\nmean_spacing_error：72.53421，col=306，count=3637，rate=8.41353%，jerk=0.00421，miniumu_ttc=106.86133\nEpoch: 4| Train Loss: 0.1475378\nmean_spacing_error：52.09934，col=342，count=3637，rate=9.40335%，jerk=0.01111，miniumu_ttc=185.44748\nEpoch: 5| Train Loss: 0.1390884\nmean_spacing_error：45.04301，col=180，count=3637，rate=4.94913%，jerk=0.04327，miniumu_ttc=53.23900\n   \n13/500. loss: 0.004987208793560664\n13/500. mserror: 78.49961853027344  col: 440  count: 3637  jerk: 0.0009577086893841624  ttc: 89.92930603027344\nEpoch: 1| Train Loss: 0.1547549\nmean_spacing_error：96.45742，col=322，count=3637，rate=8.85345%，jerk=0.00077，miniumu_ttc=152.37587\nEpoch: 2| Train Loss: 0.1518473\nmean_spacing_error：86.15362，col=345，count=3637，rate=9.48584%，jerk=0.00070，miniumu_ttc=106.05378\nEpoch: 3| Train Loss: 0.1508671\nmean_spacing_error：84.39636，col=315，count=3637，rate=8.66098%，jerk=0.00170，miniumu_ttc=120.63660\nEpoch: 4| Train Loss: 0.1484108\nmean_spacing_error：62.04908，col=346，count=3637，rate=9.51334%，jerk=0.00825，miniumu_ttc=222.20915\nEpoch: 5| Train Loss: 0.1454219\nmean_spacing_error：57.51263，col=464，count=3637，rate=12.75777%，jerk=0.05085，miniumu_ttc=21.81549\n   \n14/500. loss: 0.0032044326265652976\n14/500. mserror: 77.47481536865234  col: 458  count: 3637  jerk: 0.0008278219611383975  ttc: 76.77747344970703\nEpoch: 1| Train Loss: 0.1552983\nmean_spacing_error：100.61021，col=300，count=3637，rate=8.24856%，jerk=0.00029，miniumu_ttc=161.81744\nEpoch: 2| Train Loss: 0.1516476\nmean_spacing_error：93.14678，col=329，count=3637，rate=9.04592%，jerk=0.00056，miniumu_ttc=85.37141\nEpoch: 3| Train Loss: 0.1512284\nmean_spacing_error：94.06799，col=314，count=3637，rate=8.63349%，jerk=0.00039，miniumu_ttc=135.04488\nEpoch: 4| Train Loss: 0.1512471\nmean_spacing_error：82.84679，col=320，count=3637，rate=8.79846%，jerk=0.00173，miniumu_ttc=99.06104\nEpoch: 5| Train Loss: 0.1488646\nmean_spacing_error：35.97729，col=350，count=3637，rate=9.62332%，jerk=0.01755，miniumu_ttc=98.56312\n   \n15/500. loss: 0.0048014043519894285\n15/500. mserror: 79.40843200683594  col: 430  count: 3637  jerk: 0.0008481849799863994  ttc: 73.26435852050781\nEpoch: 1| Train Loss: 0.1553443\nmean_spacing_error：97.73239，col=306，count=3637，rate=8.41353%，jerk=0.00015，miniumu_ttc=99.86611\nEpoch: 2| Train Loss: 0.1515002\nmean_spacing_error：94.22684，col=314，count=3637，rate=8.63349%，jerk=0.00039，miniumu_ttc=132.12892\nEpoch: 3| Train Loss: 0.1513876\nmean_spacing_error：93.09621，col=305，count=3637，rate=8.38603%，jerk=0.00093，miniumu_ttc=233.67476\nEpoch: 4| Train Loss: 0.1510281\nmean_spacing_error：77.83120，col=319，count=3637，rate=8.77097%，jerk=0.00251，miniumu_ttc=113.22604\nEpoch: 5| Train Loss: 0.1481419\nmean_spacing_error：59.59193，col=299，count=3637，rate=8.22106%，jerk=0.00859，miniumu_ttc=99.19364\n   \n16/500. loss: 0.004740680878361066\n16/500. mserror: 81.78983306884766  col: 400  count: 3637  jerk: 0.000610643473919481  ttc: 152.85633850097656\nEpoch: 1| Train Loss: 0.1539769\nmean_spacing_error：97.99487，col=302，count=3637，rate=8.30355%，jerk=0.00021，miniumu_ttc=102.67019\nEpoch: 2| Train Loss: 0.1523583\nmean_spacing_error：86.85938，col=347，count=3637，rate=9.54083%，jerk=0.00133，miniumu_ttc=184.20561\nEpoch: 3| Train Loss: 0.1505998\nmean_spacing_error：92.94995，col=299，count=3637，rate=8.22106%，jerk=0.00111，miniumu_ttc=101.42381\nEpoch: 4| Train Loss: 0.1497830\nmean_spacing_error：66.76736，col=303，count=3637，rate=8.33104%，jerk=0.00573，miniumu_ttc=305.02798\nEpoch: 5| Train Loss: 0.1446109\nmean_spacing_error：46.55676，col=188，count=3637，rate=5.16910%，jerk=0.02231，miniumu_ttc=89.61107\n   \n17/500. loss: 0.0026665482049187026\n17/500. mserror: 83.82951354980469  col: 388  count: 3637  jerk: 0.0003488227666821331  ttc: 95.78008270263672\nEpoch: 1| Train Loss: 0.1540103\nmean_spacing_error：98.07986，col=299，count=3637，rate=8.22106%，jerk=0.00026，miniumu_ttc=108.57625\nEpoch: 2| Train Loss: 0.1513640\nmean_spacing_error：91.10730，col=314，count=3637，rate=8.63349%，jerk=0.00103，miniumu_ttc=131.02513\nEpoch: 3| Train Loss: 0.1509564\nmean_spacing_error：85.00590，col=307，count=3637，rate=8.44102%，jerk=0.00207，miniumu_ttc=113.23962\nEpoch: 4| Train Loss: 0.1490350\nmean_spacing_error：67.11719，col=299，count=3637，rate=8.22106%，jerk=0.00743，miniumu_ttc=88.18562\nEpoch: 5| Train Loss: 0.1421430\nmean_spacing_error：30.40499，col=237，count=3637，rate=6.51636%，jerk=0.03306，miniumu_ttc=210.56602\n   \n18/500. loss: 0.002921776846051216\n18/500. mserror: 84.98087310791016  col: 379  count: 3637  jerk: 0.00024333561304956675  ttc: 83.41893005371094\nEpoch: 1| Train Loss: 0.1542314\nmean_spacing_error：100.21955，col=290，count=3637，rate=7.97360%，jerk=0.00045，miniumu_ttc=130.98210\nEpoch: 2| Train Loss: 0.1510849\nmean_spacing_error：90.31365，col=307，count=3637，rate=8.44102%，jerk=0.00115，miniumu_ttc=88.34064\nEpoch: 3| Train Loss: 0.1497097\nmean_spacing_error：70.14082，col=319，count=3637，rate=8.77097%，jerk=0.00719，miniumu_ttc=108.47666\nEpoch: 4| Train Loss: 0.1391656\nmean_spacing_error：39.17331，col=259，count=3637，rate=7.12125%，jerk=0.04917，miniumu_ttc=44.11666\nEpoch: 5| Train Loss: 0.1364344\nmean_spacing_error：34.66758，col=202，count=3637，rate=5.55403%，jerk=0.04980，miniumu_ttc=110.81538\n   \n19/500. loss: 0.005733139192064603\n19/500. mserror: 85.23595428466797  col: 371  count: 3637  jerk: 0.00031342654256150126  ttc: 348.7688293457031\nEpoch: 1| Train Loss: 0.1536961\nmean_spacing_error：98.20832，col=299，count=3637，rate=8.22106%，jerk=0.00030，miniumu_ttc=108.88686\nEpoch: 2| Train Loss: 0.1513334\nmean_spacing_error：89.60099，col=336，count=3637，rate=9.23838%，jerk=0.00076，miniumu_ttc=781.61536\nEpoch: 3| Train Loss: 0.1499785\nmean_spacing_error：83.16642，col=314，count=3637，rate=8.63349%，jerk=0.00336，miniumu_ttc=138.87067\nEpoch: 4| Train Loss: 0.1436369\nmean_spacing_error：46.01668，col=222，count=3637，rate=6.10393%，jerk=0.02269，miniumu_ttc=90.51126\nEpoch: 5| Train Loss: 0.1303369\nmean_spacing_error：46.04172，col=149，count=3637，rate=4.09678%，jerk=0.03149，miniumu_ttc=78.01100\n   \n20/500. loss: 0.002968748720983664\n20/500. mserror: 83.94390106201172  col: 381  count: 3637  jerk: 0.0004809123638551682  ttc: 120.38370513916016\nEpoch: 1| Train Loss: 0.1539353\nmean_spacing_error：98.97549，col=295，count=3637，rate=8.11108%，jerk=0.00033，miniumu_ttc=109.67924\nEpoch: 2| Train Loss: 0.1506981\nmean_spacing_error：90.08784，col=320，count=3637，rate=8.79846%，jerk=0.00111，miniumu_ttc=975.55914\nEpoch: 3| Train Loss: 0.1470238\nmean_spacing_error：79.34660，col=234，count=3637，rate=6.43387%，jerk=0.00697，miniumu_ttc=90.09544\nEpoch: 4| Train Loss: 0.1397863\nmean_spacing_error：56.38278，col=98，count=3637，rate=2.69453%，jerk=0.03350，miniumu_ttc=48.97762\nEpoch: 5| Train Loss: 0.1245348\nmean_spacing_error：71.56333，col=123，count=3637，rate=3.38191%，jerk=0.02931，miniumu_ttc=57.57840\n   \n21/500. loss: 0.0027093676229317984\n21/500. mserror: 82.508544921875  col: 389  count: 3637  jerk: 0.0006311776814982295  ttc: 140.3699951171875\nEpoch: 1| Train Loss: 0.1538207\nmean_spacing_error：99.16809，col=299，count=3637，rate=8.22106%，jerk=0.00010，miniumu_ttc=129.34656\nEpoch: 2| Train Loss: 0.1513306\nmean_spacing_error：93.47839，col=321，count=3637，rate=8.82596%，jerk=0.00026，miniumu_ttc=114.68929\nEpoch: 3| Train Loss: 0.1511664\nmean_spacing_error：91.36127，col=322，count=3637，rate=8.85345%，jerk=0.00081，miniumu_ttc=104.14388\nEpoch: 4| Train Loss: 0.1491767\nmean_spacing_error：67.29208，col=274，count=3637，rate=7.53368%，jerk=0.00874，miniumu_ttc=120.81081\nEpoch: 5| Train Loss: 0.1394213\nmean_spacing_error：26.70077，col=88，count=3637，rate=2.41958%，jerk=0.06037，miniumu_ttc=82.96432\n   \n22/500. loss: 0.004493815203507741\n22/500. mserror: 80.89179992675781  col: 402  count: 3637  jerk: 0.0007812160765752196  ttc: 88.61747741699219\nEpoch: 1| Train Loss: 0.1539202\nmean_spacing_error：100.19469，col=296，count=3637，rate=8.13858%，jerk=0.00016，miniumu_ttc=132.11536\nEpoch: 2| Train Loss: 0.1521794\nmean_spacing_error：79.77383，col=387，count=3637，rate=10.64064%，jerk=0.00226，miniumu_ttc=101.12457\nEpoch: 3| Train Loss: 0.1501701\nmean_spacing_error：71.88746，col=345，count=3637，rate=9.48584%，jerk=0.00405，miniumu_ttc=91.97778\nEpoch: 4| Train Loss: 0.1471833\nmean_spacing_error：35.79649，col=293，count=3637，rate=8.05609%，jerk=0.02607，miniumu_ttc=197.12161\nEpoch: 5| Train Loss: 0.1469400\nmean_spacing_error：69.41682，col=785，count=3637，rate=21.58372%，jerk=0.08530，miniumu_ttc=34.12219\n   \n23/500. loss: 0.0029156357049942017\n23/500. mserror: 79.60511779785156  col: 410  count: 3637  jerk: 0.00098420272115618  ttc: 106.18547821044922\nEpoch: 1| Train Loss: 0.1539487\nmean_spacing_error：101.71779，col=294，count=3637，rate=8.08359%，jerk=0.00009，miniumu_ttc=202.60178\nEpoch: 2| Train Loss: 0.1515008\nmean_spacing_error：91.73110，col=323，count=3637，rate=8.88095%，jerk=0.00074，miniumu_ttc=86.18508\nEpoch: 3| Train Loss: 0.1514889\nmean_spacing_error：89.36285，col=330，count=3637，rate=9.07341%，jerk=0.00087，miniumu_ttc=95.05872\nEpoch: 4| Train Loss: 0.1507531\nmean_spacing_error：83.72162，col=305，count=3637，rate=8.38603%，jerk=0.00234，miniumu_ttc=99.77307\nEpoch: 5| Train Loss: 0.1487786\nmean_spacing_error：59.16972，col=325，count=3637，rate=8.93594%，jerk=0.00998，miniumu_ttc=132.84563\n   \n24/500. loss: 0.002914740703999996\n24/500. mserror: 78.79322052001953  col: 417  count: 3637  jerk: 0.001175831537693739  ttc: 112.64387512207031\nEpoch: 1| Train Loss: 0.1542601\nmean_spacing_error：102.36275，col=288，count=3637，rate=7.91861%，jerk=0.00028，miniumu_ttc=127.49862\nEpoch: 2| Train Loss: 0.1515225\nmean_spacing_error：89.56829，col=326，count=3637，rate=8.96343%，jerk=0.00091，miniumu_ttc=157.04643\nEpoch: 3| Train Loss: 0.1509299\nmean_spacing_error：88.18002，col=307，count=3637，rate=8.44102%，jerk=0.00175，miniumu_ttc=169.56320\nEpoch: 4| Train Loss: 0.1503324\nmean_spacing_error：76.28562，col=395，count=3637，rate=10.86060%，jerk=0.00473，miniumu_ttc=112.36195\nEpoch: 5| Train Loss: 0.1492321\nmean_spacing_error：62.53739，col=307，count=3637，rate=8.44102%，jerk=0.00876，miniumu_ttc=126.60744\n   \n25/500. loss: 0.0044002945845325785\n25/500. mserror: 78.2872543334961  col: 409  count: 3637  jerk: 0.001430098433047533  ttc: 803.7254028320312\nEpoch: 1| Train Loss: 0.1539692\nmean_spacing_error：102.76330，col=288，count=3637，rate=7.91861%，jerk=0.00020，miniumu_ttc=113.96797\nEpoch: 2| Train Loss: 0.1519213\nmean_spacing_error：87.71977，col=358，count=3637，rate=9.84328%，jerk=0.00099，miniumu_ttc=83.63340\nEpoch: 3| Train Loss: 0.1510199\nmean_spacing_error：89.20575，col=331，count=3637，rate=9.10091%，jerk=0.00097，miniumu_ttc=109.30256\nEpoch: 4| Train Loss: 0.1502240\nmean_spacing_error：60.82899，col=313，count=3637，rate=8.60599%，jerk=0.00699，miniumu_ttc=135.58813\nEpoch: 5| Train Loss: 0.1472806\nmean_spacing_error：37.43665，col=390，count=3637，rate=10.72312%，jerk=0.02236，miniumu_ttc=105.02756\n   \n26/500. loss: 0.004646396264433861\n26/500. mserror: 78.82299041748047  col: 391  count: 3637  jerk: 0.0015770923346281052  ttc: 95.00653076171875\nEpoch: 1| Train Loss: 0.1536991\nmean_spacing_error：102.74430，col=289，count=3637，rate=7.94611%，jerk=0.00014，miniumu_ttc=110.36714\nEpoch: 2| Train Loss: 0.1516997\nmean_spacing_error：81.86116，col=387，count=3637，rate=10.64064%，jerk=0.00211，miniumu_ttc=85.00655\nEpoch: 3| Train Loss: 0.1505276\nmean_spacing_error：74.49625，col=307，count=3637，rate=8.44102%，jerk=0.00393，miniumu_ttc=99.95345\nEpoch: 4| Train Loss: 0.1487535\nmean_spacing_error：46.09145，col=347，count=3637，rate=9.54083%，jerk=0.01361，miniumu_ttc=86.54803\nEpoch: 5| Train Loss: 0.1417151\nmean_spacing_error：70.35894，col=330，count=3637，rate=9.07341%，jerk=0.06535，miniumu_ttc=17.16047\n   \n27/500. loss: 0.003008659929037094\n27/500. mserror: 78.00595092773438  col: 384  count: 3637  jerk: 0.001965821487829089  ttc: 90.688720703125\nEpoch: 1| Train Loss: 0.1536104\nmean_spacing_error：101.94184，col=294，count=3637，rate=8.08359%，jerk=0.00009，miniumu_ttc=202.11520\nEpoch: 2| Train Loss: 0.1521786\nmean_spacing_error：71.60715，col=408，count=3637，rate=11.21804%，jerk=0.00267，miniumu_ttc=74.80044\nEpoch: 3| Train Loss: 0.1498580\nmean_spacing_error：59.45754，col=324，count=3637，rate=8.90844%，jerk=0.00623，miniumu_ttc=75.17194\nEpoch: 4| Train Loss: 0.1482120\nmean_spacing_error：52.85865，col=295，count=3637，rate=8.11108%，jerk=0.01364，miniumu_ttc=202.20703\nEpoch: 5| Train Loss: 0.1405255\nmean_spacing_error：44.97582，col=302，count=3637，rate=8.30355%，jerk=0.05781，miniumu_ttc=78.76225\n   \n28/500. loss: 0.003372702126701673\n28/500. mserror: 78.23465728759766  col: 365  count: 3637  jerk: 0.002300162799656391  ttc: 252.8479461669922\nEpoch: 1| Train Loss: 0.1534300\nmean_spacing_error：100.04431，col=295，count=3637，rate=8.11108%，jerk=0.00028，miniumu_ttc=142.38730\nEpoch: 2| Train Loss: 0.1528699\nmean_spacing_error：62.40240，col=374，count=3637，rate=10.28320%，jerk=0.00488，miniumu_ttc=89.20727\nEpoch: 3| Train Loss: 0.1481016\nmean_spacing_error：43.30891，col=333，count=3637，rate=9.15590%，jerk=0.01302，miniumu_ttc=960.23254\nEpoch: 4| Train Loss: 0.1453089\nmean_spacing_error：42.85191，col=285，count=3637，rate=7.83613%，jerk=0.02752，miniumu_ttc=1662.83557\nEpoch: 5| Train Loss: 0.1445511\nmean_spacing_error：42.30104，col=253，count=3637，rate=6.95628%，jerk=0.04491，miniumu_ttc=54.96389\n   \n29/500. loss: 0.00321886253853639\n29/500. mserror: 75.93679809570312  col: 362  count: 3637  jerk: 0.003076269757002592  ttc: 151.91748046875\nEpoch: 1| Train Loss: 0.1534004\nmean_spacing_error：100.45361，col=295，count=3637，rate=8.11108%，jerk=0.00022，miniumu_ttc=139.54338\nEpoch: 2| Train Loss: 0.1529373\nmean_spacing_error：54.75948，col=405，count=3637，rate=11.13555%，jerk=0.00586，miniumu_ttc=134.88820\nEpoch: 3| Train Loss: 0.1466438\nmean_spacing_error：45.72026，col=292，count=3637，rate=8.02859%，jerk=0.01582，miniumu_ttc=98.35896\nEpoch: 4| Train Loss: 0.1440902\nmean_spacing_error：38.47937，col=279，count=3637，rate=7.67116%，jerk=0.05660，miniumu_ttc=228.58119\nEpoch: 5| Train Loss: 0.1382440\nmean_spacing_error：58.31510，col=263，count=3637，rate=7.23123%，jerk=0.05273，miniumu_ttc=123.38338\n   \n30/500. loss: 0.004178135966261228\n30/500. mserror: 80.90946197509766  col: 341  count: 3637  jerk: 0.0022345345932990313  ttc: 129.78396606445312\nEpoch: 1| Train Loss: 0.1535680\nmean_spacing_error：99.31557，col=295，count=3637，rate=8.11108%，jerk=0.00047，miniumu_ttc=135.51530\nEpoch: 2| Train Loss: 0.1523341\nmean_spacing_error：84.25363，col=358，count=3637，rate=9.84328%，jerk=0.00075，miniumu_ttc=140.05878\nEpoch: 3| Train Loss: 0.1509489\nmean_spacing_error：83.53442，col=311，count=3637，rate=8.55100%，jerk=0.00236，miniumu_ttc=92.85617\nEpoch: 4| Train Loss: 0.1490484\nmean_spacing_error：53.54178，col=315，count=3637，rate=8.66098%，jerk=0.01210，miniumu_ttc=148.90192\nEpoch: 5| Train Loss: 0.1455201\nmean_spacing_error：83.40806，col=605，count=3637，rate=16.63459%，jerk=0.04474，miniumu_ttc=42.87903\n   \n31/500. loss: 0.0028827162459492683\n31/500. mserror: 70.13404083251953  col: 343  count: 3637  jerk: 0.004504697863012552  ttc: 161.99192810058594\nEpoch: 1| Train Loss: 0.1538134\nmean_spacing_error：99.61141，col=288，count=3637，rate=7.91861%，jerk=0.00115，miniumu_ttc=111.47665\nEpoch: 2| Train Loss: 0.1517306\nmean_spacing_error：74.81141，col=356，count=3637，rate=9.78829%，jerk=0.00280，miniumu_ttc=104.68356\nEpoch: 3| Train Loss: 0.1501706\nmean_spacing_error：59.95894，col=319，count=3637，rate=8.77097%，jerk=0.00735，miniumu_ttc=94.69952\nEpoch: 4| Train Loss: 0.1476340\nmean_spacing_error：34.75521，col=408，count=3637，rate=11.21804%，jerk=0.02837，miniumu_ttc=102.17580\nEpoch: 5| Train Loss: 0.1440748\nmean_spacing_error：83.56663，col=436，count=3637，rate=11.98790%，jerk=0.10125，miniumu_ttc=24.09686\n   \n32/500. loss: 0.002576524702211221\n32/500. mserror: 68.24241638183594  col: 412  count: 3637  jerk: 0.008644243702292442  ttc: 77.72881317138672\nEpoch: 1| Train Loss: 0.1512751\nmean_spacing_error：86.95234，col=279，count=3637，rate=7.67116%，jerk=0.00462，miniumu_ttc=213.02223\nEpoch: 2| Train Loss: 0.1609215\nmean_spacing_error：58.11436，col=502，count=3637，rate=13.80258%，jerk=0.06010，miniumu_ttc=43.12289\nEpoch: 3| Train Loss: 0.1373941\nmean_spacing_error：41.94718，col=204，count=3637，rate=5.60902%，jerk=0.02770，miniumu_ttc=152.28072\nEpoch: 4| Train Loss: 0.1342895\nmean_spacing_error：29.90165，col=193，count=3637，rate=5.30657%，jerk=0.03991，miniumu_ttc=140.59929\nEpoch: 5| Train Loss: 0.1284536\nmean_spacing_error：27.38161，col=95，count=3637，rate=2.61204%，jerk=0.06986，miniumu_ttc=72.96951\n   \n33/500. loss: 0.002557754827042421\n33/500. mserror: 73.26528930664062  col: 309  count: 3637  jerk: 0.004417503252625465  ttc: 106.31784057617188\nEpoch: 1| Train Loss: 0.1543494\nmean_spacing_error：103.75248，col=288，count=3637，rate=7.91861%，jerk=0.00063，miniumu_ttc=137.85127\nEpoch: 2| Train Loss: 0.1516500\nmean_spacing_error：82.88104，col=345，count=3637，rate=9.48584%，jerk=0.00146，miniumu_ttc=88.76954\nEpoch: 3| Train Loss: 0.1496551\nmean_spacing_error：78.78715，col=306，count=3637，rate=8.41353%，jerk=0.00333，miniumu_ttc=133.90013\nEpoch: 4| Train Loss: 0.1452443\nmean_spacing_error：58.85835，col=245，count=3637，rate=6.73632%，jerk=0.01119，miniumu_ttc=99.09615\nEpoch: 5| Train Loss: 0.1306700\nmean_spacing_error：43.07108，col=138，count=3637，rate=3.79434%，jerk=0.03711，miniumu_ttc=74.56319\n   \n34/500. loss: 0.004445925354957581\n34/500. mserror: 88.7467269897461  col: 287  count: 3637  jerk: 0.003064086427912116  ttc: 747.9163208007812\nEpoch: 1| Train Loss: 0.1523980\nmean_spacing_error：84.44227，col=324，count=3637，rate=8.90844%，jerk=0.00166，miniumu_ttc=118.95368\nEpoch: 2| Train Loss: 0.1505514\nmean_spacing_error：90.96824，col=272，count=3637，rate=7.47869%，jerk=0.00293，miniumu_ttc=127.96126\nEpoch: 3| Train Loss: 0.1472619\nmean_spacing_error：83.65337，col=257，count=3637，rate=7.06626%，jerk=0.00479，miniumu_ttc=329.36087\nEpoch: 4| Train Loss: 0.1387647\nmean_spacing_error：67.55582，col=78，count=3637，rate=2.14462%，jerk=0.02039，miniumu_ttc=118.12071\nEpoch: 5| Train Loss: 0.1263411\nmean_spacing_error：65.10314，col=307，count=3637，rate=8.44102%，jerk=0.02011，miniumu_ttc=128.72438\n   \n35/500. loss: 0.0024173809215426445\n35/500. mserror: 94.51864624023438  col: 280  count: 3637  jerk: 0.00262144161388278  ttc: 97.98469543457031\nEpoch: 1| Train Loss: 0.1531363\nmean_spacing_error：84.95043，col=325，count=3637，rate=8.93594%，jerk=0.00150，miniumu_ttc=91.92732\nEpoch: 2| Train Loss: 0.1505218\nmean_spacing_error：89.37307，col=283，count=3637，rate=7.78114%，jerk=0.00311，miniumu_ttc=106.49889\nEpoch: 3| Train Loss: 0.1475569\nmean_spacing_error：85.62508，col=209，count=3637，rate=5.74649%，jerk=0.00607，miniumu_ttc=88.58833\nEpoch: 4| Train Loss: 0.1391129\nmean_spacing_error：66.86388，col=102，count=3637，rate=2.80451%，jerk=0.01859，miniumu_ttc=93.25542\nEpoch: 5| Train Loss: 0.1262882\nmean_spacing_error：76.31236，col=21，count=3637，rate=0.57740%，jerk=0.03437，miniumu_ttc=78.18887\n   \n36/500. loss: 0.004045011475682259\n36/500. mserror: 92.0654067993164  col: 279  count: 3637  jerk: 0.002962361555546522  ttc: 93.5755844116211\nEpoch: 1| Train Loss: 0.1522966\nmean_spacing_error：84.11667，col=324，count=3637，rate=8.90844%，jerk=0.00175，miniumu_ttc=137.15237\nEpoch: 2| Train Loss: 0.1516696\nmean_spacing_error：90.58380，col=238，count=3637，rate=6.54385%，jerk=0.00430，miniumu_ttc=118.59355\nEpoch: 3| Train Loss: 0.1454199\nmean_spacing_error：94.35172，col=103，count=3637，rate=2.83200%，jerk=0.01606，miniumu_ttc=275.70346\nEpoch: 4| Train Loss: 0.1337569\nmean_spacing_error：78.61076，col=123，count=3637，rate=3.38191%，jerk=0.01551，miniumu_ttc=117.41612\nEpoch: 5| Train Loss: 0.1268308\nmean_spacing_error：73.44790，col=118，count=3637，rate=3.24443%，jerk=0.02176，miniumu_ttc=94.34594\n   \n37/500. loss: 0.002989154619475206\n37/500. mserror: 83.95724487304688  col: 289  count: 3637  jerk: 0.0031654774211347103  ttc: 156.22726440429688\nEpoch: 1| Train Loss: 0.1509658\nmean_spacing_error：83.63383，col=300，count=3637，rate=8.24856%，jerk=0.00273，miniumu_ttc=411.92349\nEpoch: 2| Train Loss: 0.1498149\nmean_spacing_error：88.17590，col=150，count=3637，rate=4.12428%，jerk=0.00936，miniumu_ttc=182.26897\nEpoch: 3| Train Loss: 0.1435343\nmean_spacing_error：76.01310，col=199，count=3637，rate=5.47154%，jerk=0.01043，miniumu_ttc=99.83004\nEpoch: 4| Train Loss: 0.1290782\nmean_spacing_error：62.52157，col=156，count=3637，rate=4.28925%，jerk=0.01801，miniumu_ttc=69.34026\nEpoch: 5| Train Loss: 0.1230174\nmean_spacing_error：49.00718，col=149，count=3637，rate=4.09678%，jerk=0.02924，miniumu_ttc=67.74425\n   \n38/500. loss: 0.0025710522507627807\n38/500. mserror: 76.77284240722656  col: 315  count: 3637  jerk: 0.002959060249850154  ttc: 113.26288604736328\nEpoch: 1| Train Loss: 0.1502475\nmean_spacing_error：83.11616，col=312，count=3637，rate=8.57850%，jerk=0.00242，miniumu_ttc=247.81401\nEpoch: 2| Train Loss: 0.1494138\nmean_spacing_error：78.33868，col=167，count=3637，rate=4.59170%，jerk=0.00955，miniumu_ttc=103.11589\nEpoch: 3| Train Loss: 0.1364465\nmean_spacing_error：74.82788，col=104，count=3637，rate=2.85950%，jerk=0.02019，miniumu_ttc=100.45607\nEpoch: 4| Train Loss: 0.1229405\nmean_spacing_error：69.47222，col=115，count=3637，rate=3.16195%，jerk=0.03275，miniumu_ttc=726.71143\nEpoch: 5| Train Loss: 0.1218351\nmean_spacing_error：45.22374，col=99，count=3637，rate=2.72202%，jerk=0.03344，miniumu_ttc=67.75886\n   \n39/500. loss: 0.00331171415746212\n39/500. mserror: 74.07515716552734  col: 330  count: 3637  jerk: 0.003034336259588599  ttc: 95.9425048828125\nEpoch: 1| Train Loss: 0.1502709\nmean_spacing_error：76.67480，col=326，count=3637，rate=8.96343%，jerk=0.00328，miniumu_ttc=101.09412\nEpoch: 2| Train Loss: 0.1472457\nmean_spacing_error：57.47651，col=176，count=3637，rate=4.83915%，jerk=0.01513，miniumu_ttc=301.73630\nEpoch: 3| Train Loss: 0.1345119\nmean_spacing_error：41.47879，col=225，count=3637，rate=6.18642%，jerk=0.02830，miniumu_ttc=76.80132\nEpoch: 4| Train Loss: 0.1265679\nmean_spacing_error：36.13990，col=93，count=3637，rate=2.55705%，jerk=0.05520，miniumu_ttc=113.43834\nEpoch: 5| Train Loss: 0.1126366\nmean_spacing_error：26.80572，col=66，count=3637，rate=1.81468%，jerk=0.06042，miniumu_ttc=75.67647\n   \n40/500. loss: 0.0020421138033270836\n40/500. mserror: 72.54841613769531  col: 327  count: 3637  jerk: 0.0034728222526609898  ttc: 101.7700424194336\nEpoch: 1| Train Loss: 0.1499291\nmean_spacing_error：75.24237，col=332，count=3637，rate=9.12840%，jerk=0.00383，miniumu_ttc=103.57649\nEpoch: 2| Train Loss: 0.1534727\nmean_spacing_error：48.33587，col=348，count=3637，rate=9.56833%，jerk=0.01011，miniumu_ttc=76.36801\nEpoch: 3| Train Loss: 0.1391299\nmean_spacing_error：36.31036，col=175，count=3637，rate=4.81166%，jerk=0.03093，miniumu_ttc=539.53943\nEpoch: 4| Train Loss: 0.1330079\nmean_spacing_error：38.71288，col=307，count=3637，rate=8.44102%，jerk=0.04301，miniumu_ttc=47.89498\nEpoch: 5| Train Loss: 0.1229039\nmean_spacing_error：39.47184，col=338，count=3637，rate=9.29337%，jerk=0.07490，miniumu_ttc=180.34447\n   \n41/500. loss: 0.0030879070982337\n41/500. mserror: 70.1072769165039  col: 315  count: 3637  jerk: 0.004543699789792299  ttc: 87.20397186279297\nEpoch: 1| Train Loss: 0.1507251\nmean_spacing_error：82.12086，col=284，count=3637，rate=7.80863%，jerk=0.00333，miniumu_ttc=283.27408\nEpoch: 2| Train Loss: 0.1429260\nmean_spacing_error：65.76689，col=105，count=3637，rate=2.88699%，jerk=0.01861，miniumu_ttc=120.69521\nEpoch: 3| Train Loss: 0.1335969\nmean_spacing_error：57.92402，col=115，count=3637，rate=3.16195%，jerk=0.02043，miniumu_ttc=85.42035\nEpoch: 4| Train Loss: 0.1236347\nmean_spacing_error：47.93155，col=93，count=3637，rate=2.55705%，jerk=0.03122，miniumu_ttc=110.24922\nEpoch: 5| Train Loss: 0.1178702\nmean_spacing_error：39.05317，col=118，count=3637，rate=3.24443%，jerk=0.03229，miniumu_ttc=106.16514\n   \n42/500. loss: 0.003032894184192022\n42/500. mserror: 67.6624526977539  col: 293  count: 3637  jerk: 0.0061064050532877445  ttc: 76.43575286865234\nEpoch: 1| Train Loss: 0.1504498\nmean_spacing_error：85.71410，col=251，count=3637，rate=6.90129%，jerk=0.00482，miniumu_ttc=90.47839\nEpoch: 2| Train Loss: 0.1423718\nmean_spacing_error：84.44269，col=54，count=3637，rate=1.48474%，jerk=0.02052，miniumu_ttc=124.09344\nEpoch: 3| Train Loss: 0.1316032\nmean_spacing_error：78.87687，col=141，count=3637，rate=3.87682%，jerk=0.01170，miniumu_ttc=103.75390\nEpoch: 4| Train Loss: 0.1276139\nmean_spacing_error：77.36591，col=425，count=3637，rate=11.68546%，jerk=0.00766，miniumu_ttc=57.04075\nEpoch: 5| Train Loss: 0.1250665\nmean_spacing_error：77.31038，col=123，count=3637，rate=3.38191%，jerk=0.01759，miniumu_ttc=110.20380\n   \n43/500. loss: 0.003946265205740929\n43/500. mserror: 70.98152923583984  col: 226  count: 3637  jerk: 0.008045045658946037  ttc: 141.11810302734375\nEpoch: 1| Train Loss: 0.1525776\nmean_spacing_error：86.88976，col=276，count=3637，rate=7.58867%，jerk=0.00408，miniumu_ttc=176.95404\nEpoch: 2| Train Loss: 0.1450824\nmean_spacing_error：88.08922，col=137，count=3637，rate=3.76684%，jerk=0.00970，miniumu_ttc=102.46744\nEpoch: 3| Train Loss: 0.1373333\nmean_spacing_error：78.86701，col=60，count=3637，rate=1.64971%，jerk=0.02035，miniumu_ttc=116.51707\nEpoch: 4| Train Loss: 0.1284714\nmean_spacing_error：74.83984，col=82，count=3637，rate=2.25461%，jerk=0.02075，miniumu_ttc=101.47533\nEpoch: 5| Train Loss: 0.1252598\nmean_spacing_error：88.14140，col=40，count=3637，rate=1.09981%，jerk=0.02747，miniumu_ttc=90.90925\n   \n44/500. loss: 0.0021658016679187617\n44/500. mserror: 71.59859466552734  col: 203  count: 3637  jerk: 0.009938279166817665  ttc: 122.37863159179688\nEpoch: 1| Train Loss: 0.1555490\nmean_spacing_error：126.37312，col=171，count=3637，rate=4.70168%，jerk=0.00436，miniumu_ttc=109.43980\nEpoch: 2| Train Loss: 0.1491848\nmean_spacing_error：79.99571，col=284，count=3637，rate=7.80863%，jerk=0.00405，miniumu_ttc=136.13048\nEpoch: 3| Train Loss: 0.1390593\nmean_spacing_error：77.91895，col=87，count=3637，rate=2.39208%，jerk=0.01895，miniumu_ttc=107.91434\nEpoch: 4| Train Loss: 0.1287328\nmean_spacing_error：69.80122，col=123，count=3637，rate=3.38191%，jerk=0.01692，miniumu_ttc=88.62584\nEpoch: 5| Train Loss: 0.1240773\nmean_spacing_error：63.40550，col=128，count=3637，rate=3.51938%，jerk=0.02469，miniumu_ttc=103.51534\n   \n45/500. loss: 0.00272444107880195\n45/500. mserror: 60.84756088256836  col: 208  count: 3637  jerk: 0.011811522766947746  ttc: 102.53515625\nEpoch: 1| Train Loss: 0.1544617\nmean_spacing_error：129.00983，col=169，count=3637，rate=4.64669%，jerk=0.00467，miniumu_ttc=121.48515\nEpoch: 2| Train Loss: 0.1499565\nmean_spacing_error：82.39219，col=316，count=3637，rate=8.68848%，jerk=0.00324，miniumu_ttc=149.70374\nEpoch: 3| Train Loss: 0.1413370\nmean_spacing_error：88.09019，col=118，count=3637，rate=3.24443%，jerk=0.01115，miniumu_ttc=88.88004\nEpoch: 4| Train Loss: 0.1353975\nmean_spacing_error：69.50732，col=96，count=3637，rate=2.63954%，jerk=0.01768，miniumu_ttc=87.61097\nEpoch: 5| Train Loss: 0.1251639\nmean_spacing_error：71.86434，col=36，count=3637，rate=0.98983%，jerk=0.03335，miniumu_ttc=73.13857\n   \n46/500. loss: 0.003907659401496251\n46/500. mserror: 57.144657135009766  col: 217  count: 3637  jerk: 0.013955539092421532  ttc: 171.8362274169922\nEpoch: 1| Train Loss: 0.1542691\nmean_spacing_error：135.05807，col=121，count=3637，rate=3.32692%，jerk=0.00802，miniumu_ttc=181.40192\nEpoch: 2| Train Loss: 0.1472560\nmean_spacing_error：83.76700，col=254，count=3637，rate=6.98378%，jerk=0.00557，miniumu_ttc=75.05116\nEpoch: 3| Train Loss: 0.1343590\nmean_spacing_error：76.46722，col=93，count=3637，rate=2.55705%，jerk=0.02490，miniumu_ttc=172.13350\nEpoch: 4| Train Loss: 0.1286601\nmean_spacing_error：68.74984，col=96，count=3637，rate=2.63954%，jerk=0.01903，miniumu_ttc=117.10099\nEpoch: 5| Train Loss: 0.1241146\nmean_spacing_error：72.84143，col=80，count=3637，rate=2.19962%，jerk=0.02549，miniumu_ttc=125.49506\n   \n47/500. loss: 0.003969701627890269\n47/500. mserror: 54.023372650146484  col: 192  count: 3637  jerk: 0.016893574967980385  ttc: 63.795841217041016\nEpoch: 1| Train Loss: 0.1545553\nmean_spacing_error：121.81144，col=162，count=3637，rate=4.45422%，jerk=0.00617，miniumu_ttc=383.36069\nEpoch: 2| Train Loss: 0.1488228\nmean_spacing_error：80.02396，col=302，count=3637，rate=8.30355%，jerk=0.00392，miniumu_ttc=91.54432\nEpoch: 3| Train Loss: 0.1393982\nmean_spacing_error：85.04627，col=135，count=3637，rate=3.71185%，jerk=0.01038，miniumu_ttc=121.63499\nEpoch: 4| Train Loss: 0.1347300\nmean_spacing_error：74.67804，col=129，count=3637，rate=3.54688%，jerk=0.01357，miniumu_ttc=303.99081\nEpoch: 5| Train Loss: 0.1287776\nmean_spacing_error：70.61585，col=160，count=3637，rate=4.39923%，jerk=0.01494，miniumu_ttc=73.92185\n   \n48/500. loss: 0.0028591115648547807\n48/500. mserror: 50.78409957885742  col: 161  count: 3637  jerk: 0.020687604323029518  ttc: 104.08137512207031\nEpoch: 1| Train Loss: 0.1558428\nmean_spacing_error：120.95435，col=145，count=3637，rate=3.98680%，jerk=0.00762，miniumu_ttc=184.43176\nEpoch: 2| Train Loss: 0.1470375\nmean_spacing_error：76.72838，col=283，count=3637，rate=7.78114%，jerk=0.00517，miniumu_ttc=192.89857\nEpoch: 3| Train Loss: 0.1363832\nmean_spacing_error：77.28969，col=107，count=3637，rate=2.94199%，jerk=0.01446，miniumu_ttc=80.26495\nEpoch: 4| Train Loss: 0.1320714\nmean_spacing_error：70.57167，col=79，count=3637，rate=2.17212%，jerk=0.01987，miniumu_ttc=185.04237\nEpoch: 5| Train Loss: 0.1284890\nmean_spacing_error：72.76685，col=181，count=3637，rate=4.97663%，jerk=0.01369，miniumu_ttc=93.49479\n   \n49/500. loss: 0.0037970508759220443\n49/500. mserror: 46.064239501953125  col: 122  count: 3637  jerk: 0.02852196805179119  ttc: 79.84973907470703\nEpoch: 1| Train Loss: 0.1621880\nmean_spacing_error：117.32661，col=127，count=3637，rate=3.49189%，jerk=0.00943，miniumu_ttc=130.92815\nEpoch: 2| Train Loss: 0.1451858\nmean_spacing_error：76.40243，col=267，count=3637，rate=7.34122%，jerk=0.00616，miniumu_ttc=173.16818\nEpoch: 3| Train Loss: 0.1347413\nmean_spacing_error：76.96751，col=104，count=3637，rate=2.85950%，jerk=0.01455，miniumu_ttc=308.79666\nEpoch: 4| Train Loss: 0.1296382\nmean_spacing_error：70.23351，col=126，count=3637，rate=3.46439%，jerk=0.01768，miniumu_ttc=85.73679\nEpoch: 5| Train Loss: 0.1292928\nmean_spacing_error：59.83448，col=86，count=3637，rate=2.36459%，jerk=0.02496，miniumu_ttc=68.76526\n   \n50/500. loss: 0.0035931256910165152\n50/500. mserror: 51.33969497680664  col: 77  count: 3637  jerk: 0.03916085138916969  ttc: 131.19358825683594\nEpoch: 1| Train Loss: 0.1759617\nmean_spacing_error：114.11243，col=105，count=3637，rate=2.88699%，jerk=0.01331，miniumu_ttc=121.14376\nEpoch: 2| Train Loss: 0.1404592\nmean_spacing_error：66.98067，col=346，count=3637，rate=9.51334%，jerk=0.00901，miniumu_ttc=87.63435\nEpoch: 3| Train Loss: 0.1307388\nmean_spacing_error：54.35859，col=78，count=3637，rate=2.14462%，jerk=0.02691，miniumu_ttc=69.49150\nEpoch: 4| Train Loss: 0.1292070\nmean_spacing_error：58.98960，col=159，count=3637，rate=4.37173%，jerk=0.02121，miniumu_ttc=204.74840\nEpoch: 5| Train Loss: 0.1220127\nmean_spacing_error：75.66677，col=72，count=3637，rate=1.97965%，jerk=0.02097，miniumu_ttc=169.95758\n   \n51/500. loss: 0.0033144718036055565\n51/500. mserror: 46.39248275756836  col: 145  count: 3637  jerk: 0.03861693665385246  ttc: 538.3751220703125\nEpoch: 1| Train Loss: 0.1716746\nmean_spacing_error：106.25961，col=106，count=3637，rate=2.91449%，jerk=0.01331，miniumu_ttc=118.72169\nEpoch: 2| Train Loss: 0.1365118\nmean_spacing_error：59.74098，col=175，count=3637，rate=4.81166%，jerk=0.01512，miniumu_ttc=80.82700\nEpoch: 3| Train Loss: 0.1276673\nmean_spacing_error：58.43862，col=127，count=3637，rate=3.49189%，jerk=0.02046，miniumu_ttc=115.97770\nEpoch: 4| Train Loss: 0.1254373\nmean_spacing_error：64.36872，col=75，count=3637，rate=2.06214%，jerk=0.02840，miniumu_ttc=120.32495\nEpoch: 5| Train Loss: 0.1184002\nmean_spacing_error：78.82876，col=31，count=3637，rate=0.85235%，jerk=0.03026，miniumu_ttc=120.98629\n   \n52/500. loss: 0.0021882842605312667\n52/500. mserror: 48.541770935058594  col: 182  count: 3637  jerk: 0.03459133207798004  ttc: 67.13408660888672\nEpoch: 1| Train Loss: 0.1698160\nmean_spacing_error：113.47961，col=105，count=3637，rate=2.88699%，jerk=0.01262，miniumu_ttc=294.07440\nEpoch: 2| Train Loss: 0.1370420\nmean_spacing_error：68.04179，col=230，count=3637，rate=6.32389%，jerk=0.01064，miniumu_ttc=83.94726\nEpoch: 3| Train Loss: 0.1308982\nmean_spacing_error：56.59648，col=73，count=3637，rate=2.00715%，jerk=0.02833，miniumu_ttc=126.76203\nEpoch: 4| Train Loss: 0.1264181\nmean_spacing_error：52.11577，col=126，count=3637，rate=3.46439%，jerk=0.02575，miniumu_ttc=84.99698\nEpoch: 5| Train Loss: 0.1199446\nmean_spacing_error：56.31674，col=51，count=3637，rate=1.40225%，jerk=0.02997，miniumu_ttc=123.79629\n   \n53/500. loss: 0.0035582507650057473\n53/500. mserror: 47.55051803588867  col: 114  count: 3637  jerk: 0.035244978964328766  ttc: 57.88154983520508\nEpoch: 1| Train Loss: 0.1727250\nmean_spacing_error：111.97270，col=106，count=3637，rate=2.91449%，jerk=0.01201，miniumu_ttc=162.44798\nEpoch: 2| Train Loss: 0.1351455\nmean_spacing_error：68.41648，col=127，count=3637，rate=3.49189%，jerk=0.01531，miniumu_ttc=116.29649\nEpoch: 3| Train Loss: 0.1271797\nmean_spacing_error：59.58206，col=118，count=3637，rate=3.24443%，jerk=0.02256，miniumu_ttc=65.30013\nEpoch: 4| Train Loss: 0.1264344\nmean_spacing_error：56.72854，col=111，count=3637，rate=3.05197%，jerk=0.02386，miniumu_ttc=73.27570\nEpoch: 5| Train Loss: 0.1174896\nmean_spacing_error：72.16238，col=89，count=3637，rate=2.44707%，jerk=0.01911，miniumu_ttc=89.69836\n   \n54/500. loss: 0.00211419848104318\n54/500. mserror: 57.55512619018555  col: 67  count: 3637  jerk: 0.04001368582248688  ttc: 70.92349243164062\nEpoch: 1| Train Loss: 0.1833173\nmean_spacing_error：148.03273，col=66，count=3637，rate=1.81468%，jerk=0.01656，miniumu_ttc=102.17375\nEpoch: 2| Train Loss: 0.1441156\nmean_spacing_error：73.32082，col=221，count=3637，rate=6.07644%，jerk=0.00934，miniumu_ttc=375.12326\nEpoch: 3| Train Loss: 0.1322424\nmean_spacing_error：72.79613，col=71，count=3637，rate=1.95216%，jerk=0.02409，miniumu_ttc=156.19350\nEpoch: 4| Train Loss: 0.1335847\nmean_spacing_error：69.88148，col=119，count=3637，rate=3.27193%，jerk=0.01531，miniumu_ttc=105.51482\nEpoch: 5| Train Loss: 0.1250105\nmean_spacing_error：73.77391，col=44，count=3637，rate=1.20979%，jerk=0.02388，miniumu_ttc=182.43614\n   \n55/500. loss: 0.002292043219010035\n55/500. mserror: 52.24038314819336  col: 89  count: 3637  jerk: 0.03626560792326927  ttc: 67.64595794677734\nEpoch: 1| Train Loss: 0.1721913\nmean_spacing_error：120.92731，col=84，count=3637，rate=2.30960%，jerk=0.01410，miniumu_ttc=81.04434\nEpoch: 2| Train Loss: 0.1345904\nmean_spacing_error：70.41281，col=139，count=3637，rate=3.82183%，jerk=0.01504，miniumu_ttc=78.31742\nEpoch: 3| Train Loss: 0.1264885\nmean_spacing_error：61.25758，col=74，count=3637，rate=2.03464%，jerk=0.02494，miniumu_ttc=142.95743\nEpoch: 4| Train Loss: 0.1188479\nmean_spacing_error：60.45088，col=35，count=3637，rate=0.96233%，jerk=0.02828，miniumu_ttc=80.40566\nEpoch: 5| Train Loss: 0.1276438\nmean_spacing_error：66.43373，col=155，count=3637，rate=4.26175%，jerk=0.01635，miniumu_ttc=122.00223\n   \n56/500. loss: 0.0021897529562314353\n56/500. mserror: 50.833763122558594  col: 128  count: 3637  jerk: 0.030159883201122284  ttc: 68.52078247070312\nEpoch: 1| Train Loss: 0.1638863\nmean_spacing_error：115.99631，col=99，count=3637，rate=2.72202%，jerk=0.01423，miniumu_ttc=111.36246\nEpoch: 2| Train Loss: 0.1314470\nmean_spacing_error：68.20724，col=94，count=3637，rate=2.58455%，jerk=0.01862，miniumu_ttc=141.34941\nEpoch: 3| Train Loss: 0.1235276\nmean_spacing_error：61.76332，col=144，count=3637，rate=3.95931%，jerk=0.01958，miniumu_ttc=93.09670\nEpoch: 4| Train Loss: 0.1168627\nmean_spacing_error：56.27463，col=54，count=3637，rate=1.48474%，jerk=0.02729，miniumu_ttc=77.62228\nEpoch: 5| Train Loss: 0.1275244\nmean_spacing_error：61.73321，col=107，count=3637，rate=2.94199%，jerk=0.01945，miniumu_ttc=89.55198\n   \n57/500. loss: 0.0038497066125273705\n57/500. mserror: 50.40228271484375  col: 144  count: 3637  jerk: 0.03054259531199932  ttc: 69.42203521728516\nEpoch: 1| Train Loss: 0.1612330\nmean_spacing_error：118.54721，col=98，count=3637，rate=2.69453%，jerk=0.01260，miniumu_ttc=81.60621\nEpoch: 2| Train Loss: 0.1334504\nmean_spacing_error：72.47297，col=121，count=3637，rate=3.32692%，jerk=0.01493，miniumu_ttc=135.56662\nEpoch: 3| Train Loss: 0.1217427\nmean_spacing_error：60.66418，col=97，count=3637，rate=2.66703%，jerk=0.02307，miniumu_ttc=87.08930\nEpoch: 4| Train Loss: 0.1175522\nmean_spacing_error：64.33984，col=55，count=3637，rate=1.51224%，jerk=0.02439，miniumu_ttc=81.51795\nEpoch: 5| Train Loss: 0.1268913\nmean_spacing_error：70.30257，col=179，count=3637，rate=4.92164%，jerk=0.01539，miniumu_ttc=275.60501\n   \n58/500. loss: 0.0026268468548854194\n58/500. mserror: 50.31437683105469  col: 155  count: 3637  jerk: 0.03128548339009285  ttc: 112.84695434570312\nEpoch: 1| Train Loss: 0.1628483\nmean_spacing_error：122.50794，col=90，count=3637，rate=2.47457%，jerk=0.01123，miniumu_ttc=94.71845\nEpoch: 2| Train Loss: 0.1353970\nmean_spacing_error：76.46762，col=144，count=3637，rate=3.95931%，jerk=0.01261，miniumu_ttc=108.10487\nEpoch: 3| Train Loss: 0.1236435\nmean_spacing_error：63.09845，col=98，count=3637，rate=2.69453%，jerk=0.02181，miniumu_ttc=80.10799\nEpoch: 4| Train Loss: 0.1186374\nmean_spacing_error：62.15788，col=56，count=3637，rate=1.53973%，jerk=0.02821，miniumu_ttc=86.35677\nEpoch: 5| Train Loss: 0.1172244\nmean_spacing_error：65.66537，col=85，count=3637，rate=2.33709%，jerk=0.02747，miniumu_ttc=81.75269\n   \n59/500. loss: 0.003749329907198747\n59/500. mserror: 47.428611755371094  col: 118  count: 3637  jerk: 0.03607858717441559  ttc: 82.44718933105469\nEpoch: 1| Train Loss: 0.1699148\nmean_spacing_error：131.61520，col=72，count=3637，rate=1.97965%，jerk=0.01213，miniumu_ttc=134.55807\nEpoch: 2| Train Loss: 0.1376559\nmean_spacing_error：75.15955，col=154，count=3637，rate=4.23426%，jerk=0.01194，miniumu_ttc=107.95680\nEpoch: 3| Train Loss: 0.1248848\nmean_spacing_error：65.25283，col=84，count=3637，rate=2.30960%，jerk=0.02037，miniumu_ttc=80.18330\nEpoch: 4| Train Loss: 0.1198878\nmean_spacing_error：64.97556，col=41，count=3637，rate=1.12730%，jerk=0.02539，miniumu_ttc=95.80672\nEpoch: 5| Train Loss: 0.1191982\nmean_spacing_error：68.74429，col=75，count=3637，rate=2.06214%，jerk=0.02220，miniumu_ttc=94.26417\n   \n60/500. loss: 0.003317865232626597\n60/500. mserror: 46.110069274902344  col: 58  count: 3637  jerk: 0.04695505276322365  ttc: 77.27132415771484\nEpoch: 1| Train Loss: 0.1755579\nmean_spacing_error：116.66380，col=82，count=3637，rate=2.25461%，jerk=0.01313，miniumu_ttc=112.59136\nEpoch: 2| Train Loss: 0.1380846\nmean_spacing_error：69.79912，col=129，count=3637，rate=3.54688%，jerk=0.01427，miniumu_ttc=128.98294\nEpoch: 3| Train Loss: 0.1281341\nmean_spacing_error：62.28162，col=91，count=3637，rate=2.50206%，jerk=0.02013，miniumu_ttc=91.44766\nEpoch: 4| Train Loss: 0.1193968\nmean_spacing_error：67.82788，col=86，count=3637，rate=2.36459%，jerk=0.02105，miniumu_ttc=89.64648\nEpoch: 5| Train Loss: 0.1332706\nmean_spacing_error：72.74670，col=65，count=3637，rate=1.78719%，jerk=0.01967，miniumu_ttc=125.58679\n   \n61/500. loss: 0.0022828769870102406\n61/500. mserror: 45.07323455810547  col: 37  count: 3637  jerk: 0.053341567516326904  ttc: 78.70695495605469\nEpoch: 1| Train Loss: 0.1785399\nmean_spacing_error：114.84796，col=75，count=3637，rate=2.06214%，jerk=0.01510，miniumu_ttc=144.88867\nEpoch: 2| Train Loss: 0.1446765\nmean_spacing_error：72.74336，col=209，count=3637，rate=5.74649%，jerk=0.01032，miniumu_ttc=91.40881\nEpoch: 3| Train Loss: 0.1289709\nmean_spacing_error：66.89309，col=70，count=3637，rate=1.92466%，jerk=0.02415，miniumu_ttc=79.47062\nEpoch: 4| Train Loss: 0.1353768\nmean_spacing_error：61.24255，col=96，count=3637，rate=2.63954%，jerk=0.02100，miniumu_ttc=69.50243\nEpoch: 5| Train Loss: 0.1216442\nmean_spacing_error：64.14681，col=51，count=3637，rate=1.40225%，jerk=0.02478，miniumu_ttc=226.60526\n   \n62/500. loss: 0.0038041993975639343\n62/500. mserror: 40.6983642578125  col: 50  count: 3637  jerk: 0.055933404713869095  ttc: 73.12564849853516\nEpoch: 1| Train Loss: 0.1902230\nmean_spacing_error：114.14180，col=69，count=3637，rate=1.89717%，jerk=0.01756，miniumu_ttc=126.48901\nEpoch: 2| Train Loss: 0.1455123\nmean_spacing_error：70.73583，col=198，count=3637，rate=5.44405%，jerk=0.01143，miniumu_ttc=72.45978\nEpoch: 3| Train Loss: 0.1263705\nmean_spacing_error：57.39997，col=98，count=3637，rate=2.69453%，jerk=0.02241，miniumu_ttc=93.07013\nEpoch: 4| Train Loss: 0.1223621\nmean_spacing_error：47.17982，col=82，count=3637，rate=2.25461%，jerk=0.03056，miniumu_ttc=57.16385\nEpoch: 5| Train Loss: 0.1261698\nmean_spacing_error：47.12490，col=65，count=3637，rate=1.78719%，jerk=0.03085，miniumu_ttc=63.62006\n   \n63/500. loss: 0.003635251894593239\n63/500. mserror: 41.28016662597656  col: 75  count: 3637  jerk: 0.051126714795827866  ttc: 37.46453094482422\nEpoch: 1| Train Loss: 0.1946285\nmean_spacing_error：97.00992，col=92，count=3637，rate=2.52956%，jerk=0.01434，miniumu_ttc=140.98035\nEpoch: 2| Train Loss: 0.1373223\nmean_spacing_error：65.67834，col=185，count=3637，rate=5.08661%，jerk=0.01400，miniumu_ttc=87.38033\nEpoch: 3| Train Loss: 0.1251179\nmean_spacing_error：66.94489，col=115，count=3637，rate=3.16195%，jerk=0.01732，miniumu_ttc=70.33894\nEpoch: 4| Train Loss: 0.1219280\nmean_spacing_error：63.77096，col=104，count=3637，rate=2.85950%，jerk=0.02027，miniumu_ttc=82.34354\nEpoch: 5| Train Loss: 0.1184532\nmean_spacing_error：60.10212，col=75，count=3637，rate=2.06214%，jerk=0.02689，miniumu_ttc=62.76544\n   \n64/500. loss: 0.003314133733510971\n64/500. mserror: 39.13681411743164  col: 60  count: 3637  jerk: 0.04810861870646477  ttc: 67.22215270996094\nEpoch: 1| Train Loss: 0.1928113\nmean_spacing_error：108.03517，col=73，count=3637，rate=2.00715%，jerk=0.01551，miniumu_ttc=373.18082\nEpoch: 2| Train Loss: 0.1353503\nmean_spacing_error：66.71078，col=178，count=3637，rate=4.89414%，jerk=0.01422，miniumu_ttc=64.04572\nEpoch: 3| Train Loss: 0.1277146\nmean_spacing_error：59.93311，col=91，count=3637，rate=2.50206%，jerk=0.02303，miniumu_ttc=65.31462\nEpoch: 4| Train Loss: 0.1209241\nmean_spacing_error：56.72793，col=66，count=3637，rate=1.81468%，jerk=0.02673，miniumu_ttc=86.17446\nEpoch: 5| Train Loss: 0.1173189\nmean_spacing_error：57.96495，col=48，count=3637，rate=1.31977%，jerk=0.02644，miniumu_ttc=136.56979\n   \n65/500. loss: 0.003718021015326182\n65/500. mserror: 40.34596252441406  col: 82  count: 3637  jerk: 0.0407969206571579  ttc: 53.160430908203125\nEpoch: 1| Train Loss: 0.1819147\nmean_spacing_error：133.97075，col=58，count=3637，rate=1.59472%，jerk=0.01684，miniumu_ttc=995.46277\nEpoch: 2| Train Loss: 0.1404302\nmean_spacing_error：70.52866，col=565，count=3637，rate=15.53478%，jerk=0.00218，miniumu_ttc=46.64431\nEpoch: 3| Train Loss: 0.1251225\nmean_spacing_error：59.07109，col=164，count=3637，rate=4.50921%，jerk=0.01945，miniumu_ttc=125.00069\nEpoch: 4| Train Loss: 0.1216679\nmean_spacing_error：59.03927，col=81，count=3637，rate=2.22711%，jerk=0.02506，miniumu_ttc=63.38790\nEpoch: 5| Train Loss: 0.1176637\nmean_spacing_error：65.03202，col=89，count=3637，rate=2.44707%，jerk=0.02562，miniumu_ttc=89.55515\n   \n66/500. loss: 0.003565241893132528\n66/500. mserror: 45.41195297241211  col: 118  count: 3637  jerk: 0.032623741775751114  ttc: 52.343318939208984\nEpoch: 1| Train Loss: 0.1759527\nmean_spacing_error：148.89722，col=55，count=3637，rate=1.51224%，jerk=0.02098，miniumu_ttc=111.12112\nEpoch: 2| Train Loss: 0.1443799\nmean_spacing_error：62.74659，col=457，count=3637，rate=12.56530%，jerk=0.00847，miniumu_ttc=87.58258\nEpoch: 3| Train Loss: 0.1255959\nmean_spacing_error：54.70660，col=194，count=3637，rate=5.33407%，jerk=0.01825，miniumu_ttc=64.96593\nEpoch: 4| Train Loss: 0.1193059\nmean_spacing_error：45.09050，col=41，count=3637，rate=1.12730%，jerk=0.03550，miniumu_ttc=126.89909\nEpoch: 5| Train Loss: 0.1173585\nmean_spacing_error：44.81219，col=93，count=3637，rate=2.55705%，jerk=0.03094，miniumu_ttc=96.14487\n   \n67/500. loss: 0.00202500664939483\n67/500. mserror: 44.454010009765625  col: 115  count: 3637  jerk: 0.029587818309664726  ttc: 56.270172119140625\nEpoch: 1| Train Loss: 0.1741479\nmean_spacing_error：131.23843，col=83，count=3637，rate=2.28210%，jerk=0.01311，miniumu_ttc=113.05918\nEpoch: 2| Train Loss: 0.1399619\nmean_spacing_error：69.32363，col=600，count=3637，rate=16.49711%，jerk=0.00522，miniumu_ttc=105.23430\nEpoch: 3| Train Loss: 0.1254533\nmean_spacing_error：45.77821，col=131，count=3637，rate=3.60187%，jerk=0.02615，miniumu_ttc=85.36882\nEpoch: 4| Train Loss: 0.1170876\nmean_spacing_error：36.08892，col=91，count=3637，rate=2.50206%，jerk=0.03747，miniumu_ttc=109.15752\nEpoch: 5| Train Loss: 0.1142847\nmean_spacing_error：42.48248，col=59，count=3637，rate=1.62222%，jerk=0.03493，miniumu_ttc=113.36601\n   \n68/500. loss: 0.002100233609477679\n68/500. mserror: 43.63017654418945  col: 88  count: 3637  jerk: 0.029867159202694893  ttc: 146.64195251464844\nEpoch: 1| Train Loss: 0.1757004\nmean_spacing_error：103.57162，col=157，count=3637，rate=4.31674%，jerk=0.00782，miniumu_ttc=246.54822\nEpoch: 2| Train Loss: 0.1330345\nmean_spacing_error：74.33985，col=235，count=3637，rate=6.46137%，jerk=0.01073，miniumu_ttc=116.48442\nEpoch: 3| Train Loss: 0.1244580\nmean_spacing_error：58.59227，col=134，count=3637，rate=3.68436%，jerk=0.02269，miniumu_ttc=123.92056\nEpoch: 4| Train Loss: 0.1173524\nmean_spacing_error：51.93096，col=28，count=3637，rate=0.76987%，jerk=0.03669，miniumu_ttc=71.66429\nEpoch: 5| Train Loss: 0.1217006\nmean_spacing_error：53.75066，col=92，count=3637，rate=2.52956%，jerk=0.02707，miniumu_ttc=146.72081\n   \n69/500. loss: 0.002032784124215444\n69/500. mserror: 44.507713317871094  col: 89  count: 3637  jerk: 0.029015224426984787  ttc: 84.41484832763672\nEpoch: 1| Train Loss: 0.1663762\nmean_spacing_error：225.09181，col=48，count=3637，rate=1.31977%，jerk=0.02521，miniumu_ttc=144.79333\nEpoch: 2| Train Loss: 0.1579723\nmean_spacing_error：60.21634，col=139，count=3637，rate=3.82183%，jerk=0.01495，miniumu_ttc=67.14331\nEpoch: 3| Train Loss: 0.1309662\nmean_spacing_error：53.65032，col=84，count=3637，rate=2.30960%，jerk=0.02192，miniumu_ttc=168.93123\nEpoch: 4| Train Loss: 0.1190827\nmean_spacing_error：42.49509，col=40，count=3637，rate=1.09981%，jerk=0.03436，miniumu_ttc=104.85059\nEpoch: 5| Train Loss: 0.1164780\nmean_spacing_error：44.03352，col=66，count=3637，rate=1.81468%，jerk=0.03700，miniumu_ttc=77.06644\n   \n70/500. loss: 0.003929259876410167\n70/500. mserror: 43.3633918762207  col: 83  count: 3637  jerk: 0.030394306406378746  ttc: 179.97119140625\nEpoch: 1| Train Loss: 0.1744982\nmean_spacing_error：188.83612，col=29，count=3637，rate=0.79736%，jerk=0.03036，miniumu_ttc=103.96609\nEpoch: 2| Train Loss: 0.1518571\nmean_spacing_error：45.04771，col=261，count=3637，rate=7.17624%，jerk=0.01878，miniumu_ttc=74.48534\nEpoch: 3| Train Loss: 0.1250891\nmean_spacing_error：39.14014，col=21，count=3637，rate=0.57740%，jerk=0.04469，miniumu_ttc=101.58842\nEpoch: 4| Train Loss: 0.1240521\nmean_spacing_error：29.81999，col=145，count=3637，rate=3.98680%，jerk=0.06029，miniumu_ttc=76.56154\nEpoch: 5| Train Loss: 0.1137785\nmean_spacing_error：28.15511，col=88，count=3637，rate=2.41958%，jerk=0.08247，miniumu_ttc=123.21739\n   \n71/500. loss: 0.0021761351575454078\n71/500. mserror: 41.019588470458984  col: 111  count: 3637  jerk: 0.030646758154034615  ttc: 85.60215759277344\nEpoch: 1| Train Loss: 0.1814238\nmean_spacing_error：193.64059，col=35，count=3637，rate=0.96233%，jerk=0.02635，miniumu_ttc=183.63608\nEpoch: 2| Train Loss: 0.1484882\nmean_spacing_error：56.49508，col=226，count=3637，rate=6.21391%，jerk=0.01538，miniumu_ttc=2009.34351\nEpoch: 3| Train Loss: 0.1268359\nmean_spacing_error：43.42747，col=34，count=3637，rate=0.93484%，jerk=0.03824，miniumu_ttc=88.86755\nEpoch: 4| Train Loss: 0.1195752\nmean_spacing_error：35.07698，col=116，count=3637，rate=3.18944%，jerk=0.04016，miniumu_ttc=98.09070\nEpoch: 5| Train Loss: 0.1130540\nmean_spacing_error：30.91172，col=70，count=3637，rate=1.92466%，jerk=0.04686，miniumu_ttc=80.90501\n   \n72/500. loss: 0.0024676574394106865\n72/500. mserror: 41.43697738647461  col: 140  count: 3637  jerk: 0.031343236565589905  ttc: 48.06175994873047\nEpoch: 1| Train Loss: 0.1894359\nmean_spacing_error：218.52896，col=48，count=3637，rate=1.31977%，jerk=0.02033，miniumu_ttc=110.81691\nEpoch: 2| Train Loss: 0.1563883\nmean_spacing_error：75.11957，col=438，count=3637，rate=12.04289%，jerk=0.00400，miniumu_ttc=70.88219\nEpoch: 3| Train Loss: 0.1373457\nmean_spacing_error：68.01894，col=105，count=3637，rate=2.88699%，jerk=0.01670，miniumu_ttc=106.98920\nEpoch: 4| Train Loss: 0.1311138\nmean_spacing_error：61.46925，col=197，count=3637，rate=5.41655%，jerk=0.01578，miniumu_ttc=460.95074\nEpoch: 5| Train Loss: 0.1234396\nmean_spacing_error：50.61165，col=158，count=3637，rate=4.34424%，jerk=0.02408，miniumu_ttc=65.05457\n   \n73/500. loss: 0.0033237350483735404\n73/500. mserror: 37.80166244506836  col: 97  count: 3637  jerk: 0.03665076196193695  ttc: 77.30755615234375\nEpoch: 1| Train Loss: 0.1793284\nmean_spacing_error：181.07944，col=48，count=3637，rate=1.31977%，jerk=0.02706，miniumu_ttc=128.27747\nEpoch: 2| Train Loss: 0.1481106\nmean_spacing_error：56.89648，col=175，count=3637，rate=4.81166%，jerk=0.01486，miniumu_ttc=147.85495\nEpoch: 3| Train Loss: 0.1259491\nmean_spacing_error：45.28094，col=90，count=3637，rate=2.47457%，jerk=0.02693，miniumu_ttc=76.14444\nEpoch: 4| Train Loss: 0.1205485\nmean_spacing_error：37.33999，col=55，count=3637，rate=1.51224%，jerk=0.04219，miniumu_ttc=95.91102\nEpoch: 5| Train Loss: 0.1169022\nmean_spacing_error：33.68426，col=63，count=3637，rate=1.73220%，jerk=0.04273，miniumu_ttc=793.91467\n   \n74/500. loss: 0.00205199575672547\n74/500. mserror: 38.429996490478516  col: 79  count: 3637  jerk: 0.04102948307991028  ttc: 185.31158447265625\nEpoch: 1| Train Loss: 0.1783692\nmean_spacing_error：165.73470，col=50，count=3637，rate=1.37476%，jerk=0.02994，miniumu_ttc=149.35835\nEpoch: 2| Train Loss: 0.1527819\nmean_spacing_error：43.05853，col=251，count=3637，rate=6.90129%，jerk=0.01939，miniumu_ttc=120.09402\nEpoch: 3| Train Loss: 0.1280434\nmean_spacing_error：38.96620，col=87，count=3637，rate=2.39208%，jerk=0.03267，miniumu_ttc=116.32418\nEpoch: 4| Train Loss: 0.1213059\nmean_spacing_error：36.88362，col=55，count=3637，rate=1.51224%，jerk=0.05677，miniumu_ttc=43.78418\nEpoch: 5| Train Loss: 0.1137807\nmean_spacing_error：46.28355，col=62，count=3637，rate=1.70470%，jerk=0.03378，miniumu_ttc=98.52984\n   \n75/500. loss: 0.0035900926838318505\n75/500. mserror: 35.887115478515625  col: 87  count: 3637  jerk: 0.04203170910477638  ttc: 113.76644897460938\nEpoch: 1| Train Loss: 0.1797495\nmean_spacing_error：185.12105，col=40，count=3637，rate=1.09981%，jerk=0.03000，miniumu_ttc=131.25516\nEpoch: 2| Train Loss: 0.1491476\nmean_spacing_error：43.56262，col=218，count=3637，rate=5.99395%，jerk=0.01935，miniumu_ttc=66.49136\nEpoch: 3| Train Loss: 0.1309109\nmean_spacing_error：38.31199，col=63，count=3637，rate=1.73220%，jerk=0.04081，miniumu_ttc=231.06575\nEpoch: 4| Train Loss: 0.1237944\nmean_spacing_error：28.94777，col=115，count=3637，rate=3.16195%，jerk=0.04696，miniumu_ttc=78.62834\nEpoch: 5| Train Loss: 0.1155200\nmean_spacing_error：33.17390，col=19，count=3637，rate=0.52241%，jerk=0.05624，miniumu_ttc=1175.99780\n   \n76/500. loss: 0.0020981493095556893\n76/500. mserror: 34.499332427978516  col: 111  count: 3637  jerk: 0.04118307679891586  ttc: 105.37380981445312\nEpoch: 1| Train Loss: 0.1673264\nmean_spacing_error：65.22062，col=132，count=3637，rate=3.62936%，jerk=0.01655，miniumu_ttc=116.97066\nEpoch: 2| Train Loss: 0.1306558\nmean_spacing_error：60.88748，col=69，count=3637，rate=1.89717%，jerk=0.02254，miniumu_ttc=139.73090\nEpoch: 3| Train Loss: 0.1231874\nmean_spacing_error：43.29113，col=96，count=3637，rate=2.63954%，jerk=0.03377，miniumu_ttc=92.53834\nEpoch: 4| Train Loss: 0.1192395\nmean_spacing_error：39.65148，col=156，count=3637，rate=4.28925%，jerk=0.03147，miniumu_ttc=76.87246\nEpoch: 5| Train Loss: 0.1137206\n","output_type":"stream"}]},{"cell_type":"code","source":"import numbers\nfrom copy import copy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport random\n\n\ndef extract_top_level_dict(current_dict):\n    \"\"\"\n    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n    :param value: Param value\n    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n    :return: A dictionary graph of the params already added to the graph.\n    \"\"\"\n    output_dict = dict()\n    for key in current_dict.keys():\n        name = key.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"block_dict.\", \"\")\n        name = name.replace(\"module-\", \"\")\n        top_level = name.split(\".\")[0]\n        sub_level = \".\".join(name.split(\".\")[1:])\n\n        if top_level not in output_dict:\n            if sub_level == \"\":\n                output_dict[top_level] = current_dict[key]\n            else:\n                output_dict[top_level] = {sub_level: current_dict[key]}\n        else:\n            new_item = {key: value for key, value in output_dict[top_level].items()}\n            new_item[sub_level] = current_dict[key]\n            output_dict[top_level] = new_item\n\n    #print(current_dict.keys(), output_dict.keys())\n    return output_dict\n\n\nclass MetaLinearLayer(nn.Module):\n    def __init__(self, input_shape, num_filters, use_bias):\n        \"\"\"\n        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n        learning setting.\n        :param input_shape: The shape of the input data, in the form (b, f)\n        :param num_filters: Number of output filters\n        :param use_bias: Whether to use biases or not.\n        \"\"\"\n        super(MetaLinearLayer, self).__init__()\n        b, c = input_shape\n\n        self.use_bias = use_bias\n        self.weights = nn.Parameter(torch.ones(num_filters, c))\n        nn.init.xavier_uniform_(self.weights)\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.zeros(num_filters))\n\n    def forward(self, x, params=None):\n        \"\"\"\n        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n        Otherwise passed params will be used to execute the function.\n        :param x: Input data batch, in the form (b, f)\n        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n        Otherwise the external are used.\n        :return: The result of the linear function.\n        \"\"\"\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n            if self.use_bias:\n                (weight, bias) = params[\"weights\"], params[\"bias\"]\n            else:\n                (weight) = params[\"weights\"]\n                bias = None\n        else:\n            pass\n            #print('no inner loop params', self)\n\n            if self.use_bias:\n                weight, bias = self.weights, self.bias\n            else:\n                weight = self.weights\n                bias = None\n        # print(x.shape)\n        out = F.linear(input=x, weight=weight, bias=bias)\n        return out\n\n\nclass MetaStepLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        super(MetaStepLossNetwork, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        out = x\n\n        self.linear1 = MetaLinearLayer(input_shape=self.input_shape,\n                                                    num_filters=self.input_dim, use_bias=True)\n\n        self.linear2 = MetaLinearLayer(input_shape=(1, self.input_dim),\n                                                    num_filters=1, use_bias=True)\n\n\n        out = self.linear1(out)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n    def forward(self, x, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n\n        linear1_params = None\n        linear2_params = None\n\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n\n            linear1_params = params['linear1']\n            linear2_params = params['linear2']\n\n        out = x\n        \n        out = self.linear1(out, linear1_params)\n        out = F.relu_(out)\n        out = self.linear2(out, linear2_params)\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass MetaLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        \"\"\"\n        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n        used at inference time. Enables inner loop optimization readily.\n        :param im_shape: The input image batch shape.\n        :param num_output_classes: The number of output classes of the network.\n        :param args: A named tuple containing the system's hyperparameters.\n        :param device: The device to run this on.\n        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n        be enabled. \n        \"\"\"\n        super(MetaLossNetwork, self).__init__()\n        \n        self.device = device\n        # self.args = args\n        self.notspi = notspi\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        self.layer_dict = nn.ModuleDict()\n        # ф\n        for i in range(self.num_steps): \n            self.layer_dict['step{}'.format(i)] = MetaStepLossNetwork(self.input_dim, notspi=self.notspi, device=self.device)\n\n            out = self.layer_dict['step{}'.format(i)](x)\n\n    def forward(self, x, num_step, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n        param_dict = dict()\n\n        if params is not None: \n            # params = {key: value[0] for key, value in params.items()}\n            param_dict = extract_top_level_dict(current_dict=params)\n            \n        for name, param in self.layer_dict.named_parameters():\n            path_bits = name.split(\".\")\n            layer_name = path_bits[0]\n            if layer_name not in param_dict:\n                param_dict[layer_name] = None\n\n            \n        out = x\n        \n        out = self.layer_dict['step{}'.format(num_step)](out, param_dict['step{}'.format(num_step)])\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass StepLossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(StepLossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n        output_dim = num_loss_net_layers * 2 * 2 # 2 for weight and bias, another 2 for multiplier and offset\n\n        self.linear1 = nn.Linear(input_dim, input_dim)\n        self.activation = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(input_dim, output_dim)\n\n        self.multiplier_bias = nn.Parameter(torch.zeros(output_dim // 2))\n        self.offset_bias = nn.Parameter(torch.zeros(output_dim // 2))\n\n    def forward(self, task_state, num_step, loss_params):\n        # ψ\n        out = self.linear1(task_state)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n        generated_multiplier, generated_offset = torch.chunk(out, chunks=2, dim=-1)\n\n        i = 0\n        updated_loss_weights = dict()\n        for key, val in loss_params.items():\n            if 'step{}'.format(num_step) in key:\n                updated_loss_weights[key] = (1 + self.multiplier_bias[i] * generated_multiplier[i]) * val + \\\n                                             self.offset_bias[i] * generated_offset[i]\n                i+=1\n\n        return updated_loss_weights\n\n\nclass LossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(LossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.loss_adapter = nn.ModuleList()\n        for i in range(self.num_steps): \n            self.loss_adapter.append(StepLossAdapter(input_dim, num_loss_net_layers, notspi=notspi, device=device))\n\n    def forward(self, task_state, num_step, loss_params):\n        return self.loss_adapter[num_step](task_state, num_step, loss_params)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nfrom torch import _weight_norm\nimport numpy as np\nfrom math import sqrt\nimport random\n\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim_in = input_dim\n        self.dim_k = input_dim//2\n        self.dim_v = input_dim//2\n        self.num_heads = num_heads\n        self.linear_q = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_k = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_v = nn.Linear(input_dim, input_dim, bias=True)\n        self._norm_fact = 1 / sqrt((input_dim//2) // num_heads)\n\n    def forward(self, x, param=None):\n        batch, n, dim_in = x.shape\n        assert dim_in == self.dim_in\n\n        nh = self.num_heads\n        dk = self.dim_k // nh  # dim_k of each head\n        dv = self.dim_v // nh  # dim_v of each head\n        if param == None:\n            q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            v = self.linear_v(x).reshape(batch, n, nh, dv*2).transpose(1, 2)  # (batch, nh, n, dv)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1]).reshape(batch, n, nh, dk).transpose(1, 2)\n            k = F.linear(x, weight=param[2], bias=param[3]).reshape(batch, n, nh, dk).transpose(1, 2)\n            v = F.linear(x, weight=param[4], bias=param[5]).reshape(batch, n, nh, dv*2).transpose(1, 2)\n        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact  # batch, nh, n, n\n        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n\n        att = torch.matmul(dist, v)  # batch, nh, n, dv\n        out = att.transpose(1, 2).reshape(batch, n, self.dim_in)  # batch, n, dim_v\n        \n        # out = att.reshape(att.shape[0], -1)\n\n        return out\n    \n\nclass conv1d(nn.Module):\n    def _init_(self):\n        super(conv1d, self).__init__()\n    \n    def forward(self, x, weight, bias, stride, padding, dilation):\n        return F.conv1d(x, weight, bias=bias, stride=stride, padding=padding, dilation=dilation)\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm1 = nn.BatchNorm1d(n_outputs)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        # self.relu1 = nn.Tanh()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm2 = nn.BatchNorm1d(n_outputs)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        # self.relu2 = nn.Tanh()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.batchnorm1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.batchnorm2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        # self.relu = nn.Tanh()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        self.kernel_size = kernel_size\n        self.dropout = dropout\n        self.num_inputs = num_inputs\n        self.num_channels = num_channels\n        layers = []\n        # self.vars_bn = nn.ParameterList()\n        num_levels = len(num_channels)\n        # num_levels = num_channels\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n        self.token = nn.Parameter(torch.ones(1, num_channels[-1]))\n        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=10, nhead= 5,\\\n        #                     dim_feedforward=10, batch_first=True)\n        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n        # self.features = nn.LSTM(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.features = nn.GRU(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.attention = TemporalAttention(num_channels[-1])\n        self.attention = MultiHeadSelfAttention(num_channels[-1])\n        self.mlp = nn.Sequential(\n            nn.Linear(num_channels[-1], num_channels[-1]),\n            # nn.LeakyReLU(negative_slope=0.01),\n            nn.ReLU(), \n            nn.Linear(num_channels[-1], 1)\n        )\n        # self.features = nn.Linear(num_channels[-1], num_channels[-1])\n        # self.decoder = nn.Linear(num_channels[-1], 1)\n\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=num_channels[i-1], nhead= 4,\\\n        #                     dim_feedforward=num_channels[i-1], batch_first=False)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        # self.tanh = nn.Tanh()\n\n    def forward(self, x, weights=None):\n        device =  x[0].device\n        if weights == None:\n            # transformer output:\n            # x = self.transformer_encoder(x) + x\n            \n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n            # output = self.attention(output)\n            # linear output:\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n            # h_n = self.features(output)\n            \n            # LSTM output:\n            # enc_x, (h_n, c_n) = self.features(output)\n            \n            # GRU output:\n            # enc_x, h_n = self.features(output)\n\n            # if len(h_n.shape) == 3:\n            #     h_n = h_n[-1] # (32,16)\n            \n            # 通过线性层和激活函数得到最终输出\n            # out = self.decoder(F.relu(h_n))\n            # out = self.decoder(F.leaky_relu(h_n, negative_slope=0.01))\n            out = self.mlp(output)\n        else:\n            # TemporalBlock0\n            # 实际权重 = 模数 * 方向向量\n            ks = self.kernel_size\n            weight1 = _weight_norm(weights[3], weights[2], 0)\n            conv1d1 = weight_norm(nn.Conv1d(self.num_inputs, self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d1._conv_forward(x, weight=weight1, bias=weights[1])\n            # output1 = F.conv1d(x, weight=weights[1], bias=weights[2], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean1 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp1(output1), running_mean=running_mean1, running_var=running_var1, weight=weights[4], bias=weights[5])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            weight2 = _weight_norm(weights[8], weights[7], 0)\n            conv1d2 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d2._conv_forward(output1, weight=weight2, bias=weights[6])\n            # output1 = F.conv1d(output1, weight=weights[5], bias=weights[6], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean2 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp2(output1), running_mean=running_mean2, running_var=running_var2, weight=weights[9], bias=weights[10])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            res1 = F.conv1d(x, weights[11], bias=weights[12], stride=1)\n            # output1 = F.tanh(output1 + res1)\n            output1 = F.relu(output1 + res1)\n\n            # TemporalBlock1\n            weight3 = _weight_norm(weights[15], weights[14], 0)\n            conv1d3 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d3._conv_forward(output1, weight=weight3, bias=weights[13])\n            # output2 = F.conv1d(output1, weight=weights[11], bias=weights[12], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean3 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp1(output2), running_mean=running_mean3, running_var=running_var3, weight=weights[16], bias=weights[17])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            weight4 = _weight_norm(weights[20], weights[19], 0)\n            conv1d4 = weight_norm(nn.Conv1d(self.num_channels[1], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d4._conv_forward(output2, weight=weight4, bias=weights[18])\n            # output2 = F.conv1d(output2, weight=weights[15], bias=weights[16], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean4 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var4 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp2(output2), running_mean=running_mean4, running_var=running_var4, weight=weights[21], bias=weights[22])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            res2 = F.conv1d(output1, weights[23], bias=weights[24], stride=1)\n            # output2 = F.tanh(output2 + res2)\n            output = F.relu(output2 + res2)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[25:31])\n            output = output[:, -1, :]\n\n            # output = output.transpose(1, 2)\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n\n            # out = F.leaky_relu(F.linear(output, weight=weights[31] , bias=weights[32]), negative_slope=0.01)\n            out = F.relu(F.linear(output, weight=weights[31] , bias=weights[32]))\n            out = F.linear(out, weight=weights[33], bias=weights[34])\n\n        return out\n    \n\n\n# MAML-跟车  version1\n# %matplotlib inline\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport copy\nimport random\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\n# from net.TCN import TemporalConvNet\n# from net.TCN_wn_bn import TemporalConvNet\n# from net.TCN_layer import TemporalConvNet\n# from net.TCN import MultiHeadSelfAttention\n# from net.meta_neural_network_architectures import MetaLossNetwork, LossAdapter\n\n# 设置随机种子\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/MAML_MeTAL_log\")\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\n# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\ngpu = 0\ndevice = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu')\nprint(device)\n\n\n# 保存日志\ndef get_logger(filename, verbosity=1, name=None):\n    # 设置不同verbosity对应的日志级别\n    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n    # 设置日志输出格式\n    formatter = logging.Formatter(\n        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level_dict[verbosity])\n    # 创建文件处理器，将日志写入文件\n    fh = logging.FileHandler(filename, \"w\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # 创建控制台处理器，将日志输出到控制台\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n# 日志保存路径\n# dataset = 'NGSIM'\nlogger = get_logger(f'/kaggle/working/MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_ht.log')\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len, Ts = 0.1):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n        self.Ts = Ts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / self.Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(\n                nn.Linear(input_size,256),\n                nn.ReLU(),\n                nn.Linear(256,256),\n                nn.ReLU(),\n                nn.Linear(256,1)\n            )\n        self.input_size = input_size\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights=None):\n        if weights == None:\n            x = self.net(x)\n        else:\n            x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n            x=F.relu(x)\n            x=F.linear(x,weights[2],weights[3])\n            x=F.relu(x)\n            x=F.linear(x,weights[4],weights[5])\n        return x\n\n# 1dcnn模型\nclass CNN1D(nn.Module):\n    def __init__(self, input_size=3, num_classes=1, dropout=0.2):\n        super(CNN1D, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(in_channels=input_size, out_channels=8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(8),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout))\n        self.attention = MultiHeadSelfAttention(32)\n        self.mlp = nn.Sequential(\n            nn.Linear(32, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes)\n        )\n        self.token = nn.Parameter(torch.ones(1, 32))\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.dropout = dropout\n\n    def forward(self, x, weights=None): # input x(150, 3, 10)\n        if weights == None:\n            output = self.net(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n\n            out = self.mlp(output)\n        else:\n            # bn = nn.BatchNorm1d(32)\n            # print(bn.running_mean, bn.running_var, bn.weight, bn.bias)\n            running_mean1 = nn.Parameter(torch.zeros(8), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(8), requires_grad=False).to(device)\n            output = F.conv1d(x, weights[1],weights[2], stride=1, padding=1)   # (150, 32, 10)\n            output = F.batch_norm(output, running_mean=running_mean1, running_var=running_var1, weight=weights[3], bias=weights[4])\n            output = F.relu(output)   # (150, 32, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            running_mean2 = nn.Parameter(torch.zeros(16), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(16), requires_grad=False).to(device)\n            output = F.conv1d(output, weights[5],weights[6], stride=1, padding=1)   # (150, 64, 10)\n            output = F.batch_norm(output, running_mean=running_mean2, running_var=running_var2, weight=weights[7], bias=weights[8])\n            output = F.relu(output)   # (150, 64, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            running_mean3 = nn.Parameter(torch.zeros(32), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(32), requires_grad=False).to(device)\n            output = F.conv1d(output, weights[9],weights[10], stride=1, padding=1)   # (150, 128, 10)\n            output = F.batch_norm(output, running_mean=running_mean3, running_var=running_var3, weight=weights[11], bias=weights[12])\n            output = F.relu(output)   # (150, 128, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[13:19])\n            output = output[:, -1, :]\n\n            out = F.linear(output, weights[19], weights[20]) # (150, 128)\n            out = F.relu(out)   # (150, 128)\n            out = F.linear(out, weights[21], weights[22]) # (150, 1)\n        return out\n\n# 计算ttc\ndef calculate_safety(ttc):\n    minimum_ttc = min(ttc)\n    return minimum_ttc\n# 多数据同时验证\ndef model_evaluate(model,his_horizon,testdata):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    model.eval()\n\n    jerk_set = torch.empty(0).to(device)\n    error_set = torch.empty(0).to(device)\n    minimum_ttc_set = torch.empty(0).to(device)\n\n    criterion = nn.MSELoss()\n    count = 0\n    col = 0\n    for i, item in tqdm(enumerate(testdata)):#每个样本挨个出来\n        \n        x_data, y_data = item['inputs'], item['label']\n        x_data = torch.stack(x_data)#3x149\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        count += B\n        x_data_orig = x_data.clone().detach()#提前克隆的副本\n        \n        col_list = torch.full((B,), -1).to(device)\n        acc_batch = torch.empty(B,0).to(device)\n        ttc_batch = torch.empty(B,0).to(device)\n\n        \n        for frame in range(his_horizon, T):#对32个样本检测\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]   # (20, 3, 10)\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            #x = x.transpose(0,1).reshape(B, -1)\n            acc_pre = model(x).squeeze()\n            if acc_pre.dim() == 0:\n                acc_pre = acc_pre.unsqueeze(0)\n            acc_batch = torch.cat((acc_batch, acc_pre.unsqueeze(1)), dim=1)#记录所有acc     \n            if model_type == 'cnn1d' or model_type == 'tcn':\n                if frame < T-1:\n                    # 根据当前速度和加速度计算下一时间速度\n                    sv_spd_ = x_data[:, 1, frame] + acc_pre*Ts # (32)\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n                    # 计算下一时间速度的相对速度\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n                    # 该时刻真实相对速度\n                    delta_v = x_data[:, -1, frame] # (32)\n                    # 通过两车车距加上相对位移得到下一时间段车距 ？\n                    spacing_ = x_data[:, 0, frame] + Ts*(delta_v + delta_v_)/2 # (32)\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    # update 根据计算得到的值，更新下一时间的值\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[:, :, frame + 1] = next_frame_data\n            else:\n                if frame < T-1:#开环训练，不断依照预测值更新下一个时间节点的数据\n                    sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts#新的自车速度\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)#保证速度不小于等于0\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_#vt临时计算的\n                    delta_v = x_data[frame, :, -1]#v0原有的\n                    spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2#新的距离\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[frame + 1] = next_frame_data\n            if spacing_.dim() == 0:\n                ttc_batch_ = (-spacing_ / delta_v_).unsqueeze(0)\n            else:\n                ttc_batch_ = (-spacing_ / delta_v_)\n                \n            ttc_batch = torch.cat((ttc_batch, ttc_batch_.unsqueeze(1)), dim=1)\n        for i in range(B):#对32个样本统一处理\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                spacing_obs = x_data_orig[i,0,...]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[i, 0, :col_list[i]]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[i, 0, :]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            else:\n                spacing_obs = x_data_orig[...,i,0]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[:col_list[i],i, 0]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[:,i, 0]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            acc_batch_ = acc_batch_.cpu().detach().numpy()\n            jerk_single = np.mean(np.abs(np.diff(acc_batch_)/Ts))\n            TTC_single = [x for x in ttc_batch_ if x >= 0]#除去其中TTC小于0的\n            if len(TTC_single) > 0:\n                minimum_ttc_single = calculate_safety(TTC_single)#该样本的最小TTC\n                minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)      \n            jerk_set= torch.cat((jerk_set,torch.tensor(jerk_single).unsqueeze(0).to(device)), dim=0)\n            error_set= torch.cat((error_set,torch.tensor(error_single).unsqueeze(0).to(device)), dim=0)\n        col = col + torch.sum(col_list != -1).item()\n    if len(minimum_ttc_set) == 0:\n        ttc = 0\n    else:\n        ttc = sum(minimum_ttc_set)/len(minimum_ttc_set)\n    error = sum(error_set)/len(error_set)\n    return error,col,count,sum(jerk_set)/len(jerk_set),ttc#还需修改（以MSE为基准）\n\n\ndef finetuning(best_mse_state):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    # Train\n    Ts = 0.1\n    max_len = 150\n    og_net = maml.net\n    # 创建一个与原始网络结构相同的虚拟网络\n    if model_type == 'nn':\n        dummy_net = nn_model(input_size = his_horizon*3)\n    elif model_type == 'tcn':\n        dummy_net = TemporalConvNet(num_inputs=3, num_channels=(16,32))\n    elif model_type == 'cnn1d':\n        dummy_net = CNN1D()\n        # dummy_net = cbrd(num_inputs=3, num_output=1)\n    dummy_net=dummy_net.to(device)\n\n    # 多gpu训练\n    # if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n    #     dummy_net = torch.nn.DataParallel(dummy_net)  # 自动选择gpu\n    # dummy_net.to(device)\n\n    # 加载原始网络的权重\n    # weight_path = torch.load(state_path)\n    # dummy_net.load_state_dict(weight_path.state_dict())\n    dummy_net.load_state_dict(og_net.state_dict())\n    # 进行迭代，每次更新虚拟网络的参数\n    num_shots=5\n    lr = 0.01\n    loss_fn=nn.MSELoss()\n    optim=torch.optim.Adam\n    opt=optim(dummy_net.parameters(),lr=lr, weight_decay=3e-4)\n\n\n    # 数据集划分\n    def split_data(data,data_ratio):\n        # random.seed(SEED)\n        # np.random.seed(SEED)\n        # torch.manual_seed(SEED)\n        if data_ratio > 0 and data_ratio <= 1:\n            # 如果小于等于1，根据输入百分比计算获取数据集的数量\n            data_size=int(len(data)*data_ratio)\n        else:\n            # 如果大于1，则data_ratio为获取数据集中的数量\n            data_size = data_ratio\n        return data[:data_size]\n    # np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n    # 获取数据集的数量\n    K=40\n    dataset_train = split_data(train_data, K)\n    dataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len, Ts = Ts)\n    train_loader = DataLoader(\n                dataset_loader_train,\n                batch_size=10,\n                shuffle=True,\n                num_workers=1,\n                drop_last=True)\n    dataset_test = test_data\n    dataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len, Ts = Ts)\n    test_loader = DataLoader(\n                dataset_loader_test,\n                batch_size=96,\n                shuffle=False,\n                num_workers=1,\n                drop_last=False)\n    # dataset_val = NGSIM_val\n    # dataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len, Ts = Ts)\n    # val_loader = DataLoader(\n    #             dataset_loader_val,\n    #             batch_size=10,\n    #             shuffle=True,\n    #             num_workers=1,\n    #             drop_last=True)\n\n\n    # 初始化变量\n    train_loss_his = [] # 训练损失\n    test_error_his = [] # 测试误差\n    best_train_loss = None # 最佳训练损失\n\n    # 训练过程\n    best_error = 10000\n    for epoch in tqdm(range(num_shots)):\n        train_losses = [] # 记录每个epoch的训练损失\n        validation_losses = [] # 记录每个epoch的验证损失\n        jerk_val = 0\n        dummy_net.train()\n        # 遍历数据集\n        for i, item in enumerate(train_loader):\n            # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n            # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n            x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n            # Put T into the first dimension, B, T, d -> T, B, d\n            # 将x_data中3个(32,374)连接，转换成(3,32,374)\n            x_data = torch.stack(x_data)\n            # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                # x->(bs,3,149)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n                B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            else:\n                # x->(149,bs,3)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n                T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            # print(T,B,d)  # 149  20  3\n                \n            y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n            y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n            # 从历史数据时间步开始遍历\n            for frame in range(his_horizon, T):\n                if model_type == 'cnn1d' or model_type == 'tcn':\n                    x= x_data[:, :, frame-his_horizon:frame]\n                else:\n                    x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n                if model_type == 'nn':\n                    x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n                # 根据his_horizon个数据预测加速度\n                acc_pre = dummy_net(x).squeeze() # (32)\n                y_pre[frame - his_horizon] = acc_pre\n            #计算损失并进行反传及优化\n            loss = loss_fn(y_pre, y_label)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n            opt.step()\n\n            train_losses.append(loss.item())\n        # 计算本轮平均损失\n        train_loss = np.mean(train_losses)\n\n        train_loss_his.append(train_loss)\n#         print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n        logger.info(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n        mean_spacing_error,col,count,jerk,miniumu_ttc = model_evaluate(dummy_net,his_horizon,test_loader)\n        logger.info(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n        if epoch == 0:\n            first_step_error = mean_spacing_error\n        if mean_spacing_error < best_error:\n            best_error = mean_spacing_error\n        if mean_spacing_error < best_mse_state:\n            best_mse_state = mean_spacing_error\n            # save the best model\n            with open(save_path, 'wb') as f:\n                torch.save(dummy_net, f)\n    return best_error, first_step_error, best_mse_state\n\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion, metal=None):\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n            \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x, temp_weights).squeeze() # (32)\n            # acc_pre = net.module(x, temp_weights).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        if metal == None:\n            return loss\n        else:\n            return loss, y_pre\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n            if adapt_tasks:\n                task_net = HighD_net\n            # print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Lyft_net\n            # print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD1_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD1_net\n            # print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD2_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD2_net\n            # print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = NGSIM_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Waymo_net\n            # print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len, Ts = Ts)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len, Ts = Ts)\n        if adapt_tasks:\n            return dataset_loader1, dataset_loader2, task_net\n        else:\n            return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self, epoch):\n        # 从1到5中随机选择3个不重复的数\n        # 创建一个新的随机数生成器实例\n        rng = np.random.RandomState(epoch)\n        data_choice = rng.choice(self.data_range, size=self.n, replace=False) + 1\n        # data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\ndef attenuate_init(attenuator, panning, task_embeddings, names_weights_copy):\n        ## Attenuate\n        updated_names_weights_copy = list()\n        \n        # 生成衰减参数和偏移参数\n        if attenuator == None:\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(key + beta[i])\n                i+=1\n        elif panning == None:\n            gamma = attenuator(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key)\n                i+=1\n        else:\n            gamma = attenuator(task_embeddings)\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key + beta[i])\n                i+=1\n            \n        return updated_names_weights_copy\n\n\ndef get_inner_loop_parameter_dict(params):\n    \"\"\"\n    Returns a dictionary with the parameters to use for inner loop updates.\n    :param params: A dictionary of the network's parameters.\n    :return: A dictionary of the parameters to use for the inner loop optimization process.\n    \"\"\"\n    param_dict = dict()\n    for name, param in params:\n        if param.requires_grad:\n            param_dict[name] = param.to(device)\n\n    return param_dict\n\ndef get_per_step_loss_importance_vector(inner_step, current_epoch):\n        \"\"\"\n        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n        loss towards the optimization loss.\n        :return: A tensor to be used to compute the weighted average of the loss, useful for\n        the MSL (Multi Step Loss) mechanism.\n        \"\"\"\n        multi_step_loss_num_epochs = 20\n        loss_weights = np.ones(shape=(inner_step)) * (\n                1.0 / inner_step)\n        decay_rate = 1.0 / inner_step / multi_step_loss_num_epochs\n        min_value_for_non_final_losses = 0.03 / inner_step\n        for i in range(len(loss_weights) - 1):\n            curr_value = np.maximum(loss_weights[i] - (current_epoch * decay_rate), min_value_for_non_final_losses)\n            loss_weights[i] = curr_value\n\n        curr_value = np.minimum(\n            loss_weights[-1] + (current_epoch * (inner_step - 1) * decay_rate),\n            1.0 - ((inner_step - 1) * min_value_for_non_final_losses))\n        loss_weights[-1] = curr_value\n        loss_weights = torch.Tensor(loss_weights).to(device)\n        return loss_weights\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k, weight_decay, inner_step):  # (net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=150)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.inner_step = inner_step  # 内部循环更新的步数\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.meta_test = []\n        self.finetuning_error = []\n        self.plot_every = 10  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.first_add = 0  # 用于添加优化器优化的参数\n        self.all_best_state = 10000 # 用于存储微调后最好mse时的模型权重\n        self.batch_size = int(self.k/2)\n        # 设置权重衰减因子\n        self.weight_decay = weight_decay\n        self.num_layers = len(self.weights)\n        params = [{'params': self.weights}]\n        base_learner_num_layers = len(self.weights)\n        if MeTAL:\n            \n            support_meta_loss_num_dim = base_learner_num_layers + 2 + 1\n            support_adapter_num_dim = base_learner_num_layers + 1\n\n            self.meta_loss = MetaLossNetwork(support_meta_loss_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_loss_adapter = LossAdapter(support_adapter_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_loss_adapter.parameters()}\n            params.append(new_params)\n\n        if semi_supervised:\n            query_num_dim = base_learner_num_layers + 1 + 1\n            self.meta_query_loss = MetaLossNetwork(query_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_query_loss_adapter = LossAdapter(query_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_query_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_query_loss_adapter.parameters()}\n            params.append(new_params)\n\n        if attenuate:  #L2F\n            self.attenuator = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Sigmoid()\n            ).to(device=device)\n            new_params = {'params': self.attenuator.parameters()}\n            params.append(new_params)\n        if tapt:  #bias\n            self.panning = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Tanh()\n            ).to(device=device)\n            new_params = {'params': self.panning.parameters()}\n            params.append(new_params)\n        if alfa:  # ALFA\n            input_dim = self.num_layers*2\n            self.regularizer = nn.Sequential(\n                nn.Linear(input_dim, input_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(input_dim, input_dim)\n            ).to(device=device)\n            new_params = {'params': self.regularizer.parameters()}\n            params.append(new_params)\n        self.meta_optimiser = torch.optim.Adam(params,lr=self.beta, weight_decay=3e-4)\n\n    def inner_loop(self, task):\n        taskloss = []\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        if adapt_tasks:\n            dataset_loader1, dataset_loader2, task_net = task.sample_data(size=self.k)\n            task_optimiser = torch.optim.Adam([{'params':task_net.parameters()}],lr=0.001, weight_decay=3e-4)\n        else:\n            dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        for step in range(self.inner_step):\n            if step == 0 and adapt_tasks or attenuate or tapt:\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n                if adapt_tasks:     # every tasks\n                    grads = tuple(grads[i] for i in range(len(grads)))\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    temp_weights = attenuate_init(task_net, None, layerwise_mean_grads, temp_weights)  # 得到新的权重\n\n                if attenuate or tapt:    #L2F\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    if attenuate and tapt:\n                        temp_weights = attenuate_init(self.attenuator, self.panning, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    elif attenuate and not tapt:\n                        temp_weights = attenuate_init(self.attenuator, None, layerwise_mean_grads, temp_weights)\n                    elif not attenuate and tapt:\n                        temp_weights = attenuate_init(None, self.panning, layerwise_mean_grads, temp_weights)\n            support_loss, support_preds = inner_train(dataloader1, self.net, temp_weights, self.criterion, metal=1)\n            if MeTAL:\n                support_task_state = []\n                # 获取支持集 Loss 和 预测值\n                \n                support_loss /= int(self.batch_size)\n                support_task_state.append(support_loss)\n\n                # 将每层模型权重的平均值添加到task_state中\n                for v in temp_weights:\n                    support_task_state.append(v.mean())\n\n                support_task_state = torch.stack(support_task_state)\n                # 归一化\n                adapt_support_task_state = (support_task_state - support_task_state.mean())/(support_task_state.std() + 1e-12)                                                 \n                # 损失函数网络参数更新\n                updated_meta_loss_weights = self.meta_loss_adapter(adapt_support_task_state, step, self.names_loss_weights_copy)\n                for i, item in enumerate(dataloader1):\n                    acceleration = item['label']\n                # 将预测值添加到task_state\n                support_task_state = torch.cat((\n                    support_task_state.view(1, -1).expand(support_preds.size(1), -1),\n                    support_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                    acceleration.mean(dim=1, keepdim=True).float().to(device)\n                ), -1)\n\n                support_task_state = (support_task_state - support_task_state.mean()) / (support_task_state.std() + 1e-12)\n                # 计算support_loss\n                meta_support_loss = self.meta_loss(support_task_state, step, params=updated_meta_loss_weights).mean().squeeze()\n            if semi_supervised:\n                dataloader2 = DataLoader(\n                        dataset_loader2,\n                        batch_size=self.batch_size,\n                        shuffle=True,\n                        num_workers=1,\n                        drop_last=False)\n                query_loss, query_preds = inner_train(dataloader2, self.net, temp_weights, self.criterion, metal=1)\n                query_loss /= int(self.batch_size)\n                query_task_state = []\n\n                for i, item in enumerate(dataloader2):\n                    acceleration = item['label']\n#                     query_task_state.append(query_loss)\n\n                for v in temp_weights:\n                    query_task_state.append(v.mean())\n                query_task_state = torch.stack(query_task_state)\n                query_task_state = torch.cat((\n                            query_task_state.view(1, -1).expand(query_preds.size(1), -1),\n                            query_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                            acceleration.mean(dim=1, keepdim=True).float().to(device)\n                ), -1)\n\n                query_task_state = (query_task_state - query_task_state.mean())/(query_task_state.std() + 1e-12)\n                updated_meta_query_loss_weights = self.meta_query_loss_adapter(query_task_state.mean(0), step, self.names_query_loss_weights_copy)\n\n                meta_query_loss = self.meta_query_loss(query_task_state, step, params=updated_meta_query_loss_weights).mean().squeeze()\n\n                loss = support_loss  + meta_query_loss\n            else:\n                # 训练过程\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n            grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            if alfa:    #ALFA\n                # 用于存储权重的平均值和梯度的平均值\n                per_step_task_embedding = []\n                for v in temp_weights:\n                    per_step_task_embedding.append(v.mean())\n            \n                for i in range(len(grads)):\n                    per_step_task_embedding.append(grads[i].mean())\n\n                per_step_task_embedding = torch.stack(per_step_task_embedding)\n\n                generated_params = self.regularizer(per_step_task_embedding)\n\n                generated_alpha, generated_beta = torch.split(generated_params, split_size_or_sections=self.num_layers)\n                generated_alpha_params = []\n                generated_beta_params = []\n                g = 0\n                for key in temp_weights:\n                    generated_alpha_params.append(generated_alpha[g])\n                    generated_beta_params.append(generated_beta[g])\n                    g+=1\n                # 初始化ALFA自适应参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    if random_init:\n                        self.names_beta_dict_per_param = nn.ParameterDict()\n                    self.names_alpha_dict = nn.ParameterDict()\n                    self.names_beta_dict = nn.ParameterDict()\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    for idx, param in enumerate(temp_weights):\n\n                        if random_init:\n                        # per-param weight decay for random init\n                            self.names_beta_dict_per_param[str(idx)] = nn.Parameter(\n                                data=torch.ones(param.shape) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_learning_rates)\n\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step),\n                                requires_grad=use_learnable_beta)\n                            \n                            self.names_beta_dict_per_param.to(device)\n                        else:\n                            # per-step per-layer meta-learnable weight decay bias term (for more stable training and better performance by 2~3%)\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_beta)\n                        \n                        # per-step per-layer meta-learnable learning rate bias term (for more stable training and better performance by 2~3%)\n                        self.names_alpha_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                        \n                        self.names_beta_dict.to(device)\n                        self.names_alpha_dict.to(device)\n                    # 将额外的参数添加到优化器中\n                    if random_init:\n                        self.meta_optimiser.add_param_group({'params': self.names_beta_dict_per_param.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_beta_dict.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_alpha_dict.parameters()})\n            else:\n                # 使用可学习的参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    self.names_learning_rates_dict = nn.ParameterDict()\n                    for idx, param in enumerate(temp_weights):\n                        self.names_learning_rates_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                    self.names_learning_rates_dict.to(device)\n                    if use_learnable_alpha:\n                        self.meta_optimiser.add_param_group({'params': self.names_learning_rates_dict.parameters()})\n                        \n                    if inner_update == \"L2\" and self.weight_decay != None:\n                        self.names_weight_decay_dict = nn.ParameterDict()\n                        for idx, param in enumerate(temp_weights):\n                            self.names_weight_decay_dict[str(idx)] = nn.Parameter(\n                                    data=torch.ones(self.inner_step) * init_weight_decay,\n                                    requires_grad=use_learnable_beta)\n                        self.names_weight_decay_dict.to(device)\n                        if use_learnable_beta:\n                            self.meta_optimiser.add_param_group({'params': self.names_weight_decay_dict.parameters()})\n\n            if inner_update == \"L2\" and self.weight_decay != None:\n                # temp_weights = [w - self.alpha * (g + (self.weight_decay * w)) for w, g in zip(temp_weights, grads)]\n                # temp_weights = [((1 - self.alpha * self.weight_decay) * w) - (self.alpha * g) for w, g in zip(temp_weights, grads)]\n                temp_weights = [((1 - self.names_learning_rates_dict[str(key)][step] * self.names_weight_decay_dict[str(key)][step]) * w) - (self.names_learning_rates_dict[str(key)][step] * g) for key, (w, g) in enumerate(zip(temp_weights, grads))]\n            elif inner_update == \"ALFA\" and alfa:\n                if random_init:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step] * self.names_beta_dict_per_param[str(key)]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n                else:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n            else:\n                temp_weights = [w - self.names_learning_rates_dict[str(key)][step] * g for key, (w, g) in enumerate(zip(temp_weights, grads))]  # 临时参数更新 梯度下降\n\n            if use_multi_step_loss_optimization and self.epoch < 20:\n                dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                taskloss.append(self.per_step_loss_importance_vectors[step] * metaloss)\n            else:\n                if step == (self.inner_step - 1):\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                    taskloss.append(metaloss)\n        task_losses = torch.sum(torch.stack(taskloss))\n        if adapt_tasks:\n            task_optimiser.zero_grad()\n            metaloss.backward(retain_graph=True)\n            task_optimiser.step()\n        return task_losses\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        best_loss = 10000\n        best_first_step_error = 10000\n        best_index = 0\n        for epoch in tqdm(range(1, num_epochs + 1)):\n            self.epoch = epoch\n            if MeTAL:\n                self.names_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_loss.named_parameters())\n            if semi_supervised:\n                self.names_query_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_query_loss.named_parameters())\n            total_loss = 0\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task(epoch)\n            for i in tasks:\n                if use_multi_step_loss_optimization:\n                    self.per_step_loss_importance_vectors = get_per_step_loss_importance_vector(self.inner_step, epoch)\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            # metaloss_sum.backward(retain_graph=True)\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights, retain_graph=True)  # 计算元学习损失对参数的梯度\n            # # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n\n            if MeTAL:\n                meta_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss.parameters(), meta_loss_grads):\n                    w.grad = g\n\n                meta_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss_adapter.parameters(), meta_loss_adapter_grads):\n                    w.grad = g\n\n            if semi_supervised:\n                meta_query_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_query_loss.parameters(), meta_query_loss_grads):\n                    w.grad = g\n\n                meta_query_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_query_loss_adapter.parameters(), meta_query_loss_adapter_grads):\n                    w.grad = g\n\n            if not alfa and use_learnable_alpha:\n                learning_rates_grads = torch.autograd.grad(metaloss_sum, list(self.names_learning_rates_dict.parameters()), retain_graph=True)\n                for w, g in zip(self.names_learning_rates_dict.parameters(), learning_rates_grads):\n                    w.grad = g\n                if inner_update == \"L2\" and self.weight_decay != None and use_learnable_beta:\n                    weight_decay_grads = torch.autograd.grad(metaloss_sum, list(self.names_weight_decay_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_weight_decay_dict.parameters(), weight_decay_grads):\n                        w.grad = g\n\n            if attenuate:\n                attenuate_grads = torch.autograd.grad(metaloss_sum, list(self.attenuator.parameters()), retain_graph=True)\n                for w, g in zip(self.attenuator.parameters(), attenuate_grads):\n                    w.grad = g\n            if tapt:\n                tapt_grads = torch.autograd.grad(metaloss_sum, list(self.panning.parameters()), retain_graph=True)\n                for w, g in zip(self.panning.parameters(), tapt_grads):\n                    w.grad = g\n            if alfa:\n                alfa_grads = torch.autograd.grad(metaloss_sum, list(self.regularizer.parameters()), retain_graph=True)\n                for w, g in zip(self.regularizer.parameters(), alfa_grads):\n                    w.grad = g\n                if use_learnable_beta:\n                    names_beta_grads = torch.autograd.grad(metaloss_sum, list(self.names_beta_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict.parameters(), names_beta_grads):\n                        w.grad = g\n                if use_learnable_alpha:\n                    names_alpha_grads = torch.autograd.grad(metaloss_sum, self.names_alpha_dict.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_alpha_dict.parameters(), names_alpha_grads):\n                        w.grad = g\n                if random_init:\n                    random_init_grads = torch.autograd.grad(metaloss_sum, self.names_beta_dict_per_param.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict_per_param.parameters(), random_init_grads):\n                        w.grad = g\n            \n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            mserror,col,count,jerk,ttc = model_evaluate(self.net,his_horizon,meta_loader)\n            if epoch % self.print_every == 0:\n                # print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                logger.info(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                logger.info(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n                if mserror < best_loss:\n                    best_loss = mserror\n                    best_index = epoch\n                    # with open(file_path, 'wb') as f:\n                    #     torch.save(self.net, f)\n                self.meta_losses.append(total_loss / self.print_every)\n                self.meta_test.append(mserror.cpu().detach().numpy() / self.print_every)\n            # finetuning()\n            fine_error, first_step_error, best_mse_state = finetuning(self.all_best_state)\n            self.all_best_state = best_mse_state\n            if first_step_error < best_first_step_error:\n                best_first_step_epoch = epoch\n                best_first_step_error = first_step_error\n            self.finetuning_error.append(fine_error.cpu().detach().numpy())\n            # if epoch % self.plot_every == 0:\n            #     epoch_path = f\"/home/ubuntu/fedavg/FollowNet-car/MAML_state/maml_l2f_test/MAML_nn_a0.01_b0.0001_{epoch}E.pt\" # 元初始化参数\n            #     # weight_change = torch.load(file_path)\n            #     with open(epoch_path, 'wb') as f:\n            #         torch.save(self.net, f)\n        logger.info(\"({}) best_loss:{}\".format(best_index, best_loss))\n        logger.info(\"({}) best_loss:{}\".format(best_first_step_epoch, best_first_step_error))\n\ndataset = 'Waymo'\nmodel_type = 'tcn'\nif dataset == 'SPMD1':\n    train_data = SPMD1_train\n    test_data = SPMD1_test\nelif dataset == 'SPMD2':\n    train_data = SPMD2_train\n    test_data = SPMD2_test\nelif dataset == 'Waymo':\n    train_data = Waymo_train\n    test_data = Waymo_test\nelif dataset == 'NGSIM':\n    train_data = NGSIM_train\n    test_data = NGSIM_test\nelif dataset == 'Lyft':\n    train_data = Lyft_train\n    test_data = Lyft_test\nelif dataset == 'HighD':\n    train_data = HighD_train\n    test_data = HighD_test\n# 定义保存文件的文件夹路径\n# file_path = \"/home/ubuntu/fedavg/FollowNet-car/MAML_state/MAML_MeTAL_nn_a0.01_b0.0001_k300_E50.pt\" # 元初始化参数\nsave_folder = '/kaggle/working/'\nsave = f'MeTAL_Waymo_{model_type}_{dataset}_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n# 定义文件保存路径\nsave_path = os.path.join(save_folder, save)\nmeta_test = test_data\nmeta_loader_test = ImitationCarFolData(meta_test, max_len = max_len, Ts = Ts)\nmeta_loader = DataLoader(\n            meta_loader_test,\n            batch_size=96,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n# 模型初始化\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'tcn':\n    net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\nelif model_type == 'cnn1d':\n    net = CNN1D().to(device)\n    # net = cbrd(num_inputs=3, num_output=1).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n\n# model_state = list(net.parameters())\n# print(model_state)\n\n# 多gpu训练\n# if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n#     net = torch.nn.DataParallel(net)  # 自动选择gpu\n# net.to(device)\n\n# 内部循环更新策略\nMeTAL = False   # 任务自适应loss函数\nsemi_supervised = True  # 使用查询集 作为半监督\nadapt_tasks = False  # 权重初始化 每个任务用一个\nattenuate = False  # 权重初始化 所有任务共用 权重衰减\ntapt = False  # 权重初始化 所有任务共用 权重偏差\nalfa = False        # 自适应学习优化超参数\nrandom_init = False\nuse_learnable_learning_rates = random_init\nuse_learnable_beta = False  # 使优化参数beta可学习\nuse_learnable_alpha = False  # 使优化参数alpha可学习\nuse_multi_step_loss_optimization = False  # 内部循环多步时，使用最后一步的Loss，还是使用多步的Loss\ninner_update = 'ALFA'\nif adapt_tasks:\n    num_layers = len(list(net.parameters()))\n    HighD_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Lyft_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD1_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD2_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Waymo_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.001,beta=0.01,tasks=data_tasks,k=200,weight_decay=0.0001,inner_step=1)\nmaml.outer_loop(num_epochs=500)\n\n# plt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(maml.meta_losses)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_Loss.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.meta_test)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_mserror.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.finetuning_error)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_finetuning_mserror.png'))\nplt.show()\nplt.close()\n# 将数据保存到文件中\nnp.save('/kaggle/working/kaggle_maml_meta_losses.npy', np.array(maml.meta_losses))\nnp.save('/kaggle/working/kaggle_maml_meta_test.npy', np.array(maml.meta_test))\nnp.save('/kaggle/working/kaggle_maml_finetuning_error.npy', np.array(maml.finetuning_error))\n\nprint('----------------------')\nlogger.info(\"----------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T03:26:45.887598Z","iopub.execute_input":"2024-07-23T03:26:45.888007Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/500 [00:00<?, ?it/s]\n0it [00:00, ?it/s]\u001b[A/tmp/ipykernel_42/1383994605.py:939: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)\n\n1it [00:01,  1.75s/it]\u001b[A\n2it [00:02,  1.35s/it]\u001b[A\n3it [00:03,  1.20s/it]\u001b[A\n[2024-07-23 03:28:22,569][1383994605.py][line:1670][INFO] 1/500. loss: 0.004363507653276126\n[2024-07-23 03:28:22,571][1383994605.py][line:1671][INFO] 1/500. mserror: 162.15927124023438  col: 70  count: 216  jerk: 0.0029613738879561424  ttc: 157.29205322265625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:28:26,713][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4733091\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.72s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.59s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.35s/it]\u001b[A\u001b[A\n[2024-07-23 03:28:30,896][1383994605.py][line:1089][INFO] mean_spacing_error：202.65480，col=64，count=216，rate=29.62963%，jerk=0.00184，miniumu_ttc=176.26738\n\n 20%|██        | 1/5 [00:08<00:33,  8.32s/it]\u001b[A[2024-07-23 03:28:35,172][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4353640\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.80s/it]\u001b[A\u001b[A\n\n2it [00:02,  1.42s/it]\u001b[A\u001b[A\n\n3it [00:03,  1.26s/it]\u001b[A\u001b[A\n[2024-07-23 03:28:39,082][1383994605.py][line:1089][INFO] mean_spacing_error：169.46217，col=69，count=216，rate=31.94444%，jerk=0.00234，miniumu_ttc=161.70561\n\n 40%|████      | 2/5 [00:16<00:24,  8.24s/it]\u001b[A[2024-07-23 03:28:43,375][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.4255572\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.72s/it]\u001b[A\u001b[A\n\n2it [00:02,  1.41s/it]\u001b[A\u001b[A\n\n3it [00:03,  1.25s/it]\u001b[A\u001b[A\n[2024-07-23 03:28:47,239][1383994605.py][line:1089][INFO] mean_spacing_error：151.66002，col=75，count=216，rate=34.72222%，jerk=0.00277，miniumu_ttc=161.08328\n\n 60%|██████    | 3/5 [00:24<00:16,  8.20s/it]\u001b[A[2024-07-23 03:28:51,709][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.4183105\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.88s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.48s/it]\u001b[A\u001b[A\n\n3it [00:03,  1.31s/it]\u001b[A\u001b[A\n[2024-07-23 03:28:55,777][1383994605.py][line:1089][INFO] mean_spacing_error：128.74947，col=73，count=216，rate=33.79630%，jerk=0.00989，miniumu_ttc=132.09012\n\n 80%|████████  | 4/5 [00:33<00:08,  8.34s/it]\u001b[A[2024-07-23 03:29:00,226][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3728429\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.99s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.53s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.36s/it]\u001b[A\u001b[A\n[2024-07-23 03:29:04,440][1383994605.py][line:1089][INFO] mean_spacing_error：37.17441，col=68，count=216，rate=31.48148%，jerk=0.06840，miniumu_ttc=91.81949\n\n100%|██████████| 5/5 [00:41<00:00,  8.37s/it]\u001b[A\n  0%|          | 1/500 [01:45<14:36:03, 105.34s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:01,  1.65s/it]\u001b[A\n2it [00:02,  1.36s/it]\u001b[A\n3it [00:03,  1.26s/it]\u001b[A\n[2024-07-23 03:30:21,403][1383994605.py][line:1670][INFO] 2/500. loss: 0.004475843471785386\n[2024-07-23 03:30:21,410][1383994605.py][line:1671][INFO] 2/500. mserror: 189.11279296875  col: 66  count: 216  jerk: 0.00015898382116574794  ttc: 171.4899444580078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:30:26,118][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4587487\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.03s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.56s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.43s/it]\u001b[A\u001b[A\n[2024-07-23 03:30:30,604][1383994605.py][line:1089][INFO] mean_spacing_error：186.11447，col=68，count=216，rate=31.48148%，jerk=0.00315，miniumu_ttc=166.66570\n\n 20%|██        | 1/5 [00:09<00:36,  9.18s/it]\u001b[A[2024-07-23 03:30:35,136][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4290309\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.04s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.59s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.46s/it]\u001b[A\u001b[A\n[2024-07-23 03:30:39,683][1383994605.py][line:1089][INFO] mean_spacing_error：170.06920，col=70，count=216，rate=32.40741%，jerk=0.00115，miniumu_ttc=154.80432\n\n 40%|████      | 2/5 [00:18<00:27,  9.12s/it]\u001b[A[2024-07-23 03:30:44,197][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.4244932\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.04s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.57s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.44s/it]\u001b[A\u001b[A\n[2024-07-23 03:30:48,672][1383994605.py][line:1089][INFO] mean_spacing_error：146.08495，col=77，count=216，rate=35.64815%，jerk=0.00275，miniumu_ttc=110.76617\n\n 60%|██████    | 3/5 [00:27<00:18,  9.06s/it]\u001b[A[2024-07-23 03:30:53,343][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.4166054\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.98s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.57s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.49s/it]\u001b[A\u001b[A\n[2024-07-23 03:30:57,960][1383994605.py][line:1089][INFO] mean_spacing_error：105.92847，col=75，count=216，rate=34.72222%，jerk=0.01283，miniumu_ttc=127.26392\n\n 80%|████████  | 4/5 [00:36<00:09,  9.15s/it]\u001b[A[2024-07-23 03:31:02,600][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3686691\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.02s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.61s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.47s/it]\u001b[A\u001b[A\n[2024-07-23 03:31:07,144][1383994605.py][line:1089][INFO] mean_spacing_error：32.22518，col=68，count=216，rate=31.48148%，jerk=0.07179，miniumu_ttc=141.77113\n\n100%|██████████| 5/5 [00:45<00:00,  9.15s/it]\u001b[A\n  0%|          | 2/500 [03:48<15:59:17, 115.58s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:01,  1.89s/it]\u001b[A\n2it [00:03,  1.59s/it]\u001b[A\n3it [00:04,  1.45s/it]\u001b[A\n[2024-07-23 03:32:27,420][1383994605.py][line:1670][INFO] 3/500. loss: 0.0041397251188755035\n[2024-07-23 03:32:27,429][1383994605.py][line:1671][INFO] 3/500. mserror: 182.04698181152344  col: 66  count: 216  jerk: 0.0002644116466399282  ttc: 160.76405334472656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:32:31,846][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4813796\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.15s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.66s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.52s/it]\u001b[A\u001b[A\n[2024-07-23 03:32:36,573][1383994605.py][line:1089][INFO] mean_spacing_error：116.82438，col=87，count=216，rate=40.27778%，jerk=0.00623，miniumu_ttc=123.02340\n\n 20%|██        | 1/5 [00:09<00:36,  9.14s/it]\u001b[A[2024-07-23 03:32:41,177][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4190156\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.98s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.55s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.46s/it]\u001b[A\u001b[A\n[2024-07-23 03:32:45,732][1383994605.py][line:1089][INFO] mean_spacing_error：155.79805，col=70，count=216，rate=32.40741%，jerk=0.00619，miniumu_ttc=154.80325\n\n 40%|████      | 2/5 [00:18<00:27,  9.15s/it]\u001b[A[2024-07-23 03:32:50,347][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3887243\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.01s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.57s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.45s/it]\u001b[A\u001b[A\n[2024-07-23 03:32:54,868][1383994605.py][line:1089][INFO] mean_spacing_error：35.30721，col=69，count=216，rate=31.94444%，jerk=0.07054，miniumu_ttc=140.32852\n\n 60%|██████    | 3/5 [00:27<00:18,  9.14s/it]\u001b[A[2024-07-23 03:32:59,458][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3191091\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.99s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.58s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.47s/it]\u001b[A\u001b[A\n[2024-07-23 03:33:04,031][1383994605.py][line:1089][INFO] mean_spacing_error：24.49467，col=13，count=216，rate=6.01852%，jerk=0.16380，miniumu_ttc=257.09647\n\n 80%|████████  | 4/5 [00:36<00:09,  9.16s/it]\u001b[A[2024-07-23 03:33:08,755][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3141102\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:01,  1.99s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.56s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.44s/it]\u001b[A\u001b[A\n[2024-07-23 03:33:13,249][1383994605.py][line:1089][INFO] mean_spacing_error：57.70479，col=1，count=216，rate=0.46296%，jerk=0.14702，miniumu_ttc=165.66473\n\n100%|██████████| 5/5 [00:45<00:00,  9.16s/it]\u001b[A\n  1%|          | 3/500 [05:54<16:37:00, 120.36s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.68s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 03:35:39,376][1383994605.py][line:1670][INFO] 4/500. loss: 0.002123743606110414\n[2024-07-23 03:35:39,387][1383994605.py][line:1671][INFO] 4/500. mserror: 171.9567413330078  col: 70  count: 216  jerk: 0.0004146716382820159  ttc: 155.3701629638672\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:35:44,544][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4676956\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 03:35:49,948][1383994605.py][line:1089][INFO] mean_spacing_error：109.68744，col=87，count=216，rate=40.27778%，jerk=0.00795，miniumu_ttc=112.98225\n\n 20%|██        | 1/5 [00:10<00:42, 10.56s/it]\u001b[A[2024-07-23 03:35:55,352][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4162315\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 03:36:00,567][1383994605.py][line:1089][INFO] mean_spacing_error：114.56612，col=66，count=216，rate=30.55556%，jerk=0.01957，miniumu_ttc=161.77249\n\n 40%|████      | 2/5 [00:21<00:31, 10.60s/it]\u001b[A[2024-07-23 03:36:06,124][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3624891\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 03:36:11,387][1383994605.py][line:1089][INFO] mean_spacing_error：30.40129，col=63，count=216，rate=29.16667%，jerk=0.08190，miniumu_ttc=137.30521\n\n 60%|██████    | 3/5 [00:32<00:21, 10.70s/it]\u001b[A[2024-07-23 03:36:16,794][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3028274\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 03:36:21,928][1383994605.py][line:1089][INFO] mean_spacing_error：21.27061，col=22，count=216，rate=10.18519%，jerk=0.13907，miniumu_ttc=188.02921\n\n 80%|████████  | 4/5 [00:42<00:10, 10.64s/it]\u001b[A[2024-07-23 03:36:27,301][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2718423\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 03:36:32,524][1383994605.py][line:1089][INFO] mean_spacing_error：33.92568，col=13，count=216，rate=6.01852%，jerk=0.13422，miniumu_ttc=150.96805\n\n100%|██████████| 5/5 [00:53<00:00, 10.63s/it]\u001b[A\n  1%|          | 4/500 [09:13<20:52:35, 151.52s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 03:38:50,372][1383994605.py][line:1670][INFO] 5/500. loss: 0.003934630813697974\n[2024-07-23 03:38:50,383][1383994605.py][line:1671][INFO] 5/500. mserror: 165.72125244140625  col: 72  count: 216  jerk: 0.0005599999567493796  ttc: 154.12698364257812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:38:55,512][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4605715\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 03:39:01,023][1383994605.py][line:1089][INFO] mean_spacing_error：93.90117，col=84，count=216，rate=38.88889%，jerk=0.01004，miniumu_ttc=93.47857\n\n 20%|██        | 1/5 [00:10<00:42, 10.63s/it]\u001b[A[2024-07-23 03:39:06,339][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4055218\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 03:39:11,622][1383994605.py][line:1089][INFO] mean_spacing_error：66.29776，col=71，count=216，rate=32.87037%，jerk=0.04075，miniumu_ttc=159.30824\n\n 40%|████      | 2/5 [00:21<00:31, 10.62s/it]\u001b[A[2024-07-23 03:39:16,979][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3477924\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 03:39:22,189][1383994605.py][line:1089][INFO] mean_spacing_error：27.31234，col=61，count=216，rate=28.24074%，jerk=0.10579，miniumu_ttc=151.93617\n\n 60%|██████    | 3/5 [00:31<00:21, 10.59s/it]\u001b[A[2024-07-23 03:39:27,627][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3042146\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 03:39:33,096][1383994605.py][line:1089][INFO] mean_spacing_error：41.82733，col=12，count=216，rate=5.55556%，jerk=0.11368，miniumu_ttc=256.09534\n\n 80%|████████  | 4/5 [00:42<00:10, 10.71s/it]\u001b[A[2024-07-23 03:39:38,498][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2658432\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 03:39:43,955][1383994605.py][line:1089][INFO] mean_spacing_error：29.80707，col=8，count=216，rate=3.70370%，jerk=0.15244，miniumu_ttc=135.75714\n\n100%|██████████| 5/5 [00:53<00:00, 10.71s/it]\u001b[A\n  1%|          | 5/500 [12:24<22:48:50, 165.92s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.67s/it]\u001b[A\n[2024-07-23 03:42:06,266][1383994605.py][line:1670][INFO] 6/500. loss: 0.0034806973611315093\n[2024-07-23 03:42:06,282][1383994605.py][line:1671][INFO] 6/500. mserror: 162.61683654785156  col: 73  count: 216  jerk: 0.0006844251765869558  ttc: 207.35362243652344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:42:11,541][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4664838\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 03:42:16,938][1383994605.py][line:1089][INFO] mean_spacing_error：113.80473，col=89，count=216，rate=41.20370%，jerk=0.00458，miniumu_ttc=110.83294\n\n 20%|██        | 1/5 [00:10<00:42, 10.64s/it]\u001b[A[2024-07-23 03:42:22,433][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4129790\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:42:27,962][1383994605.py][line:1089][INFO] mean_spacing_error：94.27018，col=73，count=216，rate=33.79630%，jerk=0.02040，miniumu_ttc=266.12405\n\n 40%|████      | 2/5 [00:21<00:32, 10.87s/it]\u001b[A[2024-07-23 03:42:34,029][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3658050\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 03:42:39,773][1383994605.py][line:1089][INFO] mean_spacing_error：32.64133，col=71，count=216，rate=32.87037%，jerk=0.07280，miniumu_ttc=145.27168\n\n 60%|██████    | 3/5 [00:33<00:22, 11.30s/it]\u001b[A[2024-07-23 03:42:45,460][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3122996\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 03:42:51,072][1383994605.py][line:1089][INFO] mean_spacing_error：36.48201，col=37，count=216，rate=17.12963%，jerk=0.10462，miniumu_ttc=175.53755\n\n 80%|████████  | 4/5 [00:44<00:11, 11.30s/it]\u001b[A[2024-07-23 03:42:56,846][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2863276\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 03:43:02,439][1383994605.py][line:1089][INFO] mean_spacing_error：31.49695，col=1，count=216，rate=0.46296%，jerk=0.17310，miniumu_ttc=143.59187\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n  1%|          | 6/500 [15:43<24:17:12, 176.99s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.47s/it]\u001b[A\n2it [00:03,  1.86s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 03:45:54,958][1383994605.py][line:1670][INFO] 7/500. loss: 0.0015745563432574272\n[2024-07-23 03:45:54,971][1383994605.py][line:1671][INFO] 7/500. mserror: 160.12350463867188  col: 73  count: 216  jerk: 0.0008224471821449697  ttc: 211.10829162597656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:46:00,305][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4647361\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:46:05,853][1383994605.py][line:1089][INFO] mean_spacing_error：117.08815，col=89，count=216，rate=41.20370%，jerk=0.00382，miniumu_ttc=95.29489\n\n 20%|██        | 1/5 [00:10<00:43, 10.87s/it]\u001b[A[2024-07-23 03:46:11,408][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4138305\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 03:46:17,025][1383994605.py][line:1089][INFO] mean_spacing_error：104.73756，col=73，count=216，rate=33.79630%，jerk=0.01876，miniumu_ttc=261.75906\n\n 40%|████      | 2/5 [00:22<00:33, 11.05s/it]\u001b[A[2024-07-23 03:46:22,519][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3710619\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 03:46:28,497][1383994605.py][line:1089][INFO] mean_spacing_error：38.73015，col=70，count=216，rate=32.40741%，jerk=0.07688，miniumu_ttc=159.83530\n\n 60%|██████    | 3/5 [00:33<00:22, 11.24s/it]\u001b[A[2024-07-23 03:46:34,084][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3132481\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:46:39,621][1383994605.py][line:1089][INFO] mean_spacing_error：36.29832，col=44，count=216，rate=20.37037%，jerk=0.10282，miniumu_ttc=196.26448\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 03:46:45,146][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2679892\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:46:50,684][1383994605.py][line:1089][INFO] mean_spacing_error：44.40738，col=5，count=216，rate=2.31481%，jerk=0.15554，miniumu_ttc=175.98372\n\n100%|██████████| 5/5 [00:55<00:00, 11.14s/it]\u001b[A\n  1%|▏         | 7/500 [19:31<26:31:55, 193.74s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 03:48:36,509][1383994605.py][line:1670][INFO] 8/500. loss: 0.0041487688819567365\n[2024-07-23 03:48:36,517][1383994605.py][line:1671][INFO] 8/500. mserror: 165.08743286132812  col: 73  count: 216  jerk: 0.0003161538625136018  ttc: 226.7806396484375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:48:41,523][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4661901\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.14s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 03:48:47,037][1383994605.py][line:1089][INFO] mean_spacing_error：108.19682，col=88，count=216，rate=40.74074%，jerk=0.00505，miniumu_ttc=97.91315\n\n 20%|██        | 1/5 [00:10<00:42, 10.51s/it]\u001b[A[2024-07-23 03:48:52,343][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4083037\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 03:48:57,931][1383994605.py][line:1089][INFO] mean_spacing_error：74.48544，col=75，count=216，rate=34.72222%，jerk=0.03246，miniumu_ttc=219.73769\n\n 40%|████      | 2/5 [00:21<00:32, 10.73s/it]\u001b[A[2024-07-23 03:49:03,609][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3784293\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 03:49:09,057][1383994605.py][line:1089][INFO] mean_spacing_error：32.47018，col=82，count=216，rate=37.96296%，jerk=0.09680，miniumu_ttc=136.38966\n\n 60%|██████    | 3/5 [00:32<00:21, 10.92s/it]\u001b[A[2024-07-23 03:49:14,370][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3160191\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 03:49:19,748][1383994605.py][line:1089][INFO] mean_spacing_error：56.97817，col=29，count=216，rate=13.42593%，jerk=0.09085，miniumu_ttc=198.63498\n\n 80%|████████  | 4/5 [00:43<00:10, 10.83s/it]\u001b[A[2024-07-23 03:49:25,105][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2793974\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 03:49:30,616][1383994605.py][line:1089][INFO] mean_spacing_error：50.68491，col=0，count=216，rate=0.00000%，jerk=0.18763，miniumu_ttc=147.44379\n\n100%|██████████| 5/5 [00:54<00:00, 10.82s/it]\u001b[A\n  2%|▏         | 8/500 [22:11<25:00:24, 182.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.68s/it]\u001b[A\n3it [00:04,  1.59s/it]\u001b[A\n[2024-07-23 03:51:53,814][1383994605.py][line:1670][INFO] 9/500. loss: 0.002012506748239199\n[2024-07-23 03:51:53,826][1383994605.py][line:1671][INFO] 9/500. mserror: 163.3455810546875  col: 73  count: 216  jerk: 0.0005343050579540431  ttc: 211.1222686767578\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:51:58,946][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4596821\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 03:52:04,295][1383994605.py][line:1089][INFO] mean_spacing_error：94.43698，col=88，count=216，rate=40.74074%，jerk=0.01040，miniumu_ttc=93.48310\n\n 20%|██        | 1/5 [00:10<00:41, 10.46s/it]\u001b[A[2024-07-23 03:52:09,771][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4024359\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 03:52:15,223][1383994605.py][line:1089][INFO] mean_spacing_error：67.00953，col=81，count=216，rate=37.50000%，jerk=0.04021，miniumu_ttc=111.53398\n\n 40%|████      | 2/5 [00:21<00:32, 10.74s/it]\u001b[A[2024-07-23 03:52:20,586][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3698419\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 03:52:25,962][1383994605.py][line:1089][INFO] mean_spacing_error：39.91091，col=83，count=216，rate=38.42593%，jerk=0.08477，miniumu_ttc=216.76326\n\n 60%|██████    | 3/5 [00:32<00:21, 10.74s/it]\u001b[A[2024-07-23 03:52:31,293][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3207229\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 03:52:36,562][1383994605.py][line:1089][INFO] mean_spacing_error：58.90547，col=60，count=216，rate=27.77778%，jerk=0.06521，miniumu_ttc=173.99187\n\n 80%|████████  | 4/5 [00:42<00:10, 10.69s/it]\u001b[A[2024-07-23 03:52:41,968][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2857063\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 03:52:47,606][1383994605.py][line:1089][INFO] mean_spacing_error：46.52886，col=20，count=216，rate=9.25926%，jerk=0.12560，miniumu_ttc=336.72327\n\n100%|██████████| 5/5 [00:53<00:00, 10.76s/it]\u001b[A\n  2%|▏         | 9/500 [25:28<25:33:13, 187.36s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 03:55:08,083][1383994605.py][line:1670][INFO] 10/500. loss: 0.0014559078651169937\n[2024-07-23 03:55:08,097][1383994605.py][line:1671][INFO] 10/500. mserror: 163.7195587158203  col: 72  count: 216  jerk: 0.0005519295227713883  ttc: 231.10806274414062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:55:13,213][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4525834\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:55:18,766][1383994605.py][line:1089][INFO] mean_spacing_error：70.39622，col=89，count=216，rate=41.20370%，jerk=0.01775，miniumu_ttc=96.27645\n\n 20%|██        | 1/5 [00:10<00:42, 10.66s/it]\u001b[A[2024-07-23 03:55:24,294][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.4001361\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 03:55:29,841][1383994605.py][line:1089][INFO] mean_spacing_error：54.44453，col=81，count=216，rate=37.50000%，jerk=0.04654，miniumu_ttc=107.56758\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 03:55:35,220][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3580852\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 03:55:40,635][1383994605.py][line:1089][INFO] mean_spacing_error：44.28839，col=76，count=216，rate=35.18519%，jerk=0.06406，miniumu_ttc=143.28140\n\n 60%|██████    | 3/5 [00:32<00:21, 10.85s/it]\u001b[A[2024-07-23 03:55:46,097][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3233681\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 03:55:51,482][1383994605.py][line:1089][INFO] mean_spacing_error：59.46005，col=64，count=216，rate=29.62963%，jerk=0.06025，miniumu_ttc=155.40201\n\n 80%|████████  | 4/5 [00:43<00:10, 10.85s/it]\u001b[A[2024-07-23 03:55:57,222][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2946270\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 03:56:02,629][1383994605.py][line:1089][INFO] mean_spacing_error：41.61812，col=53，count=216，rate=24.53704%，jerk=0.09512，miniumu_ttc=150.83189\n\n100%|██████████| 5/5 [00:54<00:00, 10.91s/it]\u001b[A\n  2%|▏         | 10/500 [28:43<25:49:23, 189.72s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 03:58:25,533][1383994605.py][line:1670][INFO] 11/500. loss: 0.0036729279284675917\n[2024-07-23 03:58:25,546][1383994605.py][line:1671][INFO] 11/500. mserror: 165.14334106445312  col: 72  count: 216  jerk: 0.0006054352270439267  ttc: 153.24314880371094\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 03:58:31,312][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4412899\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 03:58:37,019][1383994605.py][line:1089][INFO] mean_spacing_error：46.32148，col=87，count=216，rate=40.27778%，jerk=0.03186，miniumu_ttc=93.29391\n\n 20%|██        | 1/5 [00:11<00:45, 11.46s/it]\u001b[A[2024-07-23 03:58:42,315][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3906747\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 03:58:47,883][1383994605.py][line:1089][INFO] mean_spacing_error：41.75991，col=84，count=216，rate=38.88889%，jerk=0.05786，miniumu_ttc=145.04237\n\n 40%|████      | 2/5 [00:22<00:33, 11.11s/it]\u001b[A[2024-07-23 03:58:53,142][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3470545\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 03:58:58,564][1383994605.py][line:1089][INFO] mean_spacing_error：39.07681，col=70，count=216，rate=32.40741%，jerk=0.07000，miniumu_ttc=172.76219\n\n 60%|██████    | 3/5 [00:33<00:21, 10.91s/it]\u001b[A[2024-07-23 03:59:04,017][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3123692\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 03:59:09,650][1383994605.py][line:1089][INFO] mean_spacing_error：41.39822，col=64，count=216，rate=29.62963%，jerk=0.07342，miniumu_ttc=153.53223\n\n 80%|████████  | 4/5 [00:44<00:10, 10.98s/it]\u001b[A[2024-07-23 03:59:15,042][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2836636\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 03:59:20,547][1383994605.py][line:1089][INFO] mean_spacing_error：41.30558，col=40，count=216，rate=18.51852%，jerk=0.10628，miniumu_ttc=163.36678\n\n100%|██████████| 5/5 [00:55<00:00, 11.00s/it]\u001b[A\n  2%|▏         | 11/500 [32:01<26:06:45, 192.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 04:02:02,150][1383994605.py][line:1670][INFO] 12/500. loss: 0.0022851498797535896\n[2024-07-23 04:02:02,170][1383994605.py][line:1671][INFO] 12/500. mserror: 163.81466674804688  col: 72  count: 216  jerk: 0.0009371360065415502  ttc: 787.3055419921875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:02:08,055][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4396741\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 04:02:13,627][1383994605.py][line:1089][INFO] mean_spacing_error：61.81676，col=83，count=216，rate=38.42593%，jerk=0.02472，miniumu_ttc=112.06791\n\n 20%|██        | 1/5 [00:11<00:45, 11.45s/it]\u001b[A[2024-07-23 04:02:19,861][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3963738\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:02:25,591][1383994605.py][line:1089][INFO] mean_spacing_error：43.96226，col=80，count=216，rate=37.03704%，jerk=0.04835，miniumu_ttc=177.29303\n\n 40%|████      | 2/5 [00:23<00:35, 11.76s/it]\u001b[A[2024-07-23 04:02:31,958][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3645106\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 04:02:37,440][1383994605.py][line:1089][INFO] mean_spacing_error：33.59634，col=78，count=216，rate=36.11111%，jerk=0.08832，miniumu_ttc=133.52467\n\n 60%|██████    | 3/5 [00:35<00:23, 11.80s/it]\u001b[A[2024-07-23 04:02:43,223][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3257435\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:02:48,848][1383994605.py][line:1089][INFO] mean_spacing_error：46.33536，col=64，count=216，rate=29.62963%，jerk=0.05793，miniumu_ttc=159.44275\n\n 80%|████████  | 4/5 [00:46<00:11, 11.64s/it]\u001b[A[2024-07-23 04:02:54,753][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2961381\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 04:03:00,550][1383994605.py][line:1089][INFO] mean_spacing_error：32.78065，col=56，count=216，rate=25.92593%，jerk=0.08694，miniumu_ttc=154.05054\n\n100%|██████████| 5/5 [00:58<00:00, 11.68s/it]\u001b[A\n  2%|▏         | 12/500 [35:41<27:12:14, 200.68s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.08s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 04:04:47,868][1383994605.py][line:1670][INFO] 13/500. loss: 0.004336942608157794\n[2024-07-23 04:04:47,880][1383994605.py][line:1671][INFO] 13/500. mserror: 160.85604858398438  col: 72  count: 216  jerk: 0.0015110603999346495  ttc: 286.3955383300781\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:04:52,985][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4480610\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.12s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:04:58,658][1383994605.py][line:1089][INFO] mean_spacing_error：66.53570，col=102，count=216，rate=47.22222%，jerk=0.01820，miniumu_ttc=19.09207\n\n 20%|██        | 1/5 [00:10<00:43, 10.77s/it]\u001b[A[2024-07-23 04:05:03,957][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3880808\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 04:05:09,387][1383994605.py][line:1089][INFO] mean_spacing_error：52.13948，col=89，count=216，rate=41.20370%，jerk=0.07107，miniumu_ttc=147.45943\n\n 40%|████      | 2/5 [00:21<00:32, 10.74s/it]\u001b[A[2024-07-23 04:05:14,672][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3598045\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 04:05:20,086][1383994605.py][line:1089][INFO] mean_spacing_error：42.88389，col=66，count=216，rate=30.55556%，jerk=0.06090，miniumu_ttc=153.95964\n\n 60%|██████    | 3/5 [00:32<00:21, 10.73s/it]\u001b[A[2024-07-23 04:05:25,315][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3245647\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:05:30,933][1383994605.py][line:1089][INFO] mean_spacing_error：41.84476，col=59，count=216，rate=27.31481%，jerk=0.07927，miniumu_ttc=162.52489\n\n 80%|████████  | 4/5 [00:43<00:10, 10.77s/it]\u001b[A[2024-07-23 04:05:36,201][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2862084\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 04:05:41,613][1383994605.py][line:1089][INFO] mean_spacing_error：30.64481，col=36，count=216，rate=16.66667%，jerk=0.10991，miniumu_ttc=183.50301\n\n100%|██████████| 5/5 [00:53<00:00, 10.75s/it]\u001b[A\n  3%|▎         | 13/500 [38:22<25:31:26, 188.68s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:01,  1.99s/it]\u001b[A\n2it [00:03,  1.65s/it]\u001b[A\n3it [00:04,  1.55s/it]\u001b[A\n[2024-07-23 04:07:20,284][1383994605.py][line:1670][INFO] 14/500. loss: 0.004484308262666066\n[2024-07-23 04:07:20,298][1383994605.py][line:1671][INFO] 14/500. mserror: 157.17306518554688  col: 72  count: 216  jerk: 0.0021858366671949625  ttc: 216.12445068359375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:07:25,233][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4477351\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 04:07:30,426][1383994605.py][line:1089][INFO] mean_spacing_error：66.07317，col=86，count=216，rate=39.81481%，jerk=0.01956，miniumu_ttc=88.65350\n\n 20%|██        | 1/5 [00:10<00:40, 10.13s/it]\u001b[A[2024-07-23 04:07:35,874][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3869713\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 04:07:41,233][1383994605.py][line:1089][INFO] mean_spacing_error：47.52273，col=88，count=216，rate=40.74074%，jerk=0.08018，miniumu_ttc=156.23380\n\n 40%|████      | 2/5 [00:20<00:31, 10.52s/it]\u001b[A[2024-07-23 04:07:46,613][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3616588\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 04:07:52,024][1383994605.py][line:1089][INFO] mean_spacing_error：37.57438，col=69，count=216，rate=31.94444%，jerk=0.06819，miniumu_ttc=146.85527\n\n 60%|██████    | 3/5 [00:31<00:21, 10.65s/it]\u001b[A[2024-07-23 04:07:57,392][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3259922\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 04:08:02,630][1383994605.py][line:1089][INFO] mean_spacing_error：32.82589，col=61，count=216，rate=28.24074%，jerk=0.07614，miniumu_ttc=167.95590\n\n 80%|████████  | 4/5 [00:42<00:10, 10.63s/it]\u001b[A[2024-07-23 04:08:08,180][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2852302\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 04:08:13,589][1383994605.py][line:1089][INFO] mean_spacing_error：31.30565，col=34，count=216，rate=15.74074%，jerk=0.11045，miniumu_ttc=144.02367\n\n100%|██████████| 5/5 [00:53<00:00, 10.66s/it]\u001b[A\n  3%|▎         | 14/500 [40:54<23:58:28, 177.59s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.39s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 04:09:46,260][1383994605.py][line:1670][INFO] 15/500. loss: 0.0034845403085152307\n[2024-07-23 04:09:46,270][1383994605.py][line:1671][INFO] 15/500. mserror: 161.03355407714844  col: 72  count: 216  jerk: 0.0016853989800438285  ttc: 676.6099853515625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:09:51,303][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4405092\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 04:09:56,681][1383994605.py][line:1089][INFO] mean_spacing_error：43.28195，col=86，count=216，rate=39.81481%，jerk=0.03154，miniumu_ttc=90.58206\n\n 20%|██        | 1/5 [00:10<00:41, 10.40s/it]\u001b[A[2024-07-23 04:10:01,985][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3887114\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 04:10:07,441][1383994605.py][line:1089][INFO] mean_spacing_error：37.70528，col=80，count=216，rate=37.03704%，jerk=0.05331，miniumu_ttc=152.35385\n\n 40%|████      | 2/5 [00:21<00:31, 10.61s/it]\u001b[A[2024-07-23 04:10:12,884][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3598614\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.62s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 04:10:18,602][1383994605.py][line:1089][INFO] mean_spacing_error：34.09376，col=72，count=216，rate=33.33333%，jerk=0.07877，miniumu_ttc=140.51497\n\n 60%|██████    | 3/5 [00:32<00:21, 10.86s/it]\u001b[A[2024-07-23 04:10:24,009][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3284292\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 04:10:29,384][1383994605.py][line:1089][INFO] mean_spacing_error：46.45660，col=61，count=216，rate=28.24074%，jerk=0.06758，miniumu_ttc=161.17198\n\n 80%|████████  | 4/5 [00:43<00:10, 10.83s/it]\u001b[A[2024-07-23 04:10:34,750][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2912661\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 04:10:40,170][1383994605.py][line:1089][INFO] mean_spacing_error：33.17615，col=39，count=216，rate=18.05556%，jerk=0.09836，miniumu_ttc=146.77271\n\n100%|██████████| 5/5 [00:53<00:00, 10.78s/it]\u001b[A\n  3%|▎         | 15/500 [43:21<22:39:58, 168.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.31s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 04:12:57,805][1383994605.py][line:1670][INFO] 16/500. loss: 0.00345491711050272\n[2024-07-23 04:12:57,816][1383994605.py][line:1671][INFO] 16/500. mserror: 166.12013244628906  col: 70  count: 216  jerk: 0.0012221483048051596  ttc: 152.3966827392578\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:13:03,145][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4421709\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 04:13:08,613][1383994605.py][line:1089][INFO] mean_spacing_error：39.15657，col=90，count=216，rate=41.66667%，jerk=0.04439，miniumu_ttc=83.86994\n\n 20%|██        | 1/5 [00:10<00:43, 10.79s/it]\u001b[A[2024-07-23 04:13:13,880][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3866328\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 04:13:19,333][1383994605.py][line:1089][INFO] mean_spacing_error：31.50289，col=82，count=216，rate=37.96296%，jerk=0.07009，miniumu_ttc=152.35027\n\n 40%|████      | 2/5 [00:21<00:32, 10.75s/it]\u001b[A[2024-07-23 04:13:24,771][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3540220\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 04:13:30,430][1383994605.py][line:1089][INFO] mean_spacing_error：37.95913，col=70，count=216，rate=32.40741%，jerk=0.06984，miniumu_ttc=155.98277\n\n 60%|██████    | 3/5 [00:32<00:21, 10.91s/it]\u001b[A[2024-07-23 04:13:36,025][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3219393\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:13:41,593][1383994605.py][line:1089][INFO] mean_spacing_error：32.72370，col=67，count=216，rate=31.01852%，jerk=0.07861，miniumu_ttc=149.04930\n\n 80%|████████  | 4/5 [00:43<00:11, 11.01s/it]\u001b[A[2024-07-23 04:13:47,296][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2904208\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 04:13:52,782][1383994605.py][line:1089][INFO] mean_spacing_error：51.36483，col=45，count=216，rate=20.83333%，jerk=0.08649，miniumu_ttc=173.96732\n\n100%|██████████| 5/5 [00:54<00:00, 10.99s/it]\u001b[A\n  3%|▎         | 16/500 [46:33<23:36:20, 175.58s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.68s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 04:16:18,588][1383994605.py][line:1670][INFO] 17/500. loss: 0.0036930199712514877\n[2024-07-23 04:16:18,596][1383994605.py][line:1671][INFO] 17/500. mserror: 170.01193237304688  col: 70  count: 216  jerk: 0.001055240398272872  ttc: 154.96717834472656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:16:23,616][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4387899\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 04:16:29,141][1383994605.py][line:1089][INFO] mean_spacing_error：33.28241，col=98，count=216，rate=45.37037%，jerk=0.04894，miniumu_ttc=88.50549\n\n 20%|██        | 1/5 [00:10<00:42, 10.53s/it]\u001b[A[2024-07-23 04:16:34,539][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3866508\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:16:40,163][1383994605.py][line:1089][INFO] mean_spacing_error：35.59652，col=85，count=216，rate=39.35185%，jerk=0.06931，miniumu_ttc=106.43102\n\n 40%|████      | 2/5 [00:21<00:32, 10.82s/it]\u001b[A[2024-07-23 04:16:45,415][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3507847\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 04:16:50,806][1383994605.py][line:1089][INFO] mean_spacing_error：33.83297，col=72，count=216，rate=33.33333%，jerk=0.06728，miniumu_ttc=148.74594\n\n 60%|██████    | 3/5 [00:32<00:21, 10.74s/it]\u001b[A[2024-07-23 04:16:56,233][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3238609\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 04:17:01,593][1383994605.py][line:1089][INFO] mean_spacing_error：34.14041，col=67，count=216，rate=31.01852%，jerk=0.06665，miniumu_ttc=147.19212\n\n 80%|████████  | 4/5 [00:42<00:10, 10.76s/it]\u001b[A[2024-07-23 04:17:07,350][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2929911\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:17:13,034][1383994605.py][line:1089][INFO] mean_spacing_error：30.31380，col=56，count=216，rate=25.92593%，jerk=0.09446，miniumu_ttc=144.66402\n\n100%|██████████| 5/5 [00:54<00:00, 10.89s/it]\u001b[A\n  3%|▎         | 17/500 [49:53<24:33:07, 183.00s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 04:19:35,721][1383994605.py][line:1670][INFO] 18/500. loss: 0.0041108957181374235\n[2024-07-23 04:19:35,736][1383994605.py][line:1671][INFO] 18/500. mserror: 170.59149169921875  col: 70  count: 216  jerk: 0.0014108703471720219  ttc: 156.52621459960938\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:19:41,037][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4420823\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 04:19:46,755][1383994605.py][line:1089][INFO] mean_spacing_error：51.33915，col=101，count=216，rate=46.75926%，jerk=0.03124，miniumu_ttc=13.29022\n\n 20%|██        | 1/5 [00:11<00:44, 11.01s/it]\u001b[A[2024-07-23 04:19:52,567][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3837462\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 04:19:58,277][1383994605.py][line:1089][INFO] mean_spacing_error：35.54208，col=90，count=216，rate=41.66667%，jerk=0.07602，miniumu_ttc=101.61656\n\n 40%|████      | 2/5 [00:22<00:33, 11.31s/it]\u001b[A[2024-07-23 04:20:04,067][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3580486\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:20:09,674][1383994605.py][line:1089][INFO] mean_spacing_error：39.98827，col=70，count=216，rate=32.40741%，jerk=0.06478，miniumu_ttc=164.26569\n\n 60%|██████    | 3/5 [00:33<00:22, 11.36s/it]\u001b[A[2024-07-23 04:20:15,283][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3342533\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 04:20:21,155][1383994605.py][line:1089][INFO] mean_spacing_error：32.97059，col=66，count=216，rate=30.55556%，jerk=0.07147，miniumu_ttc=154.18198\n\n 80%|████████  | 4/5 [00:45<00:11, 11.40s/it]\u001b[A[2024-07-23 04:20:26,902][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3069728\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:20:32,584][1383994605.py][line:1089][INFO] mean_spacing_error：31.75048，col=64，count=216，rate=29.62963%，jerk=0.07922，miniumu_ttc=145.92538\n\n100%|██████████| 5/5 [00:56<00:00, 11.37s/it]\u001b[A\n  4%|▎         | 18/500 [53:13<25:10:05, 187.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.86s/it]\u001b[A\n[2024-07-23 04:22:24,459][1383994605.py][line:1670][INFO] 19/500. loss: 0.0041258251294493675\n[2024-07-23 04:22:24,478][1383994605.py][line:1671][INFO] 19/500. mserror: 165.4558563232422  col: 70  count: 216  jerk: 0.00253144814632833  ttc: 154.24588012695312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:22:30,049][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4421932\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 04:22:35,808][1383994605.py][line:1089][INFO] mean_spacing_error：65.97997，col=97，count=216，rate=44.90741%，jerk=0.02127，miniumu_ttc=13.03936\n\n 20%|██        | 1/5 [00:11<00:45, 11.32s/it]\u001b[A[2024-07-23 04:22:41,233][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3848776\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 04:22:46,870][1383994605.py][line:1089][INFO] mean_spacing_error：50.33349，col=81，count=216，rate=37.50000%，jerk=0.04994，miniumu_ttc=182.74814\n\n 40%|████      | 2/5 [00:22<00:33, 11.17s/it]\u001b[A[2024-07-23 04:22:52,325][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3504134\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 04:22:57,900][1383994605.py][line:1089][INFO] mean_spacing_error：40.60574，col=74，count=216，rate=34.25926%，jerk=0.07616，miniumu_ttc=145.94241\n\n 60%|██████    | 3/5 [00:33<00:22, 11.10s/it]\u001b[A[2024-07-23 04:23:03,671][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3250575\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:23:09,281][1383994605.py][line:1089][INFO] mean_spacing_error：39.71040，col=68，count=216，rate=31.48148%，jerk=0.07361，miniumu_ttc=152.39134\n\n 80%|████████  | 4/5 [00:44<00:11, 11.21s/it]\u001b[A[2024-07-23 04:23:14,695][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2961554\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 04:23:20,460][1383994605.py][line:1089][INFO] mean_spacing_error：34.04090，col=61，count=216，rate=28.24074%，jerk=0.09969，miniumu_ttc=145.33754\n\n100%|██████████| 5/5 [00:55<00:00, 11.19s/it]\u001b[A\n  4%|▍         | 19/500 [56:01<24:18:30, 181.93s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 04:25:45,345][1383994605.py][line:1670][INFO] 20/500. loss: 0.004045288388927777\n[2024-07-23 04:25:45,361][1383994605.py][line:1671][INFO] 20/500. mserror: 153.7904052734375  col: 70  count: 216  jerk: 0.004871859215199947  ttc: 162.0417022705078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:25:51,012][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4458238\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 04:25:56,808][1383994605.py][line:1089][INFO] mean_spacing_error：54.72974，col=98，count=216，rate=45.37037%，jerk=0.02940，miniumu_ttc=23.81714\n\n 20%|██        | 1/5 [00:11<00:45, 11.44s/it]\u001b[A[2024-07-23 04:26:02,722][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3846806\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 04:26:08,502][1383994605.py][line:1089][INFO] mean_spacing_error：31.03470，col=83，count=216，rate=38.42593%，jerk=0.05948，miniumu_ttc=167.95935\n\n 40%|████      | 2/5 [00:23<00:34, 11.59s/it]\u001b[A[2024-07-23 04:26:14,118][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3573948\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 04:26:19,833][1383994605.py][line:1089][INFO] mean_spacing_error：30.81524，col=74，count=216，rate=34.25926%，jerk=0.08266，miniumu_ttc=148.53833\n\n 60%|██████    | 3/5 [00:34<00:22, 11.47s/it]\u001b[A[2024-07-23 04:26:25,480][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3308910\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:26:31,248][1383994605.py][line:1089][INFO] mean_spacing_error：44.64124，col=65，count=216，rate=30.09259%，jerk=0.06137，miniumu_ttc=157.47957\n\n 80%|████████  | 4/5 [00:45<00:11, 11.45s/it]\u001b[A[2024-07-23 04:26:37,130][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3019358\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:26:42,874][1383994605.py][line:1089][INFO] mean_spacing_error：33.51896，col=60，count=216，rate=27.77778%，jerk=0.08689，miniumu_ttc=157.93481\n\n100%|██████████| 5/5 [00:57<00:00, 11.50s/it]\u001b[A\n  4%|▍         | 20/500 [59:23<25:04:41, 188.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 04:29:09,327][1383994605.py][line:1670][INFO] 21/500. loss: 0.003511195071041584\n[2024-07-23 04:29:09,339][1383994605.py][line:1671][INFO] 21/500. mserror: 142.65489196777344  col: 71  count: 216  jerk: 0.007328984327614307  ttc: 1478.5394287109375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:29:14,715][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4606873\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 04:29:20,758][1383994605.py][line:1089][INFO] mean_spacing_error：57.45815，col=89，count=216，rate=41.20370%，jerk=0.02342，miniumu_ttc=21.06814\n\n 20%|██        | 1/5 [00:11<00:45, 11.41s/it]\u001b[A[2024-07-23 04:29:26,581][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3900802\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:29:32,155][1383994605.py][line:1089][INFO] mean_spacing_error：52.05601，col=76，count=216，rate=35.18519%，jerk=0.03823，miniumu_ttc=235.68068\n\n 40%|████      | 2/5 [00:22<00:34, 11.40s/it]\u001b[A[2024-07-23 04:29:37,867][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3581681\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 04:29:43,577][1383994605.py][line:1089][INFO] mean_spacing_error：30.89165，col=80，count=216，rate=37.03704%，jerk=0.09608，miniumu_ttc=129.82431\n\n 60%|██████    | 3/5 [00:34<00:22, 11.41s/it]\u001b[A[2024-07-23 04:29:49,408][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3302803\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 04:29:55,323][1383994605.py][line:1089][INFO] mean_spacing_error：36.87781，col=66，count=216，rate=30.55556%，jerk=0.06418，miniumu_ttc=151.21819\n\n 80%|████████  | 4/5 [00:45<00:11, 11.54s/it]\u001b[A[2024-07-23 04:30:01,106][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3090791\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:30:06,797][1383994605.py][line:1089][INFO] mean_spacing_error：32.34694，col=47，count=216，rate=21.75926%，jerk=0.08528，miniumu_ttc=180.79681\n\n100%|██████████| 5/5 [00:57<00:00, 11.49s/it]\u001b[A\n  4%|▍         | 21/500 [1:02:47<25:39:32, 192.84s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 04:31:59,973][1383994605.py][line:1670][INFO] 22/500. loss: 0.0022029951214790344\n[2024-07-23 04:32:00,002][1383994605.py][line:1671][INFO] 22/500. mserror: 155.41880798339844  col: 68  count: 216  jerk: 0.0051514506340026855  ttc: 153.43247985839844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:32:05,395][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4420418\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 04:32:11,343][1383994605.py][line:1089][INFO] mean_spacing_error：36.47649，col=87，count=216，rate=40.27778%，jerk=0.03888，miniumu_ttc=95.01251\n\n 20%|██        | 1/5 [00:11<00:45, 11.34s/it]\u001b[A[2024-07-23 04:32:16,896][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3753896\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 04:32:22,922][1383994605.py][line:1089][INFO] mean_spacing_error：37.56865，col=74，count=216，rate=34.25926%，jerk=0.05076，miniumu_ttc=134.70068\n\n 40%|████      | 2/5 [00:22<00:34, 11.48s/it]\u001b[A[2024-07-23 04:32:28,724][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3452507\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 04:32:34,749][1383994605.py][line:1089][INFO] mean_spacing_error：28.23867，col=72，count=216，rate=33.33333%，jerk=0.08572，miniumu_ttc=150.11397\n\n 60%|██████    | 3/5 [00:34<00:23, 11.64s/it]\u001b[A[2024-07-23 04:32:40,333][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3132628\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:32:46,065][1383994605.py][line:1089][INFO] mean_spacing_error：37.45287，col=63，count=216，rate=29.16667%，jerk=0.06836，miniumu_ttc=158.91249\n\n 80%|████████  | 4/5 [00:46<00:11, 11.51s/it]\u001b[A[2024-07-23 04:32:51,642][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2878669\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:32:57,388][1383994605.py][line:1089][INFO] mean_spacing_error：28.09878，col=55，count=216，rate=25.46296%，jerk=0.09657，miniumu_ttc=155.23248\n\n100%|██████████| 5/5 [00:57<00:00, 11.48s/it]\u001b[A\n  4%|▍         | 22/500 [1:05:38<24:43:09, 186.17s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.67s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 04:35:25,649][1383994605.py][line:1670][INFO] 23/500. loss: 0.004020156959692637\n[2024-07-23 04:35:25,663][1383994605.py][line:1671][INFO] 23/500. mserror: 162.91018676757812  col: 67  count: 216  jerk: 0.004537815228104591  ttc: 157.03689575195312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:35:30,947][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4310047\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:35:36,657][1383994605.py][line:1089][INFO] mean_spacing_error：39.52946，col=81，count=216，rate=37.50000%，jerk=0.04036，miniumu_ttc=103.14318\n\n 20%|██        | 1/5 [00:11<00:44, 11.00s/it]\u001b[A[2024-07-23 04:35:42,215][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3710311\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 04:35:48,020][1383994605.py][line:1089][INFO] mean_spacing_error：37.23868，col=75，count=216，rate=34.72222%，jerk=0.06308，miniumu_ttc=177.61656\n\n 40%|████      | 2/5 [00:22<00:33, 11.21s/it]\u001b[A[2024-07-23 04:35:53,609][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3388447\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:35:59,139][1383994605.py][line:1089][INFO] mean_spacing_error：36.28054，col=65，count=216，rate=30.09259%，jerk=0.08031，miniumu_ttc=158.63242\n\n 60%|██████    | 3/5 [00:33<00:22, 11.17s/it]\u001b[A[2024-07-23 04:36:04,823][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3098950\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 04:36:10,280][1383994605.py][line:1089][INFO] mean_spacing_error：35.20441，col=60，count=216，rate=27.77778%，jerk=0.08276，miniumu_ttc=163.14349\n\n 80%|████████  | 4/5 [00:44<00:11, 11.15s/it]\u001b[A[2024-07-23 04:36:15,845][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2823684\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 04:36:21,302][1383994605.py][line:1089][INFO] mean_spacing_error：34.15846，col=50，count=216，rate=23.14815%，jerk=0.09568，miniumu_ttc=141.71127\n\n100%|██████████| 5/5 [00:55<00:00, 11.13s/it]\u001b[A\n  5%|▍         | 23/500 [1:09:02<25:22:19, 191.49s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.86s/it]\u001b[A\n2it [00:05,  2.52s/it]\u001b[A\n3it [00:06,  2.31s/it]\u001b[A\n[2024-07-23 04:38:59,335][1383994605.py][line:1670][INFO] 24/500. loss: 0.004023434904714425\n[2024-07-23 04:38:59,351][1383994605.py][line:1671][INFO] 24/500. mserror: 154.75799560546875  col: 67  count: 216  jerk: 0.006187686696648598  ttc: 153.6689910888672\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:39:05,585][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4331493\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 04:39:11,370][1383994605.py][line:1089][INFO] mean_spacing_error：34.08862，col=78，count=216，rate=36.11111%，jerk=0.04753，miniumu_ttc=107.60696\n\n 20%|██        | 1/5 [00:12<00:48, 12.01s/it]\u001b[A[2024-07-23 04:39:17,692][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3672417\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.62s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 04:39:23,766][1383994605.py][line:1089][INFO] mean_spacing_error：36.39317，col=70，count=216，rate=32.40741%，jerk=0.05804，miniumu_ttc=155.57948\n\n 40%|████      | 2/5 [00:24<00:36, 12.24s/it]\u001b[A[2024-07-23 04:39:30,002][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3348805\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 04:39:35,933][1383994605.py][line:1089][INFO] mean_spacing_error：29.03904，col=65，count=216，rate=30.09259%，jerk=0.09510，miniumu_ttc=156.06264\n\n 60%|██████    | 3/5 [00:36<00:24, 12.20s/it]\u001b[A[2024-07-23 04:39:41,998][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3064410\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 04:39:47,985][1383994605.py][line:1089][INFO] mean_spacing_error：48.98623，col=47，count=216，rate=21.75926%，jerk=0.08660，miniumu_ttc=220.95383\n\n 80%|████████  | 4/5 [00:48<00:12, 12.14s/it]\u001b[A[2024-07-23 04:39:54,470][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2777232\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.92s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.26s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.12s/it]\u001b[A\u001b[A\n[2024-07-23 04:40:01,110][1383994605.py][line:1089][INFO] mean_spacing_error：30.30190，col=40，count=216，rate=18.51852%，jerk=0.11951，miniumu_ttc=135.22733\n\n100%|██████████| 5/5 [01:01<00:00, 12.35s/it]\u001b[A\n  5%|▍         | 24/500 [1:12:42<26:26:34, 199.99s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.83s/it]\u001b[A\n3it [00:05,  1.73s/it]\u001b[A\n[2024-07-23 04:42:44,789][1383994605.py][line:1670][INFO] 25/500. loss: 0.002060130083312591\n[2024-07-23 04:42:44,805][1383994605.py][line:1671][INFO] 25/500. mserror: 135.97718811035156  col: 68  count: 216  jerk: 0.009176074527204037  ttc: 297.8138122558594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:42:50,677][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4588274\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 04:42:56,547][1383994605.py][line:1089][INFO] mean_spacing_error：41.40992，col=79，count=216，rate=36.57407%，jerk=0.03712，miniumu_ttc=95.75761\n\n 20%|██        | 1/5 [00:11<00:46, 11.73s/it]\u001b[A[2024-07-23 04:43:02,638][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3639850\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 04:43:08,474][1383994605.py][line:1089][INFO] mean_spacing_error：46.39054，col=71，count=216，rate=32.87037%，jerk=0.05281，miniumu_ttc=285.21664\n\n 40%|████      | 2/5 [00:23<00:35, 11.85s/it]\u001b[A[2024-07-23 04:43:14,130][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3295469\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 04:43:20,220][1383994605.py][line:1089][INFO] mean_spacing_error：30.92735，col=62，count=216，rate=28.70370%，jerk=0.08639，miniumu_ttc=151.31001\n\n 60%|██████    | 3/5 [00:35<00:23, 11.80s/it]\u001b[A[2024-07-23 04:43:26,702][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2966061\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:43:32,385][1383994605.py][line:1089][INFO] mean_spacing_error：36.75993，col=42，count=216，rate=19.44444%，jerk=0.08895，miniumu_ttc=150.41100\n\n 80%|████████  | 4/5 [00:47<00:11, 11.94s/it]\u001b[A[2024-07-23 04:43:38,379][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2580030\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.61s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 04:43:44,512][1383994605.py][line:1089][INFO] mean_spacing_error：37.27707，col=16，count=216，rate=7.40741%，jerk=0.13412，miniumu_ttc=155.99332\n\n100%|██████████| 5/5 [00:59<00:00, 11.94s/it]\u001b[A\n  5%|▌         | 25/500 [1:16:25<27:18:49, 207.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 04:45:33,400][1383994605.py][line:1670][INFO] 26/500. loss: 0.0033341978366176286\n[2024-07-23 04:45:33,410][1383994605.py][line:1671][INFO] 26/500. mserror: 110.33799743652344  col: 71  count: 216  jerk: 0.014809442684054375  ttc: 164.43629455566406\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:45:38,873][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4845131\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:45:44,627][1383994605.py][line:1089][INFO] mean_spacing_error：34.08786，col=80，count=216，rate=37.03704%，jerk=0.04944，miniumu_ttc=94.45139\n\n 20%|██        | 1/5 [00:11<00:44, 11.21s/it]\u001b[A[2024-07-23 04:45:50,778][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3587764\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 04:45:56,854][1383994605.py][line:1089][INFO] mean_spacing_error：46.88510，col=68，count=216，rate=31.48148%，jerk=0.05786，miniumu_ttc=160.02814\n\n 40%|████      | 2/5 [00:23<00:35, 11.81s/it]\u001b[A[2024-07-23 04:46:02,531][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3195400\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.74s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.14s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.01s/it]\u001b[A\u001b[A\n[2024-07-23 04:46:08,837][1383994605.py][line:1089][INFO] mean_spacing_error：28.30565，col=52，count=216，rate=24.07407%，jerk=0.11267，miniumu_ttc=165.81575\n\n 60%|██████    | 3/5 [00:35<00:23, 11.89s/it]\u001b[A[2024-07-23 04:46:14,766][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2937834\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 04:46:20,664][1383994605.py][line:1089][INFO] mean_spacing_error：52.17564，col=15，count=216，rate=6.94444%，jerk=0.14735，miniumu_ttc=163.79906\n\n 80%|████████  | 4/5 [00:47<00:11, 11.86s/it]\u001b[A[2024-07-23 04:46:26,183][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2514832\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:46:31,760][1383994605.py][line:1089][INFO] mean_spacing_error：55.51458，col=11，count=216，rate=5.09259%，jerk=0.16061，miniumu_ttc=176.70497\n\n100%|██████████| 5/5 [00:58<00:00, 11.67s/it]\u001b[A\n  5%|▌         | 26/500 [1:19:12<25:41:06, 195.08s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 04:48:17,358][1383994605.py][line:1670][INFO] 27/500. loss: 0.004247887370487054\n[2024-07-23 04:48:17,370][1383994605.py][line:1671][INFO] 27/500. mserror: 131.7247314453125  col: 67  count: 216  jerk: 0.011817497201263905  ttc: 152.51307678222656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:48:22,562][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4524185\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 04:48:28,218][1383994605.py][line:1089][INFO] mean_spacing_error：39.64262，col=71，count=216，rate=32.87037%，jerk=0.05593，miniumu_ttc=140.38361\n\n 20%|██        | 1/5 [00:10<00:43, 10.84s/it]\u001b[A[2024-07-23 04:48:33,758][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3478087\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 04:48:39,446][1383994605.py][line:1089][INFO] mean_spacing_error：46.27362，col=58，count=216，rate=26.85185%，jerk=0.06599，miniumu_ttc=297.19940\n\n 40%|████      | 2/5 [00:22<00:33, 11.07s/it]\u001b[A[2024-07-23 04:48:45,045][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3118732\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 04:48:50,990][1383994605.py][line:1089][INFO] mean_spacing_error：29.02048，col=14，count=216，rate=6.48148%，jerk=0.14879，miniumu_ttc=201.80725\n\n 60%|██████    | 3/5 [00:33<00:22, 11.29s/it]\u001b[A[2024-07-23 04:48:56,476][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2869364\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.78s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 04:49:02,609][1383994605.py][line:1089][INFO] mean_spacing_error：55.51102，col=1，count=216，rate=0.46296%，jerk=0.18953，miniumu_ttc=154.52396\n\n 80%|████████  | 4/5 [00:45<00:11, 11.42s/it]\u001b[A[2024-07-23 04:49:08,269][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2454880\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 04:49:13,971][1383994605.py][line:1089][INFO] mean_spacing_error：31.66054，col=12，count=216，rate=5.55556%，jerk=0.14155，miniumu_ttc=133.89091\n\n100%|██████████| 5/5 [00:56<00:00, 11.32s/it]\u001b[A\n  5%|▌         | 27/500 [1:21:54<24:20:08, 185.22s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.08s/it]\u001b[A\n2it [00:03,  1.69s/it]\u001b[A\n3it [00:04,  1.59s/it]\u001b[A\n[2024-07-23 04:51:37,377][1383994605.py][line:1670][INFO] 28/500. loss: 0.0013506272807717323\n[2024-07-23 04:51:37,389][1383994605.py][line:1671][INFO] 28/500. mserror: 146.38722229003906  col: 65  count: 216  jerk: 0.010522277094423771  ttc: 156.6954803466797\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:51:42,883][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4385161\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:51:48,440][1383994605.py][line:1089][INFO] mean_spacing_error：47.66901，col=67，count=216，rate=31.01852%，jerk=0.05245，miniumu_ttc=142.65753\n\n 20%|██        | 1/5 [00:11<00:44, 11.04s/it]\u001b[A[2024-07-23 04:51:53,895][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3421925\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:51:59,450][1383994605.py][line:1089][INFO] mean_spacing_error：55.72125，col=52，count=216，rate=24.07407%，jerk=0.06678，miniumu_ttc=197.12148\n\n 40%|████      | 2/5 [00:22<00:33, 11.02s/it]\u001b[A[2024-07-23 04:52:05,003][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3015067\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 04:52:10,758][1383994605.py][line:1089][INFO] mean_spacing_error：45.56321，col=13，count=216，rate=6.01852%，jerk=0.15033，miniumu_ttc=177.07344\n\n 60%|██████    | 3/5 [00:33<00:22, 11.15s/it]\u001b[A[2024-07-23 04:52:16,223][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2764685\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 04:52:21,738][1383994605.py][line:1089][INFO] mean_spacing_error：63.35764，col=8，count=216，rate=3.70370%，jerk=0.17442，miniumu_ttc=146.33250\n\n 80%|████████  | 4/5 [00:44<00:11, 11.08s/it]\u001b[A[2024-07-23 04:52:27,336][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2328495\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 04:52:32,682][1383994605.py][line:1089][INFO] mean_spacing_error：35.14635，col=12，count=216，rate=5.55556%，jerk=0.13647，miniumu_ttc=121.07635\n\n100%|██████████| 5/5 [00:55<00:00, 11.06s/it]\u001b[A\n  6%|▌         | 28/500 [1:25:13<24:48:53, 189.27s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 04:54:16,454][1383994605.py][line:1670][INFO] 29/500. loss: 0.004187531458834807\n[2024-07-23 04:54:16,471][1383994605.py][line:1671][INFO] 29/500. mserror: 142.31027221679688  col: 63  count: 216  jerk: 0.012433498166501522  ttc: 157.9910888671875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:54:21,808][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4340664\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 04:54:27,367][1383994605.py][line:1089][INFO] mean_spacing_error：42.13103，col=82，count=216，rate=37.96296%，jerk=0.05826，miniumu_ttc=20.90847\n\n 20%|██        | 1/5 [00:10<00:43, 10.89s/it]\u001b[A[2024-07-23 04:54:32,586][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3301428\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 04:54:38,075][1383994605.py][line:1089][INFO] mean_spacing_error：42.94144，col=28，count=216，rate=12.96296%，jerk=0.10652，miniumu_ttc=184.83252\n\n 40%|████      | 2/5 [00:21<00:32, 10.78s/it]\u001b[A[2024-07-23 04:54:43,483][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2849762\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 04:54:48,991][1383994605.py][line:1089][INFO] mean_spacing_error：43.13146，col=10，count=216，rate=4.62963%，jerk=0.15951，miniumu_ttc=184.50171\n\n 60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\u001b[A[2024-07-23 04:54:54,478][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2648582\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.70s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 04:55:00,425][1383994605.py][line:1089][INFO] mean_spacing_error：38.02849，col=9，count=216，rate=4.16667%，jerk=0.15923，miniumu_ttc=154.44820\n\n 80%|████████  | 4/5 [00:43<00:11, 11.08s/it]\u001b[A[2024-07-23 04:55:05,929][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2254941\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 04:55:11,366][1383994605.py][line:1089][INFO] mean_spacing_error：28.58772，col=13，count=216，rate=6.01852%，jerk=0.14847，miniumu_ttc=135.80356\n\n100%|██████████| 5/5 [00:54<00:00, 10.98s/it]\u001b[A\n  6%|▌         | 29/500 [1:27:52<23:33:43, 180.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 04:56:51,379][1383994605.py][line:1670][INFO] 30/500. loss: 0.0019516702741384506\n[2024-07-23 04:56:51,391][1383994605.py][line:1671][INFO] 30/500. mserror: 128.90284729003906  col: 64  count: 216  jerk: 0.01501974556595087  ttc: 151.68389892578125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 04:56:57,207][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4427153\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 04:57:02,800][1383994605.py][line:1089][INFO] mean_spacing_error：33.96239，col=130，count=216，rate=60.18519%，jerk=0.07760，miniumu_ttc=14.62890\n\n 20%|██        | 1/5 [00:11<00:45, 11.40s/it]\u001b[A[2024-07-23 04:57:08,516][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3288930\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 04:57:13,906][1383994605.py][line:1089][INFO] mean_spacing_error：56.57711，col=32，count=216，rate=14.81481%，jerk=0.11482，miniumu_ttc=200.20894\n\n 40%|████      | 2/5 [00:22<00:33, 11.22s/it]\u001b[A[2024-07-23 04:57:19,452][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2890194\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 04:57:24,861][1383994605.py][line:1089][INFO] mean_spacing_error：35.39136，col=13，count=216，rate=6.01852%，jerk=0.15065，miniumu_ttc=144.88510\n\n 60%|██████    | 3/5 [00:33<00:22, 11.10s/it]\u001b[A[2024-07-23 04:57:30,663][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2670377\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 04:57:36,280][1383994605.py][line:1089][INFO] mean_spacing_error：30.91496，col=11，count=216，rate=5.09259%，jerk=0.13442，miniumu_ttc=104.73752\n\n 80%|████████  | 4/5 [00:44<00:11, 11.23s/it]\u001b[A[2024-07-23 04:57:41,924][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2448031\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 04:57:47,541][1383994605.py][line:1089][INFO] mean_spacing_error：49.29087，col=8，count=216，rate=3.70370%，jerk=0.11365，miniumu_ttc=124.35994\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n  6%|▌         | 30/500 [1:30:28<22:34:30, 172.92s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.41s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 05:00:06,436][1383994605.py][line:1670][INFO] 31/500. loss: 0.003693827117482821\n[2024-07-23 05:00:06,450][1383994605.py][line:1671][INFO] 31/500. mserror: 114.609375  col: 62  count: 216  jerk: 0.020014546811580658  ttc: 150.89901733398438\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:00:12,024][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4243226\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:00:17,656][1383994605.py][line:1089][INFO] mean_spacing_error：34.59421，col=72，count=216，rate=33.33333%，jerk=0.10070，miniumu_ttc=17.28316\n\n 20%|██        | 1/5 [00:11<00:44, 11.20s/it]\u001b[A[2024-07-23 05:00:23,403][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3425524\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:00:29,000][1383994605.py][line:1089][INFO] mean_spacing_error：69.37467，col=17，count=216，rate=7.87037%，jerk=0.12501，miniumu_ttc=172.07138\n\n 40%|████      | 2/5 [00:22<00:33, 11.28s/it]\u001b[A[2024-07-23 05:00:34,710][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2938817\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 05:00:40,754][1383994605.py][line:1089][INFO] mean_spacing_error：46.82822，col=30，count=216，rate=13.88889%，jerk=0.09832，miniumu_ttc=102.26173\n\n 60%|██████    | 3/5 [00:34<00:23, 11.50s/it]\u001b[A[2024-07-23 05:00:46,402][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2761393\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:00:52,158][1383994605.py][line:1089][INFO] mean_spacing_error：40.36255，col=14，count=216，rate=6.48148%，jerk=0.11135，miniumu_ttc=108.10200\n\n 80%|████████  | 4/5 [00:45<00:11, 11.47s/it]\u001b[A[2024-07-23 05:00:57,879][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2338896\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 05:01:03,824][1383994605.py][line:1089][INFO] mean_spacing_error：28.36485，col=13，count=216，rate=6.01852%，jerk=0.13821，miniumu_ttc=172.40163\n\n100%|██████████| 5/5 [00:57<00:00, 11.47s/it]\u001b[A\n  6%|▌         | 31/500 [1:33:44<23:26:24, 179.92s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.73s/it]\u001b[A\n[2024-07-23 05:03:32,814][1383994605.py][line:1670][INFO] 32/500. loss: 0.0034255866582194963\n[2024-07-23 05:03:32,824][1383994605.py][line:1671][INFO] 32/500. mserror: 106.5362777709961  col: 57  count: 216  jerk: 0.025957956910133362  ttc: 156.6731719970703\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:03:38,558][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3631192\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 05:03:44,729][1383994605.py][line:1089][INFO] mean_spacing_error：59.29911，col=20，count=216，rate=9.25926%，jerk=0.09989，miniumu_ttc=94.51077\n\n 20%|██        | 1/5 [00:11<00:47, 11.89s/it]\u001b[A[2024-07-23 05:03:51,317][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2937189\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:03:57,096][1383994605.py][line:1089][INFO] mean_spacing_error：43.10169，col=19，count=216，rate=8.79630%，jerk=0.11635，miniumu_ttc=170.99475\n\n 40%|████      | 2/5 [00:24<00:36, 12.17s/it]\u001b[A[2024-07-23 05:04:02,903][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3087448\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 05:04:08,893][1383994605.py][line:1089][INFO] mean_spacing_error：45.17977，col=23，count=216，rate=10.64815%，jerk=0.09461，miniumu_ttc=134.05612\n\n 60%|██████    | 3/5 [00:36<00:24, 12.00s/it]\u001b[A[2024-07-23 05:04:14,584][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2753923\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 05:04:20,273][1383994605.py][line:1089][INFO] mean_spacing_error：49.45718，col=7，count=216，rate=3.24074%，jerk=0.13648，miniumu_ttc=202.61447\n\n 80%|████████  | 4/5 [00:47<00:11, 11.76s/it]\u001b[A[2024-07-23 05:04:26,388][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2422391\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:04:32,160][1383994605.py][line:1089][INFO] mean_spacing_error：24.60752，col=24，count=216，rate=11.11111%，jerk=0.13299，miniumu_ttc=138.31102\n\n100%|██████████| 5/5 [00:59<00:00, 11.87s/it]\u001b[A\n  6%|▋         | 32/500 [1:37:13<24:29:53, 188.45s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.45s/it]\u001b[A\n2it [00:04,  2.09s/it]\u001b[A\n3it [00:05,  1.93s/it]\u001b[A\n[2024-07-23 05:07:03,216][1383994605.py][line:1670][INFO] 33/500. loss: 0.0021374781305591264\n[2024-07-23 05:07:03,232][1383994605.py][line:1671][INFO] 33/500. mserror: 94.60176849365234  col: 53  count: 216  jerk: 0.03321457281708717  ttc: 501.1064453125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:07:08,784][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3567812\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:07:14,558][1383994605.py][line:1089][INFO] mean_spacing_error：66.51453，col=34，count=216，rate=15.74074%，jerk=0.08182，miniumu_ttc=130.38557\n\n 20%|██        | 1/5 [00:11<00:45, 11.31s/it]\u001b[A[2024-07-23 05:07:20,342][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3126282\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 05:07:26,007][1383994605.py][line:1089][INFO] mean_spacing_error：37.30975，col=11，count=216，rate=5.09259%，jerk=0.15552，miniumu_ttc=144.74564\n\n 40%|████      | 2/5 [00:22<00:34, 11.39s/it]\u001b[A[2024-07-23 05:07:31,804][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2727706\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.95s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.25s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.06s/it]\u001b[A\u001b[A\n[2024-07-23 05:07:38,265][1383994605.py][line:1089][INFO] mean_spacing_error：32.84779，col=19，count=216，rate=8.79630%，jerk=0.12319，miniumu_ttc=117.01900\n\n 60%|██████    | 3/5 [00:35<00:23, 11.79s/it]\u001b[A[2024-07-23 05:07:44,165][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2660228\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.71s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.16s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.01s/it]\u001b[A\u001b[A\n[2024-07-23 05:07:50,490][1383994605.py][line:1089][INFO] mean_spacing_error：47.43256，col=6，count=216，rate=2.77778%，jerk=0.12446，miniumu_ttc=143.37105\n\n 80%|████████  | 4/5 [00:47<00:11, 11.96s/it]\u001b[A[2024-07-23 05:07:56,702][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2381185\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.62s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.02s/it]\u001b[A\u001b[A\n[2024-07-23 05:08:03,055][1383994605.py][line:1089][INFO] mean_spacing_error：25.73063，col=14，count=216，rate=6.48148%，jerk=0.12314，miniumu_ttc=114.26094\n\n100%|██████████| 5/5 [00:59<00:00, 11.97s/it]\u001b[A\n  7%|▋         | 33/500 [1:40:43<25:19:14, 195.19s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.50s/it]\u001b[A\n2it [00:04,  2.05s/it]\u001b[A\n3it [00:05,  1.89s/it]\u001b[A\n[2024-07-23 05:10:42,741][1383994605.py][line:1670][INFO] 34/500. loss: 0.0027739073460300765\n[2024-07-23 05:10:42,755][1383994605.py][line:1671][INFO] 34/500. mserror: 66.97423553466797  col: 59  count: 216  jerk: 0.04387221857905388  ttc: 136.5601043701172\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:10:49,341][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3773789\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 05:10:55,028][1383994605.py][line:1089][INFO] mean_spacing_error：84.97792，col=50，count=216，rate=23.14815%，jerk=0.04911，miniumu_ttc=155.89156\n\n 20%|██        | 1/5 [00:12<00:49, 12.27s/it]\u001b[A[2024-07-23 05:11:00,981][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3365834\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 05:11:06,826][1383994605.py][line:1089][INFO] mean_spacing_error：48.42566，col=15，count=216，rate=6.94444%，jerk=0.15170，miniumu_ttc=136.96655\n\n 40%|████      | 2/5 [00:24<00:35, 11.99s/it]\u001b[A[2024-07-23 05:11:12,626][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2708820\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 05:11:18,582][1383994605.py][line:1089][INFO] mean_spacing_error：25.06834，col=22，count=216，rate=10.18519%，jerk=0.12708，miniumu_ttc=143.39041\n\n 60%|██████    | 3/5 [00:35<00:23, 11.88s/it]\u001b[A[2024-07-23 05:11:24,336][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2630613\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:11:29,934][1383994605.py][line:1089][INFO] mean_spacing_error：32.37294，col=10，count=216，rate=4.62963%，jerk=0.13485，miniumu_ttc=152.40898\n\n 80%|████████  | 4/5 [00:47<00:11, 11.68s/it]\u001b[A[2024-07-23 05:11:36,022][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2304923\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 05:11:41,850][1383994605.py][line:1089][INFO] mean_spacing_error：29.39149，col=14，count=216，rate=6.48148%，jerk=0.13358，miniumu_ttc=143.91156\n\n100%|██████████| 5/5 [00:59<00:00, 11.82s/it]\u001b[A\n  7%|▋         | 34/500 [1:44:22<26:10:58, 202.27s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.34s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 05:14:19,695][1383994605.py][line:1670][INFO] 35/500. loss: 0.00329709704965353\n[2024-07-23 05:14:19,708][1383994605.py][line:1671][INFO] 35/500. mserror: 69.32298278808594  col: 46  count: 216  jerk: 0.05028597638010979  ttc: 154.2354278564453\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:14:25,014][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3849695\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 05:14:30,896][1383994605.py][line:1089][INFO] mean_spacing_error：91.96770，col=49，count=216，rate=22.68519%，jerk=0.04465，miniumu_ttc=250.72223\n\n 20%|██        | 1/5 [00:11<00:44, 11.18s/it]\u001b[A[2024-07-23 05:14:36,659][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3578447\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:14:42,392][1383994605.py][line:1089][INFO] mean_spacing_error：50.13750，col=12，count=216，rate=5.55556%，jerk=0.12671，miniumu_ttc=154.79651\n\n 40%|████      | 2/5 [00:22<00:34, 11.37s/it]\u001b[A[2024-07-23 05:14:48,132][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2725283\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 05:14:54,038][1383994605.py][line:1089][INFO] mean_spacing_error：35.75636，col=19，count=216，rate=8.79630%，jerk=0.16698，miniumu_ttc=137.66318\n\n 60%|██████    | 3/5 [00:34<00:22, 11.49s/it]\u001b[A[2024-07-23 05:14:59,884][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2810173\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 05:15:05,919][1383994605.py][line:1089][INFO] mean_spacing_error：34.12075，col=11，count=216，rate=5.09259%，jerk=0.14888，miniumu_ttc=173.17012\n\n 80%|████████  | 4/5 [00:46<00:11, 11.65s/it]\u001b[A[2024-07-23 05:15:11,559][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2522211\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 05:15:17,445][1383994605.py][line:1089][INFO] mean_spacing_error：35.13183，col=13，count=216，rate=6.01852%，jerk=0.12896，miniumu_ttc=128.61206\n\n100%|██████████| 5/5 [00:57<00:00, 11.55s/it]\u001b[A\n  7%|▋         | 35/500 [1:47:58<26:38:31, 206.26s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.33s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 05:17:50,809][1383994605.py][line:1670][INFO] 36/500. loss: 0.0015635962287584941\n[2024-07-23 05:17:50,823][1383994605.py][line:1671][INFO] 36/500. mserror: 101.41510009765625  col: 32  count: 216  jerk: 0.05072268843650818  ttc: 674.0701904296875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:17:56,333][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3923591\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 05:18:02,405][1383994605.py][line:1089][INFO] mean_spacing_error：140.70557，col=53，count=216，rate=24.53704%，jerk=0.03097，miniumu_ttc=162.09796\n\n 20%|██        | 1/5 [00:11<00:46, 11.57s/it]\u001b[A[2024-07-23 05:18:08,769][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3482905\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.79s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.20s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.07s/it]\u001b[A\u001b[A\n[2024-07-23 05:18:15,269][1383994605.py][line:1089][INFO] mean_spacing_error：35.87237，col=17，count=216，rate=7.87037%，jerk=0.12576，miniumu_ttc=89.98643\n\n 40%|████      | 2/5 [00:24<00:36, 12.33s/it]\u001b[A[2024-07-23 05:18:22,516][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2738540\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 05:18:28,558][1383994605.py][line:1089][INFO] mean_spacing_error：34.37099，col=19，count=216，rate=8.79630%，jerk=0.15168，miniumu_ttc=96.47501\n\n 60%|██████    | 3/5 [00:37<00:25, 12.77s/it]\u001b[A[2024-07-23 05:18:34,650][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2687604\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.60s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 05:18:40,829][1383994605.py][line:1089][INFO] mean_spacing_error：38.48648，col=6，count=216，rate=2.77778%，jerk=0.13802，miniumu_ttc=179.43593\n\n 80%|████████  | 4/5 [00:49<00:12, 12.57s/it]\u001b[A[2024-07-23 05:18:46,999][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2397537\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 05:18:52,851][1383994605.py][line:1089][INFO] mean_spacing_error：32.26051，col=19，count=216，rate=8.79630%，jerk=0.12079，miniumu_ttc=137.88481\n\n100%|██████████| 5/5 [01:02<00:00, 12.40s/it]\u001b[A\n  7%|▋         | 36/500 [1:51:33<26:56:16, 209.00s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.84s/it]\u001b[A\n[2024-07-23 05:20:49,093][1383994605.py][line:1670][INFO] 37/500. loss: 0.004119347470502059\n[2024-07-23 05:20:49,113][1383994605.py][line:1671][INFO] 37/500. mserror: 74.32803344726562  col: 46  count: 216  jerk: 0.046547550708055496  ttc: 159.91058349609375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:20:54,352][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3883776\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 05:21:00,025][1383994605.py][line:1089][INFO] mean_spacing_error：115.86558，col=34，count=216，rate=15.74074%，jerk=0.04421，miniumu_ttc=204.29030\n\n 20%|██        | 1/5 [00:10<00:43, 10.91s/it]\u001b[A[2024-07-23 05:21:05,510][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3554842\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:21:11,304][1383994605.py][line:1089][INFO] mean_spacing_error：96.28170，col=10，count=216，rate=4.62963%，jerk=0.09257，miniumu_ttc=405.48996\n\n 40%|████      | 2/5 [00:22<00:33, 11.13s/it]\u001b[A[2024-07-23 05:21:16,800][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2767413\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 05:21:22,587][1383994605.py][line:1089][INFO] mean_spacing_error：36.38160，col=15，count=216，rate=6.94444%，jerk=0.16159，miniumu_ttc=20.62166\n\n 60%|██████    | 3/5 [00:33<00:22, 11.20s/it]\u001b[A[2024-07-23 05:21:28,183][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2663593\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:21:33,948][1383994605.py][line:1089][INFO] mean_spacing_error：34.88506，col=7，count=216，rate=3.24074%，jerk=0.17313，miniumu_ttc=122.08372\n\n 80%|████████  | 4/5 [00:44<00:11, 11.26s/it]\u001b[A[2024-07-23 05:21:39,888][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2361019\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 05:21:46,019][1383994605.py][line:1089][INFO] mean_spacing_error：49.98073，col=10，count=216，rate=4.62963%，jerk=0.11588，miniumu_ttc=302.03888\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n  7%|▋         | 37/500 [1:54:26<25:29:52, 198.26s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 05:24:14,792][1383994605.py][line:1670][INFO] 38/500. loss: 0.0035627279430627823\n[2024-07-23 05:24:14,806][1383994605.py][line:1671][INFO] 38/500. mserror: 71.62388610839844  col: 58  count: 216  jerk: 0.04311004653573036  ttc: 138.82949829101562\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:24:20,228][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3822146\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 05:24:25,648][1383994605.py][line:1089][INFO] mean_spacing_error：115.48031，col=23，count=216，rate=10.64815%，jerk=0.06001，miniumu_ttc=222.87994\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 05:24:31,142][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3327126\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 05:24:36,982][1383994605.py][line:1089][INFO] mean_spacing_error：59.11570，col=13，count=216，rate=6.01852%，jerk=0.10342，miniumu_ttc=157.33275\n\n 40%|████      | 2/5 [00:22<00:33, 11.13s/it]\u001b[A[2024-07-23 05:24:42,751][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2646802\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:24:48,385][1383994605.py][line:1089][INFO] mean_spacing_error：31.36304，col=27，count=216，rate=12.50000%，jerk=0.13221，miniumu_ttc=34.36584\n\n 60%|██████    | 3/5 [00:33<00:22, 11.26s/it]\u001b[A[2024-07-23 05:24:54,510][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2600570\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 05:25:00,330][1383994605.py][line:1089][INFO] mean_spacing_error：39.11166，col=7，count=216，rate=3.24074%，jerk=0.14289，miniumu_ttc=120.56326\n\n 80%|████████  | 4/5 [00:45<00:11, 11.53s/it]\u001b[A[2024-07-23 05:25:06,908][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2328987\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 05:25:12,454][1383994605.py][line:1089][INFO] mean_spacing_error：33.70484，col=16，count=216，rate=7.40741%，jerk=0.12021，miniumu_ttc=144.96732\n\n100%|██████████| 5/5 [00:57<00:00, 11.53s/it]\u001b[A\n  8%|▊         | 38/500 [1:57:53<25:45:29, 200.71s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 05:27:39,720][1383994605.py][line:1670][INFO] 39/500. loss: 0.0015326999127864838\n[2024-07-23 05:27:39,732][1383994605.py][line:1671][INFO] 39/500. mserror: 93.6341323852539  col: 57  count: 216  jerk: 0.03334517776966095  ttc: 156.61483764648438\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:27:45,169][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3635097\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 05:27:50,739][1383994605.py][line:1089][INFO] mean_spacing_error：117.49605，col=15，count=216，rate=6.94444%，jerk=0.07661，miniumu_ttc=235.39343\n\n 20%|██        | 1/5 [00:11<00:43, 11.00s/it]\u001b[A[2024-07-23 05:27:56,359][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3289829\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:28:02,137][1383994605.py][line:1089][INFO] mean_spacing_error：53.67793，col=10，count=216，rate=4.62963%，jerk=0.12235，miniumu_ttc=141.73489\n\n 40%|████      | 2/5 [00:22<00:33, 11.24s/it]\u001b[A[2024-07-23 05:28:08,133][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2643979\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 05:28:13,713][1383994605.py][line:1089][INFO] mean_spacing_error：31.98483，col=23，count=216，rate=10.64815%，jerk=0.13682，miniumu_ttc=97.51367\n\n 60%|██████    | 3/5 [00:33<00:22, 11.39s/it]\u001b[A[2024-07-23 05:28:19,679][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2625190\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 05:28:25,233][1383994605.py][line:1089][INFO] mean_spacing_error：37.44296，col=8，count=216，rate=3.70370%，jerk=0.13672，miniumu_ttc=156.49583\n\n 80%|████████  | 4/5 [00:45<00:11, 11.44s/it]\u001b[A[2024-07-23 05:28:30,793][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2410180\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:28:36,330][1383994605.py][line:1089][INFO] mean_spacing_error：32.57324，col=14，count=216，rate=6.48148%，jerk=0.12802，miniumu_ttc=144.80844\n\n100%|██████████| 5/5 [00:56<00:00, 11.32s/it]\u001b[A\n  8%|▊         | 39/500 [2:01:17<25:49:24, 201.66s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.54s/it]\u001b[A\n2it [00:04,  2.16s/it]\u001b[A\n3it [00:05,  1.99s/it]\u001b[A\n[2024-07-23 05:31:05,217][1383994605.py][line:1670][INFO] 40/500. loss: 0.001884646713733673\n[2024-07-23 05:31:05,234][1383994605.py][line:1671][INFO] 40/500. mserror: 129.36402893066406  col: 52  count: 216  jerk: 0.025229541584849358  ttc: 169.8287811279297\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:31:10,597][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3676680\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 05:31:16,242][1383994605.py][line:1089][INFO] mean_spacing_error：102.13657，col=12，count=216，rate=5.55556%，jerk=0.08910，miniumu_ttc=238.88817\n\n 20%|██        | 1/5 [00:11<00:44, 11.00s/it]\u001b[A[2024-07-23 05:31:21,959][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3065780\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 05:31:27,964][1383994605.py][line:1089][INFO] mean_spacing_error：50.25388，col=9，count=216，rate=4.16667%，jerk=0.16362，miniumu_ttc=167.92987\n\n 40%|████      | 2/5 [00:22<00:34, 11.43s/it]\u001b[A[2024-07-23 05:31:33,579][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2521812\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 05:31:39,294][1383994605.py][line:1089][INFO] mean_spacing_error：34.89191，col=11，count=216，rate=5.09259%，jerk=0.14464，miniumu_ttc=161.32141\n\n 60%|██████    | 3/5 [00:34<00:22, 11.38s/it]\u001b[A[2024-07-23 05:31:44,841][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2479749\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:31:50,463][1383994605.py][line:1089][INFO] mean_spacing_error：33.70877，col=6，count=216，rate=2.77778%，jerk=0.15107，miniumu_ttc=165.66724\n\n 80%|████████  | 4/5 [00:45<00:11, 11.30s/it]\u001b[A[2024-07-23 05:31:56,026][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2283441\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 05:32:01,898][1383994605.py][line:1089][INFO] mean_spacing_error：26.30122，col=13，count=216，rate=6.01852%，jerk=0.14472，miniumu_ttc=102.70961\n\n100%|██████████| 5/5 [00:56<00:00, 11.33s/it]\u001b[A\n  8%|▊         | 40/500 [2:04:42<25:55:02, 202.83s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.08s/it]\u001b[A\n2it [00:03,  1.69s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 05:33:41,609][1383994605.py][line:1670][INFO] 41/500. loss: 0.003993049263954163\n[2024-07-23 05:33:41,619][1383994605.py][line:1671][INFO] 41/500. mserror: 138.72979736328125  col: 51  count: 216  jerk: 0.022939415648579597  ttc: 167.936767578125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:33:46,748][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4041620\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:33:52,384][1383994605.py][line:1089][INFO] mean_spacing_error：67.91054，col=20，count=216，rate=9.25926%，jerk=0.10650，miniumu_ttc=246.26945\n\n 20%|██        | 1/5 [00:10<00:43, 10.76s/it]\u001b[A[2024-07-23 05:33:58,070][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3115186\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:34:03,614][1383994605.py][line:1089][INFO] mean_spacing_error：48.16325，col=10，count=216，rate=4.62963%，jerk=0.15131，miniumu_ttc=187.62756\n\n 40%|████      | 2/5 [00:21<00:33, 11.04s/it]\u001b[A[2024-07-23 05:34:09,772][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2602638\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 05:34:15,259][1383994605.py][line:1089][INFO] mean_spacing_error：31.11304，col=14，count=216，rate=6.48148%，jerk=0.13232，miniumu_ttc=116.73236\n\n 60%|██████    | 3/5 [00:33<00:22, 11.31s/it]\u001b[A[2024-07-23 05:34:20,621][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2607343\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.10s/it]\u001b[A\u001b[A\n[2024-07-23 05:34:27,225][1383994605.py][line:1089][INFO] mean_spacing_error：47.15398，col=7，count=216，rate=3.24074%，jerk=0.14735，miniumu_ttc=201.69159\n\n 80%|████████  | 4/5 [00:45<00:11, 11.58s/it]\u001b[A[2024-07-23 05:34:34,271][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2352673\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:34:40,051][1383994605.py][line:1089][INFO] mean_spacing_error：32.58438，col=16，count=216，rate=7.40741%，jerk=0.12870，miniumu_ttc=132.74466\n\n100%|██████████| 5/5 [00:58<00:00, 11.69s/it]\u001b[A\n  8%|▊         | 41/500 [2:07:20<24:09:05, 189.42s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.03s/it]\u001b[A\n2it [00:03,  1.65s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 05:36:23,294][1383994605.py][line:1670][INFO] 42/500. loss: 0.0038040941581130028\n[2024-07-23 05:36:23,296][1383994605.py][line:1671][INFO] 42/500. mserror: 128.10446166992188  col: 55  count: 216  jerk: 0.02272690460085869  ttc: 158.86886596679688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:36:28,740][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3838624\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.16s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.71s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 05:36:33,992][1383994605.py][line:1089][INFO] mean_spacing_error：72.54121，col=27，count=216，rate=12.50000%，jerk=0.08894，miniumu_ttc=187.19933\n\n 20%|██        | 1/5 [00:10<00:42, 10.68s/it]\u001b[A[2024-07-23 05:36:39,802][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3077650\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 05:36:45,293][1383994605.py][line:1089][INFO] mean_spacing_error：42.50471，col=11，count=216，rate=5.09259%，jerk=0.15174，miniumu_ttc=192.31372\n\n 40%|████      | 2/5 [00:21<00:33, 11.05s/it]\u001b[A[2024-07-23 05:36:51,071][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2584537\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 05:36:56,391][1383994605.py][line:1089][INFO] mean_spacing_error：32.19830，col=12，count=216，rate=5.55556%，jerk=0.13373，miniumu_ttc=153.08876\n\n 60%|██████    | 3/5 [00:33<00:22, 11.07s/it]\u001b[A[2024-07-23 05:37:02,230][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2533919\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 05:37:07,684][1383994605.py][line:1089][INFO] mean_spacing_error：43.85029，col=6，count=216，rate=2.77778%，jerk=0.16324，miniumu_ttc=199.99001\n\n 80%|████████  | 4/5 [00:44<00:11, 11.16s/it]\u001b[A[2024-07-23 05:37:13,246][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2317959\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 05:37:18,881][1383994605.py][line:1089][INFO] mean_spacing_error：29.59877，col=16，count=216，rate=7.40741%，jerk=0.12392，miniumu_ttc=140.94069\n\n100%|██████████| 5/5 [00:55<00:00, 11.12s/it]\u001b[A\n  8%|▊         | 42/500 [2:09:59<22:55:54, 180.25s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.03s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 05:38:54,414][1383994605.py][line:1670][INFO] 43/500. loss: 0.001972279200951258\n[2024-07-23 05:38:54,424][1383994605.py][line:1671][INFO] 43/500. mserror: 116.69076538085938  col: 60  count: 216  jerk: 0.022028347477316856  ttc: 152.71096801757812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:38:59,659][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3770353\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 05:39:05,059][1383994605.py][line:1089][INFO] mean_spacing_error：76.58833，col=30，count=216，rate=13.88889%，jerk=0.06916，miniumu_ttc=105.55318\n\n 20%|██        | 1/5 [00:10<00:42, 10.63s/it]\u001b[A[2024-07-23 05:39:10,637][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3021506\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 05:39:15,979][1383994605.py][line:1089][INFO] mean_spacing_error：34.31406，col=16，count=216，rate=7.40741%，jerk=0.13780，miniumu_ttc=168.57784\n\n 40%|████      | 2/5 [00:21<00:32, 10.80s/it]\u001b[A[2024-07-23 05:39:21,342][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2591173\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 05:39:26,836][1383994605.py][line:1089][INFO] mean_spacing_error：42.83904，col=48，count=216，rate=22.22222%，jerk=0.09111，miniumu_ttc=149.15054\n\n 60%|██████    | 3/5 [00:32<00:21, 10.83s/it]\u001b[A[2024-07-23 05:39:32,179][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2607869\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.14s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.71s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.62s/it]\u001b[A\u001b[A\n[2024-07-23 05:39:37,327][1383994605.py][line:1089][INFO] mean_spacing_error：49.87577，col=7，count=216，rate=3.24074%，jerk=0.11804，miniumu_ttc=156.45572\n\n 80%|████████  | 4/5 [00:42<00:10, 10.69s/it]\u001b[A[2024-07-23 05:39:43,041][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2344159\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:39:48,805][1383994605.py][line:1089][INFO] mean_spacing_error：29.97254，col=13，count=216，rate=6.01852%，jerk=0.13280，miniumu_ttc=108.23890\n\n100%|██████████| 5/5 [00:54<00:00, 10.88s/it]\u001b[A\n  9%|▊         | 43/500 [2:12:29<21:43:36, 171.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 05:42:10,195][1383994605.py][line:1670][INFO] 44/500. loss: 0.0020723159735401473\n[2024-07-23 05:42:10,205][1383994605.py][line:1671][INFO] 44/500. mserror: 112.40994262695312  col: 62  count: 216  jerk: 0.021807022392749786  ttc: 168.11480712890625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:42:15,457][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3728722\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 05:42:20,764][1383994605.py][line:1089][INFO] mean_spacing_error：78.06519，col=25，count=216，rate=11.57407%，jerk=0.06945，miniumu_ttc=122.58482\n\n 20%|██        | 1/5 [00:10<00:42, 10.55s/it]\u001b[A[2024-07-23 05:42:26,399][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3006704\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 05:42:31,728][1383994605.py][line:1089][INFO] mean_spacing_error：37.06971，col=13，count=216，rate=6.01852%，jerk=0.14448，miniumu_ttc=173.13281\n\n 40%|████      | 2/5 [00:21<00:32, 10.79s/it]\u001b[A[2024-07-23 05:42:37,755][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2617286\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:42:43,327][1383994605.py][line:1089][INFO] mean_spacing_error：40.74965，col=33，count=216，rate=15.27778%，jerk=0.09378，miniumu_ttc=98.23941\n\n 60%|██████    | 3/5 [00:33<00:22, 11.17s/it]\u001b[A[2024-07-23 05:42:49,456][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2644326\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 05:42:54,828][1383994605.py][line:1089][INFO] mean_spacing_error：60.45179，col=9，count=216，rate=4.16667%，jerk=0.09341，miniumu_ttc=143.08449\n\n 80%|████████  | 4/5 [00:44<00:11, 11.29s/it]\u001b[A[2024-07-23 05:43:00,423][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2420935\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 05:43:06,422][1383994605.py][line:1089][INFO] mean_spacing_error：29.38692，col=15，count=216，rate=6.94444%，jerk=0.11453，miniumu_ttc=102.99609\n\n100%|██████████| 5/5 [00:56<00:00, 11.24s/it]\u001b[A\n  9%|▉         | 44/500 [2:15:47<22:41:02, 179.08s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.04s/it]\u001b[A\n2it [00:03,  1.65s/it]\u001b[A\n3it [00:04,  1.56s/it]\u001b[A\n[2024-07-23 05:45:24,415][1383994605.py][line:1670][INFO] 45/500. loss: 0.003579314798116684\n[2024-07-23 05:45:24,427][1383994605.py][line:1671][INFO] 45/500. mserror: 112.26792907714844  col: 59  count: 216  jerk: 0.023759569972753525  ttc: 160.0243377685547\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:45:29,632][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3715232\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 05:45:35,102][1383994605.py][line:1089][INFO] mean_spacing_error：73.52314，col=34，count=216，rate=15.74074%，jerk=0.06205，miniumu_ttc=220.74315\n\n 20%|██        | 1/5 [00:10<00:42, 10.67s/it]\u001b[A[2024-07-23 05:45:40,604][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2955128\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 05:45:46,386][1383994605.py][line:1089][INFO] mean_spacing_error：42.89635，col=15，count=216，rate=6.94444%，jerk=0.14916，miniumu_ttc=184.01759\n\n 40%|████      | 2/5 [00:21<00:33, 11.03s/it]\u001b[A[2024-07-23 05:45:51,944][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2494841\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:45:57,685][1383994605.py][line:1089][INFO] mean_spacing_error：33.84404，col=30，count=216，rate=13.88889%，jerk=0.11711，miniumu_ttc=86.23067\n\n 60%|██████    | 3/5 [00:33<00:22, 11.15s/it]\u001b[A[2024-07-23 05:46:03,328][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2480472\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:46:08,933][1383994605.py][line:1089][INFO] mean_spacing_error：43.72307，col=10，count=216，rate=4.62963%，jerk=0.11353，miniumu_ttc=107.07581\n\n 80%|████████  | 4/5 [00:44<00:11, 11.19s/it]\u001b[A[2024-07-23 05:46:14,402][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2174969\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 05:46:20,298][1383994605.py][line:1089][INFO] mean_spacing_error：31.24248，col=19，count=216，rate=8.79630%，jerk=0.11531，miniumu_ttc=134.36539\n\n100%|██████████| 5/5 [00:55<00:00, 11.18s/it]\u001b[A\n  9%|▉         | 45/500 [2:19:01<23:11:46, 183.53s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 05:48:07,837][1383994605.py][line:1670][INFO] 46/500. loss: 0.0018315635000665982\n[2024-07-23 05:48:07,856][1383994605.py][line:1671][INFO] 46/500. mserror: 113.40350341796875  col: 57  count: 216  jerk: 0.025343969464302063  ttc: 153.42169189453125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:48:13,043][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3677743\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:48:18,580][1383994605.py][line:1089][INFO] mean_spacing_error：68.65437，col=30，count=216，rate=13.88889%，jerk=0.07136，miniumu_ttc=112.89610\n\n 20%|██        | 1/5 [00:10<00:42, 10.73s/it]\u001b[A[2024-07-23 05:48:23,952][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2977336\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:48:29,604][1383994605.py][line:1089][INFO] mean_spacing_error：40.15045，col=12，count=216，rate=5.55556%，jerk=0.15029，miniumu_ttc=170.24049\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 05:48:35,164][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2532821\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:48:40,767][1383994605.py][line:1089][INFO] mean_spacing_error：33.03893，col=22，count=216，rate=10.18519%，jerk=0.11798，miniumu_ttc=94.77537\n\n 60%|██████    | 3/5 [00:32<00:22, 11.02s/it]\u001b[A[2024-07-23 05:48:46,280][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2500165\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 05:48:51,775][1383994605.py][line:1089][INFO] mean_spacing_error：39.45554，col=12，count=216，rate=5.55556%，jerk=0.11586，miniumu_ttc=104.21490\n\n 80%|████████  | 4/5 [00:43<00:11, 11.02s/it]\u001b[A[2024-07-23 05:48:57,526][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2198031\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 05:49:03,228][1383994605.py][line:1089][INFO] mean_spacing_error：44.63122，col=43，count=216，rate=19.90741%，jerk=0.12223，miniumu_ttc=90.16508\n\n100%|██████████| 5/5 [00:55<00:00, 11.07s/it]\u001b[A\n  9%|▉         | 46/500 [2:21:44<22:21:54, 177.35s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:04,  2.05s/it]\u001b[A\n3it [00:05,  1.88s/it]\u001b[A\n[2024-07-23 05:50:46,009][1383994605.py][line:1670][INFO] 47/500. loss: 0.0018165841077764828\n[2024-07-23 05:50:46,022][1383994605.py][line:1671][INFO] 47/500. mserror: 111.20711517333984  col: 57  count: 216  jerk: 0.02599240094423294  ttc: 152.16468811035156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:50:51,650][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3706454\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 05:50:57,139][1383994605.py][line:1089][INFO] mean_spacing_error：66.05329，col=46，count=216，rate=21.29630%，jerk=0.06069，miniumu_ttc=104.55412\n\n 20%|██        | 1/5 [00:11<00:44, 11.11s/it]\u001b[A[2024-07-23 05:51:02,589][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2959273\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 05:51:08,280][1383994605.py][line:1089][INFO] mean_spacing_error：42.03071，col=9，count=216，rate=4.16667%，jerk=0.17430，miniumu_ttc=138.79301\n\n 40%|████      | 2/5 [00:22<00:33, 11.12s/it]\u001b[A[2024-07-23 05:51:13,830][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2594492\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 05:51:19,644][1383994605.py][line:1089][INFO] mean_spacing_error：39.28067，col=18，count=216，rate=8.33333%，jerk=0.13815，miniumu_ttc=100.33857\n\n 60%|██████    | 3/5 [00:33<00:22, 11.24s/it]\u001b[A[2024-07-23 05:51:25,469][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2445120\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 05:51:31,179][1383994605.py][line:1089][INFO] mean_spacing_error：46.66445，col=24，count=216，rate=11.11111%，jerk=0.12355，miniumu_ttc=86.52213\n\n 80%|████████  | 4/5 [00:45<00:11, 11.35s/it]\u001b[A[2024-07-23 05:51:37,179][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2246234\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 05:51:43,240][1383994605.py][line:1089][INFO] mean_spacing_error：68.74850，col=75，count=216，rate=34.72222%，jerk=0.12121，miniumu_ttc=77.66615\n\n100%|██████████| 5/5 [00:57<00:00, 11.44s/it]\u001b[A\n  9%|▉         | 47/500 [2:24:24<21:39:42, 172.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.04s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 05:53:24,170][1383994605.py][line:1670][INFO] 48/500. loss: 0.003850153647363186\n[2024-07-23 05:53:24,180][1383994605.py][line:1671][INFO] 48/500. mserror: 106.21236419677734  col: 57  count: 216  jerk: 0.02839139848947525  ttc: 161.94015502929688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:53:29,218][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3685920\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 05:53:34,670][1383994605.py][line:1089][INFO] mean_spacing_error：77.70858，col=53，count=216，rate=24.53704%，jerk=0.05241，miniumu_ttc=109.14285\n\n 20%|██        | 1/5 [00:10<00:41, 10.49s/it]\u001b[A[2024-07-23 05:53:40,159][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3029436\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:53:45,697][1383994605.py][line:1089][INFO] mean_spacing_error：41.21988，col=11，count=216，rate=5.09259%，jerk=0.15868，miniumu_ttc=194.96049\n\n 40%|████      | 2/5 [00:21<00:32, 10.80s/it]\u001b[A[2024-07-23 05:53:51,474][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2534707\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:53:57,113][1383994605.py][line:1089][INFO] mean_spacing_error：36.03154，col=19，count=216，rate=8.79630%，jerk=0.14403，miniumu_ttc=26.48059\n\n 60%|██████    | 3/5 [00:32<00:22, 11.08s/it]\u001b[A[2024-07-23 05:54:02,837][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2493747\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:54:08,392][1383994605.py][line:1089][INFO] mean_spacing_error：39.14614，col=11，count=216，rate=5.09259%，jerk=0.12246，miniumu_ttc=97.73960\n\n 80%|████████  | 4/5 [00:44<00:11, 11.16s/it]\u001b[A[2024-07-23 05:54:13,889][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2304272\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 05:54:19,469][1383994605.py][line:1089][INFO] mean_spacing_error：38.31186，col=17，count=216，rate=7.87037%，jerk=0.11190，miniumu_ttc=114.02773\n\n100%|██████████| 5/5 [00:55<00:00, 11.06s/it]\u001b[A\n 10%|▉         | 48/500 [2:27:00<21:00:51, 167.37s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.68s/it]\u001b[A\n[2024-07-23 05:56:41,143][1383994605.py][line:1670][INFO] 49/500. loss: 0.0029537854716181755\n[2024-07-23 05:56:41,156][1383994605.py][line:1671][INFO] 49/500. mserror: 98.99381256103516  col: 51  count: 216  jerk: 0.032693929970264435  ttc: 157.10655212402344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:56:46,682][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3727899\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 05:56:52,320][1383994605.py][line:1089][INFO] mean_spacing_error：84.54311，col=44，count=216，rate=20.37037%，jerk=0.04938，miniumu_ttc=111.38735\n\n 20%|██        | 1/5 [00:11<00:44, 11.15s/it]\u001b[A[2024-07-23 05:56:58,133][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3041931\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 05:57:03,832][1383994605.py][line:1089][INFO] mean_spacing_error：40.89834，col=12，count=216，rate=5.55556%，jerk=0.17158，miniumu_ttc=101.88030\n\n 40%|████      | 2/5 [00:22<00:34, 11.36s/it]\u001b[A[2024-07-23 05:57:09,472][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2570747\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 05:57:15,071][1383994605.py][line:1089][INFO] mean_spacing_error：39.45449，col=13，count=216，rate=6.01852%，jerk=0.16380，miniumu_ttc=93.83315\n\n 60%|██████    | 3/5 [00:33<00:22, 11.31s/it]\u001b[A[2024-07-23 05:57:20,808][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2637968\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 05:57:26,751][1383994605.py][line:1089][INFO] mean_spacing_error：33.90506，col=12，count=216，rate=5.55556%，jerk=0.11956，miniumu_ttc=152.09715\n\n 80%|████████  | 4/5 [00:45<00:11, 11.45s/it]\u001b[A[2024-07-23 05:57:32,461][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2313813\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:57:38,202][1383994605.py][line:1089][INFO] mean_spacing_error：35.43554，col=16，count=216，rate=7.40741%，jerk=0.10901，miniumu_ttc=147.46779\n\n100%|██████████| 5/5 [00:57<00:00, 11.41s/it]\u001b[A\n 10%|▉         | 49/500 [2:30:19<22:08:48, 176.78s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.72s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 05:59:23,957][1383994605.py][line:1670][INFO] 50/500. loss: 0.0019259665471812089\n[2024-07-23 05:59:23,978][1383994605.py][line:1671][INFO] 50/500. mserror: 93.91905212402344  col: 48  count: 216  jerk: 0.03575494512915611  ttc: 151.7825927734375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 05:59:29,281][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3804122\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 05:59:35,267][1383994605.py][line:1089][INFO] mean_spacing_error：94.59055，col=53，count=216，rate=24.53704%，jerk=0.04271，miniumu_ttc=109.30994\n\n 20%|██        | 1/5 [00:11<00:45, 11.29s/it]\u001b[A[2024-07-23 05:59:40,833][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3112233\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 05:59:46,546][1383994605.py][line:1089][INFO] mean_spacing_error：47.31667，col=9，count=216，rate=4.16667%，jerk=0.15730，miniumu_ttc=139.90034\n\n 40%|████      | 2/5 [00:22<00:33, 11.28s/it]\u001b[A[2024-07-23 05:59:52,095][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2482331\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 05:59:57,841][1383994605.py][line:1089][INFO] mean_spacing_error：37.93244，col=20，count=216，rate=9.25926%，jerk=0.13909，miniumu_ttc=18.63722\n\n 60%|██████    | 3/5 [00:33<00:22, 11.29s/it]\u001b[A[2024-07-23 06:00:03,793][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2537461\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.69s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.12s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.01s/it]\u001b[A\u001b[A\n[2024-07-23 06:00:10,135][1383994605.py][line:1089][INFO] mean_spacing_error：38.74910，col=11，count=216，rate=5.09259%，jerk=0.11590，miniumu_ttc=149.89661\n\n 80%|████████  | 4/5 [00:46<00:11, 11.69s/it]\u001b[A[2024-07-23 06:00:15,882][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2269355\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 06:00:21,727][1383994605.py][line:1089][INFO] mean_spacing_error：41.83965，col=8，count=216，rate=3.70370%，jerk=0.12406，miniumu_ttc=150.23457\n\n100%|██████████| 5/5 [00:57<00:00, 11.55s/it]\u001b[A\n 10%|█         | 50/500 [2:33:02<21:36:03, 172.81s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 06:02:57,327][1383994605.py][line:1670][INFO] 51/500. loss: 0.0013494581604997318\n[2024-07-23 06:02:57,341][1383994605.py][line:1671][INFO] 51/500. mserror: 91.12877655029297  col: 48  count: 216  jerk: 0.03695649281144142  ttc: 153.71463012695312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:03:02,852][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3864873\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.65s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 06:03:08,967][1383994605.py][line:1089][INFO] mean_spacing_error：93.74185，col=55，count=216，rate=25.46296%，jerk=0.04191，miniumu_ttc=98.17789\n\n 20%|██        | 1/5 [00:11<00:46, 11.63s/it]\u001b[A[2024-07-23 06:03:14,703][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3253587\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 06:03:20,668][1383994605.py][line:1089][INFO] mean_spacing_error：69.46483，col=4，count=216，rate=1.85185%，jerk=0.17883，miniumu_ttc=218.49905\n\n 40%|████      | 2/5 [00:23<00:34, 11.66s/it]\u001b[A[2024-07-23 06:03:26,450][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2443686\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:03:32,176][1383994605.py][line:1089][INFO] mean_spacing_error：29.05454，col=19，count=216，rate=8.79630%，jerk=0.14747，miniumu_ttc=19.97020\n\n 60%|██████    | 3/5 [00:34<00:23, 11.59s/it]\u001b[A[2024-07-23 06:03:37,779][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2624935\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 06:03:43,452][1383994605.py][line:1089][INFO] mean_spacing_error：34.56993，col=10，count=216，rate=4.62963%，jerk=0.11989，miniumu_ttc=141.03641\n\n 80%|████████  | 4/5 [00:46<00:11, 11.47s/it]\u001b[A[2024-07-23 06:03:49,307][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2374236\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 06:03:55,224][1383994605.py][line:1089][INFO] mean_spacing_error：45.83655，col=13，count=216，rate=6.01852%，jerk=0.10046，miniumu_ttc=259.87314\n\n100%|██████████| 5/5 [00:57<00:00, 11.58s/it]\u001b[A\n 10%|█         | 51/500 [2:36:36<23:04:29, 185.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.25s/it]\u001b[A\n2it [00:04,  2.07s/it]\u001b[A\n3it [00:05,  1.87s/it]\u001b[A\n[2024-07-23 06:06:28,480][1383994605.py][line:1670][INFO] 52/500. loss: 0.002060018479824066\n[2024-07-23 06:06:28,493][1383994605.py][line:1671][INFO] 52/500. mserror: 96.82372283935547  col: 48  count: 216  jerk: 0.03603310510516167  ttc: 158.31468200683594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:06:34,040][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3722231\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 06:06:39,754][1383994605.py][line:1089][INFO] mean_spacing_error：95.36654，col=44，count=216，rate=20.37037%，jerk=0.04593，miniumu_ttc=149.21352\n\n 20%|██        | 1/5 [00:11<00:45, 11.25s/it]\u001b[A[2024-07-23 06:06:45,410][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3048394\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:06:51,194][1383994605.py][line:1089][INFO] mean_spacing_error：52.75039，col=21，count=216，rate=9.72222%，jerk=0.16862，miniumu_ttc=37.64590\n\n 40%|████      | 2/5 [00:22<00:34, 11.37s/it]\u001b[A[2024-07-23 06:06:57,025][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2388097\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.60s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.35s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.11s/it]\u001b[A\u001b[A\n[2024-07-23 06:07:03,659][1383994605.py][line:1089][INFO] mean_spacing_error：32.04708，col=12，count=216，rate=5.55556%，jerk=0.15658，miniumu_ttc=103.14762\n\n 60%|██████    | 3/5 [00:35<00:23, 11.87s/it]\u001b[A[2024-07-23 06:07:09,570][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2302343\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 06:07:15,548][1383994605.py][line:1089][INFO] mean_spacing_error：26.54451，col=11，count=216，rate=5.09259%，jerk=0.14603，miniumu_ttc=137.01106\n\n 80%|████████  | 4/5 [00:47<00:11, 11.88s/it]\u001b[A[2024-07-23 06:07:21,616][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2184354\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.71s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.15s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.03s/it]\u001b[A\u001b[A\n[2024-07-23 06:07:27,993][1383994605.py][line:1089][INFO] mean_spacing_error：22.89961，col=12，count=216，rate=5.55556%，jerk=0.15205，miniumu_ttc=167.92764\n\n100%|██████████| 5/5 [00:59<00:00, 11.90s/it]\u001b[A\n 10%|█         | 52/500 [2:40:08<24:03:37, 193.34s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 06:09:55,751][1383994605.py][line:1670][INFO] 53/500. loss: 0.0015287427231669426\n[2024-07-23 06:09:55,756][1383994605.py][line:1671][INFO] 53/500. mserror: 102.18946838378906  col: 48  count: 216  jerk: 0.033923711627721786  ttc: 163.79673767089844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:10:00,917][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3698450\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 06:10:06,532][1383994605.py][line:1089][INFO] mean_spacing_error：85.84390，col=39，count=216，rate=18.05556%，jerk=0.05056，miniumu_ttc=251.03740\n\n 20%|██        | 1/5 [00:10<00:43, 10.77s/it]\u001b[A[2024-07-23 06:10:12,050][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3078995\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 06:10:17,705][1383994605.py][line:1089][INFO] mean_spacing_error：38.31902，col=12，count=216，rate=5.55556%，jerk=0.17183，miniumu_ttc=111.36925\n\n 40%|████      | 2/5 [00:21<00:33, 11.00s/it]\u001b[A[2024-07-23 06:10:23,203][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2307511\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 06:10:29,096][1383994605.py][line:1089][INFO] mean_spacing_error：28.95143，col=13，count=216，rate=6.01852%，jerk=0.14543，miniumu_ttc=96.90646\n\n 60%|██████    | 3/5 [00:33<00:22, 11.18s/it]\u001b[A[2024-07-23 06:10:35,046][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2248947\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:05,  5.92s/it]\u001b[A\u001b[A\n\n2it [00:07,  3.34s/it]\u001b[A\u001b[A\n\n3it [00:08,  2.98s/it]\u001b[A\u001b[A\n[2024-07-23 06:10:44,267][1383994605.py][line:1089][INFO] mean_spacing_error：24.67682，col=10，count=216，rate=4.62963%，jerk=0.14965，miniumu_ttc=102.93580\n\n 80%|████████  | 4/5 [00:48<00:12, 12.76s/it]\u001b[A[2024-07-23 06:10:50,565][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2123575\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.84s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.15s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.02s/it]\u001b[A\u001b[A\n[2024-07-23 06:10:56,899][1383994605.py][line:1089][INFO] mean_spacing_error：24.46790，col=13，count=216，rate=6.01852%，jerk=0.15138，miniumu_ttc=91.47684\n\n100%|██████████| 5/5 [01:01<00:00, 12.23s/it]\u001b[A\n 11%|█         | 53/500 [2:43:37<24:35:08, 198.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 06:13:32,864][1383994605.py][line:1670][INFO] 54/500. loss: 0.0031969094028075538\n[2024-07-23 06:13:32,873][1383994605.py][line:1671][INFO] 54/500. mserror: 103.8111343383789  col: 47  count: 216  jerk: 0.03465675935149193  ttc: 163.4333953857422\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:13:38,575][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3665140\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 06:13:44,397][1383994605.py][line:1089][INFO] mean_spacing_error：78.91016，col=38，count=216，rate=17.59259%，jerk=0.05527，miniumu_ttc=164.86795\n\n 20%|██        | 1/5 [00:11<00:46, 11.51s/it]\u001b[A[2024-07-23 06:13:50,179][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3053495\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.76s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 06:13:56,226][1383994605.py][line:1089][INFO] mean_spacing_error：47.61628，col=16，count=216，rate=7.40741%，jerk=0.17845，miniumu_ttc=132.15138\n\n 40%|████      | 2/5 [00:23<00:35, 11.69s/it]\u001b[A[2024-07-23 06:14:01,968][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2380785\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.61s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 06:14:08,046][1383994605.py][line:1089][INFO] mean_spacing_error：32.88654，col=13，count=216，rate=6.01852%，jerk=0.14380，miniumu_ttc=100.73098\n\n 60%|██████    | 3/5 [00:35<00:23, 11.75s/it]\u001b[A[2024-07-23 06:14:13,603][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2283875\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 06:14:19,284][1383994605.py][line:1089][INFO] mean_spacing_error：36.14428，col=7，count=216，rate=3.24074%，jerk=0.14478，miniumu_ttc=163.52074\n\n 80%|████████  | 4/5 [00:46<00:11, 11.55s/it]\u001b[A[2024-07-23 06:14:25,021][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2116534\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.70s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 06:14:31,047][1383994605.py][line:1089][INFO] mean_spacing_error：25.86540，col=10，count=216，rate=4.62963%，jerk=0.16753，miniumu_ttc=124.95186\n\n100%|██████████| 5/5 [00:58<00:00, 11.63s/it]\u001b[A\n 11%|█         | 54/500 [2:47:11<25:07:49, 202.85s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.30s/it]\u001b[A\n2it [00:03,  1.88s/it]\u001b[A\n3it [00:05,  1.84s/it]\u001b[A\n[2024-07-23 06:17:04,081][1383994605.py][line:1670][INFO] 55/500. loss: 0.0033500613644719124\n[2024-07-23 06:17:04,092][1383994605.py][line:1671][INFO] 55/500. mserror: 102.33694458007812  col: 43  count: 216  jerk: 0.037903930991888046  ttc: 175.01568603515625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:17:09,753][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3647560\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 06:17:15,707][1383994605.py][line:1089][INFO] mean_spacing_error：77.92416，col=38，count=216，rate=17.59259%，jerk=0.05501，miniumu_ttc=166.51991\n\n 20%|██        | 1/5 [00:11<00:46, 11.60s/it]\u001b[A[2024-07-23 06:17:21,690][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2842419\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 06:17:27,607][1383994605.py][line:1089][INFO] mean_spacing_error：40.34312，col=14，count=216，rate=6.48148%，jerk=0.16583，miniumu_ttc=163.12798\n\n 40%|████      | 2/5 [00:23<00:35, 11.78s/it]\u001b[A[2024-07-23 06:17:33,336][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2530262\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 06:17:39,299][1383994605.py][line:1089][INFO] mean_spacing_error：32.85095，col=14，count=216，rate=6.48148%，jerk=0.14508，miniumu_ttc=88.66483\n\n 60%|██████    | 3/5 [00:35<00:23, 11.74s/it]\u001b[A[2024-07-23 06:17:45,157][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2388230\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 06:17:51,278][1383994605.py][line:1089][INFO] mean_spacing_error：54.52284，col=21，count=216，rate=9.72222%，jerk=0.15832，miniumu_ttc=163.13762\n\n 80%|████████  | 4/5 [00:47<00:11, 11.83s/it]\u001b[A[2024-07-23 06:17:57,306][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2181423\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 06:18:03,555][1383994605.py][line:1089][INFO] mean_spacing_error：22.77778，col=15，count=216，rate=6.94444%，jerk=0.15215，miniumu_ttc=93.21736\n\n100%|██████████| 5/5 [00:59<00:00, 11.89s/it]\u001b[A\n 11%|█         | 55/500 [2:50:44<25:25:59, 205.75s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 06:20:36,338][1383994605.py][line:1670][INFO] 56/500. loss: 0.00210688728839159\n[2024-07-23 06:20:36,353][1383994605.py][line:1671][INFO] 56/500. mserror: 99.71226501464844  col: 40  count: 216  jerk: 0.04070030897855759  ttc: 185.19178771972656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:20:41,744][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3675171\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.17s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 06:20:47,911][1383994605.py][line:1089][INFO] mean_spacing_error：74.36012，col=45，count=216，rate=20.83333%，jerk=0.04995，miniumu_ttc=182.94876\n\n 20%|██        | 1/5 [00:11<00:46, 11.56s/it]\u001b[A[2024-07-23 06:20:53,550][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2977246\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 06:20:59,067][1383994605.py][line:1089][INFO] mean_spacing_error：39.26383，col=13，count=216，rate=6.01852%，jerk=0.16541，miniumu_ttc=134.43883\n\n 40%|████      | 2/5 [00:22<00:33, 11.31s/it]\u001b[A[2024-07-23 06:21:04,585][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2504936\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 06:21:10,182][1383994605.py][line:1089][INFO] mean_spacing_error：32.84441，col=10，count=216，rate=4.62963%，jerk=0.16482，miniumu_ttc=109.21436\n\n 60%|██████    | 3/5 [00:33<00:22, 11.22s/it]\u001b[A[2024-07-23 06:21:15,760][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2267813\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.67s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 06:21:21,743][1383994605.py][line:1089][INFO] mean_spacing_error：65.51795，col=63，count=216，rate=29.16667%，jerk=0.13401，miniumu_ttc=8.30323\n\n 80%|████████  | 4/5 [00:45<00:11, 11.36s/it]\u001b[A[2024-07-23 06:21:27,338][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2243569\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 06:21:33,002][1383994605.py][line:1089][INFO] mean_spacing_error：23.05442，col=10，count=216，rate=4.62963%，jerk=0.14880，miniumu_ttc=129.46515\n\n100%|██████████| 5/5 [00:56<00:00, 11.33s/it]\u001b[A\n 11%|█         | 56/500 [2:54:13<25:30:45, 206.86s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.43s/it]\u001b[A\n2it [00:03,  1.92s/it]\u001b[A\n3it [00:05,  1.82s/it]\u001b[A\n[2024-07-23 06:23:29,971][1383994605.py][line:1670][INFO] 57/500. loss: 0.003597677374879519\n[2024-07-23 06:23:29,988][1383994605.py][line:1671][INFO] 57/500. mserror: 88.41287994384766  col: 42  count: 216  jerk: 0.04471461847424507  ttc: 179.67098999023438\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:23:35,917][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3678873\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 06:23:41,595][1383994605.py][line:1089][INFO] mean_spacing_error：79.80454，col=43，count=216，rate=19.90741%，jerk=0.04956，miniumu_ttc=152.68936\n\n 20%|██        | 1/5 [00:11<00:46, 11.60s/it]\u001b[A[2024-07-23 06:23:47,235][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2855220\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:23:52,962][1383994605.py][line:1089][INFO] mean_spacing_error：38.30708，col=14，count=216，rate=6.48148%，jerk=0.16518，miniumu_ttc=129.75407\n\n 40%|████      | 2/5 [00:22<00:34, 11.46s/it]\u001b[A[2024-07-23 06:23:58,844][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2496603\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 06:24:04,626][1383994605.py][line:1089][INFO] mean_spacing_error：34.34808，col=7，count=216，rate=3.24074%，jerk=0.16658，miniumu_ttc=124.54868\n\n 60%|██████    | 3/5 [00:34<00:23, 11.55s/it]\u001b[A[2024-07-23 06:24:10,279][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2300199\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 06:24:15,978][1383994605.py][line:1089][INFO] mean_spacing_error：31.63119，col=14，count=216，rate=6.48148%，jerk=0.12169，miniumu_ttc=237.30833\n\n 80%|████████  | 4/5 [00:45<00:11, 11.47s/it]\u001b[A[2024-07-23 06:24:21,418][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2196698\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 06:24:26,883][1383994605.py][line:1089][INFO] mean_spacing_error：21.02895，col=12，count=216，rate=5.55556%，jerk=0.15115，miniumu_ttc=107.73318\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 11%|█▏        | 57/500 [2:57:07<24:14:17, 196.97s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 06:26:12,021][1383994605.py][line:1670][INFO] 58/500. loss: 0.0036727950597802797\n[2024-07-23 06:26:12,031][1383994605.py][line:1671][INFO] 58/500. mserror: 77.0057144165039  col: 41  count: 216  jerk: 0.04958057031035423  ttc: 147.35867309570312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:26:17,262][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3953491\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:26:22,838][1383994605.py][line:1089][INFO] mean_spacing_error：91.75290，col=57，count=216，rate=26.38889%，jerk=0.03610，miniumu_ttc=161.58580\n\n 20%|██        | 1/5 [00:10<00:43, 10.79s/it]\u001b[A[2024-07-23 06:26:28,301][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2933414\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 06:26:33,912][1383994605.py][line:1089][INFO] mean_spacing_error：34.59251，col=17，count=216，rate=7.87037%，jerk=0.14195，miniumu_ttc=182.56013\n\n 40%|████      | 2/5 [00:21<00:32, 10.96s/it]\u001b[A[2024-07-23 06:26:39,633][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2467877\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 06:26:45,169][1383994605.py][line:1089][INFO] mean_spacing_error：36.25312，col=7，count=216，rate=3.24074%，jerk=0.16796，miniumu_ttc=141.34264\n\n 60%|██████    | 3/5 [00:33<00:22, 11.09s/it]\u001b[A[2024-07-23 06:26:50,797][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2330593\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 06:26:56,519][1383994605.py][line:1089][INFO] mean_spacing_error：35.61243，col=14，count=216，rate=6.48148%，jerk=0.11728，miniumu_ttc=320.07751\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 06:27:02,186][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2196026\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 06:27:08,057][1383994605.py][line:1089][INFO] mean_spacing_error：18.72323，col=13，count=216，rate=6.01852%，jerk=0.15818，miniumu_ttc=130.48077\n\n100%|██████████| 5/5 [00:56<00:00, 11.21s/it]\u001b[A\n 12%|█▏        | 58/500 [2:59:48<22:51:54, 186.23s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.30s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 06:28:48,495][1383994605.py][line:1670][INFO] 59/500. loss: 0.00322863832116127\n[2024-07-23 06:28:48,509][1383994605.py][line:1671][INFO] 59/500. mserror: 73.27108764648438  col: 39  count: 216  jerk: 0.05375470221042633  ttc: 146.73138427734375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:28:54,078][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3973716\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 06:28:59,805][1383994605.py][line:1089][INFO] mean_spacing_error：97.72969，col=52，count=216，rate=24.07407%，jerk=0.03756，miniumu_ttc=196.34514\n\n 20%|██        | 1/5 [00:11<00:45, 11.29s/it]\u001b[A[2024-07-23 06:29:05,706][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2885081\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 06:29:11,058][1383994605.py][line:1089][INFO] mean_spacing_error：34.61142，col=17，count=216，rate=7.87037%，jerk=0.14739，miniumu_ttc=170.15009\n\n 40%|████      | 2/5 [00:22<00:33, 11.27s/it]\u001b[A[2024-07-23 06:29:16,588][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2535674\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 06:29:22,091][1383994605.py][line:1089][INFO] mean_spacing_error：39.46026，col=14，count=216，rate=6.48148%，jerk=0.17311，miniumu_ttc=60.96532\n\n 60%|██████    | 3/5 [00:33<00:22, 11.16s/it]\u001b[A[2024-07-23 06:29:27,579][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2482934\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 06:29:32,946][1383994605.py][line:1089][INFO] mean_spacing_error：43.44539，col=13，count=216，rate=6.01852%，jerk=0.11808，miniumu_ttc=117.62990\n\n 80%|████████  | 4/5 [00:44<00:11, 11.04s/it]\u001b[A[2024-07-23 06:29:38,408][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2335696\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:29:43,960][1383994605.py][line:1089][INFO] mean_spacing_error：31.83714，col=27，count=216，rate=12.50000%，jerk=0.10113，miniumu_ttc=101.54350\n\n100%|██████████| 5/5 [00:55<00:00, 11.09s/it]\u001b[A\n 12%|█▏        | 59/500 [3:02:24<21:41:51, 177.12s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 06:32:05,503][1383994605.py][line:1670][INFO] 60/500. loss: 0.002631439516941706\n[2024-07-23 06:32:05,514][1383994605.py][line:1671][INFO] 60/500. mserror: 78.5211410522461  col: 32  count: 216  jerk: 0.05968968942761421  ttc: 178.5135040283203\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:32:10,947][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3887883\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 06:32:16,789][1383994605.py][line:1089][INFO] mean_spacing_error：91.05541，col=41，count=216，rate=18.98148%，jerk=0.04452，miniumu_ttc=169.11061\n\n 20%|██        | 1/5 [00:11<00:45, 11.26s/it]\u001b[A[2024-07-23 06:32:22,675][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2851670\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 06:32:28,726][1383994605.py][line:1089][INFO] mean_spacing_error：33.84048，col=16，count=216，rate=7.40741%，jerk=0.13503，miniumu_ttc=133.81834\n\n 40%|████      | 2/5 [00:23<00:34, 11.66s/it]\u001b[A[2024-07-23 06:32:34,542][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2388348\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 06:32:40,147][1383994605.py][line:1089][INFO] mean_spacing_error：37.98017，col=8，count=216，rate=3.70370%，jerk=0.15988，miniumu_ttc=136.96761\n\n 60%|██████    | 3/5 [00:34<00:23, 11.55s/it]\u001b[A[2024-07-23 06:32:45,914][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2276166\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 06:32:51,723][1383994605.py][line:1089][INFO] mean_spacing_error：29.78185，col=21，count=216，rate=9.72222%，jerk=0.11318，miniumu_ttc=111.11031\n\n 80%|████████  | 4/5 [00:46<00:11, 11.56s/it]\u001b[A[2024-07-23 06:32:57,322][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2201725\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 06:33:03,163][1383994605.py][line:1089][INFO] mean_spacing_error：19.30890，col=16，count=216，rate=7.40741%，jerk=0.15627，miniumu_ttc=291.88461\n\n100%|██████████| 5/5 [00:57<00:00, 11.53s/it]\u001b[A\n 12%|█▏        | 60/500 [3:05:44<22:27:31, 183.75s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.37s/it]\u001b[A\n2it [00:04,  1.96s/it]\u001b[A\n3it [00:05,  1.81s/it]\u001b[A\n[2024-07-23 06:35:40,409][1383994605.py][line:1670][INFO] 61/500. loss: 0.0032789912074804306\n[2024-07-23 06:35:40,422][1383994605.py][line:1671][INFO] 61/500. mserror: 79.9376449584961  col: 27  count: 216  jerk: 0.06493648141622543  ttc: 205.56800842285156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:35:45,870][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3973083\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:35:51,430][1383994605.py][line:1089][INFO] mean_spacing_error：94.16977，col=29，count=216，rate=13.42593%，jerk=0.05686，miniumu_ttc=208.67845\n\n 20%|██        | 1/5 [00:11<00:43, 11.00s/it]\u001b[A[2024-07-23 06:35:57,018][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2843496\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 06:36:02,936][1383994605.py][line:1089][INFO] mean_spacing_error：35.01208，col=20，count=216，rate=9.25926%，jerk=0.13394，miniumu_ttc=94.79887\n\n 40%|████      | 2/5 [00:22<00:33, 11.29s/it]\u001b[A[2024-07-23 06:36:09,281][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2413414\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:36:14,831][1383994605.py][line:1089][INFO] mean_spacing_error：41.18248，col=13，count=216，rate=6.01852%，jerk=0.13648，miniumu_ttc=155.80453\n\n 60%|██████    | 3/5 [00:34<00:23, 11.57s/it]\u001b[A[2024-07-23 06:36:20,391][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2444186\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 06:36:25,998][1383994605.py][line:1089][INFO] mean_spacing_error：35.95394，col=7，count=216，rate=3.24074%，jerk=0.14045，miniumu_ttc=193.26474\n\n 80%|████████  | 4/5 [00:45<00:11, 11.41s/it]\u001b[A[2024-07-23 06:36:31,714][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2161734\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:36:37,466][1383994605.py][line:1089][INFO] mean_spacing_error：24.08040，col=24，count=216，rate=11.11111%，jerk=0.12938，miniumu_ttc=113.25539\n\n100%|██████████| 5/5 [00:57<00:00, 11.41s/it]\u001b[A\n 12%|█▏        | 61/500 [3:09:18<23:31:28, 192.91s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 06:38:23,354][1383994605.py][line:1670][INFO] 62/500. loss: 0.003335849071542422\n[2024-07-23 06:38:23,366][1383994605.py][line:1671][INFO] 62/500. mserror: 65.6973648071289  col: 31  count: 216  jerk: 0.06508684158325195  ttc: 149.696533203125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:38:28,524][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3902584\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 06:38:34,114][1383994605.py][line:1089][INFO] mean_spacing_error：86.80917，col=47，count=216，rate=21.75926%，jerk=0.04633，miniumu_ttc=148.69516\n\n 20%|██        | 1/5 [00:10<00:42, 10.74s/it]\u001b[A[2024-07-23 06:38:39,605][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2835773\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 06:38:45,253][1383994605.py][line:1089][INFO] mean_spacing_error：80.38706，col=71，count=216，rate=32.87037%，jerk=0.12775，miniumu_ttc=10.56177\n\n 40%|████      | 2/5 [00:21<00:32, 10.97s/it]\u001b[A[2024-07-23 06:38:51,099][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2367222\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:38:56,840][1383994605.py][line:1089][INFO] mean_spacing_error：48.68708，col=19，count=216，rate=8.79630%，jerk=0.13852，miniumu_ttc=124.02460\n\n 60%|██████    | 3/5 [00:33<00:22, 11.25s/it]\u001b[A[2024-07-23 06:39:03,096][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2303748\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 06:39:09,243][1383994605.py][line:1089][INFO] mean_spacing_error：31.69863，col=11，count=216，rate=5.09259%，jerk=0.14510，miniumu_ttc=210.06876\n\n 80%|████████  | 4/5 [00:45<00:11, 11.71s/it]\u001b[A[2024-07-23 06:39:14,587][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2119694\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 06:39:20,397][1383994605.py][line:1089][INFO] mean_spacing_error：73.27811，col=93，count=216，rate=43.05556%，jerk=0.13087，miniumu_ttc=7.49743\n\n100%|██████████| 5/5 [00:57<00:00, 11.41s/it]\u001b[A\n 12%|█▏        | 62/500 [3:12:01<22:22:38, 183.92s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.03s/it]\u001b[A\n2it [00:03,  1.63s/it]\u001b[A\n3it [00:04,  1.56s/it]\u001b[A\n[2024-07-23 06:41:00,667][1383994605.py][line:1670][INFO] 63/500. loss: 0.0023770537227392197\n[2024-07-23 06:41:00,679][1383994605.py][line:1671][INFO] 63/500. mserror: 66.39624786376953  col: 42  count: 216  jerk: 0.05662231519818306  ttc: 132.38987731933594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:41:06,002][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3844981\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 06:41:11,253][1383994605.py][line:1089][INFO] mean_spacing_error：77.12814，col=37，count=216，rate=17.12963%，jerk=0.05289，miniumu_ttc=195.51140\n\n 20%|██        | 1/5 [00:10<00:42, 10.57s/it]\u001b[A[2024-07-23 06:41:16,712][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2822673\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 06:41:21,953][1383994605.py][line:1089][INFO] mean_spacing_error：40.30169，col=23，count=216，rate=10.64815%，jerk=0.13571，miniumu_ttc=29.75234\n\n 40%|████      | 2/5 [00:21<00:31, 10.65s/it]\u001b[A[2024-07-23 06:41:27,575][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2362315\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 06:41:32,941][1383994605.py][line:1089][INFO] mean_spacing_error：39.17608，col=9，count=216，rate=4.16667%，jerk=0.16330，miniumu_ttc=133.66830\n\n 60%|██████    | 3/5 [00:32<00:21, 10.80s/it]\u001b[A[2024-07-23 06:41:38,365][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2262509\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 06:41:43,868][1383994605.py][line:1089][INFO] mean_spacing_error：37.99554，col=19，count=216，rate=8.79630%，jerk=0.10679，miniumu_ttc=110.44225\n\n 80%|████████  | 4/5 [00:43<00:10, 10.85s/it]\u001b[A[2024-07-23 06:41:49,375][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2245916\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 06:41:54,844][1383994605.py][line:1089][INFO] mean_spacing_error：37.80928，col=48，count=216，rate=22.22222%，jerk=0.10733，miniumu_ttc=87.01259\n\n100%|██████████| 5/5 [00:54<00:00, 10.83s/it]\u001b[A\n 13%|█▎        | 63/500 [3:14:35<21:15:07, 175.07s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 06:44:18,925][1383994605.py][line:1670][INFO] 64/500. loss: 0.0017207117440799873\n[2024-07-23 06:44:18,935][1383994605.py][line:1671][INFO] 64/500. mserror: 77.71958923339844  col: 40  count: 216  jerk: 0.049011826515197754  ttc: 142.9287872314453\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:44:24,257][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3762896\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 06:44:29,923][1383994605.py][line:1089][INFO] mean_spacing_error：76.78249，col=36，count=216，rate=16.66667%，jerk=0.05273，miniumu_ttc=196.19292\n\n 20%|██        | 1/5 [00:10<00:43, 10.98s/it]\u001b[A[2024-07-23 06:44:35,881][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2815049\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 06:44:41,762][1383994605.py][line:1089][INFO] mean_spacing_error：33.71283，col=23，count=216，rate=10.64815%，jerk=0.12078，miniumu_ttc=102.17132\n\n 40%|████      | 2/5 [00:22<00:34, 11.49s/it]\u001b[A[2024-07-23 06:44:47,975][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2322190\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 06:44:53,751][1383994605.py][line:1089][INFO] mean_spacing_error：38.16179，col=15，count=216，rate=6.94444%，jerk=0.15801，miniumu_ttc=130.15833\n\n 60%|██████    | 3/5 [00:34<00:23, 11.72s/it]\u001b[A[2024-07-23 06:44:59,734][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2267462\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.62s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.13s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 06:45:05,835][1383994605.py][line:1089][INFO] mean_spacing_error：29.24591，col=20，count=216，rate=9.25926%，jerk=0.11560，miniumu_ttc=100.56023\n\n 80%|████████  | 4/5 [00:46<00:11, 11.86s/it]\u001b[A[2024-07-23 06:45:11,824][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2240574\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 06:45:17,362][1383994605.py][line:1089][INFO] mean_spacing_error：23.65998，col=23，count=216，rate=10.64815%，jerk=0.11419，miniumu_ttc=182.30417\n\n100%|██████████| 5/5 [00:58<00:00, 11.69s/it]\u001b[A\n 13%|█▎        | 64/500 [3:17:58<22:12:02, 183.31s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.02s/it]\u001b[A\n2it [00:03,  1.66s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 06:46:56,007][1383994605.py][line:1670][INFO] 65/500. loss: 0.0029882496843735376\n[2024-07-23 06:46:56,020][1383994605.py][line:1671][INFO] 65/500. mserror: 93.86598205566406  col: 37  count: 216  jerk: 0.04306010529398918  ttc: 174.42550659179688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:47:01,189][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3579343\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 06:47:06,536][1383994605.py][line:1089][INFO] mean_spacing_error：84.40053，col=29，count=216，rate=13.42593%，jerk=0.05878，miniumu_ttc=205.29242\n\n 20%|██        | 1/5 [00:10<00:42, 10.52s/it]\u001b[A[2024-07-23 06:47:11,953][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2898942\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 06:47:17,836][1383994605.py][line:1089][INFO] mean_spacing_error：30.58311，col=22，count=216，rate=10.18519%，jerk=0.12111，miniumu_ttc=129.35216\n\n 40%|████      | 2/5 [00:21<00:32, 10.97s/it]\u001b[A[2024-07-23 06:47:23,174][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2307348\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:47:28,733][1383994605.py][line:1089][INFO] mean_spacing_error：34.59200，col=11，count=216，rate=5.09259%，jerk=0.14714，miniumu_ttc=173.11046\n\n 60%|██████    | 3/5 [00:32<00:21, 10.94s/it]\u001b[A[2024-07-23 06:47:34,412][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2217847\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 06:47:40,359][1383994605.py][line:1089][INFO] mean_spacing_error：25.07885，col=10，count=216，rate=4.62963%，jerk=0.14728，miniumu_ttc=141.74660\n\n 80%|████████  | 4/5 [00:44<00:11, 11.21s/it]\u001b[A[2024-07-23 06:47:46,056][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2061398\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 06:47:51,899][1383994605.py][line:1089][INFO] mean_spacing_error：20.69865，col=14，count=216，rate=6.48148%，jerk=0.14687，miniumu_ttc=134.99408\n\n100%|██████████| 5/5 [00:55<00:00, 11.18s/it]\u001b[A\n 13%|█▎        | 65/500 [3:20:32<21:06:24, 174.68s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 06:49:30,132][1383994605.py][line:1670][INFO] 66/500. loss: 0.0018207672983407974\n[2024-07-23 06:49:30,139][1383994605.py][line:1671][INFO] 66/500. mserror: 103.63162231445312  col: 44  count: 216  jerk: 0.03681044280529022  ttc: 163.43177795410156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:49:35,420][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3449089\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 06:49:40,854][1383994605.py][line:1089][INFO] mean_spacing_error：88.15543，col=15，count=216，rate=6.94444%，jerk=0.07497，miniumu_ttc=258.29480\n\n 20%|██        | 1/5 [00:10<00:42, 10.70s/it]\u001b[A[2024-07-23 06:49:46,328][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2895101\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 06:49:51,813][1383994605.py][line:1089][INFO] mean_spacing_error：31.56364，col=17，count=216，rate=7.87037%，jerk=0.12042，miniumu_ttc=102.61765\n\n 40%|████      | 2/5 [00:21<00:32, 10.85s/it]\u001b[A[2024-07-23 06:49:57,576][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2392551\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 06:50:03,272][1383994605.py][line:1089][INFO] mean_spacing_error：32.07854，col=13，count=216，rate=6.01852%，jerk=0.13300，miniumu_ttc=112.88217\n\n 60%|██████    | 3/5 [00:33<00:22, 11.13s/it]\u001b[A[2024-07-23 06:50:09,121][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2363626\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 06:50:14,501][1383994605.py][line:1089][INFO] mean_spacing_error：30.00988，col=6，count=216，rate=2.77778%，jerk=0.16151，miniumu_ttc=219.65303\n\n 80%|████████  | 4/5 [00:44<00:11, 11.17s/it]\u001b[A[2024-07-23 06:50:20,089][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2210391\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 06:50:25,628][1383994605.py][line:1089][INFO] mean_spacing_error：25.21853，col=22，count=216，rate=10.18519%，jerk=0.12757，miniumu_ttc=130.97798\n\n100%|██████████| 5/5 [00:55<00:00, 11.10s/it]\u001b[A\n 13%|█▎        | 66/500 [3:23:06<20:18:01, 168.39s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.37s/it]\u001b[A\n2it [00:04,  1.95s/it]\u001b[A\n3it [00:05,  1.86s/it]\u001b[A\n[2024-07-23 06:53:00,426][1383994605.py][line:1670][INFO] 67/500. loss: 0.003134562944372495\n[2024-07-23 06:53:00,435][1383994605.py][line:1671][INFO] 67/500. mserror: 101.12454986572266  col: 54  count: 216  jerk: 0.03246479481458664  ttc: 150.37179565429688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:53:06,162][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3408614\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 06:53:11,705][1383994605.py][line:1089][INFO] mean_spacing_error：94.42095，col=10，count=216，rate=4.62963%，jerk=0.08400，miniumu_ttc=324.41779\n\n 20%|██        | 1/5 [00:11<00:45, 11.26s/it]\u001b[A[2024-07-23 06:53:17,122][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2973518\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 06:53:22,516][1383994605.py][line:1089][INFO] mean_spacing_error：32.04073，col=18，count=216，rate=8.33333%，jerk=0.11570，miniumu_ttc=97.33634\n\n 40%|████      | 2/5 [00:22<00:32, 10.99s/it]\u001b[A[2024-07-23 06:53:28,019][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2443915\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 06:53:33,588][1383994605.py][line:1089][INFO] mean_spacing_error：33.57265，col=16，count=216，rate=7.40741%，jerk=0.12703，miniumu_ttc=127.25889\n\n 60%|██████    | 3/5 [00:33<00:22, 11.03s/it]\u001b[A[2024-07-23 06:53:39,580][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2365465\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 06:53:45,175][1383994605.py][line:1089][INFO] mean_spacing_error：27.29200，col=7，count=216，rate=3.24074%，jerk=0.14761，miniumu_ttc=187.16393\n\n 80%|████████  | 4/5 [00:44<00:11, 11.25s/it]\u001b[A[2024-07-23 06:53:50,985][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2151811\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 06:53:56,856][1383994605.py][line:1089][INFO] mean_spacing_error：25.46517，col=25，count=216，rate=11.57407%，jerk=0.12647，miniumu_ttc=66.31992\n\n100%|██████████| 5/5 [00:56<00:00, 11.28s/it]\u001b[A\n 13%|█▎        | 67/500 [3:26:37<21:47:59, 181.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.36s/it]\u001b[A\n2it [00:03,  1.93s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 06:56:30,065][1383994605.py][line:1670][INFO] 68/500. loss: 0.00333338292936484\n[2024-07-23 06:56:30,073][1383994605.py][line:1671][INFO] 68/500. mserror: 96.35737609863281  col: 57  count: 216  jerk: 0.03183937817811966  ttc: 146.5759735107422\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 06:56:35,692][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3333177\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 06:56:41,512][1383994605.py][line:1089][INFO] mean_spacing_error：96.89637，col=8，count=216，rate=3.70370%，jerk=0.08619，miniumu_ttc=230.73975\n\n 20%|██        | 1/5 [00:11<00:45, 11.42s/it]\u001b[A[2024-07-23 06:56:47,444][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3212033\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 06:56:53,361][1383994605.py][line:1089][INFO] mean_spacing_error：36.78542，col=9，count=216，rate=4.16667%，jerk=0.12190，miniumu_ttc=129.02296\n\n 40%|████      | 2/5 [00:23<00:35, 11.67s/it]\u001b[A[2024-07-23 06:56:59,038][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2385784\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 06:57:04,786][1383994605.py][line:1089][INFO] mean_spacing_error：30.86363，col=12，count=216，rate=5.55556%，jerk=0.14190，miniumu_ttc=181.17776\n\n 60%|██████    | 3/5 [00:34<00:23, 11.56s/it]\u001b[A[2024-07-23 06:57:10,544][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2260950\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 06:57:16,609][1383994605.py][line:1089][INFO] mean_spacing_error：24.74449，col=8，count=216，rate=3.70370%，jerk=0.16082，miniumu_ttc=204.10944\n\n 80%|████████  | 4/5 [00:46<00:11, 11.66s/it]\u001b[A[2024-07-23 06:57:22,830][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2125797\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 06:57:29,064][1383994605.py][line:1089][INFO] mean_spacing_error：28.60040，col=25，count=216，rate=11.57407%，jerk=0.12960，miniumu_ttc=25.51171\n\n100%|██████████| 5/5 [00:58<00:00, 11.80s/it]\u001b[A\n 14%|█▎        | 68/500 [3:30:09<22:51:51, 190.54s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.40s/it]\u001b[A\n2it [00:03,  1.92s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 06:59:59,616][1383994605.py][line:1670][INFO] 69/500. loss: 0.0031141859168807664\n[2024-07-23 06:59:59,620][1383994605.py][line:1671][INFO] 69/500. mserror: 93.03054809570312  col: 58  count: 216  jerk: 0.032360583543777466  ttc: 314.4787902832031\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:00:06,113][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3269509\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 07:00:12,232][1383994605.py][line:1089][INFO] mean_spacing_error：109.08096，col=7，count=216，rate=3.24074%，jerk=0.09657，miniumu_ttc=239.72610\n\n 20%|██        | 1/5 [00:12<00:50, 12.59s/it]\u001b[A[2024-07-23 07:00:18,262][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3373464\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 07:00:24,172][1383994605.py][line:1089][INFO] mean_spacing_error：32.80912，col=16，count=216，rate=7.40741%，jerk=0.12429，miniumu_ttc=113.91840\n\n 40%|████      | 2/5 [00:24<00:36, 12.21s/it]\u001b[A[2024-07-23 07:00:30,699][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2417454\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:03,  3.14s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.33s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.14s/it]\u001b[A\u001b[A\n[2024-07-23 07:00:37,443][1383994605.py][line:1089][INFO] mean_spacing_error：31.45586，col=16，count=216，rate=7.40741%，jerk=0.12247，miniumu_ttc=124.96092\n\n 60%|██████    | 3/5 [00:37<00:25, 12.69s/it]\u001b[A[2024-07-23 07:00:43,898][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2357998\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.68s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 07:00:50,076][1383994605.py][line:1089][INFO] mean_spacing_error：27.30396，col=11，count=216，rate=5.09259%，jerk=0.13979，miniumu_ttc=148.76222\n\n 80%|████████  | 4/5 [00:50<00:12, 12.67s/it]\u001b[A[2024-07-23 07:00:55,936][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2140285\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 07:01:02,100][1383994605.py][line:1089][INFO] mean_spacing_error：27.45670，col=18，count=216，rate=8.33333%，jerk=0.13613，miniumu_ttc=90.29066\n\n100%|██████████| 5/5 [01:02<00:00, 12.49s/it]\u001b[A\n 14%|█▍        | 69/500 [3:33:43<23:37:10, 197.29s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.82s/it]\u001b[A\n[2024-07-23 07:03:02,704][1383994605.py][line:1670][INFO] 70/500. loss: 0.0035725285609563193\n[2024-07-23 07:03:02,724][1383994605.py][line:1671][INFO] 70/500. mserror: 91.53466033935547  col: 54  count: 216  jerk: 0.03546605631709099  ttc: 155.34388732910156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:03:08,093][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3210027\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 07:03:14,091][1383994605.py][line:1089][INFO] mean_spacing_error：95.07962，col=5，count=216，rate=2.31481%，jerk=0.11210，miniumu_ttc=251.38293\n\n 20%|██        | 1/5 [00:11<00:45, 11.36s/it]\u001b[A[2024-07-23 07:03:19,718][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2836949\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:03:25,429][1383994605.py][line:1089][INFO] mean_spacing_error：29.20864，col=6，count=216，rate=2.77778%，jerk=0.17049，miniumu_ttc=183.31360\n\n 40%|████      | 2/5 [00:22<00:34, 11.35s/it]\u001b[A[2024-07-23 07:03:31,094][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2530425\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:03:36,881][1383994605.py][line:1089][INFO] mean_spacing_error：33.93047，col=13，count=216，rate=6.01852%，jerk=0.12913，miniumu_ttc=152.22295\n\n 60%|██████    | 3/5 [00:34<00:22, 11.40s/it]\u001b[A[2024-07-23 07:03:42,630][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2352516\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.77s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.12s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 07:03:48,861][1383994605.py][line:1089][INFO] mean_spacing_error：29.02543，col=7，count=216，rate=3.24074%，jerk=0.13169，miniumu_ttc=184.09326\n\n 80%|████████  | 4/5 [00:46<00:11, 11.63s/it]\u001b[A[2024-07-23 07:03:54,903][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2141176\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 07:04:00,918][1383994605.py][line:1089][INFO] mean_spacing_error：21.79598，col=18，count=216，rate=8.33333%，jerk=0.13351，miniumu_ttc=63.58810\n\n100%|██████████| 5/5 [00:58<00:00, 11.64s/it]\u001b[A\n 14%|█▍        | 70/500 [3:36:41<22:54:11, 191.75s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 07:06:46,193][1383994605.py][line:1670][INFO] 71/500. loss: 0.0030316446597377458\n[2024-07-23 07:06:46,207][1383994605.py][line:1671][INFO] 71/500. mserror: 89.93782806396484  col: 46  count: 216  jerk: 0.039657048881053925  ttc: 154.11355590820312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:06:52,068][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3105947\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.16s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.00s/it]\u001b[A\u001b[A\n[2024-07-23 07:06:58,361][1383994605.py][line:1089][INFO] mean_spacing_error：105.17739，col=4，count=216，rate=1.85185%，jerk=0.11694，miniumu_ttc=241.36388\n\n 20%|██        | 1/5 [00:12<00:48, 12.15s/it]\u001b[A[2024-07-23 07:07:04,500][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2507348\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 07:07:10,309][1383994605.py][line:1089][INFO] mean_spacing_error：54.77182，col=22，count=216，rate=10.18519%，jerk=0.15602，miniumu_ttc=10.13300\n\n 40%|████      | 2/5 [00:24<00:36, 12.02s/it]\u001b[A[2024-07-23 07:07:16,213][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3083269\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 07:07:22,046][1383994605.py][line:1089][INFO] mean_spacing_error：50.66279，col=20，count=216，rate=9.25926%，jerk=0.10235，miniumu_ttc=140.27396\n\n 60%|██████    | 3/5 [00:35<00:23, 11.89s/it]\u001b[A[2024-07-23 07:07:28,227][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2562386\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 07:07:34,150][1383994605.py][line:1089][INFO] mean_spacing_error：39.74512，col=6，count=216，rate=2.77778%，jerk=0.12340，miniumu_ttc=191.81110\n\n 80%|████████  | 4/5 [00:47<00:11, 11.98s/it]\u001b[A[2024-07-23 07:07:40,243][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2223703\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 07:07:46,453][1383994605.py][line:1089][INFO] mean_spacing_error：24.47563，col=16，count=216，rate=7.40741%，jerk=0.13789，miniumu_ttc=48.18670\n\n100%|██████████| 5/5 [01:00<00:00, 12.05s/it]\u001b[A\n 14%|█▍        | 71/500 [3:40:27<24:03:30, 201.89s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 07:09:41,683][1383994605.py][line:1670][INFO] 72/500. loss: 0.003514223421613375\n[2024-07-23 07:09:41,700][1383994605.py][line:1671][INFO] 72/500. mserror: 84.27569580078125  col: 41  count: 216  jerk: 0.045194391161203384  ttc: 190.84695434570312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:09:47,353][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3076342\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 07:09:53,240][1383994605.py][line:1089][INFO] mean_spacing_error：85.74825，col=3，count=216，rate=1.38889%，jerk=0.12611，miniumu_ttc=236.69017\n\n 20%|██        | 1/5 [00:11<00:46, 11.54s/it]\u001b[A[2024-07-23 07:09:58,861][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2924127\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 07:10:04,938][1383994605.py][line:1089][INFO] mean_spacing_error：59.22246，col=35，count=216，rate=16.20370%，jerk=0.12287，miniumu_ttc=52.23144\n\n 40%|████      | 2/5 [00:23<00:34, 11.63s/it]\u001b[A[2024-07-23 07:10:10,671][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2943414\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 07:10:16,726][1383994605.py][line:1089][INFO] mean_spacing_error：48.98156，col=27，count=216，rate=12.50000%，jerk=0.09453，miniumu_ttc=96.46928\n\n 60%|██████    | 3/5 [00:35<00:23, 11.70s/it]\u001b[A[2024-07-23 07:10:22,502][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2625067\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 07:10:28,572][1383994605.py][line:1089][INFO] mean_spacing_error：56.43288，col=4，count=216，rate=1.85185%，jerk=0.13840，miniumu_ttc=213.78889\n\n 80%|████████  | 4/5 [00:46<00:11, 11.76s/it]\u001b[A[2024-07-23 07:10:34,298][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2357482\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 07:10:40,257][1383994605.py][line:1089][INFO] mean_spacing_error：25.42881，col=13，count=216，rate=6.01852%，jerk=0.15753，miniumu_ttc=89.79458\n\n100%|██████████| 5/5 [00:58<00:00, 11.71s/it]\u001b[A\n 14%|█▍        | 72/500 [3:43:21<23:00:00, 193.46s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 07:13:09,865][1383994605.py][line:1670][INFO] 73/500. loss: 0.0016085331638654072\n[2024-07-23 07:13:09,877][1383994605.py][line:1671][INFO] 73/500. mserror: 73.43415069580078  col: 43  count: 216  jerk: 0.0497937947511673  ttc: 145.6093292236328\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:13:15,095][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3157131\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:13:20,803][1383994605.py][line:1089][INFO] mean_spacing_error：74.01877，col=6，count=216，rate=2.77778%，jerk=0.11756，miniumu_ttc=224.52354\n\n 20%|██        | 1/5 [00:10<00:43, 10.93s/it]\u001b[A[2024-07-23 07:13:26,409][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3091092\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:13:31,997][1383994605.py][line:1089][INFO] mean_spacing_error：32.45368，col=19，count=216，rate=8.79630%，jerk=0.10909，miniumu_ttc=106.79303\n\n 40%|████      | 2/5 [00:22<00:33, 11.09s/it]\u001b[A[2024-07-23 07:13:38,353][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2636360\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:13:44,123][1383994605.py][line:1089][INFO] mean_spacing_error：44.69215，col=25，count=216，rate=11.57407%，jerk=0.09592，miniumu_ttc=117.18190\n\n 60%|██████    | 3/5 [00:34<00:23, 11.55s/it]\u001b[A[2024-07-23 07:13:50,023][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2438277\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 07:13:55,504][1383994605.py][line:1089][INFO] mean_spacing_error：39.43219，col=6，count=216，rate=2.77778%，jerk=0.12951，miniumu_ttc=209.36026\n\n 80%|████████  | 4/5 [00:45<00:11, 11.49s/it]\u001b[A[2024-07-23 07:14:01,409][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2215086\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 07:14:07,239][1383994605.py][line:1089][INFO] mean_spacing_error：23.48162，col=13，count=216，rate=6.01852%，jerk=0.15786，miniumu_ttc=92.73420\n\n100%|██████████| 5/5 [00:57<00:00, 11.47s/it]\u001b[A\n 15%|█▍        | 73/500 [3:46:48<23:25:37, 197.51s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 07:16:30,355][1383994605.py][line:1670][INFO] 74/500. loss: 0.0029833310594161353\n[2024-07-23 07:16:30,369][1383994605.py][line:1671][INFO] 74/500. mserror: 66.77476501464844  col: 43  count: 216  jerk: 0.05322153493762016  ttc: 141.6575469970703\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:16:35,486][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3250309\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:16:41,052][1383994605.py][line:1089][INFO] mean_spacing_error：86.32465，col=4，count=216，rate=1.85185%，jerk=0.11805，miniumu_ttc=246.89803\n\n 20%|██        | 1/5 [00:10<00:42, 10.69s/it]\u001b[A[2024-07-23 07:16:46,857][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3533235\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:16:52,584][1383994605.py][line:1089][INFO] mean_spacing_error：37.92503，col=12，count=216，rate=5.55556%，jerk=0.12070，miniumu_ttc=99.31628\n\n 40%|████      | 2/5 [00:22<00:33, 11.18s/it]\u001b[A[2024-07-23 07:16:58,238][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2463926\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:17:03,930][1383994605.py][line:1089][INFO] mean_spacing_error：34.31351，col=12，count=216，rate=5.55556%，jerk=0.13394，miniumu_ttc=139.03542\n\n 60%|██████    | 3/5 [00:33<00:22, 11.26s/it]\u001b[A[2024-07-23 07:17:09,612][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2309059\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 07:17:15,048][1383994605.py][line:1089][INFO] mean_spacing_error：30.10287，col=6，count=216，rate=2.77778%，jerk=0.17986，miniumu_ttc=177.51697\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 07:17:21,014][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2327050\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:17:26,676][1383994605.py][line:1089][INFO] mean_spacing_error：31.46301，col=17，count=216，rate=7.87037%，jerk=0.12867，miniumu_ttc=97.21499\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 15%|█▍        | 74/500 [3:50:07<23:26:27, 198.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.88s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 07:19:08,705][1383994605.py][line:1670][INFO] 75/500. loss: 0.001826920701811711\n[2024-07-23 07:19:08,716][1383994605.py][line:1671][INFO] 75/500. mserror: 67.73054504394531  col: 43  count: 216  jerk: 0.05196017771959305  ttc: 143.146728515625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:19:13,934][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3414588\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 07:19:19,369][1383994605.py][line:1089][INFO] mean_spacing_error：82.10812，col=6，count=216，rate=2.77778%，jerk=0.11630，miniumu_ttc=262.62750\n\n 20%|██        | 1/5 [00:10<00:42, 10.65s/it]\u001b[A[2024-07-23 07:19:24,868][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3515024\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:19:30,627][1383994605.py][line:1089][INFO] mean_spacing_error：58.02991，col=50，count=216，rate=23.14815%，jerk=0.09274，miniumu_ttc=85.80599\n\n 40%|████      | 2/5 [00:21<00:33, 11.00s/it]\u001b[A[2024-07-23 07:19:36,953][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2459191\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:19:42,697][1383994605.py][line:1089][INFO] mean_spacing_error：29.76440，col=13，count=216，rate=6.01852%，jerk=0.14148，miniumu_ttc=140.18645\n\n 60%|██████    | 3/5 [00:33<00:22, 11.49s/it]\u001b[A[2024-07-23 07:19:48,553][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2204690\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:19:54,136][1383994605.py][line:1089][INFO] mean_spacing_error：35.32412，col=11，count=216，rate=5.09259%，jerk=0.18403，miniumu_ttc=134.88963\n\n 80%|████████  | 4/5 [00:45<00:11, 11.47s/it]\u001b[A[2024-07-23 07:19:59,735][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2158033\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 07:20:05,603][1383994605.py][line:1089][INFO] mean_spacing_error：30.66799，col=15，count=216，rate=6.94444%，jerk=0.14031，miniumu_ttc=95.69551\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 15%|█▌        | 75/500 [3:52:46<21:59:55, 186.34s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 07:22:27,099][1383994605.py][line:1670][INFO] 76/500. loss: 0.0029606608053048453\n[2024-07-23 07:22:27,110][1383994605.py][line:1671][INFO] 76/500. mserror: 71.84048461914062  col: 40  count: 216  jerk: 0.05083434656262398  ttc: 147.536865234375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:22:32,417][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3170336\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:22:38,024][1383994605.py][line:1089][INFO] mean_spacing_error：125.80474，col=0，count=216，rate=0.00000%，jerk=0.18217，miniumu_ttc=400.79068\n\n 20%|██        | 1/5 [00:10<00:43, 10.90s/it]\u001b[A[2024-07-23 07:22:43,729][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2821140\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 07:22:49,926][1383994605.py][line:1089][INFO] mean_spacing_error：67.80335，col=99，count=216，rate=45.83333%，jerk=0.07690，miniumu_ttc=86.93616\n\n 40%|████      | 2/5 [00:22<00:34, 11.49s/it]\u001b[A[2024-07-23 07:22:55,653][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2854146\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:23:01,241][1383994605.py][line:1089][INFO] mean_spacing_error：59.33406，col=28，count=216，rate=12.96296%，jerk=0.07669，miniumu_ttc=179.91002\n\n 60%|██████    | 3/5 [00:34<00:22, 11.41s/it]\u001b[A[2024-07-23 07:23:06,863][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2472008\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 07:23:12,256][1383994605.py][line:1089][INFO] mean_spacing_error：26.48728，col=7，count=216，rate=3.24074%，jerk=0.15792，miniumu_ttc=186.34369\n\n 80%|████████  | 4/5 [00:45<00:11, 11.26s/it]\u001b[A[2024-07-23 07:23:17,874][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2214202\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:23:23,516][1383994605.py][line:1089][INFO] mean_spacing_error：30.55213，col=13，count=216，rate=6.01852%，jerk=0.17471，miniumu_ttc=92.10561\n\n100%|██████████| 5/5 [00:56<00:00, 11.28s/it]\u001b[A\n 15%|█▌        | 76/500 [3:56:04<22:21:19, 189.81s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 07:25:07,508][1383994605.py][line:1670][INFO] 77/500. loss: 0.0017696665599942207\n[2024-07-23 07:25:07,524][1383994605.py][line:1671][INFO] 77/500. mserror: 80.1685562133789  col: 38  count: 216  jerk: 0.04698474332690239  ttc: 166.76889038085938\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:25:12,804][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3181008\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:25:18,394][1383994605.py][line:1089][INFO] mean_spacing_error：120.96568，col=1，count=216，rate=0.46296%，jerk=0.15415，miniumu_ttc=395.56183\n\n 20%|██        | 1/5 [00:10<00:43, 10.87s/it]\u001b[A[2024-07-23 07:25:23,828][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2784730\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:25:29,589][1383994605.py][line:1089][INFO] mean_spacing_error：52.77192，col=83，count=216，rate=38.42593%，jerk=0.05796，miniumu_ttc=102.46487\n\n 40%|████      | 2/5 [00:22<00:33, 11.06s/it]\u001b[A[2024-07-23 07:25:35,247][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2940323\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:25:40,796][1383994605.py][line:1089][INFO] mean_spacing_error：33.96267，col=15，count=216，rate=6.94444%，jerk=0.10426，miniumu_ttc=149.19788\n\n 60%|██████    | 3/5 [00:33<00:22, 11.13s/it]\u001b[A[2024-07-23 07:25:46,336][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2436068\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:25:52,003][1383994605.py][line:1089][INFO] mean_spacing_error：31.53143，col=6，count=216，rate=2.77778%，jerk=0.16893，miniumu_ttc=192.67770\n\n 80%|████████  | 4/5 [00:44<00:11, 11.16s/it]\u001b[A[2024-07-23 07:25:57,599][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2287911\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 07:26:03,609][1383994605.py][line:1089][INFO] mean_spacing_error：39.17789，col=18，count=216，rate=8.33333%，jerk=0.16009，miniumu_ttc=91.37130\n\n100%|██████████| 5/5 [00:56<00:00, 11.22s/it]\u001b[A\n 15%|█▌        | 77/500 [3:58:44<21:15:21, 180.90s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 07:27:48,609][1383994605.py][line:1670][INFO] 78/500. loss: 0.003416940259436766\n[2024-07-23 07:27:48,680][1383994605.py][line:1671][INFO] 78/500. mserror: 81.926025390625  col: 42  count: 216  jerk: 0.044463321566581726  ttc: 185.2547607421875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:27:57,512][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3227638\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:28:03,065][1383994605.py][line:1089][INFO] mean_spacing_error：87.24881，col=4，count=216，rate=1.85185%，jerk=0.13420，miniumu_ttc=250.31146\n\n 20%|██        | 1/5 [00:14<00:57, 14.34s/it]\u001b[A[2024-07-23 07:28:08,967][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2694165\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 07:28:14,504][1383994605.py][line:1089][INFO] mean_spacing_error：64.19553，col=87，count=216，rate=40.27778%，jerk=0.07848，miniumu_ttc=26.22379\n\n 40%|████      | 2/5 [00:25<00:37, 12.63s/it]\u001b[A[2024-07-23 07:28:20,587][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2879548\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 07:28:26,077][1383994605.py][line:1089][INFO] mean_spacing_error：38.76826，col=17，count=216，rate=7.87037%，jerk=0.12079，miniumu_ttc=169.92029\n\n 60%|██████    | 3/5 [00:37<00:24, 12.15s/it]\u001b[A[2024-07-23 07:28:31,626][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2317050\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 07:28:37,245][1383994605.py][line:1089][INFO] mean_spacing_error：28.12514，col=6，count=216，rate=2.77778%，jerk=0.16665，miniumu_ttc=143.12111\n\n 80%|████████  | 4/5 [00:48<00:11, 11.76s/it]\u001b[A[2024-07-23 07:28:42,949][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2110978\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:28:48,738][1383994605.py][line:1089][INFO] mean_spacing_error：24.46546，col=16，count=216，rate=7.40741%，jerk=0.15255，miniumu_ttc=89.59293\n\n100%|██████████| 5/5 [01:00<00:00, 12.00s/it]\u001b[A\n 16%|█▌        | 78/500 [4:01:29<20:39:01, 176.16s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.25s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 07:31:07,545][1383994605.py][line:1670][INFO] 79/500. loss: 0.0031883123641212783\n[2024-07-23 07:31:07,557][1383994605.py][line:1671][INFO] 79/500. mserror: 79.61080169677734  col: 43  count: 216  jerk: 0.04575295373797417  ttc: 158.78302001953125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:31:12,927][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3126351\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 07:31:19,038][1383994605.py][line:1089][INFO] mean_spacing_error：84.16340，col=6，count=216，rate=2.77778%，jerk=0.12047，miniumu_ttc=241.72263\n\n 20%|██        | 1/5 [00:11<00:45, 11.48s/it]\u001b[A[2024-07-23 07:31:24,797][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2460942\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 07:31:30,687][1383994605.py][line:1089][INFO] mean_spacing_error：26.12082，col=16，count=216，rate=7.40741%，jerk=0.14960，miniumu_ttc=95.08363\n\n 40%|████      | 2/5 [00:23<00:34, 11.58s/it]\u001b[A[2024-07-23 07:31:36,382][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2457159\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:31:42,036][1383994605.py][line:1089][INFO] mean_spacing_error：25.49116，col=7，count=216，rate=3.24074%，jerk=0.18355，miniumu_ttc=209.79633\n\n 60%|██████    | 3/5 [00:34<00:22, 11.48s/it]\u001b[A[2024-07-23 07:31:47,890][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2401845\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.70s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 07:31:53,852][1383994605.py][line:1089][INFO] mean_spacing_error：32.19269，col=25，count=216，rate=11.57407%，jerk=0.14643，miniumu_ttc=52.85903\n\n 80%|████████  | 4/5 [00:46<00:11, 11.61s/it]\u001b[A[2024-07-23 07:31:59,696][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2662155\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 07:32:05,746][1383994605.py][line:1089][INFO] mean_spacing_error：25.19524，col=25，count=216，rate=11.57407%，jerk=0.11319，miniumu_ttc=95.05289\n\n100%|██████████| 5/5 [00:58<00:00, 11.64s/it]\u001b[A\n 16%|█▌        | 79/500 [4:04:46<21:19:59, 182.42s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.47s/it]\u001b[A\n2it [00:04,  1.97s/it]\u001b[A\n3it [00:05,  1.86s/it]\u001b[A\n[2024-07-23 07:34:33,592][1383994605.py][line:1670][INFO] 80/500. loss: 0.0015584214900930722\n[2024-07-23 07:34:33,604][1383994605.py][line:1671][INFO] 80/500. mserror: 77.37877655029297  col: 41  count: 216  jerk: 0.04726115241646767  ttc: 155.24404907226562\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:34:38,990][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2978196\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:34:44,660][1383994605.py][line:1089][INFO] mean_spacing_error：71.23735，col=6，count=216，rate=2.77778%，jerk=0.12210，miniumu_ttc=213.42618\n\n 20%|██        | 1/5 [00:11<00:44, 11.05s/it]\u001b[A[2024-07-23 07:34:50,523][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2404474\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:34:56,077][1383994605.py][line:1089][INFO] mean_spacing_error：26.28836，col=20，count=216，rate=9.25926%，jerk=0.11776，miniumu_ttc=94.81395\n\n 40%|████      | 2/5 [00:22<00:33, 11.26s/it]\u001b[A[2024-07-23 07:35:01,938][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2317069\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 07:35:07,739][1383994605.py][line:1089][INFO] mean_spacing_error：30.23705，col=6，count=216，rate=2.77778%，jerk=0.22161，miniumu_ttc=94.25436\n\n 60%|██████    | 3/5 [00:34<00:22, 11.45s/it]\u001b[A[2024-07-23 07:35:13,264][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3053428\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 07:35:18,903][1383994605.py][line:1089][INFO] mean_spacing_error：24.57105，col=8，count=216，rate=3.70370%，jerk=0.21835，miniumu_ttc=31.94439\n\n 80%|████████  | 4/5 [00:45<00:11, 11.34s/it]\u001b[A[2024-07-23 07:35:24,412][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2806130\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 07:35:29,922][1383994605.py][line:1089][INFO] mean_spacing_error：19.08174，col=13，count=216，rate=6.01852%，jerk=0.15650，miniumu_ttc=118.56365\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 16%|█▌        | 80/500 [4:08:10<22:02:36, 188.94s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.28s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 07:37:54,126][1383994605.py][line:1670][INFO] 81/500. loss: 0.0029672070716818175\n[2024-07-23 07:37:54,141][1383994605.py][line:1671][INFO] 81/500. mserror: 74.37928009033203  col: 39  count: 216  jerk: 0.050049446523189545  ttc: 166.67965698242188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:37:59,604][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2930760\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 07:38:05,713][1383994605.py][line:1089][INFO] mean_spacing_error：55.81003，col=7，count=216，rate=3.24074%，jerk=0.12294，miniumu_ttc=197.16830\n\n 20%|██        | 1/5 [00:11<00:46, 11.57s/it]\u001b[A[2024-07-23 07:38:11,482][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2342155\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.76s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.13s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 07:38:17,672][1383994605.py][line:1089][INFO] mean_spacing_error：28.56554，col=16，count=216，rate=7.40741%，jerk=0.14411，miniumu_ttc=88.16461\n\n 40%|████      | 2/5 [00:23<00:35, 11.80s/it]\u001b[A[2024-07-23 07:38:23,327][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2132633\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:38:29,084][1383994605.py][line:1089][INFO] mean_spacing_error：45.35960，col=17，count=216，rate=7.87037%，jerk=0.18635，miniumu_ttc=55.39649\n\n 60%|██████    | 3/5 [00:34<00:23, 11.62s/it]\u001b[A[2024-07-23 07:38:34,934][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2517961\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 07:38:40,769][1383994605.py][line:1089][INFO] mean_spacing_error：32.12577，col=16，count=216，rate=7.40741%，jerk=0.18790，miniumu_ttc=22.33022\n\n 80%|████████  | 4/5 [00:46<00:11, 11.65s/it]\u001b[A[2024-07-23 07:38:46,681][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2413919\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.68s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 07:38:52,873][1383994605.py][line:1089][INFO] mean_spacing_error：30.63399，col=17，count=216，rate=7.87037%，jerk=0.16788，miniumu_ttc=32.60515\n\n100%|██████████| 5/5 [00:58<00:00, 11.75s/it]\u001b[A\n 16%|█▌        | 81/500 [4:11:33<22:28:49, 193.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 07:41:31,243][1383994605.py][line:1670][INFO] 82/500. loss: 0.0031503215432167053\n[2024-07-23 07:41:31,260][1383994605.py][line:1671][INFO] 82/500. mserror: 70.34131622314453  col: 36  count: 216  jerk: 0.05552630499005318  ttc: 242.95542907714844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:41:36,739][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.4002933\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:41:42,313][1383994605.py][line:1089][INFO] mean_spacing_error：98.82983，col=0，count=216，rate=0.00000%，jerk=0.17475，miniumu_ttc=363.47928\n\n 20%|██        | 1/5 [00:11<00:44, 11.05s/it]\u001b[A[2024-07-23 07:41:48,096][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2688979\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 07:41:53,928][1383994605.py][line:1089][INFO] mean_spacing_error：58.07817，col=29，count=216，rate=13.42593%，jerk=0.07565，miniumu_ttc=92.59950\n\n 40%|████      | 2/5 [00:22<00:34, 11.38s/it]\u001b[A[2024-07-23 07:41:59,842][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2624998\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 07:42:05,742][1383994605.py][line:1089][INFO] mean_spacing_error：50.21543，col=8，count=216，rate=3.70370%，jerk=0.12369，miniumu_ttc=190.54028\n\n 60%|██████    | 3/5 [00:34<00:23, 11.58s/it]\u001b[A[2024-07-23 07:42:11,576][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2176318\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 07:42:17,406][1383994605.py][line:1089][INFO] mean_spacing_error：28.74171，col=15，count=216，rate=6.94444%，jerk=0.16583，miniumu_ttc=19.07671\n\n 80%|████████  | 4/5 [00:46<00:11, 11.61s/it]\u001b[A[2024-07-23 07:42:23,154][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2428655\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 07:42:29,215][1383994605.py][line:1089][INFO] mean_spacing_error：20.24727，col=12，count=216，rate=5.55556%，jerk=0.14823，miniumu_ttc=115.27212\n\n100%|██████████| 5/5 [00:57<00:00, 11.59s/it]\u001b[A\n 16%|█▋        | 82/500 [4:15:10<23:14:05, 200.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.38s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.85s/it]\u001b[A\n[2024-07-23 07:45:06,312][1383994605.py][line:1670][INFO] 83/500. loss: 0.00308158528059721\n[2024-07-23 07:45:06,327][1383994605.py][line:1671][INFO] 83/500. mserror: 63.975486755371094  col: 33  count: 216  jerk: 0.06375652551651001  ttc: 158.5897216796875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:45:11,987][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2936871\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:45:17,761][1383994605.py][line:1089][INFO] mean_spacing_error：101.89503，col=4，count=216，rate=1.85185%，jerk=0.12047，miniumu_ttc=269.77487\n\n 20%|██        | 1/5 [00:11<00:45, 11.42s/it]\u001b[A[2024-07-23 07:45:23,567][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2475888\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:45:29,335][1383994605.py][line:1089][INFO] mean_spacing_error：35.67321，col=14，count=216，rate=6.48148%，jerk=0.13400，miniumu_ttc=119.79390\n\n 40%|████      | 2/5 [00:22<00:34, 11.51s/it]\u001b[A[2024-07-23 07:45:34,918][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2190917\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.83s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 07:45:41,107][1383994605.py][line:1089][INFO] mean_spacing_error：31.28090，col=7，count=216，rate=3.24074%，jerk=0.18104，miniumu_ttc=122.98051\n\n 60%|██████    | 3/5 [00:34<00:23, 11.63s/it]\u001b[A[2024-07-23 07:45:46,947][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2254872\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 07:45:52,933][1383994605.py][line:1089][INFO] mean_spacing_error：25.31532，col=15，count=216，rate=6.94444%，jerk=0.18158，miniumu_ttc=34.81168\n\n 80%|████████  | 4/5 [00:46<00:11, 11.71s/it]\u001b[A[2024-07-23 07:45:58,846][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2439710\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.20s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.05s/it]\u001b[A\u001b[A\n[2024-07-23 07:46:05,276][1383994605.py][line:1089][INFO] mean_spacing_error：22.53254，col=11，count=216，rate=5.09259%，jerk=0.18393，miniumu_ttc=175.22005\n\n100%|██████████| 5/5 [00:58<00:00, 11.79s/it]\u001b[A\n 17%|█▋        | 83/500 [4:18:46<23:43:59, 204.89s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.27s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.68s/it]\u001b[A\n[2024-07-23 07:48:49,752][1383994605.py][line:1670][INFO] 84/500. loss: 0.0026452181239922843\n[2024-07-23 07:48:49,761][1383994605.py][line:1671][INFO] 84/500. mserror: 52.25074768066406  col: 31  count: 216  jerk: 0.0729314461350441  ttc: 148.9348907470703\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:48:55,129][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2893897\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 07:49:00,706][1383994605.py][line:1089][INFO] mean_spacing_error：70.47387，col=6，count=216，rate=2.77778%，jerk=0.11907，miniumu_ttc=256.51608\n\n 20%|██        | 1/5 [00:10<00:43, 10.94s/it]\u001b[A[2024-07-23 07:49:06,288][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2773790\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:49:11,952][1383994605.py][line:1089][INFO] mean_spacing_error：49.34071，col=25，count=216，rate=11.57407%，jerk=0.10123，miniumu_ttc=36.82507\n\n 40%|████      | 2/5 [00:22<00:33, 11.12s/it]\u001b[A[2024-07-23 07:49:17,415][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2536425\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 07:49:23,207][1383994605.py][line:1089][INFO] mean_spacing_error：47.21133，col=3，count=216，rate=1.38889%，jerk=0.15853，miniumu_ttc=268.77652\n\n 60%|██████    | 3/5 [00:33<00:22, 11.18s/it]\u001b[A[2024-07-23 07:49:29,011][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2241253\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:49:34,724][1383994605.py][line:1089][INFO] mean_spacing_error：38.86184，col=17，count=216，rate=7.87037%，jerk=0.15267，miniumu_ttc=88.54565\n\n 80%|████████  | 4/5 [00:44<00:11, 11.31s/it]\u001b[A[2024-07-23 07:49:40,290][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2515880\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 07:49:46,076][1383994605.py][line:1089][INFO] mean_spacing_error：31.85461，col=27，count=216，rate=12.50000%，jerk=0.12250，miniumu_ttc=166.69247\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 17%|█▋        | 84/500 [4:22:27<24:13:41, 209.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.34s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 07:51:34,189][1383994605.py][line:1670][INFO] 85/500. loss: 0.003040438207487265\n[2024-07-23 07:51:34,203][1383994605.py][line:1671][INFO] 85/500. mserror: 45.59579086303711  col: 30  count: 216  jerk: 0.08014421164989471  ttc: 144.71035766601562\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:51:39,616][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3136724\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:51:45,267][1383994605.py][line:1089][INFO] mean_spacing_error：249.83948，col=4，count=216，rate=1.85185%，jerk=0.13358，miniumu_ttc=526.88867\n\n 20%|██        | 1/5 [00:11<00:44, 11.05s/it]\u001b[A[2024-07-23 07:51:50,742][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2633162\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 07:51:56,235][1383994605.py][line:1089][INFO] mean_spacing_error：44.73600，col=11，count=216，rate=5.09259%，jerk=0.12889，miniumu_ttc=224.10678\n\n 40%|████      | 2/5 [00:22<00:33, 11.00s/it]\u001b[A[2024-07-23 07:52:02,367][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2231008\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 07:52:08,069][1383994605.py][line:1089][INFO] mean_spacing_error：31.12833，col=16，count=216，rate=7.40741%，jerk=0.15189，miniumu_ttc=15.31352\n\n 60%|██████    | 3/5 [00:33<00:22, 11.38s/it]\u001b[A[2024-07-23 07:52:13,418][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2355558\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 07:52:18,966][1383994605.py][line:1089][INFO] mean_spacing_error：27.27839，col=13，count=216，rate=6.01852%，jerk=0.13906，miniumu_ttc=181.80362\n\n 80%|████████  | 4/5 [00:44<00:11, 11.19s/it]\u001b[A[2024-07-23 07:52:24,519][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2108786\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:52:30,263][1383994605.py][line:1089][INFO] mean_spacing_error：37.71509，col=17，count=216，rate=7.87037%，jerk=0.14894，miniumu_ttc=94.15116\n\n100%|██████████| 5/5 [00:56<00:00, 11.21s/it]\u001b[A\n 17%|█▋        | 85/500 [4:25:11<22:35:49, 196.02s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.88s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 07:54:11,612][1383994605.py][line:1670][INFO] 86/500. loss: 0.002104939892888069\n[2024-07-23 07:54:11,621][1383994605.py][line:1671][INFO] 86/500. mserror: 48.3349609375  col: 29  count: 216  jerk: 0.0768355205655098  ttc: 138.77291870117188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:54:16,883][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3238050\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:54:22,619][1383994605.py][line:1089][INFO] mean_spacing_error：68.34583，col=9，count=216，rate=4.16667%，jerk=0.10713，miniumu_ttc=185.87059\n\n 20%|██        | 1/5 [00:10<00:43, 10.99s/it]\u001b[A[2024-07-23 07:54:28,237][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2638666\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 07:54:33,736][1383994605.py][line:1089][INFO] mean_spacing_error：45.04192，col=13，count=216，rate=6.01852%，jerk=0.14562，miniumu_ttc=29.14818\n\n 40%|████      | 2/5 [00:22<00:33, 11.06s/it]\u001b[A[2024-07-23 07:54:39,188][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2470246\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:54:44,743][1383994605.py][line:1089][INFO] mean_spacing_error：41.52643，col=3，count=216，rate=1.38889%，jerk=0.16546，miniumu_ttc=181.70534\n\n 60%|██████    | 3/5 [00:33<00:22, 11.04s/it]\u001b[A[2024-07-23 07:54:50,214][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2170568\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 07:54:55,511][1383994605.py][line:1089][INFO] mean_spacing_error：41.09865，col=28，count=216，rate=12.96296%，jerk=0.12432，miniumu_ttc=87.90240\n\n 80%|████████  | 4/5 [00:43<00:10, 10.93s/it]\u001b[A[2024-07-23 07:55:00,994][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2358599\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 07:55:06,459][1383994605.py][line:1089][INFO] mean_spacing_error：28.74313，col=22，count=216，rate=10.18519%，jerk=0.11811，miniumu_ttc=135.78618\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 17%|█▋        | 86/500 [4:27:47<21:10:04, 184.07s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.67s/it]\u001b[A\n[2024-07-23 07:57:23,732][1383994605.py][line:1670][INFO] 87/500. loss: 0.0020280970881382623\n[2024-07-23 07:57:23,742][1383994605.py][line:1671][INFO] 87/500. mserror: 60.14536666870117  col: 31  count: 216  jerk: 0.06622281670570374  ttc: 154.4615478515625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 07:57:29,139][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3340749\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 07:57:34,786][1383994605.py][line:1089][INFO] mean_spacing_error：52.60162，col=13，count=216，rate=6.01852%，jerk=0.12209，miniumu_ttc=170.40016\n\n 20%|██        | 1/5 [00:11<00:44, 11.04s/it]\u001b[A[2024-07-23 07:57:40,511][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2668830\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:57:46,241][1383994605.py][line:1089][INFO] mean_spacing_error：56.77420，col=30，count=216，rate=13.88889%，jerk=0.12181，miniumu_ttc=102.36060\n\n 40%|████      | 2/5 [00:22<00:33, 11.29s/it]\u001b[A[2024-07-23 07:57:52,151][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2590758\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 07:57:57,713][1383994605.py][line:1089][INFO] mean_spacing_error：28.18352，col=14，count=216，rate=6.48148%，jerk=0.11692，miniumu_ttc=104.32834\n\n 60%|██████    | 3/5 [00:33<00:22, 11.37s/it]\u001b[A[2024-07-23 07:58:03,624][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2327833\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 07:58:09,389][1383994605.py][line:1089][INFO] mean_spacing_error：30.09846，col=11，count=216，rate=5.09259%，jerk=0.14050，miniumu_ttc=141.00775\n\n 80%|████████  | 4/5 [00:45<00:11, 11.49s/it]\u001b[A[2024-07-23 07:58:14,974][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2083827\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 07:58:20,522][1383994605.py][line:1089][INFO] mean_spacing_error：36.32784，col=9，count=216，rate=4.16667%，jerk=0.17512，miniumu_ttc=129.86597\n\n100%|██████████| 5/5 [00:56<00:00, 11.36s/it]\u001b[A\n 17%|█▋        | 87/500 [4:31:01<21:27:37, 187.06s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 08:00:00,102][1383994605.py][line:1670][INFO] 88/500. loss: 0.0017927279695868492\n[2024-07-23 08:00:00,123][1383994605.py][line:1671][INFO] 88/500. mserror: 73.70366668701172  col: 37  count: 216  jerk: 0.05264889448881149  ttc: 153.1361846923828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:00:05,898][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3298403\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:00:11,483][1383994605.py][line:1089][INFO] mean_spacing_error：72.68690，col=30，count=216，rate=13.88889%，jerk=0.06654，miniumu_ttc=220.63405\n\n 20%|██        | 1/5 [00:11<00:45, 11.35s/it]\u001b[A[2024-07-23 08:00:17,145][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2994485\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 08:00:22,827][1383994605.py][line:1089][INFO] mean_spacing_error：55.31529，col=48，count=216，rate=22.22222%，jerk=0.10299，miniumu_ttc=26.38626\n\n 40%|████      | 2/5 [00:22<00:34, 11.35s/it]\u001b[A[2024-07-23 08:00:28,921][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2756599\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 08:00:35,023][1383994605.py][line:1089][INFO] mean_spacing_error：39.09905，col=6，count=216，rate=2.77778%，jerk=0.16883，miniumu_ttc=95.08727\n\n 60%|██████    | 3/5 [00:34<00:23, 11.73s/it]\u001b[A[2024-07-23 08:00:40,731][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2346512\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:00:46,286][1383994605.py][line:1089][INFO] mean_spacing_error：35.73216，col=7，count=216，rate=3.24074%，jerk=0.13553，miniumu_ttc=163.29926\n\n 80%|████████  | 4/5 [00:46<00:11, 11.55s/it]\u001b[A[2024-07-23 08:00:52,147][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2122156\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 08:00:57,879][1383994605.py][line:1089][INFO] mean_spacing_error：29.59538，col=20，count=216，rate=9.25926%，jerk=0.13363，miniumu_ttc=87.05630\n\n100%|██████████| 5/5 [00:57<00:00, 11.55s/it]\u001b[A\n 18%|█▊        | 88/500 [4:33:38<20:23:18, 178.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 08:03:23,473][1383994605.py][line:1670][INFO] 89/500. loss: 0.0031278105452656746\n[2024-07-23 08:03:23,485][1383994605.py][line:1671][INFO] 89/500. mserror: 79.26919555664062  col: 43  count: 216  jerk: 0.04670910909771919  ttc: 148.9394989013672\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:03:29,087][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3207773\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.12s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.99s/it]\u001b[A\u001b[A\n[2024-07-23 08:03:35,325][1383994605.py][line:1089][INFO] mean_spacing_error：164.82687，col=1，count=216，rate=0.46296%，jerk=0.13258，miniumu_ttc=255.92950\n\n 20%|██        | 1/5 [00:11<00:47, 11.84s/it]\u001b[A[2024-07-23 08:03:41,741][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2858957\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 08:03:47,694][1383994605.py][line:1089][INFO] mean_spacing_error：67.43687，col=50，count=216，rate=23.14815%，jerk=0.13338，miniumu_ttc=85.69408\n\n 40%|████      | 2/5 [00:24<00:36, 12.15s/it]\u001b[A[2024-07-23 08:03:53,436][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3154477\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:03:59,059][1383994605.py][line:1089][INFO] mean_spacing_error：42.52578，col=24，count=216，rate=11.11111%，jerk=0.09965，miniumu_ttc=92.11031\n\n 60%|██████    | 3/5 [00:35<00:23, 11.79s/it]\u001b[A[2024-07-23 08:04:04,838][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2633727\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 08:04:10,715][1383994605.py][line:1089][INFO] mean_spacing_error：46.27832，col=5，count=216，rate=2.31481%，jerk=0.13721，miniumu_ttc=218.14958\n\n 80%|████████  | 4/5 [00:47<00:11, 11.74s/it]\u001b[A[2024-07-23 08:04:16,752][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2289865\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 08:04:22,448][1383994605.py][line:1089][INFO] mean_spacing_error：44.25815，col=20，count=216，rate=9.25926%，jerk=0.15783，miniumu_ttc=39.58293\n\n100%|██████████| 5/5 [00:58<00:00, 11.80s/it]\u001b[A\n 18%|█▊        | 89/500 [4:37:03<21:14:41, 186.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.26s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 08:06:58,187][1383994605.py][line:1670][INFO] 90/500. loss: 0.0030546076595783234\n[2024-07-23 08:06:58,203][1383994605.py][line:1671][INFO] 90/500. mserror: 79.01680755615234  col: 49  count: 216  jerk: 0.04370763525366783  ttc: 125.29646301269531\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:07:04,354][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3121379\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 08:07:10,240][1383994605.py][line:1089][INFO] mean_spacing_error：301.37024，col=4，count=216，rate=1.85185%，jerk=0.12168，miniumu_ttc=276.37881\n\n 20%|██        | 1/5 [00:12<00:48, 12.03s/it]\u001b[A[2024-07-23 08:07:16,060][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2453591\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:07:21,640][1383994605.py][line:1089][INFO] mean_spacing_error：38.38762，col=19，count=216，rate=8.79630%，jerk=0.14313，miniumu_ttc=86.31689\n\n 40%|████      | 2/5 [00:23<00:34, 11.66s/it]\u001b[A[2024-07-23 08:07:27,509][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2383466\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:07:33,035][1383994605.py][line:1089][INFO] mean_spacing_error：28.17141，col=4，count=216，rate=1.85185%，jerk=0.17636，miniumu_ttc=182.27818\n\n 60%|██████    | 3/5 [00:34<00:23, 11.54s/it]\u001b[A[2024-07-23 08:07:39,438][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2232100\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 08:07:45,285][1383994605.py][line:1089][INFO] mean_spacing_error：33.52183，col=20，count=216，rate=9.25926%，jerk=0.14067，miniumu_ttc=89.93964\n\n 80%|████████  | 4/5 [00:47<00:11, 11.82s/it]\u001b[A[2024-07-23 08:07:51,232][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2303637\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.12s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 08:07:57,435][1383994605.py][line:1089][INFO] mean_spacing_error：28.93381，col=24，count=216，rate=11.11111%，jerk=0.10772，miniumu_ttc=126.23955\n\n100%|██████████| 5/5 [00:59<00:00, 11.85s/it]\u001b[A\n 18%|█▊        | 90/500 [4:40:38<22:10:48, 194.75s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 08:10:34,578][1383994605.py][line:1670][INFO] 91/500. loss: 0.001579419244080782\n[2024-07-23 08:10:34,590][1383994605.py][line:1671][INFO] 91/500. mserror: 80.08411407470703  col: 53  count: 216  jerk: 0.042274411767721176  ttc: 129.75587463378906\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:10:39,859][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3148347\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:10:45,354][1383994605.py][line:1089][INFO] mean_spacing_error：193.44130，col=5，count=216，rate=2.31481%，jerk=0.11649，miniumu_ttc=235.12941\n\n 20%|██        | 1/5 [00:10<00:43, 10.76s/it]\u001b[A[2024-07-23 08:10:51,277][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2400048\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 08:10:57,226][1383994605.py][line:1089][INFO] mean_spacing_error：28.22680，col=14，count=216，rate=6.48148%，jerk=0.16923，miniumu_ttc=127.50732\n\n 40%|████      | 2/5 [00:22<00:34, 11.41s/it]\u001b[A[2024-07-23 08:11:03,455][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2162433\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.85s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.19s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.02s/it]\u001b[A\u001b[A\n[2024-07-23 08:11:09,818][1383994605.py][line:1089][INFO] mean_spacing_error：59.49268，col=33，count=216，rate=15.27778%，jerk=0.15566，miniumu_ttc=41.27628\n\n 60%|██████    | 3/5 [00:35<00:23, 11.95s/it]\u001b[A[2024-07-23 08:11:15,666][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2657861\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.79s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.22s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.07s/it]\u001b[A\u001b[A\n[2024-07-23 08:11:22,167][1383994605.py][line:1089][INFO] mean_spacing_error：31.14354，col=25，count=216，rate=11.57407%，jerk=0.11577，miniumu_ttc=115.32706\n\n 80%|████████  | 4/5 [00:47<00:12, 12.11s/it]\u001b[A[2024-07-23 08:11:27,961][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2198671\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 08:11:33,929][1383994605.py][line:1089][INFO] mean_spacing_error：26.71001，col=23，count=216，rate=10.64815%，jerk=0.11890，miniumu_ttc=38.79133\n\n100%|██████████| 5/5 [00:59<00:00, 11.87s/it]\u001b[A\n 18%|█▊        | 91/500 [4:44:14<22:52:01, 201.27s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.36s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.81s/it]\u001b[A\n[2024-07-23 08:14:12,373][1383994605.py][line:1670][INFO] 92/500. loss: 0.0016391496174037457\n[2024-07-23 08:14:12,387][1383994605.py][line:1671][INFO] 92/500. mserror: 83.1702651977539  col: 48  count: 216  jerk: 0.042103033512830734  ttc: 177.39410400390625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:14:18,626][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3124479\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 08:14:24,560][1383994605.py][line:1089][INFO] mean_spacing_error：70.48775，col=9，count=216，rate=4.16667%，jerk=0.09304，miniumu_ttc=218.92654\n\n 20%|██        | 1/5 [00:12<00:48, 12.17s/it]\u001b[A[2024-07-23 08:14:30,177][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2549113\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 08:14:35,913][1383994605.py][line:1089][INFO] mean_spacing_error：42.64991，col=13，count=216，rate=6.01852%，jerk=0.16790，miniumu_ttc=97.27678\n\n 40%|████      | 2/5 [00:23<00:35, 11.69s/it]\u001b[A[2024-07-23 08:14:41,502][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2114601\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 08:14:47,209][1383994605.py][line:1089][INFO] mean_spacing_error：54.93574，col=20，count=216，rate=9.25926%，jerk=0.17575，miniumu_ttc=10.20804\n\n 60%|██████    | 3/5 [00:34<00:23, 11.51s/it]\u001b[A[2024-07-23 08:14:53,653][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2211267\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 08:14:59,752][1383994605.py][line:1089][INFO] mean_spacing_error：25.55746，col=15，count=216，rate=6.94444%，jerk=0.15429，miniumu_ttc=143.57695\n\n 80%|████████  | 4/5 [00:47<00:11, 11.92s/it]\u001b[A[2024-07-23 08:15:05,959][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2133893\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 08:15:11,798][1383994605.py][line:1089][INFO] mean_spacing_error：26.72113，col=16，count=216，rate=7.40741%，jerk=0.16527，miniumu_ttc=101.74720\n\n100%|██████████| 5/5 [00:59<00:00, 11.88s/it]\u001b[A\n 18%|█▊        | 92/500 [4:47:52<23:22:31, 206.25s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.48s/it]\u001b[A\n2it [00:04,  1.94s/it]\u001b[A\n3it [00:05,  1.83s/it]\u001b[A\n[2024-07-23 08:17:03,139][1383994605.py][line:1670][INFO] 93/500. loss: 0.0036667355646689734\n[2024-07-23 08:17:03,149][1383994605.py][line:1671][INFO] 93/500. mserror: 83.38963317871094  col: 46  count: 216  jerk: 0.043889112770557404  ttc: 264.0137634277344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:17:08,309][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3041330\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:17:13,798][1383994605.py][line:1089][INFO] mean_spacing_error：105.32853，col=8，count=216，rate=3.70370%，jerk=0.10741，miniumu_ttc=193.67555\n\n 20%|██        | 1/5 [00:10<00:42, 10.64s/it]\u001b[A[2024-07-23 08:17:19,178][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2463152\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:17:24,715][1383994605.py][line:1089][INFO] mean_spacing_error：28.51352，col=12，count=216，rate=5.55556%，jerk=0.16345，miniumu_ttc=95.56973\n\n 40%|████      | 2/5 [00:21<00:32, 10.80s/it]\u001b[A[2024-07-23 08:17:30,159][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2151385\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 08:17:35,972][1383994605.py][line:1089][INFO] mean_spacing_error：26.46862，col=15，count=216，rate=6.94444%，jerk=0.15324，miniumu_ttc=133.75546\n\n 60%|██████    | 3/5 [00:32<00:22, 11.01s/it]\u001b[A[2024-07-23 08:17:41,297][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2093380\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:17:46,882][1383994605.py][line:1089][INFO] mean_spacing_error：33.34811，col=15，count=216，rate=6.94444%，jerk=0.17779，miniumu_ttc=130.99380\n\n 80%|████████  | 4/5 [00:43<00:10, 10.97s/it]\u001b[A[2024-07-23 08:17:52,391][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2207113\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:17:57,993][1383994605.py][line:1089][INFO] mean_spacing_error：28.83278，col=16，count=216，rate=7.40741%，jerk=0.16181，miniumu_ttc=89.91278\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 19%|█▊        | 93/500 [4:50:38<21:57:34, 194.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 08:20:21,584][1383994605.py][line:1670][INFO] 94/500. loss: 0.0032389983534812927\n[2024-07-23 08:20:21,590][1383994605.py][line:1671][INFO] 94/500. mserror: 78.01893615722656  col: 40  count: 216  jerk: 0.04901312291622162  ttc: 151.82986450195312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:20:26,945][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3180747\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:20:32,457][1383994605.py][line:1089][INFO] mean_spacing_error：89.15467，col=10，count=216，rate=4.62963%，jerk=0.08196，miniumu_ttc=227.69971\n\n 20%|██        | 1/5 [00:10<00:43, 10.85s/it]\u001b[A[2024-07-23 08:20:38,104][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2535253\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 08:20:43,745][1383994605.py][line:1089][INFO] mean_spacing_error：32.84380，col=12，count=216，rate=5.55556%，jerk=0.16042，miniumu_ttc=93.30547\n\n 40%|████      | 2/5 [00:22<00:33, 11.11s/it]\u001b[A[2024-07-23 08:20:49,315][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2073934\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:20:54,914][1383994605.py][line:1089][INFO] mean_spacing_error：27.80124，col=16，count=216，rate=7.40741%，jerk=0.15257，miniumu_ttc=91.68678\n\n 60%|██████    | 3/5 [00:33<00:22, 11.14s/it]\u001b[A[2024-07-23 08:21:00,576][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2162391\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 08:21:06,252][1383994605.py][line:1089][INFO] mean_spacing_error：24.67627，col=13，count=216，rate=6.01852%，jerk=0.17788，miniumu_ttc=126.92303\n\n 80%|████████  | 4/5 [00:44<00:11, 11.22s/it]\u001b[A[2024-07-23 08:21:11,928][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2267112\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 08:21:17,759][1383994605.py][line:1089][INFO] mean_spacing_error：25.53628，col=15，count=216，rate=6.94444%，jerk=0.15971，miniumu_ttc=106.11270\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n 19%|█▉        | 94/500 [4:53:58<22:05:33, 195.90s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.35s/it]\u001b[A\n2it [00:03,  1.89s/it]\u001b[A\n3it [00:05,  1.77s/it]\u001b[A\n[2024-07-23 08:23:47,215][1383994605.py][line:1670][INFO] 95/500. loss: 0.0032441969960927963\n[2024-07-23 08:23:47,227][1383994605.py][line:1671][INFO] 95/500. mserror: 68.32378387451172  col: 39  count: 216  jerk: 0.055102381855249405  ttc: 155.93728637695312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:23:52,577][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3397404\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 08:23:58,615][1383994605.py][line:1089][INFO] mean_spacing_error：116.16051，col=8，count=216，rate=3.70370%，jerk=0.07953，miniumu_ttc=285.50778\n\n 20%|██        | 1/5 [00:11<00:45, 11.38s/it]\u001b[A[2024-07-23 08:24:04,148][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2605777\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 08:24:09,569][1383994605.py][line:1089][INFO] mean_spacing_error：43.60486，col=5，count=216，rate=2.31481%，jerk=0.16712，miniumu_ttc=156.36737\n\n 40%|████      | 2/5 [00:22<00:33, 11.13s/it]\u001b[A[2024-07-23 08:24:14,982][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2227457\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 08:24:20,370][1383994605.py][line:1089][INFO] mean_spacing_error：24.59429，col=14，count=216，rate=6.48148%，jerk=0.13643，miniumu_ttc=138.65529\n\n 60%|██████    | 3/5 [00:33<00:21, 10.98s/it]\u001b[A[2024-07-23 08:24:26,015][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2052961\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 08:24:31,430][1383994605.py][line:1089][INFO] mean_spacing_error：30.38703，col=16，count=216，rate=7.40741%，jerk=0.16175，miniumu_ttc=137.61958\n\n 80%|████████  | 4/5 [00:44<00:11, 11.01s/it]\u001b[A[2024-07-23 08:24:37,022][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2187781\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:24:42,604][1383994605.py][line:1089][INFO] mean_spacing_error：28.25844，col=8，count=216，rate=3.70370%，jerk=0.16964，miniumu_ttc=191.36163\n\n100%|██████████| 5/5 [00:55<00:00, 11.08s/it]\u001b[A\n 19%|█▉        | 95/500 [4:57:23<22:20:24, 198.58s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.36s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 08:27:07,277][1383994605.py][line:1670][INFO] 96/500. loss: 0.0015155758398274581\n[2024-07-23 08:27:07,290][1383994605.py][line:1671][INFO] 96/500. mserror: 59.828643798828125  col: 41  count: 216  jerk: 0.06016532704234123  ttc: 102.68683624267578\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:27:12,417][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3299323\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 08:27:17,736][1383994605.py][line:1089][INFO] mean_spacing_error：126.96675，col=7，count=216，rate=3.24074%，jerk=0.09822，miniumu_ttc=256.66156\n\n 20%|██        | 1/5 [00:10<00:41, 10.44s/it]\u001b[A[2024-07-23 08:27:23,742][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2631431\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 08:27:29,484][1383994605.py][line:1089][INFO] mean_spacing_error：46.44838，col=9，count=216，rate=4.16667%，jerk=0.17166，miniumu_ttc=92.92804\n\n 40%|████      | 2/5 [00:22<00:33, 11.21s/it]\u001b[A[2024-07-23 08:27:35,013][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2331986\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:27:40,561][1383994605.py][line:1089][INFO] mean_spacing_error：33.36358，col=4，count=216，rate=1.85185%，jerk=0.16699，miniumu_ttc=153.04622\n\n 60%|██████    | 3/5 [00:33<00:22, 11.15s/it]\u001b[A[2024-07-23 08:27:46,056][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2112048\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:27:51,640][1383994605.py][line:1089][INFO] mean_spacing_error：26.29327，col=19，count=216，rate=8.79630%，jerk=0.12697，miniumu_ttc=97.62529\n\n 80%|████████  | 4/5 [00:44<00:11, 11.12s/it]\u001b[A[2024-07-23 08:27:57,163][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2165566\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 08:28:02,879][1383994605.py][line:1089][INFO] mean_spacing_error：24.97580，col=9，count=216，rate=4.16667%，jerk=0.14565，miniumu_ttc=221.48508\n\n100%|██████████| 5/5 [00:55<00:00, 11.12s/it]\u001b[A\n 19%|█▉        | 96/500 [5:00:43<22:20:32, 199.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 08:30:33,248][1383994605.py][line:1670][INFO] 97/500. loss: 0.0030753944690028825\n[2024-07-23 08:30:33,259][1383994605.py][line:1671][INFO] 97/500. mserror: 58.70027542114258  col: 30  count: 216  jerk: 0.06759043782949448  ttc: 20197.22265625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:30:38,467][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3456991\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 08:30:43,886][1383994605.py][line:1089][INFO] mean_spacing_error：99.11034，col=8，count=216，rate=3.70370%，jerk=0.09626，miniumu_ttc=279.28723\n\n 20%|██        | 1/5 [00:10<00:42, 10.62s/it]\u001b[A[2024-07-23 08:30:49,598][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2619967\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:30:55,122][1383994605.py][line:1089][INFO] mean_spacing_error：46.98333，col=25，count=216，rate=11.57407%，jerk=0.12871，miniumu_ttc=12.52853\n\n 40%|████      | 2/5 [00:21<00:32, 10.99s/it]\u001b[A[2024-07-23 08:31:00,564][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2504235\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:31:06,137][1383994605.py][line:1089][INFO] mean_spacing_error：32.10417，col=12，count=216，rate=5.55556%，jerk=0.13422，miniumu_ttc=182.77946\n\n 60%|██████    | 3/5 [00:32<00:21, 10.99s/it]\u001b[A[2024-07-23 08:31:11,591][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2157382\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:31:17,096][1383994605.py][line:1089][INFO] mean_spacing_error：28.59517，col=12，count=216，rate=5.55556%，jerk=0.15458，miniumu_ttc=132.96063\n\n 80%|████████  | 4/5 [00:43<00:10, 10.98s/it]\u001b[A[2024-07-23 08:31:22,888][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2091010\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:31:28,431][1383994605.py][line:1089][INFO] mean_spacing_error：27.66941，col=6，count=216，rate=2.77778%，jerk=0.17957，miniumu_ttc=185.94318\n\n100%|██████████| 5/5 [00:55<00:00, 11.04s/it]\u001b[A\n 19%|█▉        | 97/500 [5:04:09<22:30:15, 201.03s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.72s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 08:33:55,043][1383994605.py][line:1670][INFO] 98/500. loss: 0.001413512509316206\n[2024-07-23 08:33:55,052][1383994605.py][line:1671][INFO] 98/500. mserror: 50.98775863647461  col: 31  count: 216  jerk: 0.07248692214488983  ttc: 144.52346801757812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:34:00,765][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3549117\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 08:34:06,414][1383994605.py][line:1089][INFO] mean_spacing_error：89.35258，col=6，count=216，rate=2.77778%，jerk=0.10441，miniumu_ttc=262.96027\n\n 20%|██        | 1/5 [00:11<00:45, 11.34s/it]\u001b[A[2024-07-23 08:34:12,197][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2804695\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:34:17,750][1383994605.py][line:1089][INFO] mean_spacing_error：72.13992，col=44，count=216，rate=20.37037%，jerk=0.16449，miniumu_ttc=7.91486\n\n 40%|████      | 2/5 [00:22<00:34, 11.34s/it]\u001b[A[2024-07-23 08:34:23,459][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3001215\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:34:29,079][1383994605.py][line:1089][INFO] mean_spacing_error：47.71025，col=21，count=216，rate=9.72222%，jerk=0.12900，miniumu_ttc=130.46365\n\n 60%|██████    | 3/5 [00:34<00:22, 11.34s/it]\u001b[A[2024-07-23 08:34:34,851][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2648618\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:34:40,366][1383994605.py][line:1089][INFO] mean_spacing_error：51.51945，col=10，count=216，rate=4.62963%，jerk=0.09838，miniumu_ttc=191.90631\n\n 80%|████████  | 4/5 [00:45<00:11, 11.32s/it]\u001b[A[2024-07-23 08:34:46,166][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2281054\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:34:51,711][1383994605.py][line:1089][INFO] mean_spacing_error：33.05785，col=16，count=216，rate=7.40741%，jerk=0.13405，miniumu_ttc=99.51073\n\n100%|██████████| 5/5 [00:56<00:00, 11.33s/it]\u001b[A\n 20%|█▉        | 98/500 [5:07:32<22:31:23, 201.70s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.03s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 08:37:20,645][1383994605.py][line:1670][INFO] 99/500. loss: 0.0029767745484908423\n[2024-07-23 08:37:20,654][1383994605.py][line:1671][INFO] 99/500. mserror: 43.63984680175781  col: 35  count: 216  jerk: 0.07465871423482895  ttc: 96.4999008178711\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:37:25,815][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3548629\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 08:37:31,095][1383994605.py][line:1089][INFO] mean_spacing_error：78.80511，col=7，count=216，rate=3.24074%，jerk=0.10007，miniumu_ttc=212.21304\n\n 20%|██        | 1/5 [00:10<00:41, 10.43s/it]\u001b[A[2024-07-23 08:37:36,495][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2629143\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 08:37:42,243][1383994605.py][line:1089][INFO] mean_spacing_error：43.12909，col=20，count=216，rate=9.25926%，jerk=0.13599，miniumu_ttc=12.40256\n\n 40%|████      | 2/5 [00:21<00:32, 10.85s/it]\u001b[A[2024-07-23 08:37:47,747][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2404548\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 08:37:53,069][1383994605.py][line:1089][INFO] mean_spacing_error：35.74234，col=3，count=216，rate=1.38889%，jerk=0.16291，miniumu_ttc=181.75479\n\n 60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\u001b[A[2024-07-23 08:37:58,561][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2205230\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:38:04,093][1383994605.py][line:1089][INFO] mean_spacing_error：32.06710，col=17，count=216，rate=7.87037%，jerk=0.14309，miniumu_ttc=103.45075\n\n 80%|████████  | 4/5 [00:43<00:10, 10.91s/it]\u001b[A[2024-07-23 08:38:09,515][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2167323\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:38:15,119][1383994605.py][line:1089][INFO] mean_spacing_error：27.68608，col=15，count=216，rate=6.94444%，jerk=0.16515，miniumu_ttc=59.96795\n\n100%|██████████| 5/5 [00:54<00:00, 10.89s/it]\u001b[A\n 20%|█▉        | 99/500 [5:10:56<22:31:27, 202.21s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.03s/it]\u001b[A\n2it [00:03,  1.95s/it]\u001b[A\n3it [00:05,  1.77s/it]\u001b[A\n[2024-07-23 08:39:58,783][1383994605.py][line:1670][INFO] 100/500. loss: 0.001847446585694949\n[2024-07-23 08:39:58,793][1383994605.py][line:1671][INFO] 100/500. mserror: 41.60456466674805  col: 43  count: 216  jerk: 0.0739302858710289  ttc: 90.17999267578125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:40:04,011][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3401196\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 08:40:09,405][1383994605.py][line:1089][INFO] mean_spacing_error：113.46476，col=4，count=216，rate=1.85185%，jerk=0.10392，miniumu_ttc=231.66389\n\n 20%|██        | 1/5 [00:10<00:42, 10.60s/it]\u001b[A[2024-07-23 08:40:14,749][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2662935\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 08:40:20,406][1383994605.py][line:1089][INFO] mean_spacing_error：57.36394，col=30，count=216，rate=13.88889%，jerk=0.10647，miniumu_ttc=86.97102\n\n 40%|████      | 2/5 [00:21<00:32, 10.84s/it]\u001b[A[2024-07-23 08:40:25,706][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2588234\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:40:31,249][1383994605.py][line:1089][INFO] mean_spacing_error：40.80354，col=2，count=216，rate=0.92593%，jerk=0.16707，miniumu_ttc=205.56482\n\n 60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\u001b[A[2024-07-23 08:40:36,542][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2283829\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 08:40:42,241][1383994605.py][line:1089][INFO] mean_spacing_error：43.70744，col=19，count=216，rate=8.79630%，jerk=0.18214，miniumu_ttc=31.33280\n\n 80%|████████  | 4/5 [00:43<00:10, 10.90s/it]\u001b[A[2024-07-23 08:40:47,634][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2622370\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 08:40:53,377][1383994605.py][line:1089][INFO] mean_spacing_error：31.33639，col=25，count=216，rate=11.57407%，jerk=0.12925，miniumu_ttc=155.39200\n\n100%|██████████| 5/5 [00:54<00:00, 10.92s/it]\u001b[A\n 20%|██        | 100/500 [5:13:34<21:00:09, 189.02s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 08:43:12,727][1383994605.py][line:1670][INFO] 101/500. loss: 0.0029905013119181\n[2024-07-23 08:43:12,735][1383994605.py][line:1671][INFO] 101/500. mserror: 42.100040435791016  col: 42  count: 216  jerk: 0.07139333337545395  ttc: 93.15026092529297\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:43:17,814][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3308844\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 08:43:23,152][1383994605.py][line:1089][INFO] mean_spacing_error：126.63119，col=4，count=216，rate=1.85185%，jerk=0.12006，miniumu_ttc=454.09833\n\n 20%|██        | 1/5 [00:10<00:41, 10.41s/it]\u001b[A[2024-07-23 08:43:28,747][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2469338\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.65s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 08:43:34,525][1383994605.py][line:1089][INFO] mean_spacing_error：43.24792，col=15，count=216，rate=6.94444%，jerk=0.11715，miniumu_ttc=210.99225\n\n 40%|████      | 2/5 [00:21<00:32, 10.98s/it]\u001b[A[2024-07-23 08:43:40,018][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2187537\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 08:43:45,404][1383994605.py][line:1089][INFO] mean_spacing_error：24.05837，col=14，count=216，rate=6.48148%，jerk=0.13323，miniumu_ttc=114.34508\n\n 60%|██████    | 3/5 [00:32<00:21, 10.93s/it]\u001b[A[2024-07-23 08:43:50,988][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2027180\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 08:43:56,395][1383994605.py][line:1089][INFO] mean_spacing_error：31.71096，col=15，count=216，rate=6.94444%，jerk=0.17502，miniumu_ttc=131.17198\n\n 80%|████████  | 4/5 [00:43<00:10, 10.96s/it]\u001b[A[2024-07-23 08:44:01,864][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2066380\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:44:07,497][1383994605.py][line:1089][INFO] mean_spacing_error：24.52223，col=16，count=216，rate=7.40741%，jerk=0.17703，miniumu_ttc=131.15742\n\n100%|██████████| 5/5 [00:54<00:00, 10.95s/it]\u001b[A\n 20%|██        | 101/500 [5:16:48<21:07:12, 190.56s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.72s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 08:45:48,177][1383994605.py][line:1670][INFO] 102/500. loss: 0.0033290584882100425\n[2024-07-23 08:45:48,188][1383994605.py][line:1671][INFO] 102/500. mserror: 46.80488967895508  col: 37  count: 216  jerk: 0.06790591031312943  ttc: 115.50999450683594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:45:53,426][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3371875\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:45:58,959][1383994605.py][line:1089][INFO] mean_spacing_error：62.19832，col=10，count=216，rate=4.62963%，jerk=0.09369，miniumu_ttc=204.03688\n\n 20%|██        | 1/5 [00:10<00:43, 10.76s/it]\u001b[A[2024-07-23 08:46:04,295][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2362901\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:46:09,864][1383994605.py][line:1089][INFO] mean_spacing_error：40.93365，col=13，count=216，rate=6.01852%，jerk=0.15929，miniumu_ttc=133.13951\n\n 40%|████      | 2/5 [00:21<00:32, 10.85s/it]\u001b[A[2024-07-23 08:46:15,295][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2206507\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:46:20,913][1383994605.py][line:1089][INFO] mean_spacing_error：24.38543，col=13，count=216，rate=6.01852%，jerk=0.14310，miniumu_ttc=131.66794\n\n 60%|██████    | 3/5 [00:32<00:21, 10.94s/it]\u001b[A[2024-07-23 08:46:26,505][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2068936\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 08:46:31,960][1383994605.py][line:1089][INFO] mean_spacing_error：25.30531，col=19，count=216，rate=8.79630%，jerk=0.12825，miniumu_ttc=154.64415\n\n 80%|████████  | 4/5 [00:43<00:10, 10.98s/it]\u001b[A[2024-07-23 08:46:37,375][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1996124\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 08:46:43,090][1383994605.py][line:1089][INFO] mean_spacing_error：24.05104，col=12，count=216，rate=5.55556%，jerk=0.15570，miniumu_ttc=176.64403\n\n100%|██████████| 5/5 [00:54<00:00, 10.98s/it]\u001b[A\n 20%|██        | 102/500 [5:19:24<19:54:26, 180.07s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 08:49:03,587][1383994605.py][line:1670][INFO] 103/500. loss: 0.0029122444490591684\n[2024-07-23 08:49:03,600][1383994605.py][line:1671][INFO] 103/500. mserror: 50.78078842163086  col: 38  count: 216  jerk: 0.0649818554520607  ttc: 135.52780151367188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:49:08,684][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3323475\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 08:49:13,987][1383994605.py][line:1089][INFO] mean_spacing_error：120.94093，col=8，count=216，rate=3.70370%，jerk=0.09200，miniumu_ttc=274.62646\n\n 20%|██        | 1/5 [00:10<00:41, 10.39s/it]\u001b[A[2024-07-23 08:49:19,484][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2889454\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 08:49:25,253][1383994605.py][line:1089][INFO] mean_spacing_error：58.67638，col=29，count=216，rate=13.42593%，jerk=0.12434，miniumu_ttc=81.55473\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 08:49:30,696][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2482505\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 08:49:36,105][1383994605.py][line:1089][INFO] mean_spacing_error：44.04824，col=14，count=216，rate=6.48148%，jerk=0.15647，miniumu_ttc=163.42133\n\n 60%|██████    | 3/5 [00:32<00:21, 10.88s/it]\u001b[A[2024-07-23 08:49:41,570][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2155988\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:49:47,067][1383994605.py][line:1089][INFO] mean_spacing_error：27.58099，col=7，count=216，rate=3.24074%，jerk=0.14331，miniumu_ttc=191.50826\n\n 80%|████████  | 4/5 [00:43<00:10, 10.91s/it]\u001b[A[2024-07-23 08:49:52,587][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2040980\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:49:58,240][1383994605.py][line:1089][INFO] mean_spacing_error：26.33439，col=13，count=216，rate=6.01852%，jerk=0.15725，miniumu_ttc=159.88097\n\n100%|██████████| 5/5 [00:54<00:00, 10.93s/it]\u001b[A\n 21%|██        | 103/500 [5:22:39<20:21:21, 184.59s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 08:52:21,415][1383994605.py][line:1670][INFO] 104/500. loss: 0.0016066112245122592\n[2024-07-23 08:52:21,427][1383994605.py][line:1671][INFO] 104/500. mserror: 47.1107177734375  col: 45  count: 216  jerk: 0.062307022511959076  ttc: 97.31914520263672\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:52:26,757][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3234052\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:52:32,378][1383994605.py][line:1089][INFO] mean_spacing_error：106.42604，col=8，count=216，rate=3.70370%，jerk=0.09520，miniumu_ttc=244.77968\n\n 20%|██        | 1/5 [00:10<00:43, 10.95s/it]\u001b[A[2024-07-23 08:52:38,061][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2655113\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 08:52:43,796][1383994605.py][line:1089][INFO] mean_spacing_error：61.99306，col=75，count=216，rate=34.72222%，jerk=0.10087，miniumu_ttc=10.00332\n\n 40%|████      | 2/5 [00:22<00:33, 11.22s/it]\u001b[A[2024-07-23 08:52:49,489][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2473870\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 08:52:54,918][1383994605.py][line:1089][INFO] mean_spacing_error：35.70774，col=7，count=216，rate=3.24074%，jerk=0.13989，miniumu_ttc=186.76123\n\n 60%|██████    | 3/5 [00:33<00:22, 11.18s/it]\u001b[A[2024-07-23 08:53:00,583][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2153390\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 08:53:06,426][1383994605.py][line:1089][INFO] mean_spacing_error：32.56584，col=14，count=216，rate=6.48148%，jerk=0.18337，miniumu_ttc=57.65126\n\n 80%|████████  | 4/5 [00:45<00:11, 11.31s/it]\u001b[A[2024-07-23 08:53:12,005][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2457681\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 08:53:17,576][1383994605.py][line:1089][INFO] mean_spacing_error：21.79842，col=11，count=216，rate=5.09259%，jerk=0.15605，miniumu_ttc=158.69948\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n 21%|██        | 104/500 [5:25:58<20:47:29, 189.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 08:55:04,842][1383994605.py][line:1670][INFO] 105/500. loss: 0.002945464104413986\n[2024-07-23 08:55:04,857][1383994605.py][line:1671][INFO] 105/500. mserror: 46.727081298828125  col: 57  count: 216  jerk: 0.06082288548350334  ttc: 88.61495208740234\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:55:10,081][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2772467\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 08:55:15,737][1383994605.py][line:1089][INFO] mean_spacing_error：52.86812，col=33，count=216，rate=15.27778%，jerk=0.07321，miniumu_ttc=147.66780\n\n 20%|██        | 1/5 [00:10<00:43, 10.88s/it]\u001b[A[2024-07-23 08:55:21,118][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2619346\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 08:55:26,540][1383994605.py][line:1089][INFO] mean_spacing_error：55.05799，col=33，count=216，rate=15.27778%，jerk=0.10713，miniumu_ttc=14.14781\n\n 40%|████      | 2/5 [00:21<00:32, 10.83s/it]\u001b[A[2024-07-23 08:55:32,396][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2222338\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 08:55:37,922][1383994605.py][line:1089][INFO] mean_spacing_error：28.29172，col=11，count=216，rate=5.09259%，jerk=0.14130，miniumu_ttc=132.88995\n\n 60%|██████    | 3/5 [00:33<00:22, 11.08s/it]\u001b[A[2024-07-23 08:55:43,431][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1920262\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 08:55:48,914][1383994605.py][line:1089][INFO] mean_spacing_error：26.80873，col=14，count=216，rate=6.48148%，jerk=0.16045，miniumu_ttc=136.84337\n\n 80%|████████  | 4/5 [00:44<00:11, 11.05s/it]\u001b[A[2024-07-23 08:55:54,278][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1902331\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 08:55:59,882][1383994605.py][line:1089][INFO] mean_spacing_error：30.01556，col=15，count=216，rate=6.94444%，jerk=0.17974，miniumu_ttc=59.42834\n\n100%|██████████| 5/5 [00:55<00:00, 11.01s/it]\u001b[A\n 21%|██        | 105/500 [5:28:40<19:51:37, 181.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 08:58:31,596][1383994605.py][line:1670][INFO] 106/500. loss: 0.0032614863788088164\n[2024-07-23 08:58:31,607][1383994605.py][line:1671][INFO] 106/500. mserror: 50.33312225341797  col: 46  count: 216  jerk: 0.05808721110224724  ttc: 102.62722778320312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 08:58:36,754][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3146075\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 08:58:42,140][1383994605.py][line:1089][INFO] mean_spacing_error：97.40075，col=8，count=216，rate=3.70370%，jerk=0.09541，miniumu_ttc=268.12589\n\n 20%|██        | 1/5 [00:10<00:42, 10.52s/it]\u001b[A[2024-07-23 08:58:47,590][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2667745\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 08:58:53,099][1383994605.py][line:1089][INFO] mean_spacing_error：42.96208，col=53，count=216，rate=24.53704%，jerk=0.10761，miniumu_ttc=10.71758\n\n 40%|████      | 2/5 [00:21<00:32, 10.78s/it]\u001b[A[2024-07-23 08:58:58,867][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2523542\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 08:59:04,514][1383994605.py][line:1089][INFO] mean_spacing_error：27.65559，col=13，count=216，rate=6.01852%，jerk=0.14092，miniumu_ttc=182.60631\n\n 60%|██████    | 3/5 [00:32<00:22, 11.07s/it]\u001b[A[2024-07-23 08:59:09,935][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2201508\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 08:59:15,327][1383994605.py][line:1089][INFO] mean_spacing_error：24.94786，col=17，count=216，rate=7.87037%，jerk=0.15894，miniumu_ttc=119.67675\n\n 80%|████████  | 4/5 [00:43<00:10, 10.97s/it]\u001b[A[2024-07-23 08:59:20,686][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2237556\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 08:59:26,027][1383994605.py][line:1089][INFO] mean_spacing_error：23.96134，col=16，count=216，rate=7.40741%，jerk=0.14393，miniumu_ttc=168.49879\n\n100%|██████████| 5/5 [00:54<00:00, 10.88s/it]\u001b[A\n 21%|██        | 106/500 [5:32:06<20:38:05, 188.54s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 09:01:05,608][1383994605.py][line:1670][INFO] 107/500. loss: 0.0033676127592722573\n[2024-07-23 09:01:05,617][1383994605.py][line:1671][INFO] 107/500. mserror: 56.97397232055664  col: 41  count: 216  jerk: 0.05694236233830452  ttc: 182.66525268554688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:01:10,687][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3062764\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:01:16,134][1383994605.py][line:1089][INFO] mean_spacing_error：57.37703，col=14，count=216，rate=6.48148%，jerk=0.08267，miniumu_ttc=207.12024\n\n 20%|██        | 1/5 [00:10<00:42, 10.51s/it]\u001b[A[2024-07-23 09:01:21,582][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2357692\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 09:01:27,145][1383994605.py][line:1089][INFO] mean_spacing_error：57.46370，col=47，count=216，rate=21.75926%，jerk=0.13657，miniumu_ttc=10.78488\n\n 40%|████      | 2/5 [00:21<00:32, 10.80s/it]\u001b[A[2024-07-23 09:01:32,517][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2405377\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.67s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.15s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.97s/it]\u001b[A\u001b[A\n[2024-07-23 09:01:38,711][1383994605.py][line:1089][INFO] mean_spacing_error：31.72098，col=2，count=216，rate=0.92593%，jerk=0.18393，miniumu_ttc=188.92413\n\n 60%|██████    | 3/5 [00:33<00:22, 11.15s/it]\u001b[A[2024-07-23 09:01:44,169][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2273641\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:01:49,753][1383994605.py][line:1089][INFO] mean_spacing_error：34.13068，col=17，count=216，rate=7.87037%，jerk=0.16977，miniumu_ttc=57.19948\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 09:01:55,329][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2431607\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:02:00,804][1383994605.py][line:1089][INFO] mean_spacing_error：37.72966，col=16，count=216，rate=7.40741%，jerk=0.14800，miniumu_ttc=172.11490\n\n100%|██████████| 5/5 [00:55<00:00, 11.04s/it]\u001b[A\n 21%|██▏       | 107/500 [5:34:41<19:28:36, 178.41s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 09:04:24,684][1383994605.py][line:1670][INFO] 108/500. loss: 0.0019763829186558723\n[2024-07-23 09:04:24,697][1383994605.py][line:1671][INFO] 108/500. mserror: 63.218727111816406  col: 37  count: 216  jerk: 0.05611858516931534  ttc: 151.27508544921875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:04:29,970][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3093155\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:04:35,404][1383994605.py][line:1089][INFO] mean_spacing_error：89.10605，col=9，count=216，rate=4.16667%，jerk=0.08586，miniumu_ttc=219.45316\n\n 20%|██        | 1/5 [00:10<00:42, 10.70s/it]\u001b[A[2024-07-23 09:04:41,070][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2595012\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 09:04:46,851][1383994605.py][line:1089][INFO] mean_spacing_error：59.14575，col=28，count=216，rate=12.96296%，jerk=0.14446，miniumu_ttc=11.70258\n\n 40%|████      | 2/5 [00:22<00:33, 11.14s/it]\u001b[A[2024-07-23 09:04:52,707][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2395184\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 09:04:58,048][1383994605.py][line:1089][INFO] mean_spacing_error：39.14226，col=9，count=216，rate=4.16667%，jerk=0.15517，miniumu_ttc=174.91327\n\n 60%|██████    | 3/5 [00:33<00:22, 11.17s/it]\u001b[A[2024-07-23 09:05:03,899][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2159220\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:05:09,475][1383994605.py][line:1089][INFO] mean_spacing_error：26.08166，col=13，count=216，rate=6.01852%，jerk=0.14259，miniumu_ttc=105.15664\n\n 80%|████████  | 4/5 [00:44<00:11, 11.27s/it]\u001b[A[2024-07-23 09:05:14,974][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2049822\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 09:05:20,840][1383994605.py][line:1089][INFO] mean_spacing_error：24.66476，col=9，count=216，rate=4.16667%，jerk=0.17037，miniumu_ttc=135.02090\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n 22%|██▏       | 108/500 [5:38:01<20:08:01, 184.90s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 09:07:05,598][1383994605.py][line:1670][INFO] 109/500. loss: 0.0035683795188864074\n[2024-07-23 09:07:05,611][1383994605.py][line:1671][INFO] 109/500. mserror: 57.66695022583008  col: 38  count: 216  jerk: 0.0585235133767128  ttc: 221.61839294433594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:07:10,824][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3117190\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.14s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.73s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 09:07:16,107][1383994605.py][line:1089][INFO] mean_spacing_error：63.50300，col=9，count=216，rate=4.16667%，jerk=0.09756，miniumu_ttc=188.17458\n\n 20%|██        | 1/5 [00:10<00:41, 10.49s/it]\u001b[A[2024-07-23 09:07:21,405][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2409901\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:07:26,946][1383994605.py][line:1089][INFO] mean_spacing_error：51.68338，col=20，count=216，rate=9.25926%，jerk=0.15950，miniumu_ttc=33.40373\n\n 40%|████      | 2/5 [00:21<00:32, 10.69s/it]\u001b[A[2024-07-23 09:07:32,261][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2391042\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:07:37,843][1383994605.py][line:1089][INFO] mean_spacing_error：38.20599，col=2，count=216，rate=0.92593%，jerk=0.16749，miniumu_ttc=190.86365\n\n 60%|██████    | 3/5 [00:32<00:21, 10.78s/it]\u001b[A[2024-07-23 09:07:43,380][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2148416\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:07:49,047][1383994605.py][line:1089][INFO] mean_spacing_error：41.27163，col=18，count=216，rate=8.33333%，jerk=0.15166，miniumu_ttc=137.64790\n\n 80%|████████  | 4/5 [00:43<00:10, 10.95s/it]\u001b[A[2024-07-23 09:07:54,656][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2230948\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 09:08:00,698][1383994605.py][line:1089][INFO] mean_spacing_error：27.96116，col=17，count=216，rate=7.87037%，jerk=0.13070，miniumu_ttc=155.45341\n\n100%|██████████| 5/5 [00:55<00:00, 11.02s/it]\u001b[A\n 22%|██▏       | 109/500 [5:40:41<19:16:00, 177.39s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 09:09:47,401][1383994605.py][line:1670][INFO] 110/500. loss: 0.00337802991271019\n[2024-07-23 09:09:47,413][1383994605.py][line:1671][INFO] 110/500. mserror: 51.75086975097656  col: 38  count: 216  jerk: 0.062200114130973816  ttc: 182.56094360351562\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:09:52,601][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3219052\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 09:09:58,013][1383994605.py][line:1089][INFO] mean_spacing_error：80.90730，col=9，count=216，rate=4.16667%，jerk=0.11223，miniumu_ttc=211.46458\n\n 20%|██        | 1/5 [00:10<00:42, 10.59s/it]\u001b[A[2024-07-23 09:10:03,565][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2372228\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 09:10:09,361][1383994605.py][line:1089][INFO] mean_spacing_error：63.09520，col=33，count=216，rate=15.27778%，jerk=0.14403，miniumu_ttc=11.15996\n\n 40%|████      | 2/5 [00:21<00:33, 11.04s/it]\u001b[A[2024-07-23 09:10:14,928][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2244079\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 09:10:20,206][1383994605.py][line:1089][INFO] mean_spacing_error：28.53938，col=13，count=216，rate=6.01852%，jerk=0.15761，miniumu_ttc=130.06166\n\n 60%|██████    | 3/5 [00:32<00:21, 10.95s/it]\u001b[A[2024-07-23 09:10:25,666][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2076742\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 09:10:31,036][1383994605.py][line:1089][INFO] mean_spacing_error：21.92272，col=6，count=216，rate=2.77778%，jerk=0.18397，miniumu_ttc=130.38171\n\n 80%|████████  | 4/5 [00:43<00:10, 10.90s/it]\u001b[A[2024-07-23 09:10:36,744][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2199638\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:10:42,343][1383994605.py][line:1089][INFO] mean_spacing_error：25.55295，col=17，count=216，rate=7.87037%，jerk=0.15664，miniumu_ttc=123.81024\n\n100%|██████████| 5/5 [00:54<00:00, 10.99s/it]\u001b[A\n 22%|██▏       | 110/500 [5:43:23<18:42:19, 172.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 09:12:20,053][1383994605.py][line:1670][INFO] 111/500. loss: 0.001788144155095021\n[2024-07-23 09:12:20,066][1383994605.py][line:1671][INFO] 111/500. mserror: 47.09572219848633  col: 39  count: 216  jerk: 0.06481705605983734  ttc: 225.50732421875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:12:25,292][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2997086\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 09:12:30,513][1383994605.py][line:1089][INFO] mean_spacing_error：85.67572，col=8，count=216，rate=3.70370%，jerk=0.11942，miniumu_ttc=225.62119\n\n 20%|██        | 1/5 [00:10<00:41, 10.44s/it]\u001b[A[2024-07-23 09:12:36,213][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2370602\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:12:41,765][1383994605.py][line:1089][INFO] mean_spacing_error：49.60229，col=17，count=216，rate=7.87037%，jerk=0.18967，miniumu_ttc=17.29504\n\n 40%|████      | 2/5 [00:21<00:32, 10.92s/it]\u001b[A[2024-07-23 09:12:48,038][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2291781\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 09:12:53,283][1383994605.py][line:1089][INFO] mean_spacing_error：32.42953，col=18，count=216，rate=8.33333%，jerk=0.14418，miniumu_ttc=92.83629\n\n 60%|██████    | 3/5 [00:33<00:22, 11.19s/it]\u001b[A[2024-07-23 09:12:58,991][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2203337\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 09:13:04,498][1383994605.py][line:1089][INFO] mean_spacing_error：35.08711，col=40，count=216，rate=18.51852%，jerk=0.10726，miniumu_ttc=89.67339\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 09:13:10,189][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2306396\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 09:13:15,512][1383994605.py][line:1089][INFO] mean_spacing_error：27.09117，col=23，count=216，rate=10.64815%，jerk=0.11432，miniumu_ttc=153.56606\n\n100%|██████████| 5/5 [00:55<00:00, 11.09s/it]\u001b[A\n 22%|██▏       | 111/500 [5:45:56<18:01:31, 166.82s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.62s/it]\u001b[A\n[2024-07-23 09:15:33,304][1383994605.py][line:1670][INFO] 112/500. loss: 0.0019519108658035595\n[2024-07-23 09:15:33,317][1383994605.py][line:1671][INFO] 112/500. mserror: 45.21061325073242  col: 39  count: 216  jerk: 0.0663672387599945  ttc: 138.6020050048828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:15:38,625][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3005869\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:15:44,155][1383994605.py][line:1089][INFO] mean_spacing_error：52.54101，col=20，count=216，rate=9.25926%，jerk=0.08066，miniumu_ttc=161.18921\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 09:15:49,826][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2526090\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:15:55,496][1383994605.py][line:1089][INFO] mean_spacing_error：35.51790，col=37，count=216，rate=17.12963%，jerk=0.10552，miniumu_ttc=26.35231\n\n 40%|████      | 2/5 [00:22<00:33, 11.14s/it]\u001b[A[2024-07-23 09:16:01,644][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2121787\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 09:16:07,447][1383994605.py][line:1089][INFO] mean_spacing_error：40.16965，col=22，count=216，rate=10.18519%，jerk=0.15291，miniumu_ttc=88.96146\n\n 60%|██████    | 3/5 [00:34<00:23, 11.50s/it]\u001b[A[2024-07-23 09:16:13,025][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2056260\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 09:16:19,058][1383994605.py][line:1089][INFO] mean_spacing_error：21.47439，col=17，count=216，rate=7.87037%，jerk=0.15808，miniumu_ttc=137.61462\n\n 80%|████████  | 4/5 [00:45<00:11, 11.55s/it]\u001b[A[2024-07-23 09:16:25,081][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2196838\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 09:16:30,989][1383994605.py][line:1089][INFO] mean_spacing_error：23.19891，col=45，count=216，rate=20.83333%，jerk=0.11158，miniumu_ttc=92.78954\n\n100%|██████████| 5/5 [00:57<00:00, 11.53s/it]\u001b[A\n 22%|██▏       | 112/500 [5:49:11<18:54:21, 175.42s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 09:19:01,870][1383994605.py][line:1670][INFO] 113/500. loss: 0.0014429582903782527\n[2024-07-23 09:19:01,882][1383994605.py][line:1671][INFO] 113/500. mserror: 44.40761184692383  col: 39  count: 216  jerk: 0.06767409294843674  ttc: 134.95928955078125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:19:07,790][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3083989\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 09:19:13,308][1383994605.py][line:1089][INFO] mean_spacing_error：52.05906，col=14，count=216，rate=6.48148%，jerk=0.08437，miniumu_ttc=153.27284\n\n 20%|██        | 1/5 [00:11<00:45, 11.42s/it]\u001b[A[2024-07-23 09:19:18,917][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2493882\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:19:24,497][1383994605.py][line:1089][INFO] mean_spacing_error：29.34619，col=21，count=216，rate=9.72222%，jerk=0.12697，miniumu_ttc=39.76716\n\n 40%|████      | 2/5 [00:22<00:33, 11.29s/it]\u001b[A[2024-07-23 09:19:30,218][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2143840\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:19:35,679][1383994605.py][line:1089][INFO] mean_spacing_error：44.27337，col=46，count=216，rate=21.29630%，jerk=0.12625，miniumu_ttc=31.09007\n\n 60%|██████    | 3/5 [00:33<00:22, 11.24s/it]\u001b[A[2024-07-23 09:19:41,551][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2170548\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 09:19:47,052][1383994605.py][line:1089][INFO] mean_spacing_error：23.11265，col=6，count=216，rate=2.77778%，jerk=0.19068，miniumu_ttc=184.10696\n\n 80%|████████  | 4/5 [00:45<00:11, 11.29s/it]\u001b[A[2024-07-23 09:19:52,758][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2160048\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:19:58,230][1383994605.py][line:1089][INFO] mean_spacing_error：24.79462，col=16，count=216，rate=7.40741%，jerk=0.17226，miniumu_ttc=130.54633\n\n100%|██████████| 5/5 [00:56<00:00, 11.27s/it]\u001b[A\n 23%|██▎       | 113/500 [5:52:39<19:52:58, 184.96s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 09:22:30,901][1383994605.py][line:1670][INFO] 114/500. loss: 0.003111142044266065\n[2024-07-23 09:22:30,914][1383994605.py][line:1671][INFO] 114/500. mserror: 45.189701080322266  col: 34  count: 216  jerk: 0.0715632513165474  ttc: 205.34146118164062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:22:36,069][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3209225\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 09:22:41,403][1383994605.py][line:1089][INFO] mean_spacing_error：66.33817，col=11，count=216，rate=5.09259%，jerk=0.08839，miniumu_ttc=205.84584\n\n 20%|██        | 1/5 [00:10<00:41, 10.49s/it]\u001b[A[2024-07-23 09:22:46,832][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2507259\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 09:22:52,464][1383994605.py][line:1089][INFO] mean_spacing_error：55.72755，col=112，count=216，rate=51.85185%，jerk=0.09236，miniumu_ttc=12.23254\n\n 40%|████      | 2/5 [00:21<00:32, 10.82s/it]\u001b[A[2024-07-23 09:22:57,808][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2399187\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 09:23:03,200][1383994605.py][line:1089][INFO] mean_spacing_error：27.10371，col=8，count=216，rate=3.70370%，jerk=0.15671，miniumu_ttc=176.87105\n\n 60%|██████    | 3/5 [00:32<00:21, 10.79s/it]\u001b[A[2024-07-23 09:23:08,638][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2268509\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 09:23:14,037][1383994605.py][line:1089][INFO] mean_spacing_error：28.04812，col=17，count=216，rate=7.87037%，jerk=0.16389，miniumu_ttc=38.07840\n\n 80%|████████  | 4/5 [00:43<00:10, 10.81s/it]\u001b[A[2024-07-23 09:23:19,477][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2370536\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:23:25,144][1383994605.py][line:1089][INFO] mean_spacing_error：17.14514，col=17，count=216，rate=7.87037%，jerk=0.13671，miniumu_ttc=143.28812\n\n100%|██████████| 5/5 [00:54<00:00, 10.85s/it]\u001b[A\n 23%|██▎       | 114/500 [5:56:06<20:32:18, 191.55s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.06s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 09:25:10,779][1383994605.py][line:1670][INFO] 115/500. loss: 0.0032750709603230157\n[2024-07-23 09:25:10,790][1383994605.py][line:1671][INFO] 115/500. mserror: 41.399078369140625  col: 31  count: 216  jerk: 0.07728070020675659  ttc: 627.6380004882812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:25:15,928][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3265384\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 09:25:21,348][1383994605.py][line:1089][INFO] mean_spacing_error：85.48441，col=11，count=216，rate=5.09259%，jerk=0.08823，miniumu_ttc=230.22443\n\n 20%|██        | 1/5 [00:10<00:42, 10.55s/it]\u001b[A[2024-07-23 09:25:26,615][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2599578\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 09:25:32,541][1383994605.py][line:1089][INFO] mean_spacing_error：50.29611，col=121，count=216，rate=56.01852%，jerk=0.08099，miniumu_ttc=7.15172\n\n 40%|████      | 2/5 [00:21<00:32, 10.93s/it]\u001b[A[2024-07-23 09:25:37,984][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2501987\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:25:43,429][1383994605.py][line:1089][INFO] mean_spacing_error：33.80244，col=9，count=216，rate=4.16667%，jerk=0.13077，miniumu_ttc=199.78845\n\n 60%|██████    | 3/5 [00:32<00:21, 10.91s/it]\u001b[A[2024-07-23 09:25:49,166][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2296852\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 09:25:54,994][1383994605.py][line:1089][INFO] mean_spacing_error：44.65394，col=22，count=216，rate=10.18519%，jerk=0.16334，miniumu_ttc=24.06343\n\n 80%|████████  | 4/5 [00:44<00:11, 11.17s/it]\u001b[A[2024-07-23 09:26:00,701][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.3385913\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 09:26:06,481][1383994605.py][line:1089][INFO] mean_spacing_error：26.55321，col=24，count=216，rate=11.11111%，jerk=0.13433，miniumu_ttc=97.76218\n\n100%|██████████| 5/5 [00:55<00:00, 11.14s/it]\u001b[A\n 23%|██▎       | 115/500 [5:58:47<19:30:57, 182.49s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.01s/it]\u001b[A\n2it [00:03,  1.64s/it]\u001b[A\n3it [00:04,  1.56s/it]\u001b[A\n[2024-07-23 09:27:44,711][1383994605.py][line:1670][INFO] 116/500. loss: 0.0033414006854097047\n[2024-07-23 09:27:44,726][1383994605.py][line:1671][INFO] 116/500. mserror: 36.69397735595703  col: 33  count: 216  jerk: 0.08029117435216904  ttc: 101.22176361083984\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:27:49,929][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3021916\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 09:27:55,171][1383994605.py][line:1089][INFO] mean_spacing_error：71.40382，col=12，count=216，rate=5.55556%，jerk=0.08338，miniumu_ttc=205.68205\n\n 20%|██        | 1/5 [00:10<00:41, 10.44s/it]\u001b[A[2024-07-23 09:28:00,516][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2448726\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:28:06,045][1383994605.py][line:1089][INFO] mean_spacing_error：45.65900，col=30，count=216，rate=13.88889%，jerk=0.12348，miniumu_ttc=16.61746\n\n 40%|████      | 2/5 [00:21<00:32, 10.70s/it]\u001b[A[2024-07-23 09:28:11,520][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2102262\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 09:28:16,910][1383994605.py][line:1089][INFO] mean_spacing_error：28.80578，col=13，count=216，rate=6.01852%，jerk=0.16294，miniumu_ttc=97.38176\n\n 60%|██████    | 3/5 [00:32<00:21, 10.77s/it]\u001b[A[2024-07-23 09:28:22,318][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2028386\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:28:27,793][1383994605.py][line:1089][INFO] mean_spacing_error：32.05885，col=11，count=216，rate=5.09259%，jerk=0.19395，miniumu_ttc=170.58771\n\n 80%|████████  | 4/5 [00:43<00:10, 10.82s/it]\u001b[A[2024-07-23 09:28:33,307][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2292930\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:28:38,777][1383994605.py][line:1089][INFO] mean_spacing_error：21.71363，col=16，count=216，rate=7.40741%，jerk=0.17937，miniumu_ttc=135.33391\n\n100%|██████████| 5/5 [00:54<00:00, 10.81s/it]\u001b[A\n 23%|██▎       | 116/500 [6:01:19<18:29:56, 173.43s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 09:30:17,146][1383994605.py][line:1670][INFO] 117/500. loss: 0.0018469756469130516\n[2024-07-23 09:30:17,159][1383994605.py][line:1671][INFO] 117/500. mserror: 37.01302719116211  col: 41  count: 216  jerk: 0.07710905373096466  ttc: 94.51959991455078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:30:22,299][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3188183\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 09:30:27,628][1383994605.py][line:1089][INFO] mean_spacing_error：159.48236，col=6，count=216，rate=2.77778%，jerk=0.09728，miniumu_ttc=274.50070\n\n 20%|██        | 1/5 [00:10<00:41, 10.46s/it]\u001b[A[2024-07-23 09:30:32,920][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3039668\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 09:30:38,431][1383994605.py][line:1089][INFO] mean_spacing_error：49.94229，col=50，count=216，rate=23.14815%，jerk=0.08201，miniumu_ttc=91.44508\n\n 40%|████      | 2/5 [00:21<00:31, 10.66s/it]\u001b[A[2024-07-23 09:30:43,885][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2526790\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 09:30:49,761][1383994605.py][line:1089][INFO] mean_spacing_error：35.50632，col=3，count=216，rate=1.38889%，jerk=0.17623，miniumu_ttc=176.28812\n\n 60%|██████    | 3/5 [00:32<00:21, 10.97s/it]\u001b[A[2024-07-23 09:30:55,141][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2172024\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 09:31:00,406][1383994605.py][line:1089][INFO] mean_spacing_error：25.39707，col=13，count=216，rate=6.01852%，jerk=0.17091，miniumu_ttc=27.08559\n\n 80%|████████  | 4/5 [00:43<00:10, 10.84s/it]\u001b[A[2024-07-23 09:31:05,949][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2263341\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.15s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 09:31:11,142][1383994605.py][line:1089][INFO] mean_spacing_error：20.48616，col=9，count=216，rate=4.16667%，jerk=0.16356，miniumu_ttc=136.87944\n\n100%|██████████| 5/5 [00:53<00:00, 10.80s/it]\u001b[A\n 23%|██▎       | 117/500 [6:03:52<17:46:41, 167.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.01s/it]\u001b[A\n2it [00:03,  1.66s/it]\u001b[A\n3it [00:04,  1.57s/it]\u001b[A\n[2024-07-23 09:32:48,138][1383994605.py][line:1670][INFO] 118/500. loss: 0.003361271694302559\n[2024-07-23 09:32:48,149][1383994605.py][line:1671][INFO] 118/500. mserror: 43.17789840698242  col: 40  count: 216  jerk: 0.07137196511030197  ttc: 105.62389373779297\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:32:53,126][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3072384\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:32:58,666][1383994605.py][line:1089][INFO] mean_spacing_error：118.01059，col=8，count=216，rate=3.70370%，jerk=0.09041，miniumu_ttc=238.84766\n\n 20%|██        | 1/5 [00:10<00:42, 10.51s/it]\u001b[A[2024-07-23 09:33:03,949][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3010525\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 09:33:09,242][1383994605.py][line:1089][INFO] mean_spacing_error：52.47966，col=66，count=216，rate=30.55556%，jerk=0.08270，miniumu_ttc=12.96308\n\n 40%|████      | 2/5 [00:21<00:31, 10.55s/it]\u001b[A[2024-07-23 09:33:14,507][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2609313\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 09:33:19,882][1383994605.py][line:1089][INFO] mean_spacing_error：45.89865，col=13，count=216，rate=6.01852%，jerk=0.12025，miniumu_ttc=174.76479\n\n 60%|██████    | 3/5 [00:31<00:21, 10.59s/it]\u001b[A[2024-07-23 09:33:25,067][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2155787\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.60s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:33:30,755][1383994605.py][line:1089][INFO] mean_spacing_error：24.08875，col=13，count=216，rate=6.01852%，jerk=0.14792，miniumu_ttc=88.53211\n\n 80%|████████  | 4/5 [00:42<00:10, 10.70s/it]\u001b[A[2024-07-23 09:33:36,174][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2131622\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 09:33:41,446][1383994605.py][line:1089][INFO] mean_spacing_error：49.18448，col=4，count=216，rate=1.85185%，jerk=0.14917，miniumu_ttc=229.09619\n\n100%|██████████| 5/5 [00:53<00:00, 10.66s/it]\u001b[A\n 24%|██▎       | 118/500 [6:06:22<17:11:48, 162.06s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.65s/it]\u001b[A\n3it [00:04,  1.57s/it]\u001b[A\n[2024-07-23 09:36:00,859][1383994605.py][line:1670][INFO] 119/500. loss: 0.001421927319218715\n[2024-07-23 09:36:00,868][1383994605.py][line:1671][INFO] 119/500. mserror: 49.862770080566406  col: 39  count: 216  jerk: 0.0666242465376854  ttc: 119.28443145751953\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:36:06,268][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3007455\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 09:36:11,473][1383994605.py][line:1089][INFO] mean_spacing_error：122.40659，col=9，count=216，rate=4.16667%，jerk=0.07969，miniumu_ttc=317.46552\n\n 20%|██        | 1/5 [00:10<00:42, 10.60s/it]\u001b[A[2024-07-23 09:36:16,773][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2790624\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 09:36:22,111][1383994605.py][line:1089][INFO] mean_spacing_error：49.70576，col=54，count=216，rate=25.00000%，jerk=0.09106，miniumu_ttc=22.48790\n\n 40%|████      | 2/5 [00:21<00:31, 10.62s/it]\u001b[A[2024-07-23 09:36:27,532][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2584392\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 09:36:32,756][1383994605.py][line:1089][INFO] mean_spacing_error：42.54661，col=3，count=216，rate=1.38889%，jerk=0.16150，miniumu_ttc=178.83531\n\n 60%|██████    | 3/5 [00:31<00:21, 10.64s/it]\u001b[A[2024-07-23 09:36:38,198][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2229608\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 09:36:43,416][1383994605.py][line:1089][INFO] mean_spacing_error：27.49119，col=16，count=216，rate=7.40741%，jerk=0.15957，miniumu_ttc=44.60414\n\n 80%|████████  | 4/5 [00:42<00:10, 10.64s/it]\u001b[A[2024-07-23 09:36:48,956][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2305871\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.15s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 09:36:54,164][1383994605.py][line:1089][INFO] mean_spacing_error：22.38567，col=11，count=216，rate=5.09259%，jerk=0.13774，miniumu_ttc=155.45636\n\n100%|██████████| 5/5 [00:53<00:00, 10.66s/it]\u001b[A\n 24%|██▍       | 119/500 [6:09:35<18:07:31, 171.26s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 09:38:40,577][1383994605.py][line:1670][INFO] 120/500. loss: 0.003008644717435042\n[2024-07-23 09:38:40,588][1383994605.py][line:1671][INFO] 120/500. mserror: 51.09214782714844  col: 38  count: 216  jerk: 0.06676501035690308  ttc: 112.7347183227539\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:38:46,148][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2994663\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:38:51,685][1383994605.py][line:1089][INFO] mean_spacing_error：96.92079，col=11，count=216，rate=5.09259%，jerk=0.07614，miniumu_ttc=210.64911\n\n 20%|██        | 1/5 [00:11<00:44, 11.09s/it]\u001b[A[2024-07-23 09:38:57,187][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2721104\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 09:39:02,889][1383994605.py][line:1089][INFO] mean_spacing_error：57.00002，col=41，count=216，rate=18.98148%，jerk=0.10040，miniumu_ttc=87.72871\n\n 40%|████      | 2/5 [00:22<00:33, 11.16s/it]\u001b[A[2024-07-23 09:39:08,355][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2350710\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:39:13,800][1383994605.py][line:1089][INFO] mean_spacing_error：36.82161，col=7，count=216，rate=3.24074%，jerk=0.16156，miniumu_ttc=173.25653\n\n 60%|██████    | 3/5 [00:33<00:22, 11.05s/it]\u001b[A[2024-07-23 09:39:19,681][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2099917\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 09:39:25,255][1383994605.py][line:1089][INFO] mean_spacing_error：22.07492，col=11，count=216，rate=5.09259%，jerk=0.15619，miniumu_ttc=94.27010\n\n 80%|████████  | 4/5 [00:44<00:11, 11.21s/it]\u001b[A[2024-07-23 09:39:30,746][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2122427\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 09:39:36,107][1383994605.py][line:1089][INFO] mean_spacing_error：27.06058，col=9，count=216，rate=4.16667%，jerk=0.17561，miniumu_ttc=139.38860\n\n100%|██████████| 5/5 [00:55<00:00, 11.11s/it]\u001b[A\n 24%|██▍       | 120/500 [6:12:17<17:46:58, 168.47s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 09:41:57,349][1383994605.py][line:1670][INFO] 121/500. loss: 0.0026669129729270935\n[2024-07-23 09:41:57,356][1383994605.py][line:1671][INFO] 121/500. mserror: 48.34568405151367  col: 37  count: 216  jerk: 0.0701170340180397  ttc: 118.0156478881836\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:42:02,817][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2957500\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 09:42:08,664][1383994605.py][line:1089][INFO] mean_spacing_error：79.69122，col=11，count=216，rate=5.09259%，jerk=0.07684，miniumu_ttc=203.28435\n\n 20%|██        | 1/5 [00:11<00:45, 11.29s/it]\u001b[A[2024-07-23 09:42:14,227][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2638473\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 09:42:20,021][1383994605.py][line:1089][INFO] mean_spacing_error：45.22088，col=19，count=216，rate=8.79630%，jerk=0.12252，miniumu_ttc=90.81578\n\n 40%|████      | 2/5 [00:22<00:33, 11.33s/it]\u001b[A[2024-07-23 09:42:25,722][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2197915\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 09:42:31,666][1383994605.py][line:1089][INFO] mean_spacing_error：39.64627，col=21，count=216，rate=9.72222%，jerk=0.12552，miniumu_ttc=98.35364\n\n 60%|██████    | 3/5 [00:34<00:22, 11.47s/it]\u001b[A[2024-07-23 09:42:37,837][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2243348\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 09:42:43,815][1383994605.py][line:1089][INFO] mean_spacing_error：29.33735，col=17，count=216，rate=7.87037%，jerk=0.13303，miniumu_ttc=138.10283\n\n 80%|████████  | 4/5 [00:46<00:11, 11.74s/it]\u001b[A[2024-07-23 09:42:49,595][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1987242\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:42:55,266][1383994605.py][line:1089][INFO] mean_spacing_error：27.34574，col=13，count=216，rate=6.01852%，jerk=0.17059，miniumu_ttc=105.69898\n\n100%|██████████| 5/5 [00:57<00:00, 11.58s/it]\u001b[A\n 24%|██▍       | 121/500 [6:15:36<18:42:18, 177.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 09:45:21,369][1383994605.py][line:1670][INFO] 122/500. loss: 0.001420883306612571\n[2024-07-23 09:45:21,381][1383994605.py][line:1671][INFO] 122/500. mserror: 43.830963134765625  col: 36  count: 216  jerk: 0.07426247745752335  ttc: 105.86558532714844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:45:26,416][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3002360\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 09:45:31,724][1383994605.py][line:1089][INFO] mean_spacing_error：84.97852，col=11，count=216，rate=5.09259%，jerk=0.08478，miniumu_ttc=202.40286\n\n 20%|██        | 1/5 [00:10<00:41, 10.34s/it]\u001b[A[2024-07-23 09:45:37,168][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2558218\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 09:45:42,867][1383994605.py][line:1089][INFO] mean_spacing_error：46.60534，col=23，count=216，rate=10.64815%，jerk=0.11804，miniumu_ttc=88.67268\n\n 40%|████      | 2/5 [00:21<00:32, 10.81s/it]\u001b[A[2024-07-23 09:45:48,322][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2204336\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 09:45:53,800][1383994605.py][line:1089][INFO] mean_spacing_error：29.30012，col=12，count=216，rate=5.55556%，jerk=0.15427，miniumu_ttc=129.28001\n\n 60%|██████    | 3/5 [00:32<00:21, 10.87s/it]\u001b[A[2024-07-23 09:45:59,373][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1974263\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:46:05,063][1383994605.py][line:1089][INFO] mean_spacing_error：23.49869，col=8，count=216，rate=3.70370%，jerk=0.19221，miniumu_ttc=90.51238\n\n 80%|████████  | 4/5 [00:43<00:11, 11.02s/it]\u001b[A[2024-07-23 09:46:10,666][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2315233\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:46:16,372][1383994605.py][line:1089][INFO] mean_spacing_error：18.11199，col=7，count=216，rate=3.24074%，jerk=0.18185，miniumu_ttc=184.90997\n\n100%|██████████| 5/5 [00:54<00:00, 11.00s/it]\u001b[A\n 24%|██▍       | 122/500 [6:18:57<19:23:37, 184.70s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.04s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 09:47:58,678][1383994605.py][line:1670][INFO] 123/500. loss: 0.0032284759605924287\n[2024-07-23 09:47:58,690][1383994605.py][line:1671][INFO] 123/500. mserror: 39.561187744140625  col: 35  count: 216  jerk: 0.08068294823169708  ttc: 96.88961029052734\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:48:03,726][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3047602\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 09:48:09,065][1383994605.py][line:1089][INFO] mean_spacing_error：81.87357，col=11，count=216，rate=5.09259%，jerk=0.07634，miniumu_ttc=200.45692\n\n 20%|██        | 1/5 [00:10<00:41, 10.37s/it]\u001b[A[2024-07-23 09:48:14,358][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2447183\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 09:48:19,875][1383994605.py][line:1089][INFO] mean_spacing_error：47.93904，col=54，count=216，rate=25.00000%，jerk=0.11067，miniumu_ttc=25.16143\n\n 40%|████      | 2/5 [00:21<00:31, 10.62s/it]\u001b[A[2024-07-23 09:48:25,170][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2194358\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 09:48:30,577][1383994605.py][line:1089][INFO] mean_spacing_error：30.87482，col=6，count=216，rate=2.77778%，jerk=0.17602，miniumu_ttc=130.65091\n\n 60%|██████    | 3/5 [00:31<00:21, 10.66s/it]\u001b[A[2024-07-23 09:48:35,978][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2079369\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 09:48:41,304][1383994605.py][line:1089][INFO] mean_spacing_error：20.47215，col=13，count=216，rate=6.01852%，jerk=0.15803，miniumu_ttc=133.83055\n\n 80%|████████  | 4/5 [00:42<00:10, 10.69s/it]\u001b[A[2024-07-23 09:48:46,659][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2090342\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 09:48:52,540][1383994605.py][line:1089][INFO] mean_spacing_error：19.27900，col=17，count=216，rate=7.87037%，jerk=0.13798，miniumu_ttc=143.14050\n\n100%|██████████| 5/5 [00:53<00:00, 10.77s/it]\u001b[A\n 25%|██▍       | 123/500 [6:21:33<18:26:44, 176.14s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.69s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 09:51:14,898][1383994605.py][line:1670][INFO] 124/500. loss: 0.001356703694909811\n[2024-07-23 09:51:14,908][1383994605.py][line:1671][INFO] 124/500. mserror: 37.25847625732422  col: 37  count: 216  jerk: 0.08308787643909454  ttc: 93.33203887939453\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:51:20,133][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3076977\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.24s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 09:51:26,347][1383994605.py][line:1089][INFO] mean_spacing_error：175.52496，col=8，count=216，rate=3.70370%，jerk=0.08027，miniumu_ttc=275.93286\n\n 20%|██        | 1/5 [00:11<00:45, 11.43s/it]\u001b[A[2024-07-23 09:51:31,968][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2724324\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:51:37,409][1383994605.py][line:1089][INFO] mean_spacing_error：51.55802，col=61，count=216，rate=28.24074%，jerk=0.09049，miniumu_ttc=19.29667\n\n 40%|████      | 2/5 [00:22<00:33, 11.21s/it]\u001b[A[2024-07-23 09:51:43,597][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2322207\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:51:49,084][1383994605.py][line:1089][INFO] mean_spacing_error：40.04736，col=10，count=216，rate=4.62963%，jerk=0.14332，miniumu_ttc=175.45486\n\n 60%|██████    | 3/5 [00:34<00:22, 11.42s/it]\u001b[A[2024-07-23 09:51:54,672][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2096628\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:52:00,103][1383994605.py][line:1089][INFO] mean_spacing_error：34.51247，col=5，count=216，rate=2.31481%，jerk=0.17381，miniumu_ttc=203.46103\n\n 80%|████████  | 4/5 [00:45<00:11, 11.26s/it]\u001b[A[2024-07-23 09:52:05,760][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1967843\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:52:11,191][1383994605.py][line:1089][INFO] mean_spacing_error：23.47467，col=14，count=216，rate=6.48148%，jerk=0.15788，miniumu_ttc=126.95774\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 25%|██▍       | 124/500 [6:24:52<19:06:08, 182.89s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 09:54:39,836][1383994605.py][line:1670][INFO] 125/500. loss: 0.003080165944993496\n[2024-07-23 09:54:39,849][1383994605.py][line:1671][INFO] 125/500. mserror: 36.862213134765625  col: 25  count: 216  jerk: 0.08830338716506958  ttc: 132.6527862548828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:54:45,468][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3130349\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 09:54:50,750][1383994605.py][line:1089][INFO] mean_spacing_error：77.59346，col=11，count=216，rate=5.09259%，jerk=0.08092，miniumu_ttc=170.16771\n\n 20%|██        | 1/5 [00:10<00:43, 10.90s/it]\u001b[A[2024-07-23 09:54:56,495][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2353860\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 09:55:01,941][1383994605.py][line:1089][INFO] mean_spacing_error：38.95068，col=12，count=216，rate=5.55556%，jerk=0.14638，miniumu_ttc=88.25857\n\n 40%|████      | 2/5 [00:22<00:33, 11.06s/it]\u001b[A[2024-07-23 09:55:07,712][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2069331\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 09:55:13,361][1383994605.py][line:1089][INFO] mean_spacing_error：29.01095，col=19，count=216，rate=8.79630%，jerk=0.12967，miniumu_ttc=96.60994\n\n 60%|██████    | 3/5 [00:33<00:22, 11.23s/it]\u001b[A[2024-07-23 09:55:19,003][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2071175\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:55:24,488][1383994605.py][line:1089][INFO] mean_spacing_error：32.83862，col=21，count=216，rate=9.72222%，jerk=0.11698，miniumu_ttc=137.94748\n\n 80%|████████  | 4/5 [00:44<00:11, 11.19s/it]\u001b[A[2024-07-23 09:55:30,240][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1992910\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 09:55:35,712][1383994605.py][line:1089][INFO] mean_spacing_error：23.96083，col=13，count=216，rate=6.01852%，jerk=0.15211，miniumu_ttc=140.55234\n\n100%|██████████| 5/5 [00:55<00:00, 11.17s/it]\u001b[A\n 25%|██▌       | 125/500 [6:28:16<19:43:39, 189.39s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 09:57:59,573][1383994605.py][line:1670][INFO] 126/500. loss: 0.0025495702090362706\n[2024-07-23 09:57:59,585][1383994605.py][line:1671][INFO] 126/500. mserror: 34.82985305786133  col: 21  count: 216  jerk: 0.09325037896633148  ttc: 109.69617462158203\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 09:58:04,969][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3158487\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:58:10,651][1383994605.py][line:1089][INFO] mean_spacing_error：67.54237，col=14，count=216，rate=6.48148%，jerk=0.07836，miniumu_ttc=164.20006\n\n 20%|██        | 1/5 [00:11<00:44, 11.06s/it]\u001b[A[2024-07-23 09:58:16,238][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2305297\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 09:58:22,378][1383994605.py][line:1089][INFO] mean_spacing_error：36.80088，col=15，count=216，rate=6.94444%，jerk=0.14824，miniumu_ttc=50.12458\n\n 40%|████      | 2/5 [00:22<00:34, 11.45s/it]\u001b[A[2024-07-23 09:58:28,251][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2018566\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 09:58:33,945][1383994605.py][line:1089][INFO] mean_spacing_error：27.58818，col=13，count=216，rate=6.01852%，jerk=0.16559，miniumu_ttc=85.90838\n\n 60%|██████    | 3/5 [00:34<00:23, 11.50s/it]\u001b[A[2024-07-23 09:58:39,626][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1937677\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 09:58:45,359][1383994605.py][line:1089][INFO] mean_spacing_error：30.44483，col=16，count=216，rate=7.40741%，jerk=0.16400，miniumu_ttc=54.50936\n\n 80%|████████  | 4/5 [00:45<00:11, 11.47s/it]\u001b[A[2024-07-23 09:58:50,987][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2070652\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 09:58:56,948][1383994605.py][line:1089][INFO] mean_spacing_error：30.62256，col=16，count=216，rate=7.40741%，jerk=0.17547，miniumu_ttc=92.61655\n\n100%|██████████| 5/5 [00:57<00:00, 11.47s/it]\u001b[A\n 25%|██▌       | 126/500 [6:31:37<20:02:38, 192.94s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 10:00:46,459][1383994605.py][line:1670][INFO] 127/500. loss: 0.003015377869208654\n[2024-07-23 10:00:46,482][1383994605.py][line:1671][INFO] 127/500. mserror: 33.574806213378906  col: 31  count: 216  jerk: 0.09143710881471634  ttc: 95.46841430664062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:00:51,793][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3146583\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:00:57,441][1383994605.py][line:1089][INFO] mean_spacing_error：68.15844，col=11，count=216，rate=5.09259%，jerk=0.08941，miniumu_ttc=199.09351\n\n 20%|██        | 1/5 [00:10<00:43, 10.96s/it]\u001b[A[2024-07-23 10:01:03,375][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2715457\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 10:01:09,149][1383994605.py][line:1089][INFO] mean_spacing_error：58.46984，col=60，count=216，rate=27.77778%，jerk=0.09296，miniumu_ttc=36.27917\n\n 40%|████      | 2/5 [00:22<00:34, 11.40s/it]\u001b[A[2024-07-23 10:01:14,739][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2363970\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 10:01:20,466][1383994605.py][line:1089][INFO] mean_spacing_error：37.24015，col=6，count=216，rate=2.77778%，jerk=0.16984，miniumu_ttc=136.27580\n\n 60%|██████    | 3/5 [00:33<00:22, 11.36s/it]\u001b[A[2024-07-23 10:01:26,176][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2195746\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 10:01:32,006][1383994605.py][line:1089][INFO] mean_spacing_error：21.00353，col=14，count=216，rate=6.48148%，jerk=0.16691，miniumu_ttc=19.25484\n\n 80%|████████  | 4/5 [00:45<00:11, 11.43s/it]\u001b[A[2024-07-23 10:01:37,731][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2328709\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:01:43,468][1383994605.py][line:1089][INFO] mean_spacing_error：19.48365，col=9，count=216，rate=4.16667%，jerk=0.14849，miniumu_ttc=215.52074\n\n100%|██████████| 5/5 [00:56<00:00, 11.40s/it]\u001b[A\n 25%|██▌       | 127/500 [6:34:24<19:10:10, 185.01s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.26s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 10:04:08,848][1383994605.py][line:1670][INFO] 128/500. loss: 0.002755179380377134\n[2024-07-23 10:04:08,860][1383994605.py][line:1671][INFO] 128/500. mserror: 34.58856201171875  col: 29  count: 216  jerk: 0.09026392549276352  ttc: 102.71705627441406\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:04:14,266][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3079413\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 10:04:19,674][1383994605.py][line:1089][INFO] mean_spacing_error：58.09788，col=11，count=216，rate=5.09259%，jerk=0.08954，miniumu_ttc=216.26910\n\n 20%|██        | 1/5 [00:10<00:43, 10.81s/it]\u001b[A[2024-07-23 10:04:24,950][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2339812\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:04:30,481][1383994605.py][line:1089][INFO] mean_spacing_error：43.72883，col=18，count=216，rate=8.33333%，jerk=0.13904，miniumu_ttc=117.15633\n\n 40%|████      | 2/5 [00:21<00:32, 10.81s/it]\u001b[A[2024-07-23 10:04:35,893][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2292902\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 10:04:41,367][1383994605.py][line:1089][INFO] mean_spacing_error：41.38105，col=22，count=216，rate=10.18519%，jerk=0.12795，miniumu_ttc=111.71091\n\n 60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\u001b[A[2024-07-23 10:04:46,983][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2175355\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 10:04:52,374][1383994605.py][line:1089][INFO] mean_spacing_error：28.74676，col=15，count=216，rate=6.94444%，jerk=0.12160，miniumu_ttc=149.51474\n\n 80%|████████  | 4/5 [00:43<00:10, 10.91s/it]\u001b[A[2024-07-23 10:04:57,854][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1964591\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 10:05:03,713][1383994605.py][line:1089][INFO] mean_spacing_error：20.24910，col=12，count=216，rate=5.55556%，jerk=0.16177，miniumu_ttc=128.88208\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 26%|██▌       | 128/500 [6:37:44<19:35:24, 189.58s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.97s/it]\u001b[A\n2it [00:04,  2.18s/it]\u001b[A\n3it [00:05,  1.99s/it]\u001b[A\n[2024-07-23 10:06:54,762][1383994605.py][line:1670][INFO] 129/500. loss: 0.0031601199880242348\n[2024-07-23 10:06:54,779][1383994605.py][line:1671][INFO] 129/500. mserror: 37.40103530883789  col: 23  count: 216  jerk: 0.08922931551933289  ttc: 111.037109375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:07:00,080][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3112695\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:07:05,684][1383994605.py][line:1089][INFO] mean_spacing_error：53.63480，col=10，count=216，rate=4.62963%，jerk=0.10277，miniumu_ttc=197.49136\n\n 20%|██        | 1/5 [00:10<00:43, 10.90s/it]\u001b[A[2024-07-23 10:07:11,048][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2154358\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:07:16,684][1383994605.py][line:1089][INFO] mean_spacing_error：60.35874，col=21，count=216，rate=9.72222%，jerk=0.16870，miniumu_ttc=21.71125\n\n 40%|████      | 2/5 [00:21<00:32, 10.96s/it]\u001b[A[2024-07-23 10:07:22,092][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2266744\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 10:07:28,054][1383994605.py][line:1089][INFO] mean_spacing_error：22.05504，col=12，count=216，rate=5.55556%，jerk=0.15009，miniumu_ttc=142.92830\n\n 60%|██████    | 3/5 [00:33<00:22, 11.15s/it]\u001b[A[2024-07-23 10:07:33,482][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2438881\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:07:39,094][1383994605.py][line:1089][INFO] mean_spacing_error：32.72540，col=30，count=216，rate=13.88889%，jerk=0.09180，miniumu_ttc=155.33321\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 10:07:44,707][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2209114\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:07:50,403][1383994605.py][line:1089][INFO] mean_spacing_error：28.33270，col=25，count=216，rate=11.57407%，jerk=0.10846，miniumu_ttc=141.52702\n\n100%|██████████| 5/5 [00:55<00:00, 11.12s/it]\u001b[A\n 26%|██▌       | 129/500 [6:40:31<18:49:47, 182.72s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 10:10:13,353][1383994605.py][line:1670][INFO] 130/500. loss: 0.0015598923588792484\n[2024-07-23 10:10:13,364][1383994605.py][line:1671][INFO] 130/500. mserror: 40.71706771850586  col: 28  count: 216  jerk: 0.08337298035621643  ttc: 111.13619995117188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:10:18,789][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3070840\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 10:10:24,298][1383994605.py][line:1089][INFO] mean_spacing_error：57.97940，col=11，count=216，rate=5.09259%，jerk=0.09486，miniumu_ttc=196.59348\n\n 20%|██        | 1/5 [00:10<00:43, 10.93s/it]\u001b[A[2024-07-23 10:10:29,917][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2243290\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 10:10:35,897][1383994605.py][line:1089][INFO] mean_spacing_error：35.98438，col=12，count=216，rate=5.55556%，jerk=0.15706，miniumu_ttc=119.55402\n\n 40%|████      | 2/5 [00:22<00:33, 11.32s/it]\u001b[A[2024-07-23 10:10:41,611][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2038109\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 10:10:47,132][1383994605.py][line:1089][INFO] mean_spacing_error：24.76428，col=12，count=216，rate=5.55556%，jerk=0.15209，miniumu_ttc=139.90739\n\n 60%|██████    | 3/5 [00:33<00:22, 11.28s/it]\u001b[A[2024-07-23 10:10:52,791][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1967957\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 10:10:58,763][1383994605.py][line:1089][INFO] mean_spacing_error：30.01690，col=22，count=216，rate=10.18519%，jerk=0.11897，miniumu_ttc=133.85620\n\n 80%|████████  | 4/5 [00:45<00:11, 11.42s/it]\u001b[A[2024-07-23 10:11:04,497][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2081277\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.69s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 10:11:10,418][1383994605.py][line:1089][INFO] mean_spacing_error：26.28452，col=17，count=216，rate=7.87037%，jerk=0.12499，miniumu_ttc=139.60648\n\n100%|██████████| 5/5 [00:57<00:00, 11.41s/it]\u001b[A\n 26%|██▌       | 130/500 [6:43:51<19:18:45, 187.91s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.31s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 10:13:40,287][1383994605.py][line:1670][INFO] 131/500. loss: 0.0028120574230949083\n[2024-07-23 10:13:40,302][1383994605.py][line:1671][INFO] 131/500. mserror: 43.483734130859375  col: 26  count: 216  jerk: 0.08156371861696243  ttc: 141.5360870361328\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:13:46,609][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3066916\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 10:13:52,593][1383994605.py][line:1089][INFO] mean_spacing_error：58.91248，col=11，count=216，rate=5.09259%，jerk=0.09476，miniumu_ttc=195.81282\n\n 20%|██        | 1/5 [00:12<00:49, 12.28s/it]\u001b[A[2024-07-23 10:13:58,378][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2243253\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 10:14:04,252][1383994605.py][line:1089][INFO] mean_spacing_error：37.22117，col=11，count=216，rate=5.09259%，jerk=0.16274，miniumu_ttc=85.59685\n\n 40%|████      | 2/5 [00:23<00:35, 11.92s/it]\u001b[A[2024-07-23 10:14:09,876][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1991283\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 10:14:15,777][1383994605.py][line:1089][INFO] mean_spacing_error：32.45543，col=16，count=216，rate=7.40741%，jerk=0.15174，miniumu_ttc=89.54798\n\n 60%|██████    | 3/5 [00:35<00:23, 11.74s/it]\u001b[A[2024-07-23 10:14:21,758][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1950173\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:14:27,337][1383994605.py][line:1089][INFO] mean_spacing_error：36.19506，col=15，count=216，rate=6.94444%，jerk=0.18095，miniumu_ttc=44.63187\n\n 80%|████████  | 4/5 [00:47<00:11, 11.67s/it]\u001b[A[2024-07-23 10:14:32,864][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2000255\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:14:38,434][1383994605.py][line:1089][INFO] mean_spacing_error：23.97229，col=11，count=216，rate=5.09259%，jerk=0.19513，miniumu_ttc=56.45670\n\n100%|██████████| 5/5 [00:58<00:00, 11.63s/it]\u001b[A\n 26%|██▌       | 131/500 [6:47:19<19:52:43, 193.94s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.47s/it]\u001b[A\n2it [00:03,  1.91s/it]\u001b[A\n3it [00:05,  1.78s/it]\u001b[A\n[2024-07-23 10:17:09,905][1383994605.py][line:1670][INFO] 132/500. loss: 0.002468206143627564\n[2024-07-23 10:17:09,918][1383994605.py][line:1671][INFO] 132/500. mserror: 42.034358978271484  col: 26  count: 216  jerk: 0.0836825892329216  ttc: 451.1328430175781\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:17:15,742][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3101715\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 10:17:21,543][1383994605.py][line:1089][INFO] mean_spacing_error：58.99953，col=11，count=216，rate=5.09259%，jerk=0.09352，miniumu_ttc=198.08592\n\n 20%|██        | 1/5 [00:11<00:46, 11.63s/it]\u001b[A[2024-07-23 10:17:27,298][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2207660\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 10:17:33,273][1383994605.py][line:1089][INFO] mean_spacing_error：40.69962，col=14，count=216，rate=6.48148%，jerk=0.17612，miniumu_ttc=16.26404\n\n 40%|████      | 2/5 [00:23<00:35, 11.68s/it]\u001b[A[2024-07-23 10:17:38,978][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2104179\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:17:44,521][1383994605.py][line:1089][INFO] mean_spacing_error：21.49674，col=14，count=216，rate=6.48148%，jerk=0.16227，miniumu_ttc=119.76201\n\n 60%|██████    | 3/5 [00:34<00:22, 11.48s/it]\u001b[A[2024-07-23 10:17:50,400][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2048335\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:17:56,093][1383994605.py][line:1089][INFO] mean_spacing_error：30.30898，col=22，count=216，rate=10.18519%，jerk=0.14344，miniumu_ttc=53.84761\n\n 80%|████████  | 4/5 [00:46<00:11, 11.52s/it]\u001b[A[2024-07-23 10:18:02,001][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2037638\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 10:18:07,865][1383994605.py][line:1089][INFO] mean_spacing_error：30.26209，col=21，count=216，rate=9.72222%，jerk=0.15612，miniumu_ttc=29.77298\n\n100%|██████████| 5/5 [00:57<00:00, 11.59s/it]\u001b[A\n 26%|██▋       | 132/500 [6:50:48<20:17:59, 198.59s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 10:19:49,341][1383994605.py][line:1670][INFO] 133/500. loss: 0.0030964945132533708\n[2024-07-23 10:19:49,361][1383994605.py][line:1671][INFO] 133/500. mserror: 41.017574310302734  col: 26  count: 216  jerk: 0.08425911515951157  ttc: 134.73605346679688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:19:54,427][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3072320\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 10:19:59,846][1383994605.py][line:1089][INFO] mean_spacing_error：54.84506，col=11，count=216，rate=5.09259%，jerk=0.09324，miniumu_ttc=184.85715\n\n 20%|██        | 1/5 [00:10<00:41, 10.48s/it]\u001b[A[2024-07-23 10:20:05,335][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2248365\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:20:11,047][1383994605.py][line:1089][INFO] mean_spacing_error：34.56125，col=15，count=216，rate=6.94444%，jerk=0.14699，miniumu_ttc=86.04232\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 10:20:16,500][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1977253\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:20:22,058][1383994605.py][line:1089][INFO] mean_spacing_error：23.15658，col=12，count=216，rate=5.55556%，jerk=0.16652，miniumu_ttc=120.89095\n\n 60%|██████    | 3/5 [00:32<00:21, 10.95s/it]\u001b[A[2024-07-23 10:20:27,517][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1965757\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 10:20:32,965][1383994605.py][line:1089][INFO] mean_spacing_error：26.87565，col=15，count=216，rate=6.94444%，jerk=0.17633，miniumu_ttc=64.24775\n\n 80%|████████  | 4/5 [00:43<00:10, 10.94s/it]\u001b[A[2024-07-23 10:20:38,461][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2043107\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 10:20:44,056][1383994605.py][line:1089][INFO] mean_spacing_error：17.62023，col=13，count=216，rate=6.01852%，jerk=0.18106，miniumu_ttc=129.50259\n\n100%|██████████| 5/5 [00:54<00:00, 10.94s/it]\u001b[A\n 27%|██▋       | 133/500 [6:53:24<18:56:52, 185.86s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.83s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 10:23:04,425][1383994605.py][line:1670][INFO] 134/500. loss: 0.0028237681835889816\n[2024-07-23 10:23:04,440][1383994605.py][line:1671][INFO] 134/500. mserror: 40.82290267944336  col: 22  count: 216  jerk: 0.0878712460398674  ttc: 143.81021118164062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:23:09,796][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3164058\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 10:23:15,307][1383994605.py][line:1089][INFO] mean_spacing_error：47.73045，col=12，count=216，rate=5.55556%，jerk=0.09675，miniumu_ttc=157.56520\n\n 20%|██        | 1/5 [00:10<00:43, 10.86s/it]\u001b[A[2024-07-23 10:23:21,153][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2259399\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 10:23:26,736][1383994605.py][line:1089][INFO] mean_spacing_error：29.65297，col=11，count=216，rate=5.09259%，jerk=0.13339，miniumu_ttc=96.31857\n\n 40%|████      | 2/5 [00:22<00:33, 11.19s/it]\u001b[A[2024-07-23 10:23:32,337][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2026838\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:23:37,965][1383994605.py][line:1089][INFO] mean_spacing_error：26.38337，col=14，count=216，rate=6.48148%，jerk=0.13137，miniumu_ttc=142.29639\n\n 60%|██████    | 3/5 [00:33<00:22, 11.21s/it]\u001b[A[2024-07-23 10:23:43,679][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2058402\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 10:23:49,638][1383994605.py][line:1089][INFO] mean_spacing_error：29.06262，col=17，count=216，rate=7.87037%，jerk=0.11863，miniumu_ttc=147.46259\n\n 80%|████████  | 4/5 [00:45<00:11, 11.40s/it]\u001b[A[2024-07-23 10:23:55,239][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1938965\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:24:00,877][1383994605.py][line:1089][INFO] mean_spacing_error：21.62454，col=13，count=216，rate=6.01852%，jerk=0.14922，miniumu_ttc=135.62202\n\n100%|██████████| 5/5 [00:56<00:00, 11.29s/it]\u001b[A\n 27%|██▋       | 134/500 [6:56:41<19:13:50, 189.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.33s/it]\u001b[A\n2it [00:03,  1.89s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 10:26:31,177][1383994605.py][line:1670][INFO] 135/500. loss: 0.0017527383752167225\n[2024-07-23 10:26:31,191][1383994605.py][line:1671][INFO] 135/500. mserror: 40.96615219116211  col: 24  count: 216  jerk: 0.08642897009849548  ttc: 141.77951049804688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:26:36,521][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3227540\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:26:42,042][1383994605.py][line:1089][INFO] mean_spacing_error：53.19068，col=10，count=216，rate=4.62963%，jerk=0.10387，miniumu_ttc=166.94872\n\n 20%|██        | 1/5 [00:10<00:43, 10.85s/it]\u001b[A[2024-07-23 10:26:47,712][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2364577\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:26:53,342][1383994605.py][line:1089][INFO] mean_spacing_error：28.26969，col=16，count=216，rate=7.40741%，jerk=0.12485，miniumu_ttc=1093.30103\n\n 40%|████      | 2/5 [00:22<00:33, 11.12s/it]\u001b[A[2024-07-23 10:26:59,176][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2276250\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 10:27:05,143][1383994605.py][line:1089][INFO] mean_spacing_error：36.46680，col=18，count=216，rate=8.33333%，jerk=0.10622，miniumu_ttc=142.72392\n\n 60%|██████    | 3/5 [00:33<00:22, 11.43s/it]\u001b[A[2024-07-23 10:27:10,897][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2098373\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:27:16,622][1383994605.py][line:1089][INFO] mean_spacing_error：30.63671，col=13，count=216，rate=6.01852%，jerk=0.13648，miniumu_ttc=139.25410\n\n 80%|████████  | 4/5 [00:45<00:11, 11.45s/it]\u001b[A[2024-07-23 10:27:22,262][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1925135\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:27:27,975][1383994605.py][line:1089][INFO] mean_spacing_error：25.38427，col=10，count=216，rate=4.62963%，jerk=0.16499，miniumu_ttc=107.87251\n\n100%|██████████| 5/5 [00:56<00:00, 11.36s/it]\u001b[A\n 27%|██▋       | 135/500 [7:00:08<19:43:25, 194.54s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 10:30:02,847][1383994605.py][line:1670][INFO] 136/500. loss: 0.0024284751464923224\n[2024-07-23 10:30:02,856][1383994605.py][line:1671][INFO] 136/500. mserror: 39.27033615112305  col: 24  count: 216  jerk: 0.08829131722450256  ttc: 145.94509887695312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:30:08,323][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3276467\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:30:13,995][1383994605.py][line:1089][INFO] mean_spacing_error：73.57233，col=6，count=216，rate=2.77778%，jerk=0.10611，miniumu_ttc=209.64197\n\n 20%|██        | 1/5 [00:11<00:44, 11.13s/it]\u001b[A[2024-07-23 10:30:19,475][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2970415\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:30:25,126][1383994605.py][line:1089][INFO] mean_spacing_error：53.31883，col=75，count=216，rate=34.72222%，jerk=0.09731，miniumu_ttc=8.97433\n\n 40%|████      | 2/5 [00:22<00:33, 11.13s/it]\u001b[A[2024-07-23 10:30:30,673][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2213659\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 10:30:36,719][1383994605.py][line:1089][INFO] mean_spacing_error：32.74328，col=12，count=216，rate=5.55556%，jerk=0.15213，miniumu_ttc=170.31798\n\n 60%|██████    | 3/5 [00:33<00:22, 11.34s/it]\u001b[A[2024-07-23 10:30:42,312][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2100462\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 10:30:48,403][1383994605.py][line:1089][INFO] mean_spacing_error：21.31058，col=7，count=216，rate=3.24074%，jerk=0.20187，miniumu_ttc=31.74993\n\n 80%|████████  | 4/5 [00:45<00:11, 11.48s/it]\u001b[A[2024-07-23 10:30:53,826][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2296198\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:30:59,386][1383994605.py][line:1089][INFO] mean_spacing_error：17.87789，col=13，count=216，rate=6.01852%，jerk=0.17507，miniumu_ttc=28.72666\n\n100%|██████████| 5/5 [00:56<00:00, 11.30s/it]\u001b[A\n 27%|██▋       | 136/500 [7:03:40<20:10:54, 199.60s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.46s/it]\u001b[A\n2it [00:04,  1.98s/it]\u001b[A\n3it [00:05,  1.91s/it]\u001b[A\n[2024-07-23 10:32:50,035][1383994605.py][line:1670][INFO] 137/500. loss: 0.0032305590187509856\n[2024-07-23 10:32:50,050][1383994605.py][line:1671][INFO] 137/500. mserror: 39.31032943725586  col: 24  count: 216  jerk: 0.08800321817398071  ttc: 150.18931579589844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:32:55,569][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3299516\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:33:01,324][1383994605.py][line:1089][INFO] mean_spacing_error：72.03803，col=8，count=216，rate=3.70370%，jerk=0.09775，miniumu_ttc=203.85477\n\n 20%|██        | 1/5 [00:11<00:45, 11.26s/it]\u001b[A[2024-07-23 10:33:06,999][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3144920\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:33:12,642][1383994605.py][line:1089][INFO] mean_spacing_error：57.91721，col=114，count=216，rate=52.77778%，jerk=0.09481，miniumu_ttc=4.00971\n\n 40%|████      | 2/5 [00:22<00:33, 11.29s/it]\u001b[A[2024-07-23 10:33:18,223][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3196449\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 10:33:24,260][1383994605.py][line:1089][INFO] mean_spacing_error：44.69656，col=21，count=216，rate=9.72222%，jerk=0.09327，miniumu_ttc=147.83940\n\n 60%|██████    | 3/5 [00:34<00:22, 11.44s/it]\u001b[A[2024-07-23 10:33:29,935][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2228406\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 10:33:35,833][1383994605.py][line:1089][INFO] mean_spacing_error：36.32115，col=17，count=216，rate=7.87037%，jerk=0.11373，miniumu_ttc=94.27985\n\n 80%|████████  | 4/5 [00:45<00:11, 11.50s/it]\u001b[A[2024-07-23 10:33:41,459][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2154682\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 10:33:47,062][1383994605.py][line:1089][INFO] mean_spacing_error：31.28932，col=13，count=216，rate=6.01852%，jerk=0.13620，miniumu_ttc=109.31948\n\n100%|██████████| 5/5 [00:57<00:00, 11.40s/it]\u001b[A\n 27%|██▋       | 137/500 [7:06:27<19:09:40, 190.03s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 10:36:21,895][1383994605.py][line:1670][INFO] 138/500. loss: 0.0024357236300905547\n[2024-07-23 10:36:21,907][1383994605.py][line:1671][INFO] 138/500. mserror: 39.289737701416016  col: 22  count: 216  jerk: 0.08914104104042053  ttc: 145.8004913330078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:36:27,142][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3301326\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:36:32,700][1383994605.py][line:1089][INFO] mean_spacing_error：65.50976，col=11，count=216，rate=5.09259%，jerk=0.08979，miniumu_ttc=197.42599\n\n 20%|██        | 1/5 [00:10<00:43, 10.79s/it]\u001b[A[2024-07-23 10:36:38,382][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3039701\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:36:43,920][1383994605.py][line:1089][INFO] mean_spacing_error：57.10477，col=128，count=216，rate=59.25926%，jerk=0.09012，miniumu_ttc=3.96188\n\n 40%|████      | 2/5 [00:22<00:33, 11.04s/it]\u001b[A[2024-07-23 10:36:49,512][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3280126\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 10:36:54,861][1383994605.py][line:1089][INFO] mean_spacing_error：42.90125，col=23，count=216，rate=10.64815%，jerk=0.09266，miniumu_ttc=154.40047\n\n 60%|██████    | 3/5 [00:32<00:22, 11.00s/it]\u001b[A[2024-07-23 10:37:00,458][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2232415\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 10:37:06,234][1383994605.py][line:1089][INFO] mean_spacing_error：33.28931，col=16，count=216，rate=7.40741%，jerk=0.11675，miniumu_ttc=96.89044\n\n 80%|████████  | 4/5 [00:44<00:11, 11.15s/it]\u001b[A[2024-07-23 10:37:11,714][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2124914\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 10:37:17,190][1383994605.py][line:1089][INFO] mean_spacing_error：30.54355，col=12，count=216，rate=5.55556%，jerk=0.14421，miniumu_ttc=107.43040\n\n100%|██████████| 5/5 [00:55<00:00, 11.06s/it]\u001b[A\n 28%|██▊       | 138/500 [7:09:58<19:42:52, 196.06s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 10:39:05,786][1383994605.py][line:1670][INFO] 139/500. loss: 0.0017771553248167038\n[2024-07-23 10:39:05,798][1383994605.py][line:1671][INFO] 139/500. mserror: 39.82876205444336  col: 23  count: 216  jerk: 0.08626263588666916  ttc: 145.50558471679688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:39:11,735][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3330943\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 10:39:17,181][1383994605.py][line:1089][INFO] mean_spacing_error：61.36200，col=11，count=216，rate=5.09259%，jerk=0.09103，miniumu_ttc=192.76428\n\n 20%|██        | 1/5 [00:10<00:43, 10.95s/it]\u001b[A[2024-07-23 10:39:22,516][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3004256\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 10:39:27,992][1383994605.py][line:1089][INFO] mean_spacing_error：59.55920，col=128，count=216，rate=59.25926%，jerk=0.08152，miniumu_ttc=4.18522\n\n 40%|████      | 2/5 [00:21<00:32, 10.87s/it]\u001b[A[2024-07-23 10:39:33,519][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3280599\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:39:39,041][1383994605.py][line:1089][INFO] mean_spacing_error：44.33606，col=18，count=216，rate=8.33333%，jerk=0.09275，miniumu_ttc=145.80264\n\n 60%|██████    | 3/5 [00:32<00:21, 10.95s/it]\u001b[A[2024-07-23 10:39:45,198][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2284274\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:39:50,953][1383994605.py][line:1089][INFO] mean_spacing_error：39.51155，col=16，count=216，rate=7.40741%，jerk=0.10864，miniumu_ttc=139.40776\n\n 80%|████████  | 4/5 [00:44<00:11, 11.33s/it]\u001b[A[2024-07-23 10:39:56,517][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2107076\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:40:02,131][1383994605.py][line:1089][INFO] mean_spacing_error：32.02999，col=13，count=216，rate=6.01852%，jerk=0.14478，miniumu_ttc=107.47121\n\n100%|██████████| 5/5 [00:55<00:00, 11.18s/it]\u001b[A\n 28%|██▊       | 139/500 [7:12:43<18:43:26, 186.72s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.25s/it]\u001b[A\n2it [00:03,  1.86s/it]\u001b[A\n3it [00:05,  1.77s/it]\u001b[A\n[2024-07-23 10:41:46,891][1383994605.py][line:1670][INFO] 140/500. loss: 0.0029481460029880204\n[2024-07-23 10:41:46,902][1383994605.py][line:1671][INFO] 140/500. mserror: 38.34922790527344  col: 23  count: 216  jerk: 0.08716977387666702  ttc: 145.09158325195312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:41:52,341][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3329446\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:41:57,901][1383994605.py][line:1089][INFO] mean_spacing_error：63.59238，col=9，count=216，rate=4.16667%，jerk=0.10167，miniumu_ttc=202.55438\n\n 20%|██        | 1/5 [00:10<00:43, 10.99s/it]\u001b[A[2024-07-23 10:42:03,438][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3160299\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:42:09,092][1383994605.py][line:1089][INFO] mean_spacing_error：63.22517，col=121，count=216，rate=56.01852%，jerk=0.09095，miniumu_ttc=3.66030\n\n 40%|████      | 2/5 [00:22<00:33, 11.11s/it]\u001b[A[2024-07-23 10:42:14,524][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3240749\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 10:42:20,028][1383994605.py][line:1089][INFO] mean_spacing_error：43.98048，col=12，count=216，rate=5.55556%，jerk=0.09642，miniumu_ttc=160.96591\n\n 60%|██████    | 3/5 [00:33<00:22, 11.03s/it]\u001b[A[2024-07-23 10:42:25,794][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2429800\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 10:42:31,248][1383994605.py][line:1089][INFO] mean_spacing_error：38.92520，col=17，count=216，rate=7.87037%，jerk=0.10343，miniumu_ttc=151.32166\n\n 80%|████████  | 4/5 [00:44<00:11, 11.10s/it]\u001b[A[2024-07-23 10:42:36,941][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2110112\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 10:42:42,377][1383994605.py][line:1089][INFO] mean_spacing_error：29.93145，col=13，count=216，rate=6.01852%，jerk=0.14819，miniumu_ttc=109.89372\n\n100%|██████████| 5/5 [00:55<00:00, 11.10s/it]\u001b[A\n 28%|██▊       | 140/500 [7:15:23<17:52:39, 178.78s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.97s/it]\u001b[A\n3it [00:05,  1.76s/it]\u001b[A\n[2024-07-23 10:45:01,940][1383994605.py][line:1670][INFO] 141/500. loss: 0.0015704003162682056\n[2024-07-23 10:45:01,953][1383994605.py][line:1671][INFO] 141/500. mserror: 36.78224182128906  col: 30  count: 216  jerk: 0.08396020531654358  ttc: 193.554443359375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:45:07,213][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3097834\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 10:45:12,553][1383994605.py][line:1089][INFO] mean_spacing_error：32.73715，col=17，count=216，rate=7.87037%，jerk=0.10599，miniumu_ttc=155.33919\n\n 20%|██        | 1/5 [00:10<00:42, 10.59s/it]\u001b[A[2024-07-23 10:45:19,035][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2349042\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 10:45:24,362][1383994605.py][line:1089][INFO] mean_spacing_error：38.88050，col=7，count=216，rate=3.24074%，jerk=0.14985，miniumu_ttc=163.83475\n\n 40%|████      | 2/5 [00:22<00:33, 11.31s/it]\u001b[A[2024-07-23 10:45:29,853][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2119085\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:45:35,550][1383994605.py][line:1089][INFO] mean_spacing_error：27.09719，col=14，count=216，rate=6.48148%，jerk=0.16797，miniumu_ttc=127.93857\n\n 60%|██████    | 3/5 [00:33<00:22, 11.25s/it]\u001b[A[2024-07-23 10:45:41,148][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1968772\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 10:45:46,671][1383994605.py][line:1089][INFO] mean_spacing_error：23.04062，col=16，count=216，rate=7.40741%，jerk=0.15450，miniumu_ttc=126.43156\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 10:45:52,188][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1927546\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:45:57,759][1383994605.py][line:1089][INFO] mean_spacing_error：18.63306，col=13，count=216，rate=6.01852%，jerk=0.15167，miniumu_ttc=131.84679\n\n100%|██████████| 5/5 [00:55<00:00, 11.16s/it]\u001b[A\n 28%|██▊       | 141/500 [7:18:38<18:19:28, 183.76s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 10:48:31,946][1383994605.py][line:1670][INFO] 142/500. loss: 0.001895241594562928\n[2024-07-23 10:48:31,960][1383994605.py][line:1671][INFO] 142/500. mserror: 38.03300857543945  col: 37  count: 216  jerk: 0.07939548790454865  ttc: 110.72615814208984\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:48:37,501][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3046664\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:48:43,236][1383994605.py][line:1089][INFO] mean_spacing_error：38.70198，col=12，count=216，rate=5.55556%，jerk=0.10537，miniumu_ttc=186.14755\n\n 20%|██        | 1/5 [00:11<00:45, 11.27s/it]\u001b[A[2024-07-23 10:48:48,881][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2339010\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:48:54,435][1383994605.py][line:1089][INFO] mean_spacing_error：34.70052，col=8，count=216，rate=3.70370%，jerk=0.14047，miniumu_ttc=148.51353\n\n 40%|████      | 2/5 [00:22<00:33, 11.23s/it]\u001b[A[2024-07-23 10:49:00,091][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2088900\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 10:49:05,683][1383994605.py][line:1089][INFO] mean_spacing_error：27.43739，col=14，count=216，rate=6.48148%，jerk=0.15512，miniumu_ttc=122.04961\n\n 60%|██████    | 3/5 [00:33<00:22, 11.24s/it]\u001b[A[2024-07-23 10:49:11,278][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2005447\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 10:49:17,067][1383994605.py][line:1089][INFO] mean_spacing_error：25.21635，col=15，count=216，rate=6.94444%，jerk=0.14428，miniumu_ttc=146.10268\n\n 80%|████████  | 4/5 [00:45<00:11, 11.30s/it]\u001b[A[2024-07-23 10:49:22,639][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1925541\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 10:49:28,243][1383994605.py][line:1089][INFO] mean_spacing_error：42.31955，col=7，count=216，rate=3.24074%，jerk=0.13383，miniumu_ttc=248.66829\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 28%|██▊       | 142/500 [7:22:09<19:04:16, 191.78s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.86s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 10:52:01,290][1383994605.py][line:1670][INFO] 143/500. loss: 0.0013995397215088208\n[2024-07-23 10:52:01,302][1383994605.py][line:1671][INFO] 143/500. mserror: 43.81747817993164  col: 42  count: 216  jerk: 0.0708845779299736  ttc: 106.88726043701172\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:52:06,626][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3024997\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 10:52:12,019][1383994605.py][line:1089][INFO] mean_spacing_error：38.15990，col=16，count=216，rate=7.40741%，jerk=0.09859，miniumu_ttc=151.48209\n\n 20%|██        | 1/5 [00:10<00:42, 10.71s/it]\u001b[A[2024-07-23 10:52:17,454][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2313896\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 10:52:23,014][1383994605.py][line:1089][INFO] mean_spacing_error：36.50447，col=8，count=216，rate=3.70370%，jerk=0.14103，miniumu_ttc=148.09604\n\n 40%|████      | 2/5 [00:21<00:32, 10.88s/it]\u001b[A[2024-07-23 10:52:28,693][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1960409\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 10:52:34,164][1383994605.py][line:1089][INFO] mean_spacing_error：30.31908，col=14，count=216，rate=6.48148%，jerk=0.14599，miniumu_ttc=145.33987\n\n 60%|██████    | 3/5 [00:32<00:22, 11.00s/it]\u001b[A[2024-07-23 10:52:39,690][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2030988\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 10:52:45,126][1383994605.py][line:1089][INFO] mean_spacing_error：23.74259，col=6，count=216，rate=2.77778%，jerk=0.17540，miniumu_ttc=183.59441\n\n 80%|████████  | 4/5 [00:43<00:10, 10.99s/it]\u001b[A[2024-07-23 10:52:50,728][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2007197\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 10:52:56,343][1383994605.py][line:1089][INFO] mean_spacing_error：34.94159，col=17，count=216，rate=7.87037%，jerk=0.18567，miniumu_ttc=18.40040\n\n100%|██████████| 5/5 [00:55<00:00, 11.01s/it]\u001b[A\n 29%|██▊       | 143/500 [7:25:37<19:30:11, 196.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 10:55:24,100][1383994605.py][line:1670][INFO] 144/500. loss: 0.0016897120513021946\n[2024-07-23 10:55:24,116][1383994605.py][line:1671][INFO] 144/500. mserror: 48.190155029296875  col: 43  count: 216  jerk: 0.06616471707820892  ttc: 127.96481323242188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:55:29,625][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3044521\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:55:35,279][1383994605.py][line:1089][INFO] mean_spacing_error：40.42221，col=16，count=216，rate=7.40741%，jerk=0.09462，miniumu_ttc=159.31804\n\n 20%|██        | 1/5 [00:11<00:44, 11.16s/it]\u001b[A[2024-07-23 10:55:41,135][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2239688\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:55:46,888][1383994605.py][line:1089][INFO] mean_spacing_error：31.38851，col=7，count=216，rate=3.24074%，jerk=0.15412，miniumu_ttc=187.85420\n\n 40%|████      | 2/5 [00:22<00:34, 11.42s/it]\u001b[A[2024-07-23 10:55:52,467][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1975293\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 10:55:58,382][1383994605.py][line:1089][INFO] mean_spacing_error：46.84425，col=21，count=216，rate=9.72222%，jerk=0.15137，miniumu_ttc=20.07267\n\n 60%|██████    | 3/5 [00:34<00:22, 11.45s/it]\u001b[A[2024-07-23 10:56:04,124][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2363484\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.19s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 10:56:10,287][1383994605.py][line:1089][INFO] mean_spacing_error：36.76001，col=20，count=216，rate=9.25926%，jerk=0.13001，miniumu_ttc=132.28133\n\n 80%|████████  | 4/5 [00:46<00:11, 11.63s/it]\u001b[A[2024-07-23 10:56:15,955][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1975964\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 10:56:21,764][1383994605.py][line:1089][INFO] mean_spacing_error：24.24975，col=19，count=216，rate=8.79630%，jerk=0.13404，miniumu_ttc=24.27994\n\n100%|██████████| 5/5 [00:57<00:00, 11.53s/it]\u001b[A\n 29%|██▉       | 144/500 [7:29:02<19:42:29, 199.30s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.27s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 10:58:53,083][1383994605.py][line:1670][INFO] 145/500. loss: 0.003291809310515722\n[2024-07-23 10:58:53,091][1383994605.py][line:1671][INFO] 145/500. mserror: 47.03917694091797  col: 37  count: 216  jerk: 0.06862549483776093  ttc: 193.22265625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 10:58:58,361][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3236697\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 10:59:04,091][1383994605.py][line:1089][INFO] mean_spacing_error：36.34328，col=20，count=216，rate=9.25926%，jerk=0.09645，miniumu_ttc=148.92793\n\n 20%|██        | 1/5 [00:10<00:43, 10.99s/it]\u001b[A[2024-07-23 10:59:09,570][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2114134\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 10:59:15,244][1383994605.py][line:1089][INFO] mean_spacing_error：32.81338，col=4，count=216，rate=1.85185%，jerk=0.22026，miniumu_ttc=63.14205\n\n 40%|████      | 2/5 [00:22<00:33, 11.08s/it]\u001b[A[2024-07-23 10:59:21,165][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2576276\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 10:59:26,797][1383994605.py][line:1089][INFO] mean_spacing_error：32.34858，col=12，count=216，rate=5.55556%，jerk=0.18545，miniumu_ttc=81.71368\n\n 60%|██████    | 3/5 [00:33<00:22, 11.30s/it]\u001b[A[2024-07-23 10:59:32,473][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2165827\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 10:59:38,279][1383994605.py][line:1089][INFO] mean_spacing_error：33.55019，col=26，count=216，rate=12.03704%，jerk=0.12280，miniumu_ttc=92.50186\n\n 80%|████████  | 4/5 [00:45<00:11, 11.37s/it]\u001b[A[2024-07-23 10:59:43,957][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2155463\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 10:59:49,886][1383994605.py][line:1089][INFO] mean_spacing_error：28.05017，col=18，count=216，rate=8.33333%，jerk=0.10912，miniumu_ttc=131.13866\n\n100%|██████████| 5/5 [00:56<00:00, 11.36s/it]\u001b[A\n 29%|██▉       | 145/500 [7:32:30<19:54:51, 201.95s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.25s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 11:02:18,737][1383994605.py][line:1670][INFO] 146/500. loss: 0.0029048466434081397\n[2024-07-23 11:02:18,749][1383994605.py][line:1671][INFO] 146/500. mserror: 40.51040267944336  col: 32  count: 216  jerk: 0.07682009041309357  ttc: 2121.615234375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:02:23,974][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3501073\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:02:29,648][1383994605.py][line:1089][INFO] mean_spacing_error：54.05546，col=11，count=216，rate=5.09259%，jerk=0.09454，miniumu_ttc=198.81950\n\n 20%|██        | 1/5 [00:10<00:43, 10.89s/it]\u001b[A[2024-07-23 11:02:35,643][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2478686\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 11:02:41,136][1383994605.py][line:1089][INFO] mean_spacing_error：32.25301，col=23，count=216，rate=10.64815%，jerk=0.13868，miniumu_ttc=81.60131\n\n 40%|████      | 2/5 [00:22<00:33, 11.24s/it]\u001b[A[2024-07-23 11:02:46,872][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2310188\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:02:52,303][1383994605.py][line:1089][INFO] mean_spacing_error：28.61850，col=5，count=216，rate=2.31481%，jerk=0.18209，miniumu_ttc=179.59306\n\n 60%|██████    | 3/5 [00:33<00:22, 11.21s/it]\u001b[A[2024-07-23 11:02:57,952][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2132330\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.26s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.01s/it]\u001b[A\u001b[A\n[2024-07-23 11:03:04,244][1383994605.py][line:1089][INFO] mean_spacing_error：23.57737，col=16，count=216，rate=7.40741%，jerk=0.14694，miniumu_ttc=102.35000\n\n 80%|████████  | 4/5 [00:45<00:11, 11.50s/it]\u001b[A[2024-07-23 11:03:09,926][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2020350\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 11:03:15,869][1383994605.py][line:1089][INFO] mean_spacing_error：19.42406，col=13，count=216，rate=6.01852%，jerk=0.14684，miniumu_ttc=113.50194\n\n100%|██████████| 5/5 [00:57<00:00, 11.43s/it]\u001b[A\n 29%|██▉       | 146/500 [7:35:56<19:58:37, 203.16s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 11:05:03,839][1383994605.py][line:1670][INFO] 147/500. loss: 0.003436378203332424\n[2024-07-23 11:05:03,857][1383994605.py][line:1671][INFO] 147/500. mserror: 29.454544067382812  col: 35  count: 216  jerk: 0.08662892132997513  ttc: 116.20266723632812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:05:09,223][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3525423\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:05:14,815][1383994605.py][line:1089][INFO] mean_spacing_error：31.20765，col=15，count=216，rate=6.94444%，jerk=0.10723，miniumu_ttc=147.92564\n\n 20%|██        | 1/5 [00:10<00:43, 10.96s/it]\u001b[A[2024-07-23 11:05:20,554][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2129013\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:05:26,185][1383994605.py][line:1089][INFO] mean_spacing_error：31.20995，col=2，count=216，rate=0.92593%，jerk=0.21362，miniumu_ttc=134.44185\n\n 40%|████      | 2/5 [00:22<00:33, 11.20s/it]\u001b[A[2024-07-23 11:05:31,570][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2365127\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 11:05:37,525][1383994605.py][line:1089][INFO] mean_spacing_error：22.29196，col=10，count=216，rate=4.62963%，jerk=0.17851，miniumu_ttc=120.22432\n\n 60%|██████    | 3/5 [00:33<00:22, 11.26s/it]\u001b[A[2024-07-23 11:05:43,432][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2000801\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 11:05:49,331][1383994605.py][line:1089][INFO] mean_spacing_error：19.06159，col=17，count=216，rate=7.87037%，jerk=0.14376，miniumu_ttc=149.58659\n\n 80%|████████  | 4/5 [00:45<00:11, 11.48s/it]\u001b[A[2024-07-23 11:05:54,901][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1887824\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.58s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 11:06:00,936][1383994605.py][line:1089][INFO] mean_spacing_error：22.76070，col=17，count=216，rate=7.87037%，jerk=0.15273，miniumu_ttc=22.38205\n\n100%|██████████| 5/5 [00:57<00:00, 11.42s/it]\u001b[A\n 29%|██▉       | 147/500 [7:38:41<18:48:01, 191.73s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:05,  1.68s/it]\u001b[A\n[2024-07-23 11:08:30,423][1383994605.py][line:1670][INFO] 148/500. loss: 0.001885161114235719\n[2024-07-23 11:08:30,434][1383994605.py][line:1671][INFO] 148/500. mserror: 26.49697494506836  col: 43  count: 216  jerk: 0.09198986738920212  ttc: 93.19581604003906\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:08:35,662][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3498542\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:08:41,190][1383994605.py][line:1089][INFO] mean_spacing_error：43.86637，col=13，count=216，rate=6.01852%，jerk=0.10261，miniumu_ttc=196.17476\n\n 20%|██        | 1/5 [00:10<00:43, 10.75s/it]\u001b[A[2024-07-23 11:08:46,800][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2676820\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 11:08:52,741][1383994605.py][line:1089][INFO] mean_spacing_error：29.65530，col=16，count=216，rate=7.40741%，jerk=0.14820，miniumu_ttc=82.51855\n\n 40%|████      | 2/5 [00:22<00:33, 11.22s/it]\u001b[A[2024-07-23 11:08:58,730][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2358179\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.19s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.04s/it]\u001b[A\u001b[A\n[2024-07-23 11:09:05,154][1383994605.py][line:1089][INFO] mean_spacing_error：29.62212，col=2，count=216，rate=0.92593%，jerk=0.21654，miniumu_ttc=101.48420\n\n 60%|██████    | 3/5 [00:34<00:23, 11.76s/it]\u001b[A[2024-07-23 11:09:10,756][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2285551\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 11:09:16,514][1383994605.py][line:1089][INFO] mean_spacing_error：23.51988，col=16，count=216，rate=7.40741%，jerk=0.17767，miniumu_ttc=17.14781\n\n 80%|████████  | 4/5 [00:46<00:11, 11.60s/it]\u001b[A[2024-07-23 11:09:22,114][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2146699\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.61s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 11:09:27,993][1383994605.py][line:1089][INFO] mean_spacing_error：17.28824，col=10，count=216，rate=4.62963%，jerk=0.15834，miniumu_ttc=155.06119\n\n100%|██████████| 5/5 [00:57<00:00, 11.51s/it]\u001b[A\n 30%|██▉       | 148/500 [7:42:08<19:11:47, 196.33s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 11:11:14,427][1383994605.py][line:1670][INFO] 149/500. loss: 0.0030510655293862023\n[2024-07-23 11:11:14,439][1383994605.py][line:1671][INFO] 149/500. mserror: 26.80137062072754  col: 45  count: 216  jerk: 0.09172448515892029  ttc: 89.44152069091797\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:11:19,798][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3424098\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 11:11:25,895][1383994605.py][line:1089][INFO] mean_spacing_error：45.61376，col=12，count=216，rate=5.55556%，jerk=0.10069，miniumu_ttc=190.81773\n\n 20%|██        | 1/5 [00:11<00:45, 11.45s/it]\u001b[A[2024-07-23 11:11:31,920][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2807968\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 11:11:37,925][1383994605.py][line:1089][INFO] mean_spacing_error：40.70046，col=44，count=216，rate=20.37037%，jerk=0.12220，miniumu_ttc=20.32545\n\n 40%|████      | 2/5 [00:23<00:35, 11.79s/it]\u001b[A[2024-07-23 11:11:44,055][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2714728\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 11:11:49,855][1383994605.py][line:1089][INFO] mean_spacing_error：55.08332，col=2，count=216，rate=0.92593%，jerk=0.16850，miniumu_ttc=221.57951\n\n 60%|██████    | 3/5 [00:35<00:23, 11.85s/it]\u001b[A[2024-07-23 11:11:55,477][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2235813\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 11:12:01,544][1383994605.py][line:1089][INFO] mean_spacing_error：44.35628，col=20，count=216，rate=9.25926%，jerk=0.17609，miniumu_ttc=27.40752\n\n 80%|████████  | 4/5 [00:47<00:11, 11.79s/it]\u001b[A[2024-07-23 11:12:07,751][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2549374\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 11:12:13,594][1383994605.py][line:1089][INFO] mean_spacing_error：25.57940，col=21，count=216，rate=9.72222%，jerk=0.12764，miniumu_ttc=133.73586\n\n100%|██████████| 5/5 [00:59<00:00, 11.83s/it]\u001b[A\n 30%|██▉       | 149/500 [7:44:54<18:14:36, 187.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 11:14:34,600][1383994605.py][line:1670][INFO] 150/500. loss: 0.0012857044736544292\n[2024-07-23 11:14:34,612][1383994605.py][line:1671][INFO] 150/500. mserror: 27.07642936706543  col: 38  count: 216  jerk: 0.0891956016421318  ttc: 191.13731384277344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:14:39,730][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3531874\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 11:14:45,739][1383994605.py][line:1089][INFO] mean_spacing_error：29.52965，col=15，count=216，rate=6.94444%，jerk=0.11220，miniumu_ttc=142.33612\n\n 20%|██        | 1/5 [00:11<00:44, 11.13s/it]\u001b[A[2024-07-23 11:14:51,307][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2125497\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:14:56,784][1383994605.py][line:1089][INFO] mean_spacing_error：31.36710，col=10，count=216，rate=4.62963%，jerk=0.18896，miniumu_ttc=40.33968\n\n 40%|████      | 2/5 [00:22<00:33, 11.08s/it]\u001b[A[2024-07-23 11:15:02,335][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2217779\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 11:15:08,256][1383994605.py][line:1089][INFO] mean_spacing_error：26.58207，col=13，count=216，rate=6.01852%，jerk=0.18286，miniumu_ttc=20.20366\n\n 60%|██████    | 3/5 [00:33<00:22, 11.26s/it]\u001b[A[2024-07-23 11:15:13,907][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1971395\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:15:19,532][1383994605.py][line:1089][INFO] mean_spacing_error：27.05305，col=17，count=216，rate=7.87037%，jerk=0.15212，miniumu_ttc=86.10657\n\n 80%|████████  | 4/5 [00:44<00:11, 11.27s/it]\u001b[A[2024-07-23 11:15:25,148][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1948670\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:15:30,712][1383994605.py][line:1089][INFO] mean_spacing_error：24.47588，col=17，count=216，rate=7.87037%，jerk=0.13718，miniumu_ttc=136.29881\n\n100%|██████████| 5/5 [00:56<00:00, 11.22s/it]\u001b[A\n 30%|███       | 150/500 [7:48:11<18:28:57, 190.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.08s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.62s/it]\u001b[A\n[2024-07-23 11:17:21,033][1383994605.py][line:1670][INFO] 151/500. loss: 0.002817395764092604\n[2024-07-23 11:17:21,045][1383994605.py][line:1671][INFO] 151/500. mserror: 31.597078323364258  col: 30  count: 216  jerk: 0.08752423524856567  ttc: 124.2591323852539\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:17:26,453][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3626174\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:17:32,108][1383994605.py][line:1089][INFO] mean_spacing_error：53.28159，col=11，count=216，rate=5.09259%，jerk=0.09697，miniumu_ttc=198.57509\n\n 20%|██        | 1/5 [00:11<00:44, 11.05s/it]\u001b[A[2024-07-23 11:17:37,634][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2909311\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:17:43,276][1383994605.py][line:1089][INFO] mean_spacing_error：50.07197，col=56，count=216，rate=25.92593%，jerk=0.14205，miniumu_ttc=6.39218\n\n 40%|████      | 2/5 [00:22<00:33, 11.12s/it]\u001b[A[2024-07-23 11:17:49,325][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3543474\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:17:54,975][1383994605.py][line:1089][INFO] mean_spacing_error：29.68837，col=14，count=216，rate=6.48148%，jerk=0.11576，miniumu_ttc=172.93512\n\n 60%|██████    | 3/5 [00:33<00:22, 11.39s/it]\u001b[A[2024-07-23 11:18:00,518][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2826671\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:18:06,157][1383994605.py][line:1089][INFO] mean_spacing_error：70.08577，col=14，count=216，rate=6.48148%，jerk=0.08077，miniumu_ttc=192.44586\n\n 80%|████████  | 4/5 [00:45<00:11, 11.31s/it]\u001b[A[2024-07-23 11:18:11,526][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2472514\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:18:17,051][1383994605.py][line:1089][INFO] mean_spacing_error：41.78379，col=27，count=216，rate=12.50000%，jerk=0.11274，miniumu_ttc=100.21127\n\n100%|██████████| 5/5 [00:56<00:00, 11.20s/it]\u001b[A\n 30%|███       | 151/500 [7:50:57<17:44:20, 182.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.67s/it]\u001b[A\n[2024-07-23 11:20:39,704][1383994605.py][line:1670][INFO] 152/500. loss: 0.0019377730786800385\n[2024-07-23 11:20:39,715][1383994605.py][line:1671][INFO] 152/500. mserror: 39.80746841430664  col: 29  count: 216  jerk: 0.08223136514425278  ttc: 186.990478515625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:20:44,973][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3405411\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 11:20:50,354][1383994605.py][line:1089][INFO] mean_spacing_error：37.74970，col=14，count=216，rate=6.48148%，jerk=0.10015，miniumu_ttc=162.86923\n\n 20%|██        | 1/5 [00:10<00:42, 10.63s/it]\u001b[A[2024-07-23 11:20:55,886][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2230903\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:21:01,427][1383994605.py][line:1089][INFO] mean_spacing_error：28.17799，col=10，count=216，rate=4.62963%，jerk=0.16457，miniumu_ttc=138.51956\n\n 40%|████      | 2/5 [00:21<00:32, 10.89s/it]\u001b[A[2024-07-23 11:21:07,156][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2199019\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:21:12,721][1383994605.py][line:1089][INFO] mean_spacing_error：36.58875，col=20，count=216，rate=9.25926%，jerk=0.12942，miniumu_ttc=131.93327\n\n 60%|██████    | 3/5 [00:33<00:22, 11.08s/it]\u001b[A[2024-07-23 11:21:18,336][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2197460\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:21:23,943][1383994605.py][line:1089][INFO] mean_spacing_error：29.94467，col=18，count=216，rate=8.33333%，jerk=0.11689，miniumu_ttc=123.22475\n\n 80%|████████  | 4/5 [00:44<00:11, 11.13s/it]\u001b[A[2024-07-23 11:21:29,569][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2004072\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:21:35,260][1383994605.py][line:1089][INFO] mean_spacing_error：20.47038，col=13，count=216，rate=6.01852%，jerk=0.15288，miniumu_ttc=92.53931\n\n100%|██████████| 5/5 [00:55<00:00, 11.11s/it]\u001b[A\n 30%|███       | 152/500 [7:54:16<18:07:46, 187.55s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 11:23:14,489][1383994605.py][line:1670][INFO] 153/500. loss: 0.00183620552221934\n[2024-07-23 11:23:14,505][1383994605.py][line:1671][INFO] 153/500. mserror: 42.36109924316406  col: 41  count: 216  jerk: 0.07365994155406952  ttc: 132.38645935058594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:23:19,624][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3209139\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 11:23:24,947][1383994605.py][line:1089][INFO] mean_spacing_error：34.15805，col=14，count=216，rate=6.48148%，jerk=0.10543，miniumu_ttc=145.16875\n\n 20%|██        | 1/5 [00:10<00:41, 10.44s/it]\u001b[A[2024-07-23 11:23:30,339][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2159898\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:23:35,791][1383994605.py][line:1089][INFO] mean_spacing_error：31.68181，col=6，count=216，rate=2.77778%，jerk=0.17543，miniumu_ttc=146.81628\n\n 40%|████      | 2/5 [00:21<00:32, 10.68s/it]\u001b[A[2024-07-23 11:23:41,240][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2039590\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:23:46,924][1383994605.py][line:1089][INFO] mean_spacing_error：31.11537，col=19，count=216，rate=8.79630%，jerk=0.15517，miniumu_ttc=87.28585\n\n 60%|██████    | 3/5 [00:32<00:21, 10.89s/it]\u001b[A[2024-07-23 11:23:52,440][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2143957\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:23:57,988][1383994605.py][line:1089][INFO] mean_spacing_error：32.26641，col=17，count=216，rate=7.87037%，jerk=0.12015，miniumu_ttc=160.39198\n\n 80%|████████  | 4/5 [00:43<00:10, 10.96s/it]\u001b[A[2024-07-23 11:24:03,710][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1994829\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:24:09,186][1383994605.py][line:1089][INFO] mean_spacing_error：26.37337，col=21，count=216，rate=9.72222%，jerk=0.13252，miniumu_ttc=87.32147\n\n100%|██████████| 5/5 [00:54<00:00, 10.94s/it]\u001b[A\n 31%|███       | 153/500 [7:56:50<17:06:19, 177.46s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.62s/it]\u001b[A\n[2024-07-23 11:26:31,429][1383994605.py][line:1670][INFO] 154/500. loss: 0.001462206554909547\n[2024-07-23 11:26:31,440][1383994605.py][line:1671][INFO] 154/500. mserror: 48.13100051879883  col: 53  count: 216  jerk: 0.06617569178342819  ttc: 94.96926879882812\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:26:36,657][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2968197\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:26:42,131][1383994605.py][line:1089][INFO] mean_spacing_error：45.24551，col=11，count=216，rate=5.09259%，jerk=0.10409，miniumu_ttc=221.17455\n\n 20%|██        | 1/5 [00:10<00:42, 10.69s/it]\u001b[A[2024-07-23 11:26:47,668][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2697715\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 11:26:53,472][1383994605.py][line:1089][INFO] mean_spacing_error：37.68134，col=40，count=216，rate=18.51852%，jerk=0.13294，miniumu_ttc=15.65110\n\n 40%|████      | 2/5 [00:22<00:33, 11.07s/it]\u001b[A[2024-07-23 11:26:59,109][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2641882\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:27:04,757][1383994605.py][line:1089][INFO] mean_spacing_error：42.18723，col=6，count=216，rate=2.77778%，jerk=0.15251，miniumu_ttc=218.17636\n\n 60%|██████    | 3/5 [00:33<00:22, 11.17s/it]\u001b[A[2024-07-23 11:27:10,332][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2120231\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:27:15,786][1383994605.py][line:1089][INFO] mean_spacing_error：42.03454，col=24，count=216，rate=11.11111%，jerk=0.16168，miniumu_ttc=12.72255\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 11:27:21,694][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2454272\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 11:27:27,546][1383994605.py][line:1089][INFO] mean_spacing_error：26.67209，col=16，count=216，rate=7.40741%，jerk=0.14099，miniumu_ttc=137.97374\n\n100%|██████████| 5/5 [00:56<00:00, 11.22s/it]\u001b[A\n 31%|███       | 154/500 [8:00:08<17:39:31, 183.73s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 11:29:59,427][1383994605.py][line:1670][INFO] 155/500. loss: 0.0013828282244503498\n[2024-07-23 11:29:59,441][1383994605.py][line:1671][INFO] 155/500. mserror: 54.92459487915039  col: 60  count: 216  jerk: 0.05868103727698326  ttc: 94.13199615478516\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:30:05,106][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2969817\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:30:10,654][1383994605.py][line:1089][INFO] mean_spacing_error：45.63568，col=12，count=216，rate=5.55556%，jerk=0.10189，miniumu_ttc=255.17465\n\n 20%|██        | 1/5 [00:11<00:44, 11.21s/it]\u001b[A[2024-07-23 11:30:16,163][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2685157\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:30:21,834][1383994605.py][line:1089][INFO] mean_spacing_error：33.39717，col=31，count=216，rate=14.35185%，jerk=0.14324，miniumu_ttc=13.25343\n\n 40%|████      | 2/5 [00:22<00:33, 11.19s/it]\u001b[A[2024-07-23 11:30:27,465][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2711934\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:30:33,110][1383994605.py][line:1089][INFO] mean_spacing_error：41.15486，col=4，count=216，rate=1.85185%，jerk=0.16226，miniumu_ttc=202.84352\n\n 60%|██████    | 3/5 [00:33<00:22, 11.23s/it]\u001b[A[2024-07-23 11:30:39,173][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2164927\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:30:44,723][1383994605.py][line:1089][INFO] mean_spacing_error：39.51184，col=30，count=216，rate=13.88889%，jerk=0.14215，miniumu_ttc=12.48707\n\n 80%|████████  | 4/5 [00:45<00:11, 11.38s/it]\u001b[A[2024-07-23 11:30:50,528][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2364386\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 11:30:56,326][1383994605.py][line:1089][INFO] mean_spacing_error：33.11107，col=23，count=216，rate=10.64815%，jerk=0.11017，miniumu_ttc=131.00481\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 31%|███       | 155/500 [8:03:37<18:19:38, 191.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:03,  1.87s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 11:32:39,939][1383994605.py][line:1670][INFO] 156/500. loss: 0.0017224807913104694\n[2024-07-23 11:32:39,955][1383994605.py][line:1671][INFO] 156/500. mserror: 62.50441360473633  col: 60  count: 216  jerk: 0.05286121368408203  ttc: 95.09992980957031\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:32:45,245][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2976407\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:32:50,996][1383994605.py][line:1089][INFO] mean_spacing_error：44.97002，col=12，count=216，rate=5.55556%，jerk=0.10119，miniumu_ttc=226.01880\n\n 20%|██        | 1/5 [00:11<00:44, 11.04s/it]\u001b[A[2024-07-23 11:32:56,586][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2708960\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:33:02,285][1383994605.py][line:1089][INFO] mean_spacing_error：35.36994，col=23，count=216，rate=10.64815%，jerk=0.16601，miniumu_ttc=11.43364\n\n 40%|████      | 2/5 [00:22<00:33, 11.18s/it]\u001b[A[2024-07-23 11:33:07,904][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2890695\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:33:13,483][1383994605.py][line:1089][INFO] mean_spacing_error：35.84732，col=4，count=216，rate=1.85185%，jerk=0.17588，miniumu_ttc=259.62424\n\n 60%|██████    | 3/5 [00:33<00:22, 11.19s/it]\u001b[A[2024-07-23 11:33:19,086][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2179224\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 11:33:24,658][1383994605.py][line:1089][INFO] mean_spacing_error：33.65213，col=19，count=216，rate=8.79630%，jerk=0.15342，miniumu_ttc=29.92211\n\n 80%|████████  | 4/5 [00:44<00:11, 11.19s/it]\u001b[A[2024-07-23 11:33:30,228][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2124610\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:33:35,667][1383994605.py][line:1089][INFO] mean_spacing_error：25.35780，col=16，count=216，rate=7.40741%，jerk=0.13657，miniumu_ttc=144.55557\n\n100%|██████████| 5/5 [00:55<00:00, 11.14s/it]\u001b[A\n 31%|███       | 156/500 [8:06:16<17:21:33, 181.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:01,  2.00s/it]\u001b[A\n2it [00:03,  1.64s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 11:35:13,739][1383994605.py][line:1670][INFO] 157/500. loss: 0.003482822639246782\n[2024-07-23 11:35:13,749][1383994605.py][line:1671][INFO] 157/500. mserror: 68.03656768798828  col: 54  count: 216  jerk: 0.05299662798643112  ttc: 126.65621948242188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:35:18,789][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2970590\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 11:35:24,027][1383994605.py][line:1089][INFO] mean_spacing_error：47.50644，col=12，count=216，rate=5.55556%，jerk=0.09810，miniumu_ttc=214.92351\n\n 20%|██        | 1/5 [00:10<00:41, 10.27s/it]\u001b[A[2024-07-23 11:35:30,027][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2735357\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 11:35:35,364][1383994605.py][line:1089][INFO] mean_spacing_error：34.74825，col=16，count=216，rate=7.40741%，jerk=0.17457，miniumu_ttc=13.57166\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 11:35:40,821][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3196630\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:35:46,299][1383994605.py][line:1089][INFO] mean_spacing_error：38.77362，col=3，count=216，rate=1.38889%，jerk=0.18443，miniumu_ttc=216.07812\n\n 60%|██████    | 3/5 [00:32<00:21, 10.91s/it]\u001b[A[2024-07-23 11:35:52,092][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2207262\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 11:35:57,916][1383994605.py][line:1089][INFO] mean_spacing_error：27.23707，col=19，count=216，rate=8.79630%，jerk=0.14390，miniumu_ttc=87.64497\n\n 80%|████████  | 4/5 [00:44<00:11, 11.19s/it]\u001b[A[2024-07-23 11:36:03,416][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2127684\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:36:08,897][1383994605.py][line:1089][INFO] mean_spacing_error：21.26844，col=8，count=216，rate=3.70370%，jerk=0.15548，miniumu_ttc=145.06549\n\n100%|██████████| 5/5 [00:55<00:00, 11.03s/it]\u001b[A\n 31%|███▏      | 157/500 [8:08:49<16:29:47, 173.14s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.83s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 11:37:49,090][1383994605.py][line:1670][INFO] 158/500. loss: 0.0033038475861152015\n[2024-07-23 11:37:49,100][1383994605.py][line:1671][INFO] 158/500. mserror: 62.39170837402344  col: 43  count: 216  jerk: 0.06033642217516899  ttc: 152.03097534179688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:37:54,328][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3239054\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 11:37:59,745][1383994605.py][line:1089][INFO] mean_spacing_error：54.73858，col=12，count=216，rate=5.55556%，jerk=0.09544，miniumu_ttc=234.54970\n\n 20%|██        | 1/5 [00:10<00:42, 10.63s/it]\u001b[A[2024-07-23 11:38:05,532][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2314378\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 11:38:10,884][1383994605.py][line:1089][INFO] mean_spacing_error：37.02617，col=13，count=216，rate=6.01852%，jerk=0.19134，miniumu_ttc=51.83943\n\n 40%|████      | 2/5 [00:21<00:32, 10.93s/it]\u001b[A[2024-07-23 11:38:16,266][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2515486\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.73s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 11:38:21,518][1383994605.py][line:1089][INFO] mean_spacing_error：24.81272，col=7，count=216，rate=3.24074%，jerk=0.19705，miniumu_ttc=62.64674\n\n 60%|██████    | 3/5 [00:32<00:21, 10.80s/it]\u001b[A[2024-07-23 11:38:26,942][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2146303\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 11:38:32,308][1383994605.py][line:1089][INFO] mean_spacing_error：19.97219，col=17，count=216，rate=7.87037%，jerk=0.14585，miniumu_ttc=93.75605\n\n 80%|████████  | 4/5 [00:43<00:10, 10.79s/it]\u001b[A[2024-07-23 11:38:37,897][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2033684\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:38:43,334][1383994605.py][line:1089][INFO] mean_spacing_error：25.51256，col=23，count=216，rate=10.64815%，jerk=0.13417，miniumu_ttc=96.52386\n\n100%|██████████| 5/5 [00:54<00:00, 10.85s/it]\u001b[A\n 32%|███▏      | 158/500 [8:11:24<15:54:56, 167.53s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.31s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 11:41:06,050][1383994605.py][line:1670][INFO] 159/500. loss: 0.0019487903142968814\n[2024-07-23 11:41:06,064][1383994605.py][line:1671][INFO] 159/500. mserror: 43.51344299316406  col: 42  count: 216  jerk: 0.07042945921421051  ttc: 176.34500122070312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:41:11,422][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3529797\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.16s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 11:41:17,498][1383994605.py][line:1089][INFO] mean_spacing_error：81.84703，col=9，count=216，rate=4.16667%，jerk=0.09438，miniumu_ttc=228.44400\n\n 20%|██        | 1/5 [00:11<00:45, 11.42s/it]\u001b[A[2024-07-23 11:41:23,324][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3237261\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.53s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 11:41:29,182][1383994605.py][line:1089][INFO] mean_spacing_error：41.86978，col=83，count=216，rate=38.42593%，jerk=0.09046，miniumu_ttc=10.67559\n\n 40%|████      | 2/5 [00:23<00:34, 11.58s/it]\u001b[A[2024-07-23 11:41:35,009][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3140808\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 11:41:40,745][1383994605.py][line:1089][INFO] mean_spacing_error：50.63460，col=23，count=216，rate=10.64815%，jerk=0.08605，miniumu_ttc=150.10240\n\n 60%|██████    | 3/5 [00:34<00:23, 11.57s/it]\u001b[A[2024-07-23 11:41:46,809][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2422021\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.68s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 11:41:52,787][1383994605.py][line:1089][INFO] mean_spacing_error：26.13541，col=13，count=216，rate=6.01852%，jerk=0.13714，miniumu_ttc=137.39696\n\n 80%|████████  | 4/5 [00:46<00:11, 11.76s/it]\u001b[A[2024-07-23 11:41:58,630][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1963196\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 11:42:04,628][1383994605.py][line:1089][INFO] mean_spacing_error：23.32294，col=6，count=216，rate=2.77778%，jerk=0.18893，miniumu_ttc=103.57223\n\n100%|██████████| 5/5 [00:58<00:00, 11.71s/it]\u001b[A\n 32%|███▏      | 159/500 [8:14:45<16:49:41, 177.66s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.92s/it]\u001b[A\n3it [00:05,  1.78s/it]\u001b[A\n[2024-07-23 11:43:57,084][1383994605.py][line:1670][INFO] 160/500. loss: 0.0029552622387806573\n[2024-07-23 11:43:57,096][1383994605.py][line:1671][INFO] 160/500. mserror: 35.10894775390625  col: 46  count: 216  jerk: 0.08272016793489456  ttc: 110.70210266113281\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:44:02,324][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3584308\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 11:44:08,207][1383994605.py][line:1089][INFO] mean_spacing_error：74.52784，col=6，count=216，rate=2.77778%，jerk=0.12476，miniumu_ttc=284.56104\n\n 20%|██        | 1/5 [00:11<00:44, 11.10s/it]\u001b[A[2024-07-23 11:44:13,591][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.3099471\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:44:19,239][1383994605.py][line:1089][INFO] mean_spacing_error：50.21674，col=103，count=216，rate=47.68519%，jerk=0.09881，miniumu_ttc=5.21805\n\n 40%|████      | 2/5 [00:22<00:33, 11.06s/it]\u001b[A[2024-07-23 11:44:24,734][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3241912\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 11:44:30,548][1383994605.py][line:1089][INFO] mean_spacing_error：35.66583，col=29，count=216，rate=13.42593%，jerk=0.09125，miniumu_ttc=149.04291\n\n 60%|██████    | 3/5 [00:33<00:22, 11.17s/it]\u001b[A[2024-07-23 11:44:36,072][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2078980\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 11:44:42,045][1383994605.py][line:1089][INFO] mean_spacing_error：25.25968，col=8，count=216，rate=3.70370%，jerk=0.19325，miniumu_ttc=42.66095\n\n 80%|████████  | 4/5 [00:44<00:11, 11.30s/it]\u001b[A[2024-07-23 11:44:47,635][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2220222\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 11:44:53,310][1383994605.py][line:1089][INFO] mean_spacing_error：21.20423，col=8，count=216，rate=3.70370%，jerk=0.18898，miniumu_ttc=127.04180\n\n100%|██████████| 5/5 [00:56<00:00, 11.24s/it]\u001b[A\n 32%|███▏      | 160/500 [8:17:34<16:31:28, 174.97s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 11:47:18,157][1383994605.py][line:1670][INFO] 161/500. loss: 0.00200671578447024\n[2024-07-23 11:47:18,171][1383994605.py][line:1671][INFO] 161/500. mserror: 36.79718017578125  col: 48  count: 216  jerk: 0.07651160657405853  ttc: 98.18753814697266\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:47:23,527][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3588817\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 11:47:29,251][1383994605.py][line:1089][INFO] mean_spacing_error：70.68298，col=6，count=216，rate=2.77778%，jerk=0.11986，miniumu_ttc=244.27283\n\n 20%|██        | 1/5 [00:11<00:44, 11.07s/it]\u001b[A[2024-07-23 11:47:35,040][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2843870\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.67s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 11:47:40,959][1383994605.py][line:1089][INFO] mean_spacing_error：36.79287，col=35，count=216，rate=16.20370%，jerk=0.12152，miniumu_ttc=82.45185\n\n 40%|████      | 2/5 [00:22<00:34, 11.45s/it]\u001b[A[2024-07-23 11:47:46,638][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2381459\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 11:47:52,350][1383994605.py][line:1089][INFO] mean_spacing_error：28.53858，col=6，count=216，rate=2.77778%，jerk=0.16828，miniumu_ttc=184.24763\n\n 60%|██████    | 3/5 [00:34<00:22, 11.42s/it]\u001b[A[2024-07-23 11:47:58,012][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2110256\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 11:48:03,870][1383994605.py][line:1089][INFO] mean_spacing_error：25.01247，col=13，count=216，rate=6.01852%，jerk=0.18212，miniumu_ttc=25.95030\n\n 80%|████████  | 4/5 [00:45<00:11, 11.46s/it]\u001b[A[2024-07-23 11:48:09,966][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2111538\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 11:48:15,537][1383994605.py][line:1089][INFO] mean_spacing_error：18.73883，col=8，count=216，rate=3.70370%，jerk=0.18124，miniumu_ttc=135.39088\n\n100%|██████████| 5/5 [00:57<00:00, 11.47s/it]\u001b[A\n 32%|███▏      | 161/500 [8:20:56<17:14:45, 183.14s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 11:50:45,047][1383994605.py][line:1670][INFO] 162/500. loss: 0.001980281745394071\n[2024-07-23 11:50:45,064][1383994605.py][line:1671][INFO] 162/500. mserror: 52.92805099487305  col: 43  count: 216  jerk: 0.06313388794660568  ttc: 186.6089630126953\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:50:50,620][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3352941\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:50:56,230][1383994605.py][line:1089][INFO] mean_spacing_error：57.03336，col=10，count=216，rate=4.62963%，jerk=0.09904，miniumu_ttc=237.86978\n\n 20%|██        | 1/5 [00:11<00:44, 11.16s/it]\u001b[A[2024-07-23 11:51:01,752][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2319283\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:51:07,404][1383994605.py][line:1089][INFO] mean_spacing_error：37.45916，col=14，count=216，rate=6.48148%，jerk=0.18409，miniumu_ttc=39.35011\n\n 40%|████      | 2/5 [00:22<00:33, 11.17s/it]\u001b[A[2024-07-23 11:51:12,802][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2633389\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 11:51:19,033][1383994605.py][line:1089][INFO] mean_spacing_error：29.37675，col=2，count=216，rate=0.92593%，jerk=0.19447，miniumu_ttc=190.34128\n\n 60%|██████    | 3/5 [00:33<00:22, 11.38s/it]\u001b[A[2024-07-23 11:51:25,059][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2255725\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 11:51:30,860][1383994605.py][line:1089][INFO] mean_spacing_error：30.86958，col=23，count=216，rate=10.64815%，jerk=0.14644，miniumu_ttc=17.85700\n\n 80%|████████  | 4/5 [00:45<00:11, 11.56s/it]\u001b[A[2024-07-23 11:51:36,577][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2477726\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 11:51:42,245][1383994605.py][line:1089][INFO] mean_spacing_error：34.44979，col=27，count=216，rate=12.50000%，jerk=0.09822，miniumu_ttc=104.82191\n\n100%|██████████| 5/5 [00:57<00:00, 11.44s/it]\u001b[A\n 32%|███▏      | 162/500 [8:24:23<17:51:32, 190.21s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.69s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 11:54:13,213][1383994605.py][line:1670][INFO] 163/500. loss: 0.002581799558053414\n[2024-07-23 11:54:13,225][1383994605.py][line:1671][INFO] 163/500. mserror: 66.16398620605469  col: 43  count: 216  jerk: 0.05702722817659378  ttc: 146.28298950195312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:54:18,407][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3259061\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 11:54:23,695][1383994605.py][line:1089][INFO] mean_spacing_error：45.12324，col=19，count=216，rate=8.79630%，jerk=0.09321，miniumu_ttc=186.20274\n\n 20%|██        | 1/5 [00:10<00:41, 10.47s/it]\u001b[A[2024-07-23 11:54:29,124][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2345165\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:54:34,668][1383994605.py][line:1089][INFO] mean_spacing_error：27.27356，col=10，count=216，rate=4.62963%，jerk=0.19493，miniumu_ttc=20.60399\n\n 40%|████      | 2/5 [00:21<00:32, 10.76s/it]\u001b[A[2024-07-23 11:54:40,081][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2567572\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 11:54:45,538][1383994605.py][line:1089][INFO] mean_spacing_error：29.78845，col=10，count=216，rate=4.62963%，jerk=0.19396，miniumu_ttc=27.17317\n\n 60%|██████    | 3/5 [00:32<00:21, 10.82s/it]\u001b[A[2024-07-23 11:54:50,967][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2161166\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 11:54:56,495][1383994605.py][line:1089][INFO] mean_spacing_error：26.16245，col=11，count=216，rate=5.09259%，jerk=0.14728，miniumu_ttc=153.90038\n\n 80%|████████  | 4/5 [00:43<00:10, 10.87s/it]\u001b[A[2024-07-23 11:55:02,106][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1935404\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.77s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 11:55:08,129][1383994605.py][line:1089][INFO] mean_spacing_error：30.44104，col=29，count=216，rate=13.42593%，jerk=0.11316，miniumu_ttc=89.50814\n\n100%|██████████| 5/5 [00:54<00:00, 10.98s/it]\u001b[A\n 33%|███▎      | 163/500 [8:27:49<18:14:46, 194.91s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 11:56:59,910][1383994605.py][line:1670][INFO] 164/500. loss: 0.0032569592197736106\n[2024-07-23 11:56:59,921][1383994605.py][line:1671][INFO] 164/500. mserror: 61.044307708740234  col: 47  count: 216  jerk: 0.05667894333600998  ttc: 143.5613555908203\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:57:04,938][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3093727\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:57:10,387][1383994605.py][line:1089][INFO] mean_spacing_error：36.56099，col=11，count=216，rate=5.09259%，jerk=0.10868，miniumu_ttc=188.57755\n\n 20%|██        | 1/5 [00:10<00:41, 10.46s/it]\u001b[A[2024-07-23 11:57:15,695][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2361743\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 11:57:21,158][1383994605.py][line:1089][INFO] mean_spacing_error：28.03571，col=10，count=216，rate=4.62963%，jerk=0.19630，miniumu_ttc=18.84192\n\n 40%|████      | 2/5 [00:21<00:31, 10.64s/it]\u001b[A[2024-07-23 11:57:26,565][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2414043\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 11:57:31,973][1383994605.py][line:1089][INFO] mean_spacing_error：26.75038，col=13，count=216，rate=6.01852%，jerk=0.17762，miniumu_ttc=16.95287\n\n 60%|██████    | 3/5 [00:32<00:21, 10.72s/it]\u001b[A[2024-07-23 11:57:37,463][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2099576\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 11:57:43,176][1383994605.py][line:1089][INFO] mean_spacing_error：25.91605，col=13，count=216，rate=6.01852%，jerk=0.13765，miniumu_ttc=151.43028\n\n 80%|████████  | 4/5 [00:43<00:10, 10.91s/it]\u001b[A[2024-07-23 11:57:48,799][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1961488\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 11:57:54,440][1383994605.py][line:1089][INFO] mean_spacing_error：30.07303，col=24，count=216，rate=11.11111%，jerk=0.11472，miniumu_ttc=91.39010\n\n100%|██████████| 5/5 [00:54<00:00, 10.90s/it]\u001b[A\n 33%|███▎      | 164/500 [8:30:35<17:23:27, 186.33s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 11:59:36,105][1383994605.py][line:1670][INFO] 165/500. loss: 0.0034086176504691443\n[2024-07-23 11:59:36,118][1383994605.py][line:1671][INFO] 165/500. mserror: 48.526424407958984  col: 53  count: 216  jerk: 0.061988141387701035  ttc: 93.55243682861328\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 11:59:41,731][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3100352\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 11:59:47,056][1383994605.py][line:1089][INFO] mean_spacing_error：33.01600，col=9，count=216，rate=4.16667%，jerk=0.12215，miniumu_ttc=187.73918\n\n 20%|██        | 1/5 [00:10<00:43, 10.94s/it]\u001b[A[2024-07-23 11:59:52,510][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2415399\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 11:59:58,000][1383994605.py][line:1089][INFO] mean_spacing_error：30.60119，col=10，count=216，rate=4.62963%，jerk=0.19695，miniumu_ttc=17.16629\n\n 40%|████      | 2/5 [00:21<00:32, 10.94s/it]\u001b[A[2024-07-23 12:00:03,617][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2395844\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 12:00:09,149][1383994605.py][line:1089][INFO] mean_spacing_error：30.76946，col=13，count=216，rate=6.01852%，jerk=0.17043，miniumu_ttc=19.64336\n\n 60%|██████    | 3/5 [00:33<00:22, 11.04s/it]\u001b[A[2024-07-23 12:00:14,634][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2157863\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:00:20,131][1383994605.py][line:1089][INFO] mean_spacing_error：29.11871，col=14，count=216，rate=6.48148%，jerk=0.12824，miniumu_ttc=140.54457\n\n 80%|████████  | 4/5 [00:44<00:11, 11.01s/it]\u001b[A[2024-07-23 12:00:25,815][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1992373\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 12:00:31,210][1383994605.py][line:1089][INFO] mean_spacing_error：31.50765，col=24，count=216，rate=11.11111%，jerk=0.11563，miniumu_ttc=54.63200\n\n100%|██████████| 5/5 [00:55<00:00, 11.02s/it]\u001b[A\n 33%|███▎      | 165/500 [8:33:12<16:30:50, 177.46s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 12:02:54,624][1383994605.py][line:1670][INFO] 166/500. loss: 0.001326127288242181\n[2024-07-23 12:02:54,636][1383994605.py][line:1671][INFO] 166/500. mserror: 40.190574645996094  col: 62  count: 216  jerk: 0.06797010451555252  ttc: 74.43917083740234\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:02:59,870][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3043881\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 12:03:05,731][1383994605.py][line:1089][INFO] mean_spacing_error：37.22841，col=8，count=216，rate=3.70370%，jerk=0.12078，miniumu_ttc=203.59322\n\n 20%|██        | 1/5 [00:11<00:44, 11.09s/it]\u001b[A[2024-07-23 12:03:11,116][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2610852\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:03:16,572][1383994605.py][line:1089][INFO] mean_spacing_error：31.83835，col=27，count=216，rate=12.50000%，jerk=0.15435，miniumu_ttc=12.29911\n\n 40%|████      | 2/5 [00:21<00:32, 10.94s/it]\u001b[A[2024-07-23 12:03:22,019][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2915156\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:03:27,459][1383994605.py][line:1089][INFO] mean_spacing_error：36.92803，col=3，count=216，rate=1.38889%，jerk=0.16229，miniumu_ttc=196.68390\n\n 60%|██████    | 3/5 [00:32<00:21, 10.92s/it]\u001b[A[2024-07-23 12:03:33,108][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2142186\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 12:03:39,249][1383994605.py][line:1089][INFO] mean_spacing_error：38.32444，col=21，count=216，rate=9.72222%，jerk=0.14527，miniumu_ttc=85.00065\n\n 80%|████████  | 4/5 [00:44<00:11, 11.26s/it]\u001b[A[2024-07-23 12:03:44,847][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2089587\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 12:03:50,745][1383994605.py][line:1089][INFO] mean_spacing_error：28.93857，col=12，count=216，rate=5.55556%，jerk=0.12618，miniumu_ttc=140.34700\n\n100%|██████████| 5/5 [00:56<00:00, 11.22s/it]\u001b[A\n 33%|███▎      | 166/500 [8:36:31<17:04:44, 184.08s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.83s/it]\u001b[A\n3it [00:05,  1.71s/it]\u001b[A\n[2024-07-23 12:06:23,963][1383994605.py][line:1670][INFO] 167/500. loss: 0.0026498946050802865\n[2024-07-23 12:06:23,974][1383994605.py][line:1671][INFO] 167/500. mserror: 33.918514251708984  col: 50  count: 216  jerk: 0.07725472748279572  ttc: 107.3016128540039\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:06:29,354][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3137690\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 12:06:35,505][1383994605.py][line:1089][INFO] mean_spacing_error：35.03292，col=12，count=216，rate=5.55556%，jerk=0.12003，miniumu_ttc=188.33775\n\n 20%|██        | 1/5 [00:11<00:46, 11.52s/it]\u001b[A[2024-07-23 12:06:41,023][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2264460\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 12:06:46,806][1383994605.py][line:1089][INFO] mean_spacing_error：27.22638，col=8，count=216，rate=3.70370%，jerk=0.15299，miniumu_ttc=127.36011\n\n 40%|████      | 2/5 [00:22<00:34, 11.39s/it]\u001b[A[2024-07-23 12:06:52,294][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2346190\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:06:57,890][1383994605.py][line:1089][INFO] mean_spacing_error：31.61205，col=13，count=216，rate=6.01852%，jerk=0.12810，miniumu_ttc=129.96538\n\n 60%|██████    | 3/5 [00:33<00:22, 11.25s/it]\u001b[A[2024-07-23 12:07:03,536][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2011215\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 12:07:09,219][1383994605.py][line:1089][INFO] mean_spacing_error：33.99286，col=13，count=216，rate=6.01852%，jerk=0.12677，miniumu_ttc=140.91791\n\n 80%|████████  | 4/5 [00:45<00:11, 11.28s/it]\u001b[A[2024-07-23 12:07:14,698][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1916813\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.25s/it]\u001b[A\u001b[A\n\n3it [00:05,  2.00s/it]\u001b[A\u001b[A\n[2024-07-23 12:07:20,968][1383994605.py][line:1089][INFO] mean_spacing_error：23.20586，col=12，count=216，rate=5.55556%，jerk=0.15749，miniumu_ttc=98.08511\n\n100%|██████████| 5/5 [00:56<00:00, 11.40s/it]\u001b[A\n 33%|███▎      | 167/500 [8:40:01<17:45:11, 191.93s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.52s/it]\u001b[A\n2it [00:04,  1.92s/it]\u001b[A\n3it [00:05,  1.81s/it]\u001b[A\n[2024-07-23 12:10:01,617][1383994605.py][line:1670][INFO] 168/500. loss: 0.0014746903131405513\n[2024-07-23 12:10:01,630][1383994605.py][line:1671][INFO] 168/500. mserror: 30.558055877685547  col: 44  count: 216  jerk: 0.08261847496032715  ttc: 90.8597412109375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:10:07,875][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3248141\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 12:10:13,420][1383994605.py][line:1089][INFO] mean_spacing_error：37.02557，col=14，count=216，rate=6.48148%，jerk=0.10693，miniumu_ttc=181.51630\n\n 20%|██        | 1/5 [00:11<00:47, 11.80s/it]\u001b[A[2024-07-23 12:10:19,148][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2126408\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:10:24,654][1383994605.py][line:1089][INFO] mean_spacing_error：29.16767，col=5，count=216，rate=2.31481%，jerk=0.17653，miniumu_ttc=120.73102\n\n 40%|████      | 2/5 [00:23<00:34, 11.46s/it]\u001b[A[2024-07-23 12:10:30,626][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2047204\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 12:10:36,467][1383994605.py][line:1089][INFO] mean_spacing_error：26.63296，col=13，count=216，rate=6.01852%，jerk=0.15207，miniumu_ttc=117.29301\n\n 60%|██████    | 3/5 [00:34<00:23, 11.62s/it]\u001b[A[2024-07-23 12:10:42,267][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1908559\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 12:10:48,286][1383994605.py][line:1089][INFO] mean_spacing_error：21.08996，col=11，count=216，rate=5.09259%，jerk=0.16066，miniumu_ttc=134.12222\n\n 80%|████████  | 4/5 [00:46<00:11, 11.70s/it]\u001b[A[2024-07-23 12:10:54,064][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1824984\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 12:10:59,749][1383994605.py][line:1089][INFO] mean_spacing_error：20.66426，col=14，count=216，rate=6.48148%，jerk=0.15380，miniumu_ttc=91.91592\n\n100%|██████████| 5/5 [00:58<00:00, 11.62s/it]\u001b[A\n 34%|███▎      | 168/500 [8:43:40<18:26:34, 199.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.26s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 12:13:31,026][1383994605.py][line:1670][INFO] 169/500. loss: 0.0029013448705275855\n[2024-07-23 12:13:31,038][1383994605.py][line:1671][INFO] 169/500. mserror: 30.495223999023438  col: 41  count: 216  jerk: 0.08331197500228882  ttc: 134.80807495117188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:13:36,321][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3229459\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 12:13:42,147][1383994605.py][line:1089][INFO] mean_spacing_error：39.87894，col=13，count=216，rate=6.01852%，jerk=0.10471，miniumu_ttc=184.23172\n\n 20%|██        | 1/5 [00:11<00:44, 11.10s/it]\u001b[A[2024-07-23 12:13:47,837][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2147483\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 12:13:53,586][1383994605.py][line:1089][INFO] mean_spacing_error：28.86987，col=6，count=216，rate=2.77778%，jerk=0.16741，miniumu_ttc=128.82033\n\n 40%|████      | 2/5 [00:22<00:33, 11.30s/it]\u001b[A[2024-07-23 12:13:59,227][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2026229\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 12:14:05,058][1383994605.py][line:1089][INFO] mean_spacing_error：26.99577，col=13，count=216，rate=6.01852%，jerk=0.14998，miniumu_ttc=128.06252\n\n 60%|██████    | 3/5 [00:34<00:22, 11.38s/it]\u001b[A[2024-07-23 12:14:10,681][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1891558\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:14:16,264][1383994605.py][line:1089][INFO] mean_spacing_error：22.69535，col=16，count=216，rate=7.40741%，jerk=0.15237，miniumu_ttc=93.91566\n\n 80%|████████  | 4/5 [00:45<00:11, 11.31s/it]\u001b[A[2024-07-23 12:14:21,728][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1864085\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:14:27,197][1383994605.py][line:1089][INFO] mean_spacing_error：20.86813，col=13，count=216，rate=6.01852%，jerk=0.19074，miniumu_ttc=27.68500\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n 34%|███▍      | 169/500 [8:47:08<18:35:35, 202.22s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 12:16:57,825][1383994605.py][line:1670][INFO] 170/500. loss: 0.0028163762763142586\n[2024-07-23 12:16:57,841][1383994605.py][line:1671][INFO] 170/500. mserror: 30.467744827270508  col: 33  count: 216  jerk: 0.08702325075864792  ttc: 114.59104919433594\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:17:03,171][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3384717\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:17:08,753][1383994605.py][line:1089][INFO] mean_spacing_error：39.30288，col=9，count=216，rate=4.16667%，jerk=0.11284，miniumu_ttc=229.83266\n\n 20%|██        | 1/5 [00:10<00:43, 10.90s/it]\u001b[A[2024-07-23 12:17:14,444][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2321642\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:05,  5.78s/it]\u001b[A\u001b[A\n\n2it [00:10,  4.90s/it]\u001b[A\u001b[A\n\n3it [00:11,  3.84s/it]\u001b[A\u001b[A\n[2024-07-23 12:17:26,246][1383994605.py][line:1089][INFO] mean_spacing_error：33.07882，col=8，count=216，rate=3.70370%，jerk=0.20810，miniumu_ttc=14.26012\n\n 40%|████      | 2/5 [00:28<00:44, 14.78s/it]\u001b[A[2024-07-23 12:17:32,064][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2434269\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 12:17:37,952][1383994605.py][line:1089][INFO] mean_spacing_error：30.27791，col=15，count=216，rate=6.94444%，jerk=0.16134，miniumu_ttc=91.78218\n\n 60%|██████    | 3/5 [00:40<00:26, 13.38s/it]\u001b[A[2024-07-23 12:17:43,777][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2112532\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 12:17:49,488][1383994605.py][line:1089][INFO] mean_spacing_error：25.79169，col=14，count=216，rate=6.48148%，jerk=0.13026，miniumu_ttc=169.69260\n\n 80%|████████  | 4/5 [00:51<00:12, 12.65s/it]\u001b[A[2024-07-23 12:17:55,207][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1960734\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 12:18:00,996][1383994605.py][line:1089][INFO] mean_spacing_error：27.62082，col=29，count=216，rate=13.42593%，jerk=0.12647，miniumu_ttc=125.22384\n\n100%|██████████| 5/5 [01:03<00:00, 12.63s/it]\u001b[A\n 34%|███▍      | 170/500 [8:50:41<18:51:21, 205.70s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 12:19:54,442][1383994605.py][line:1670][INFO] 171/500. loss: 0.0030464185401797295\n[2024-07-23 12:19:54,463][1383994605.py][line:1671][INFO] 171/500. mserror: 29.147594451904297  col: 32  count: 216  jerk: 0.09065189212560654  ttc: 122.41991424560547\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:19:59,823][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3368184\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 12:20:05,981][1383994605.py][line:1089][INFO] mean_spacing_error：44.28974，col=8，count=216，rate=3.70370%，jerk=0.11967，miniumu_ttc=210.28127\n\n 20%|██        | 1/5 [00:11<00:46, 11.51s/it]\u001b[A[2024-07-23 12:20:11,566][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2292167\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 12:20:17,424][1383994605.py][line:1089][INFO] mean_spacing_error：26.86580，col=10，count=216，rate=4.62963%，jerk=0.19417，miniumu_ttc=18.95652\n\n 40%|████      | 2/5 [00:22<00:34, 11.47s/it]\u001b[A[2024-07-23 12:20:23,070][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2253181\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 12:20:28,868][1383994605.py][line:1089][INFO] mean_spacing_error：33.79150，col=19，count=216，rate=8.79630%，jerk=0.16056，miniumu_ttc=17.14259\n\n 60%|██████    | 3/5 [00:34<00:22, 11.46s/it]\u001b[A[2024-07-23 12:20:35,000][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2157288\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 12:20:40,981][1383994605.py][line:1089][INFO] mean_spacing_error：29.55703，col=17，count=216，rate=7.87037%，jerk=0.12556，miniumu_ttc=141.48318\n\n 80%|████████  | 4/5 [00:46<00:11, 11.72s/it]\u001b[A[2024-07-23 12:20:46,673][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2005528\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 12:20:52,519][1383994605.py][line:1089][INFO] mean_spacing_error：29.10460，col=22，count=216，rate=10.18519%，jerk=0.11558，miniumu_ttc=94.21949\n\n100%|██████████| 5/5 [00:58<00:00, 11.61s/it]\u001b[A\n 34%|███▍      | 171/500 [8:53:33<17:51:42, 195.45s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.93s/it]\u001b[A\n2it [00:04,  2.13s/it]\u001b[A\n3it [00:05,  1.96s/it]\u001b[A\n[2024-07-23 12:23:18,752][1383994605.py][line:1670][INFO] 172/500. loss: 0.002426009625196457\n[2024-07-23 12:23:18,760][1383994605.py][line:1671][INFO] 172/500. mserror: 26.972726821899414  col: 35  count: 216  jerk: 0.09563662111759186  ttc: 116.98271942138672\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:23:24,165][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3050052\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:23:29,742][1383994605.py][line:1089][INFO] mean_spacing_error：41.95003，col=10，count=216，rate=4.62963%，jerk=0.11928，miniumu_ttc=200.92989\n\n 20%|██        | 1/5 [00:10<00:43, 10.97s/it]\u001b[A[2024-07-23 12:23:35,483][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2295076\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 12:23:41,607][1383994605.py][line:1089][INFO] mean_spacing_error：26.86535，col=8，count=216，rate=3.70370%，jerk=0.15074，miniumu_ttc=112.21920\n\n 40%|████      | 2/5 [00:22<00:34, 11.50s/it]\u001b[A[2024-07-23 12:23:47,990][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2539978\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.67s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.20s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.03s/it]\u001b[A\u001b[A\n[2024-07-23 12:23:54,399][1383994605.py][line:1089][INFO] mean_spacing_error：32.32571，col=8，count=216，rate=3.70370%，jerk=0.13744，miniumu_ttc=167.13602\n\n 60%|██████    | 3/5 [00:35<00:24, 12.09s/it]\u001b[A[2024-07-23 12:24:01,223][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2039121\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.80s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.14s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 12:24:07,406][1383994605.py][line:1089][INFO] mean_spacing_error：32.08313，col=18，count=216，rate=8.33333%，jerk=0.12250，miniumu_ttc=112.68771\n\n 80%|████████  | 4/5 [00:48<00:12, 12.45s/it]\u001b[A[2024-07-23 12:24:13,036][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2025444\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 12:24:18,773][1383994605.py][line:1089][INFO] mean_spacing_error：29.20547，col=14，count=216，rate=6.48148%，jerk=0.13130，miniumu_ttc=151.93893\n\n100%|██████████| 5/5 [01:00<00:00, 12.00s/it]\u001b[A\n 34%|███▍      | 172/500 [8:56:59<18:06:07, 198.68s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 12:26:47,112][1383994605.py][line:1670][INFO] 173/500. loss: 0.0018390535066525142\n[2024-07-23 12:26:47,123][1383994605.py][line:1671][INFO] 173/500. mserror: 29.829683303833008  col: 38  count: 216  jerk: 0.09338217228651047  ttc: 88.17777252197266\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:26:52,293][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3016154\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 12:26:57,911][1383994605.py][line:1089][INFO] mean_spacing_error：45.53685，col=6，count=216，rate=2.77778%，jerk=0.13868，miniumu_ttc=206.10349\n\n 20%|██        | 1/5 [00:10<00:43, 10.79s/it]\u001b[A[2024-07-23 12:27:03,549][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2721442\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 12:27:09,434][1383994605.py][line:1089][INFO] mean_spacing_error：45.22663，col=50，count=216，rate=23.14815%，jerk=0.11885，miniumu_ttc=18.62573\n\n 40%|████      | 2/5 [00:22<00:33, 11.21s/it]\u001b[A[2024-07-23 12:27:15,110][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2505706\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 12:27:20,823][1383994605.py][line:1089][INFO] mean_spacing_error：31.97241，col=11，count=216，rate=5.09259%，jerk=0.13064，miniumu_ttc=183.45258\n\n 60%|██████    | 3/5 [00:33<00:22, 11.29s/it]\u001b[A[2024-07-23 12:27:26,362][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1998530\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 12:27:32,094][1383994605.py][line:1089][INFO] mean_spacing_error：26.48287，col=13，count=216，rate=6.01852%，jerk=0.15953，miniumu_ttc=83.60217\n\n 80%|████████  | 4/5 [00:44<00:11, 11.29s/it]\u001b[A[2024-07-23 12:27:37,594][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2128875\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 12:27:43,120][1383994605.py][line:1089][INFO] mean_spacing_error：25.47845，col=6，count=216，rate=2.77778%，jerk=0.16297，miniumu_ttc=150.22235\n\n100%|██████████| 5/5 [00:56<00:00, 11.20s/it]\u001b[A\n 35%|███▍      | 173/500 [9:00:24<18:12:07, 200.39s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 12:30:18,740][1383994605.py][line:1670][INFO] 174/500. loss: 0.0016576989243427913\n[2024-07-23 12:30:18,753][1383994605.py][line:1671][INFO] 174/500. mserror: 31.593101501464844  col: 36  count: 216  jerk: 0.08891461789608002  ttc: 116.39111328125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:30:24,107][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2921460\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:30:29,606][1383994605.py][line:1089][INFO] mean_spacing_error：36.23402，col=12，count=216，rate=5.55556%，jerk=0.13344，miniumu_ttc=247.39565\n\n 20%|██        | 1/5 [00:10<00:43, 10.84s/it]\u001b[A[2024-07-23 12:30:35,414][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2552555\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.09s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 12:30:41,473][1383994605.py][line:1089][INFO] mean_spacing_error：25.51011，col=25，count=216，rate=11.57407%，jerk=0.14124，miniumu_ttc=13.73640\n\n 40%|████      | 2/5 [00:22<00:34, 11.45s/it]\u001b[A[2024-07-23 12:30:47,197][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2679395\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 12:30:52,768][1383994605.py][line:1089][INFO] mean_spacing_error：45.97058，col=5，count=216，rate=2.31481%，jerk=0.15114，miniumu_ttc=221.37195\n\n 60%|██████    | 3/5 [00:34<00:22, 11.38s/it]\u001b[A[2024-07-23 12:30:58,408][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2081872\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 12:31:04,040][1383994605.py][line:1089][INFO] mean_spacing_error：32.58571，col=19，count=216，rate=8.79630%，jerk=0.15919，miniumu_ttc=13.95068\n\n 80%|████████  | 4/5 [00:45<00:11, 11.34s/it]\u001b[A[2024-07-23 12:31:09,651][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2106359\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.64s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 12:31:15,510][1383994605.py][line:1089][INFO] mean_spacing_error：26.09147，col=16，count=216，rate=7.40741%，jerk=0.14416，miniumu_ttc=130.74487\n\n100%|██████████| 5/5 [00:56<00:00, 11.35s/it]\u001b[A\n 35%|███▍      | 174/500 [9:03:56<18:28:19, 203.99s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.34s/it]\u001b[A\n2it [00:04,  1.99s/it]\u001b[A\n3it [00:05,  1.89s/it]\u001b[A\n[2024-07-23 12:33:06,265][1383994605.py][line:1670][INFO] 175/500. loss: 0.001657103809217612\n[2024-07-23 12:33:06,276][1383994605.py][line:1671][INFO] 175/500. mserror: 37.321712493896484  col: 34  count: 216  jerk: 0.08178523927927017  ttc: 116.18780517578125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:33:11,625][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2889006\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 12:33:17,476][1383994605.py][line:1089][INFO] mean_spacing_error：32.31454，col=12，count=216，rate=5.55556%，jerk=0.13225，miniumu_ttc=176.29379\n\n 20%|██        | 1/5 [00:11<00:44, 11.20s/it]\u001b[A[2024-07-23 12:33:23,193][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2234116\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 12:33:28,897][1383994605.py][line:1089][INFO] mean_spacing_error：25.44510，col=10，count=216，rate=4.62963%，jerk=0.14543，miniumu_ttc=99.28896\n\n 40%|████      | 2/5 [00:22<00:33, 11.32s/it]\u001b[A[2024-07-23 12:33:34,515][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2378731\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 12:33:40,584][1383994605.py][line:1089][INFO] mean_spacing_error：33.97754，col=12，count=216，rate=5.55556%，jerk=0.11994，miniumu_ttc=111.30261\n\n 60%|██████    | 3/5 [00:34<00:22, 11.49s/it]\u001b[A[2024-07-23 12:33:46,427][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2059755\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.73s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.32s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.10s/it]\u001b[A\u001b[A\n[2024-07-23 12:33:52,999][1383994605.py][line:1089][INFO] mean_spacing_error：38.91257，col=24，count=216，rate=11.11111%，jerk=0.10938，miniumu_ttc=103.11412\n\n 80%|████████  | 4/5 [00:46<00:11, 11.86s/it]\u001b[A[2024-07-23 12:33:58,677][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1995032\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 12:34:04,622][1383994605.py][line:1089][INFO] mean_spacing_error：29.04051，col=9，count=216，rate=4.16667%，jerk=0.15987，miniumu_ttc=154.32326\n\n100%|██████████| 5/5 [00:58<00:00, 11.67s/it]\u001b[A\n 35%|███▌      | 175/500 [9:06:45<17:28:16, 193.53s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 12:35:48,464][1383994605.py][line:1670][INFO] 176/500. loss: 0.003085885817805926\n[2024-07-23 12:35:48,480][1383994605.py][line:1671][INFO] 176/500. mserror: 46.012969970703125  col: 33  count: 216  jerk: 0.07591577619314194  ttc: 157.68484497070312\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:35:53,723][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2878639\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 12:35:59,412][1383994605.py][line:1089][INFO] mean_spacing_error：29.78915，col=12，count=216，rate=5.55556%，jerk=0.13174，miniumu_ttc=179.60918\n\n 20%|██        | 1/5 [00:10<00:43, 10.93s/it]\u001b[A[2024-07-23 12:36:04,879][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2131431\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 12:36:10,279][1383994605.py][line:1089][INFO] mean_spacing_error：23.50513，col=9，count=216，rate=4.16667%，jerk=0.15807，miniumu_ttc=36.33540\n\n 40%|████      | 2/5 [00:21<00:32, 10.89s/it]\u001b[A[2024-07-23 12:36:15,713][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2258969\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:36:21,192][1383994605.py][line:1089][INFO] mean_spacing_error：33.08102，col=15，count=216，rate=6.94444%，jerk=0.12405，miniumu_ttc=106.07339\n\n 60%|██████    | 3/5 [00:32<00:21, 10.90s/it]\u001b[A[2024-07-23 12:36:26,653][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2009020\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:36:32,238][1383994605.py][line:1089][INFO] mean_spacing_error：38.53738，col=12，count=216，rate=5.55556%，jerk=0.12700，miniumu_ttc=146.33388\n\n 80%|████████  | 4/5 [00:43<00:10, 10.96s/it]\u001b[A[2024-07-23 12:36:37,744][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1919620\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:36:43,191][1383994605.py][line:1089][INFO] mean_spacing_error：26.36407，col=14，count=216，rate=6.48148%，jerk=0.15303，miniumu_ttc=88.09618\n\n100%|██████████| 5/5 [00:54<00:00, 10.94s/it]\u001b[A\n 35%|███▌      | 176/500 [9:09:24<16:28:23, 183.04s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 12:39:05,748][1383994605.py][line:1670][INFO] 177/500. loss: 0.0028332537040114403\n[2024-07-23 12:39:05,761][1383994605.py][line:1671][INFO] 177/500. mserror: 51.62275314331055  col: 33  count: 216  jerk: 0.07300417870283127  ttc: 145.9744873046875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:39:11,101][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2904557\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:39:16,597][1383994605.py][line:1089][INFO] mean_spacing_error：32.11254，col=11，count=216，rate=5.09259%，jerk=0.12922，miniumu_ttc=196.84662\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 12:39:22,068][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2234158\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:39:27,571][1383994605.py][line:1089][INFO] mean_spacing_error：21.15219，col=11，count=216，rate=5.09259%，jerk=0.18325，miniumu_ttc=22.74397\n\n 40%|████      | 2/5 [00:21<00:32, 10.92s/it]\u001b[A[2024-07-23 12:39:33,000][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2320686\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:39:38,481][1383994605.py][line:1089][INFO] mean_spacing_error：21.41492，col=13，count=216，rate=6.01852%，jerk=0.16677，miniumu_ttc=88.07668\n\n 60%|██████    | 3/5 [00:32<00:21, 10.91s/it]\u001b[A[2024-07-23 12:39:44,304][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1963207\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:39:49,892][1383994605.py][line:1089][INFO] mean_spacing_error：26.44292，col=12，count=216，rate=5.55556%，jerk=0.15131，miniumu_ttc=180.35669\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 12:39:55,289][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1906920\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 12:40:00,810][1383994605.py][line:1089][INFO] mean_spacing_error：37.31443，col=29，count=216，rate=13.42593%，jerk=0.14740，miniumu_ttc=80.45824\n\n100%|██████████| 5/5 [00:55<00:00, 11.01s/it]\u001b[A\n 35%|███▌      | 177/500 [9:12:41<16:48:54, 187.41s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.27s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 12:42:30,626][1383994605.py][line:1670][INFO] 178/500. loss: 0.0019111099342505138\n[2024-07-23 12:42:30,638][1383994605.py][line:1671][INFO] 178/500. mserror: 50.8026123046875  col: 33  count: 216  jerk: 0.07197654992341995  ttc: 279.3041076660156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:42:36,073][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2829050\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 12:42:41,616][1383994605.py][line:1089][INFO] mean_spacing_error：31.33119，col=13，count=216，rate=6.01852%，jerk=0.13077，miniumu_ttc=175.21387\n\n 20%|██        | 1/5 [00:10<00:43, 10.97s/it]\u001b[A[2024-07-23 12:42:47,145][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2137435\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 12:42:53,095][1383994605.py][line:1089][INFO] mean_spacing_error：24.39173，col=8，count=216，rate=3.70370%，jerk=0.15882，miniumu_ttc=111.30508\n\n 40%|████      | 2/5 [00:22<00:33, 11.27s/it]\u001b[A[2024-07-23 12:42:58,704][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2177359\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 12:43:04,438][1383994605.py][line:1089][INFO] mean_spacing_error：32.33070，col=20，count=216，rate=9.25926%，jerk=0.12302，miniumu_ttc=110.11038\n\n 60%|██████    | 3/5 [00:33<00:22, 11.30s/it]\u001b[A[2024-07-23 12:43:10,203][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2058698\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 12:43:15,856][1383994605.py][line:1089][INFO] mean_spacing_error：39.22217，col=14，count=216，rate=6.48148%，jerk=0.11721，miniumu_ttc=143.44139\n\n 80%|████████  | 4/5 [00:45<00:11, 11.35s/it]\u001b[A[2024-07-23 12:43:21,479][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1920924\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.78s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 12:43:27,568][1383994605.py][line:1089][INFO] mean_spacing_error：25.88693，col=11，count=216，rate=5.09259%，jerk=0.15144，miniumu_ttc=114.97079\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 36%|███▌      | 178/500 [9:16:08<17:16:54, 193.21s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 12:45:56,121][1383994605.py][line:1670][INFO] 179/500. loss: 0.0031549005458752313\n[2024-07-23 12:45:56,138][1383994605.py][line:1671][INFO] 179/500. mserror: 48.45631790161133  col: 37  count: 216  jerk: 0.07177803665399551  ttc: 141.4044189453125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:46:01,565][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2840058\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.68s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 12:46:07,541][1383994605.py][line:1089][INFO] mean_spacing_error：28.87433，col=12，count=216，rate=5.55556%，jerk=0.13811，miniumu_ttc=176.94264\n\n 20%|██        | 1/5 [00:11<00:45, 11.40s/it]\u001b[A[2024-07-23 12:46:13,103][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2076772\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.15s/it]\u001b[A\u001b[A\n[2024-07-23 12:46:19,834][1383994605.py][line:1089][INFO] mean_spacing_error：24.65265，col=9，count=216，rate=4.16667%，jerk=0.15173，miniumu_ttc=110.68026\n\n 40%|████      | 2/5 [00:23<00:35, 11.92s/it]\u001b[A[2024-07-23 12:46:25,687][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2013672\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:46:31,121][1383994605.py][line:1089][INFO] mean_spacing_error：37.16087，col=35，count=216，rate=16.20370%，jerk=0.11456，miniumu_ttc=21.78569\n\n 60%|██████    | 3/5 [00:34<00:23, 11.63s/it]\u001b[A[2024-07-23 12:46:37,079][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2250440\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 12:46:42,672][1383994605.py][line:1089][INFO] mean_spacing_error：43.38424，col=20，count=216，rate=9.25926%，jerk=0.10895，miniumu_ttc=138.59448\n\n 80%|████████  | 4/5 [00:46<00:11, 11.61s/it]\u001b[A[2024-07-23 12:46:48,474][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1978989\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:46:53,900][1383994605.py][line:1089][INFO] mean_spacing_error：28.14195，col=10，count=216，rate=4.62963%，jerk=0.15469，miniumu_ttc=140.80888\n\n100%|██████████| 5/5 [00:57<00:00, 11.55s/it]\u001b[A\n 36%|███▌      | 179/500 [9:19:34<17:34:45, 197.15s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.68s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 12:49:20,742][1383994605.py][line:1670][INFO] 180/500. loss: 0.00254603149369359\n[2024-07-23 12:49:20,754][1383994605.py][line:1671][INFO] 180/500. mserror: 44.72059631347656  col: 35  count: 216  jerk: 0.0758754089474678  ttc: 151.33628845214844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:49:25,975][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2899034\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 12:49:31,586][1383994605.py][line:1089][INFO] mean_spacing_error：41.19350，col=7，count=216，rate=3.24074%，jerk=0.13958，miniumu_ttc=212.34143\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 12:49:37,061][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2340129\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:49:42,533][1383994605.py][line:1089][INFO] mean_spacing_error：35.66796，col=20，count=216，rate=9.25926%，jerk=0.18686，miniumu_ttc=10.91010\n\n 40%|████      | 2/5 [00:21<00:32, 10.90s/it]\u001b[A[2024-07-23 12:49:48,268][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2485427\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:49:53,740][1383994605.py][line:1089][INFO] mean_spacing_error：21.54524，col=8，count=216，rate=3.70370%，jerk=0.16871，miniumu_ttc=146.37218\n\n 60%|██████    | 3/5 [00:32<00:22, 11.04s/it]\u001b[A[2024-07-23 12:49:59,554][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2011352\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.62s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.11s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 12:50:05,784][1383994605.py][line:1089][INFO] mean_spacing_error：26.93949，col=20，count=216，rate=9.25926%，jerk=0.12895，miniumu_ttc=98.47295\n\n 80%|████████  | 4/5 [00:45<00:11, 11.43s/it]\u001b[A[2024-07-23 12:50:11,720][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2019713\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.00s/it]\u001b[A\u001b[A\n[2024-07-23 12:50:18,017][1383994605.py][line:1089][INFO] mean_spacing_error：24.96972，col=12，count=216，rate=5.55556%，jerk=0.13989，miniumu_ttc=146.70691\n\n100%|██████████| 5/5 [00:57<00:00, 11.45s/it]\u001b[A\n 36%|███▌      | 180/500 [9:22:58<17:42:36, 199.24s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 12:52:44,976][1383994605.py][line:1670][INFO] 181/500. loss: 0.001384565606713295\n[2024-07-23 12:52:44,989][1383994605.py][line:1671][INFO] 181/500. mserror: 42.34455490112305  col: 35  count: 216  jerk: 0.07708535343408585  ttc: 143.25086975097656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:52:50,201][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2913118\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:52:55,648][1383994605.py][line:1089][INFO] mean_spacing_error：44.66657，col=7，count=216，rate=3.24074%，jerk=0.13827，miniumu_ttc=222.97658\n\n 20%|██        | 1/5 [00:10<00:42, 10.66s/it]\u001b[A[2024-07-23 12:53:01,137][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2569692\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 12:53:06,608][1383994605.py][line:1089][INFO] mean_spacing_error：33.43113，col=23，count=216，rate=10.64815%，jerk=0.17094，miniumu_ttc=17.13534\n\n 40%|████      | 2/5 [00:21<00:32, 10.83s/it]\u001b[A[2024-07-23 12:53:12,171][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2402540\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 12:53:17,588][1383994605.py][line:1089][INFO] mean_spacing_error：23.36514，col=11，count=216，rate=5.09259%，jerk=0.15410，miniumu_ttc=294.88333\n\n 60%|██████    | 3/5 [00:32<00:21, 10.90s/it]\u001b[A[2024-07-23 12:53:22,938][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2031825\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 12:53:28,350][1383994605.py][line:1089][INFO] mean_spacing_error：27.73890，col=24，count=216，rate=11.11111%，jerk=0.12836，miniumu_ttc=88.31655\n\n 80%|████████  | 4/5 [00:43<00:10, 10.85s/it]\u001b[A[2024-07-23 12:53:33,935][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2064570\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 12:53:39,728][1383994605.py][line:1089][INFO] mean_spacing_error：26.33921，col=9，count=216，rate=4.16667%，jerk=0.15088，miniumu_ttc=144.16292\n\n100%|██████████| 5/5 [00:54<00:00, 10.95s/it]\u001b[A\n 36%|███▌      | 181/500 [9:26:20<17:43:13, 199.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 12:55:30,471][1383994605.py][line:1670][INFO] 182/500. loss: 0.0028979737932483354\n[2024-07-23 12:55:30,485][1383994605.py][line:1671][INFO] 182/500. mserror: 38.18587112426758  col: 38  count: 216  jerk: 0.07865668088197708  ttc: 111.71002960205078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:55:35,660][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2869697\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 12:55:41,182][1383994605.py][line:1089][INFO] mean_spacing_error：27.42163，col=13，count=216，rate=6.01852%，jerk=0.14509，miniumu_ttc=172.14868\n\n 20%|██        | 1/5 [00:10<00:42, 10.69s/it]\u001b[A[2024-07-23 12:55:46,710][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2106409\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 12:55:52,892][1383994605.py][line:1089][INFO] mean_spacing_error：27.86652，col=10，count=216，rate=4.62963%，jerk=0.13147，miniumu_ttc=110.71833\n\n 40%|████      | 2/5 [00:22<00:33, 11.29s/it]\u001b[A[2024-07-23 12:55:58,539][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1988672\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 12:56:04,181][1383994605.py][line:1089][INFO] mean_spacing_error：34.22020，col=16，count=216，rate=7.40741%，jerk=0.12308，miniumu_ttc=123.65755\n\n 60%|██████    | 3/5 [00:33<00:22, 11.29s/it]\u001b[A[2024-07-23 12:56:10,163][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1976652\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 12:56:15,862][1383994605.py][line:1089][INFO] mean_spacing_error：29.80096，col=6，count=216，rate=2.77778%，jerk=0.18110，miniumu_ttc=220.65845\n\n 80%|████████  | 4/5 [00:45<00:11, 11.44s/it]\u001b[A[2024-07-23 12:56:21,250][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2071758\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 12:56:26,810][1383994605.py][line:1089][INFO] mean_spacing_error：29.16496，col=14，count=216，rate=6.48148%，jerk=0.18817，miniumu_ttc=18.85099\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 36%|███▋      | 182/500 [9:29:07<16:47:34, 190.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 12:58:47,790][1383994605.py][line:1670][INFO] 183/500. loss: 0.0013934684296449025\n[2024-07-23 12:58:47,802][1383994605.py][line:1671][INFO] 183/500. mserror: 37.48995590209961  col: 42  count: 216  jerk: 0.07742924243211746  ttc: 99.76606750488281\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 12:58:52,820][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2906581\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 12:58:58,140][1383994605.py][line:1089][INFO] mean_spacing_error：27.88913，col=13，count=216，rate=6.01852%，jerk=0.14432，miniumu_ttc=173.61295\n\n 20%|██        | 1/5 [00:10<00:41, 10.33s/it]\u001b[A[2024-07-23 12:59:03,463][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2151038\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 12:59:08,823][1383994605.py][line:1089][INFO] mean_spacing_error：27.42426，col=10，count=216，rate=4.62963%，jerk=0.13187，miniumu_ttc=105.53713\n\n 40%|████      | 2/5 [00:21<00:31, 10.54s/it]\u001b[A[2024-07-23 12:59:13,991][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1995398\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 12:59:19,593][1383994605.py][line:1089][INFO] mean_spacing_error：35.81992，col=24，count=216，rate=11.11111%，jerk=0.12554，miniumu_ttc=85.46404\n\n 60%|██████    | 3/5 [00:31<00:21, 10.64s/it]\u001b[A[2024-07-23 12:59:24,993][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2042576\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 12:59:30,427][1383994605.py][line:1089][INFO] mean_spacing_error：33.13678，col=10，count=216，rate=4.62963%，jerk=0.15805，miniumu_ttc=134.12683\n\n 80%|████████  | 4/5 [00:42<00:10, 10.72s/it]\u001b[A[2024-07-23 12:59:35,888][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1914923\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 12:59:41,212][1383994605.py][line:1089][INFO] mean_spacing_error：26.18704，col=17，count=216，rate=7.87037%，jerk=0.14754，miniumu_ttc=36.83563\n\n100%|██████████| 5/5 [00:53<00:00, 10.68s/it]\u001b[A\n 37%|███▋      | 183/500 [9:32:22<16:51:12, 191.39s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.06s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:05,  1.72s/it]\u001b[A\n[2024-07-23 13:01:27,173][1383994605.py][line:1670][INFO] 184/500. loss: 0.0016224818925062816\n[2024-07-23 13:01:27,190][1383994605.py][line:1671][INFO] 184/500. mserror: 39.41226577758789  col: 43  count: 216  jerk: 0.0731198713183403  ttc: 105.26531219482422\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:01:32,212][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2912268\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 13:01:37,575][1383994605.py][line:1089][INFO] mean_spacing_error：27.64513，col=9，count=216，rate=4.16667%，jerk=0.13886，miniumu_ttc=182.31792\n\n 20%|██        | 1/5 [00:10<00:41, 10.38s/it]\u001b[A[2024-07-23 13:01:42,932][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2103407\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 13:01:48,533][1383994605.py][line:1089][INFO] mean_spacing_error：25.15553，col=11，count=216，rate=5.09259%，jerk=0.13782，miniumu_ttc=120.45539\n\n 40%|████      | 2/5 [00:21<00:32, 10.72s/it]\u001b[A[2024-07-23 13:01:53,968][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1975634\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:01:59,650][1383994605.py][line:1089][INFO] mean_spacing_error：40.37713，col=36，count=216，rate=16.66667%，jerk=0.11635，miniumu_ttc=16.50116\n\n 60%|██████    | 3/5 [00:32<00:21, 10.90s/it]\u001b[A[2024-07-23 13:02:05,145][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2298769\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:02:10,618][1383994605.py][line:1089][INFO] mean_spacing_error：38.03340，col=15，count=216，rate=6.94444%，jerk=0.12722，miniumu_ttc=147.25081\n\n 80%|████████  | 4/5 [00:43<00:10, 10.93s/it]\u001b[A[2024-07-23 13:02:16,224][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1890676\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:02:21,756][1383994605.py][line:1089][INFO] mean_spacing_error：30.46174，col=23，count=216，rate=10.64815%，jerk=0.12913，miniumu_ttc=50.19711\n\n100%|██████████| 5/5 [00:54<00:00, 10.91s/it]\u001b[A\n 37%|███▋      | 184/500 [9:35:02<15:59:16, 182.14s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.54s/it]\u001b[A\n2it [00:03,  1.89s/it]\u001b[A\n3it [00:05,  1.78s/it]\u001b[A\n[2024-07-23 13:04:03,098][1383994605.py][line:1670][INFO] 185/500. loss: 0.002885029340783755\n[2024-07-23 13:04:03,111][1383994605.py][line:1671][INFO] 185/500. mserror: 41.01664733886719  col: 44  count: 216  jerk: 0.07020223885774612  ttc: 137.1502227783203\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:04:09,036][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2899073\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 13:04:14,375][1383994605.py][line:1089][INFO] mean_spacing_error：28.72341，col=13，count=216，rate=6.01852%，jerk=0.13404，miniumu_ttc=179.43495\n\n 20%|██        | 1/5 [00:11<00:45, 11.25s/it]\u001b[A[2024-07-23 13:04:19,957][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2085622\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 13:04:25,271][1383994605.py][line:1089][INFO] mean_spacing_error：25.78796，col=9，count=216，rate=4.16667%，jerk=0.14018，miniumu_ttc=136.09274\n\n 40%|████      | 2/5 [00:22<00:33, 11.04s/it]\u001b[A[2024-07-23 13:04:30,807][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1992567\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 13:04:36,213][1383994605.py][line:1089][INFO] mean_spacing_error：39.00391，col=34，count=216，rate=15.74074%，jerk=0.11515，miniumu_ttc=21.49464\n\n 60%|██████    | 3/5 [00:33<00:21, 11.00s/it]\u001b[A[2024-07-23 13:04:41,753][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2328852\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 13:04:47,252][1383994605.py][line:1089][INFO] mean_spacing_error：37.29531，col=16，count=216，rate=7.40741%，jerk=0.12250，miniumu_ttc=141.32904\n\n 80%|████████  | 4/5 [00:44<00:11, 11.01s/it]\u001b[A[2024-07-23 13:04:52,649][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1960070\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 13:04:57,982][1383994605.py][line:1089][INFO] mean_spacing_error：30.89129，col=17，count=216，rate=7.87037%，jerk=0.12404，miniumu_ttc=109.66858\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 37%|███▋      | 185/500 [9:37:38<15:15:26, 174.37s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 13:07:21,760][1383994605.py][line:1670][INFO] 186/500. loss: 0.001714770992596944\n[2024-07-23 13:07:21,773][1383994605.py][line:1671][INFO] 186/500. mserror: 42.79906463623047  col: 45  count: 216  jerk: 0.06730920076370239  ttc: 102.39266967773438\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:07:26,905][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3004406\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 13:07:32,304][1383994605.py][line:1089][INFO] mean_spacing_error：33.12345，col=8，count=216，rate=3.70370%，jerk=0.12786，miniumu_ttc=203.55664\n\n 20%|██        | 1/5 [00:10<00:42, 10.52s/it]\u001b[A[2024-07-23 13:07:37,681][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2096471\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 13:07:43,065][1383994605.py][line:1089][INFO] mean_spacing_error：19.44176，col=10，count=216，rate=4.62963%，jerk=0.16160，miniumu_ttc=114.27779\n\n 40%|████      | 2/5 [00:21<00:32, 10.67s/it]\u001b[A[2024-07-23 13:07:48,824][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2293516\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 13:07:54,188][1383994605.py][line:1089][INFO] mean_spacing_error：27.68182，col=14，count=216，rate=6.48148%，jerk=0.12843，miniumu_ttc=114.37363\n\n 60%|██████    | 3/5 [00:32<00:21, 10.88s/it]\u001b[A[2024-07-23 13:07:59,947][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2032544\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 13:08:05,838][1383994605.py][line:1089][INFO] mean_spacing_error：32.10704，col=14，count=216，rate=6.48148%，jerk=0.12667，miniumu_ttc=140.26102\n\n 80%|████████  | 4/5 [00:44<00:11, 11.18s/it]\u001b[A[2024-07-23 13:08:11,504][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1907906\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 13:08:16,898][1383994605.py][line:1089][INFO] mean_spacing_error：26.27106，col=12，count=216，rate=5.55556%，jerk=0.14285，miniumu_ttc=99.42212\n\n100%|██████████| 5/5 [00:55<00:00, 11.02s/it]\u001b[A\n 37%|███▋      | 186/500 [9:40:57<15:51:03, 181.73s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.16s/it]\u001b[A\n2it [00:05,  2.97s/it]\u001b[A\n3it [00:11,  3.91s/it]\u001b[A\n[2024-07-23 13:10:46,079][1383994605.py][line:1670][INFO] 187/500. loss: 0.003198935960729917\n[2024-07-23 13:10:46,103][1383994605.py][line:1671][INFO] 187/500. mserror: 41.683902740478516  col: 42  count: 216  jerk: 0.06945954263210297  ttc: 174.5053253173828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:10:51,487][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3039890\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:10:56,941][1383994605.py][line:1089][INFO] mean_spacing_error：47.39773，col=8，count=216，rate=3.70370%，jerk=0.12020，miniumu_ttc=268.94012\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 13:11:03,124][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2334039\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.69s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 13:11:09,283][1383994605.py][line:1089][INFO] mean_spacing_error：32.54509，col=19，count=216，rate=8.79630%，jerk=0.16875，miniumu_ttc=12.19649\n\n 40%|████      | 2/5 [00:23<00:35, 11.72s/it]\u001b[A[2024-07-23 13:11:14,887][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2328007\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:11:20,527][1383994605.py][line:1089][INFO] mean_spacing_error：22.80742，col=9，count=216，rate=4.16667%，jerk=0.16711，miniumu_ttc=167.43044\n\n 60%|██████    | 3/5 [00:34<00:23, 11.50s/it]\u001b[A[2024-07-23 13:11:26,134][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1985052\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 13:11:31,941][1383994605.py][line:1089][INFO] mean_spacing_error：24.74669，col=16，count=216，rate=7.40741%，jerk=0.13290，miniumu_ttc=96.96648\n\n 80%|████████  | 4/5 [00:45<00:11, 11.47s/it]\u001b[A[2024-07-23 13:11:38,096][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2014742\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.56s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 13:11:44,059][1383994605.py][line:1089][INFO] mean_spacing_error：24.19532，col=11，count=216，rate=5.09259%，jerk=0.13848，miniumu_ttc=147.87788\n\n100%|██████████| 5/5 [00:57<00:00, 11.59s/it]\u001b[A\n 37%|███▋      | 187/500 [9:44:24<16:27:51, 189.36s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.46s/it]\u001b[A\n2it [00:04,  2.03s/it]\u001b[A\n3it [00:05,  1.88s/it]\u001b[A\n[2024-07-23 13:14:18,576][1383994605.py][line:1670][INFO] 188/500. loss: 0.0013495724027355511\n[2024-07-23 13:14:18,589][1383994605.py][line:1671][INFO] 188/500. mserror: 38.08644485473633  col: 41  count: 216  jerk: 0.07355204969644547  ttc: 159.1888885498047\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:14:24,014][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3192988\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 13:14:29,640][1383994605.py][line:1089][INFO] mean_spacing_error：53.69644，col=9，count=216，rate=4.16667%，jerk=0.10037，miniumu_ttc=242.58267\n\n 20%|██        | 1/5 [00:11<00:44, 11.04s/it]\u001b[A[2024-07-23 13:14:35,287][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2711731\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 13:14:41,254][1383994605.py][line:1089][INFO] mean_spacing_error：41.89402，col=45，count=216，rate=20.83333%，jerk=0.13658，miniumu_ttc=12.25623\n\n 40%|████      | 2/5 [00:22<00:34, 11.39s/it]\u001b[A[2024-07-23 13:14:47,046][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3085508\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:14:52,691][1383994605.py][line:1089][INFO] mean_spacing_error：25.41267，col=13，count=216，rate=6.01852%，jerk=0.12395，miniumu_ttc=125.03115\n\n 60%|██████    | 3/5 [00:34<00:22, 11.41s/it]\u001b[A[2024-07-23 13:14:58,695][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2717506\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.12s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.96s/it]\u001b[A\u001b[A\n[2024-07-23 13:15:04,841][1383994605.py][line:1089][INFO] mean_spacing_error：62.78211，col=12，count=216，rate=5.55556%，jerk=0.08872，miniumu_ttc=172.43182\n\n 80%|████████  | 4/5 [00:46<00:11, 11.70s/it]\u001b[A[2024-07-23 13:15:10,477][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2296762\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 13:15:16,195][1383994605.py][line:1089][INFO] mean_spacing_error：35.81356，col=23，count=216，rate=10.64815%，jerk=0.11608，miniumu_ttc=89.78465\n\n100%|██████████| 5/5 [00:57<00:00, 11.52s/it]\u001b[A\n 38%|███▊      | 188/500 [9:47:57<17:00:11, 196.19s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 13:17:02,214][1383994605.py][line:1670][INFO] 189/500. loss: 0.002873421957095464\n[2024-07-23 13:17:02,224][1383994605.py][line:1671][INFO] 189/500. mserror: 33.524322509765625  col: 42  count: 216  jerk: 0.07683687657117844  ttc: 102.0678482055664\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:17:07,866][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3309349\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 13:17:13,615][1383994605.py][line:1089][INFO] mean_spacing_error：56.22102，col=9，count=216，rate=4.16667%，jerk=0.10339，miniumu_ttc=225.87648\n\n 20%|██        | 1/5 [00:11<00:45, 11.38s/it]\u001b[A[2024-07-23 13:17:19,364][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2834248\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 13:17:25,162][1383994605.py][line:1089][INFO] mean_spacing_error：37.84877，col=44，count=216，rate=20.37037%，jerk=0.12256，miniumu_ttc=12.30795\n\n 40%|████      | 2/5 [00:22<00:34, 11.48s/it]\u001b[A[2024-07-23 13:17:30,823][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2947070\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:17:36,477][1383994605.py][line:1089][INFO] mean_spacing_error：30.95383，col=18，count=216，rate=8.33333%，jerk=0.11212，miniumu_ttc=136.80432\n\n 60%|██████    | 3/5 [00:34<00:22, 11.40s/it]\u001b[A[2024-07-23 13:17:42,061][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2492488\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 13:17:47,967][1383994605.py][line:1089][INFO] mean_spacing_error：57.22396，col=18，count=216，rate=8.33333%，jerk=0.08560，miniumu_ttc=151.05925\n\n 80%|████████  | 4/5 [00:45<00:11, 11.44s/it]\u001b[A[2024-07-23 13:17:54,012][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2274916\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.45s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.10s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.99s/it]\u001b[A\u001b[A\n[2024-07-23 13:18:00,260][1383994605.py][line:1089][INFO] mean_spacing_error：34.62691，col=15，count=216，rate=6.94444%，jerk=0.11753，miniumu_ttc=93.95259\n\n100%|██████████| 5/5 [00:58<00:00, 11.61s/it]\u001b[A\n 38%|███▊      | 189/500 [9:50:41<16:06:58, 186.56s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.37s/it]\u001b[A\n2it [00:03,  1.86s/it]\u001b[A\n3it [00:05,  1.73s/it]\u001b[A\n[2024-07-23 13:20:37,619][1383994605.py][line:1670][INFO] 190/500. loss: 0.0017301530266801517\n[2024-07-23 13:20:37,629][1383994605.py][line:1671][INFO] 190/500. mserror: 32.303428649902344  col: 46  count: 216  jerk: 0.07767287641763687  ttc: 203.5433807373047\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:20:42,893][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3089843\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 13:20:48,486][1383994605.py][line:1089][INFO] mean_spacing_error：38.43001，col=7，count=216，rate=3.24074%，jerk=0.14011，miniumu_ttc=254.00142\n\n 20%|██        | 1/5 [00:10<00:43, 10.85s/it]\u001b[A[2024-07-23 13:20:53,907][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2271252\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:20:59,367][1383994605.py][line:1089][INFO] mean_spacing_error：23.33278，col=14，count=216，rate=6.48148%，jerk=0.15557，miniumu_ttc=110.18205\n\n 40%|████      | 2/5 [00:21<00:32, 10.87s/it]\u001b[A[2024-07-23 13:21:04,776][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2060944\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 13:21:10,560][1383994605.py][line:1089][INFO] mean_spacing_error：24.77455，col=15，count=216，rate=6.94444%，jerk=0.15517，miniumu_ttc=135.12962\n\n 60%|██████    | 3/5 [00:32<00:22, 11.02s/it]\u001b[A[2024-07-23 13:21:16,048][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1895365\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:21:21,561][1383994605.py][line:1089][INFO] mean_spacing_error：18.23980，col=7，count=216，rate=3.24074%，jerk=0.18194，miniumu_ttc=136.68846\n\n 80%|████████  | 4/5 [00:43<00:11, 11.01s/it]\u001b[A[2024-07-23 13:21:27,190][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1943378\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:21:32,883][1383994605.py][line:1089][INFO] mean_spacing_error：19.78502，col=16，count=216，rate=7.40741%，jerk=0.15620，miniumu_ttc=90.35398\n\n100%|██████████| 5/5 [00:55<00:00, 11.05s/it]\u001b[A\n 38%|███▊      | 190/500 [9:54:13<16:44:16, 194.38s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.17s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 13:24:01,271][1383994605.py][line:1670][INFO] 191/500. loss: 0.002826691915591558\n[2024-07-23 13:24:01,279][1383994605.py][line:1671][INFO] 191/500. mserror: 32.486907958984375  col: 46  count: 216  jerk: 0.07749752700328827  ttc: 321.9501953125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:24:06,932][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3194720\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:24:12,462][1383994605.py][line:1089][INFO] mean_spacing_error：51.17649，col=6，count=216，rate=2.77778%，jerk=0.13099，miniumu_ttc=222.53778\n\n 20%|██        | 1/5 [00:11<00:44, 11.17s/it]\u001b[A[2024-07-23 13:24:18,585][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2712478\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 13:24:24,295][1383994605.py][line:1089][INFO] mean_spacing_error：38.23428，col=54，count=216，rate=25.00000%，jerk=0.12477，miniumu_ttc=7.48704\n\n 40%|████      | 2/5 [00:23<00:34, 11.56s/it]\u001b[A[2024-07-23 13:24:30,064][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2717775\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:24:35,753][1383994605.py][line:1089][INFO] mean_spacing_error：37.97034，col=16，count=216，rate=7.40741%，jerk=0.10606，miniumu_ttc=184.59323\n\n 60%|██████    | 3/5 [00:34<00:23, 11.51s/it]\u001b[A[2024-07-23 13:24:41,557][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2063914\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 13:24:47,191][1383994605.py][line:1089][INFO] mean_spacing_error：28.43269，col=26，count=216，rate=12.03704%，jerk=0.13254，miniumu_ttc=20.05130\n\n 80%|████████  | 4/5 [00:45<00:11, 11.48s/it]\u001b[A[2024-07-23 13:24:53,173][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2065665\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 13:24:59,061][1383994605.py][line:1089][INFO] mean_spacing_error：18.88081，col=13，count=216，rate=6.01852%，jerk=0.15371，miniumu_ttc=145.81700\n\n100%|██████████| 5/5 [00:57<00:00, 11.56s/it]\u001b[A\n 38%|███▊      | 191/500 [9:57:39<16:59:16, 197.92s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.10s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 13:27:27,793][1383994605.py][line:1670][INFO] 192/500. loss: 0.0028235924740632377\n[2024-07-23 13:27:27,806][1383994605.py][line:1671][INFO] 192/500. mserror: 33.53574752807617  col: 39  count: 216  jerk: 0.07862145453691483  ttc: 136.36265563964844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:27:32,912][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3377780\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 13:27:38,236][1383994605.py][line:1089][INFO] mean_spacing_error：55.43040，col=11，count=216，rate=5.09259%，jerk=0.09535，miniumu_ttc=220.87100\n\n 20%|██        | 1/5 [00:10<00:41, 10.43s/it]\u001b[A[2024-07-23 13:27:43,599][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2743159\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:27:49,237][1383994605.py][line:1089][INFO] mean_spacing_error：40.32586，col=61，count=216，rate=28.24074%，jerk=0.10941，miniumu_ttc=9.39923\n\n 40%|████      | 2/5 [00:21<00:32, 10.77s/it]\u001b[A[2024-07-23 13:27:54,635][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3164366\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 13:28:00,220][1383994605.py][line:1089][INFO] mean_spacing_error：32.61032，col=16，count=216，rate=7.40741%，jerk=0.10932，miniumu_ttc=151.52417\n\n 60%|██████    | 3/5 [00:32<00:21, 10.86s/it]\u001b[A[2024-07-23 13:28:05,674][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2419139\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 13:28:11,110][1383994605.py][line:1089][INFO] mean_spacing_error：51.45833，col=23，count=216，rate=10.64815%，jerk=0.08727，miniumu_ttc=145.21115\n\n 80%|████████  | 4/5 [00:43<00:10, 10.87s/it]\u001b[A[2024-07-23 13:28:16,590][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2290683\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:28:22,131][1383994605.py][line:1089][INFO] mean_spacing_error：41.24137，col=24，count=216，rate=11.11111%，jerk=0.12961，miniumu_ttc=86.71009\n\n100%|██████████| 5/5 [00:54<00:00, 10.87s/it]\u001b[A\n 38%|███▊      | 192/500 [10:01:03<17:03:54, 199.46s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.68s/it]\u001b[A\n[2024-07-23 13:30:14,455][1383994605.py][line:1670][INFO] 193/500. loss: 0.0028251586481928825\n[2024-07-23 13:30:14,465][1383994605.py][line:1671][INFO] 193/500. mserror: 35.19062805175781  col: 36  count: 216  jerk: 0.07942657917737961  ttc: 142.02841186523438\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:30:19,683][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3265727\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:30:25,255][1383994605.py][line:1089][INFO] mean_spacing_error：58.68952，col=12，count=216，rate=5.55556%，jerk=0.09091，miniumu_ttc=227.23326\n\n 20%|██        | 1/5 [00:10<00:43, 10.78s/it]\u001b[A[2024-07-23 13:30:30,793][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2641089\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 13:30:36,650][1383994605.py][line:1089][INFO] mean_spacing_error：41.76013，col=60，count=216，rate=27.77778%，jerk=0.11798，miniumu_ttc=7.78347\n\n 40%|████      | 2/5 [00:22<00:33, 11.14s/it]\u001b[A[2024-07-23 13:30:42,257][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3352880\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:30:47,785][1383994605.py][line:1089][INFO] mean_spacing_error：27.84773，col=16，count=216，rate=7.40741%，jerk=0.11598，miniumu_ttc=107.58663\n\n 60%|██████    | 3/5 [00:33<00:22, 11.14s/it]\u001b[A[2024-07-23 13:30:53,589][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2887347\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:30:59,069][1383994605.py][line:1089][INFO] mean_spacing_error：70.29533，col=10，count=216，rate=4.62963%，jerk=0.09202，miniumu_ttc=211.72386\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 13:31:04,581][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2360802\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 13:31:10,333][1383994605.py][line:1089][INFO] mean_spacing_error：39.64798，col=34，count=216，rate=15.74074%，jerk=0.10067，miniumu_ttc=105.45142\n\n100%|██████████| 5/5 [00:55<00:00, 11.18s/it]\u001b[A\n 39%|███▊      | 193/500 [10:03:51<16:12:36, 190.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.13s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.73s/it]\u001b[A\n[2024-07-23 13:33:31,964][1383994605.py][line:1670][INFO] 194/500. loss: 0.0024773511104285717\n[2024-07-23 13:33:31,976][1383994605.py][line:1671][INFO] 194/500. mserror: 34.40446090698242  col: 34  count: 216  jerk: 0.08156868815422058  ttc: 581.8626708984375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:33:37,959][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3292233\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 13:33:43,387][1383994605.py][line:1089][INFO] mean_spacing_error：53.02317，col=13，count=216，rate=6.01852%，jerk=0.09445，miniumu_ttc=209.65987\n\n 20%|██        | 1/5 [00:11<00:45, 11.40s/it]\u001b[A[2024-07-23 13:33:49,394][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2612848\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 13:33:54,817][1383994605.py][line:1089][INFO] mean_spacing_error：49.00624，col=72，count=216，rate=33.33333%，jerk=0.11173，miniumu_ttc=5.88780\n\n 40%|████      | 2/5 [00:22<00:34, 11.42s/it]\u001b[A[2024-07-23 13:34:00,415][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3606673\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:34:06,048][1383994605.py][line:1089][INFO] mean_spacing_error：30.71255，col=21，count=216，rate=9.72222%，jerk=0.10780，miniumu_ttc=113.93936\n\n 60%|██████    | 3/5 [00:34<00:22, 11.33s/it]\u001b[A[2024-07-23 13:34:11,623][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2898998\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:34:17,275][1383994605.py][line:1089][INFO] mean_spacing_error：71.88220，col=10，count=216，rate=4.62963%，jerk=0.08912，miniumu_ttc=215.24265\n\n 80%|████████  | 4/5 [00:45<00:11, 11.29s/it]\u001b[A[2024-07-23 13:34:23,328][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2389460\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 13:34:28,903][1383994605.py][line:1089][INFO] mean_spacing_error：40.79675，col=35，count=216，rate=16.20370%，jerk=0.10207，miniumu_ttc=85.57406\n\n100%|██████████| 5/5 [00:56<00:00, 11.39s/it]\u001b[A\n 39%|███▉      | 194/500 [10:07:09<16:22:24, 192.63s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 13:36:57,158][1383994605.py][line:1670][INFO] 195/500. loss: 0.0033076079562306404\n[2024-07-23 13:36:57,170][1383994605.py][line:1671][INFO] 195/500. mserror: 35.024696350097656  col: 34  count: 216  jerk: 0.08226938545703888  ttc: 142.07290649414062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:37:02,458][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3314744\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:37:07,927][1383994605.py][line:1089][INFO] mean_spacing_error：54.09572，col=9，count=216，rate=4.16667%，jerk=0.09910，miniumu_ttc=249.06111\n\n 20%|██        | 1/5 [00:10<00:43, 10.75s/it]\u001b[A[2024-07-23 13:37:13,431][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2594728\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:37:18,916][1383994605.py][line:1089][INFO] mean_spacing_error：52.17550，col=82，count=216，rate=37.96296%，jerk=0.11019，miniumu_ttc=4.97877\n\n 40%|████      | 2/5 [00:21<00:32, 10.89s/it]\u001b[A[2024-07-23 13:37:24,292][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3665939\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 13:37:29,718][1383994605.py][line:1089][INFO] mean_spacing_error：31.37016，col=21，count=216，rate=9.72222%，jerk=0.10443，miniumu_ttc=129.96797\n\n 60%|██████    | 3/5 [00:32<00:21, 10.85s/it]\u001b[A[2024-07-23 13:37:35,729][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2972188\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:37:41,190][1383994605.py][line:1089][INFO] mean_spacing_error：76.76415，col=10，count=216，rate=4.62963%，jerk=0.08757，miniumu_ttc=200.65150\n\n 80%|████████  | 4/5 [00:44<00:11, 11.10s/it]\u001b[A[2024-07-23 13:37:46,737][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2395739\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:37:52,268][1383994605.py][line:1089][INFO] mean_spacing_error：41.33292，col=35，count=216，rate=16.20370%，jerk=0.10331，miniumu_ttc=84.99164\n\n100%|██████████| 5/5 [00:55<00:00, 11.02s/it]\u001b[A\n 39%|███▉      | 195/500 [10:10:33<16:35:33, 195.85s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.47s/it]\u001b[A\n2it [00:03,  1.86s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 13:39:43,146][1383994605.py][line:1670][INFO] 196/500. loss: 0.0017549412635465462\n[2024-07-23 13:39:43,156][1383994605.py][line:1671][INFO] 196/500. mserror: 38.03508758544922  col: 37  count: 216  jerk: 0.07859545946121216  ttc: 131.67596435546875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:39:48,384][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3124505\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 13:39:53,963][1383994605.py][line:1089][INFO] mean_spacing_error：51.53157，col=6，count=216，rate=2.77778%，jerk=0.12978，miniumu_ttc=219.19353\n\n 20%|██        | 1/5 [00:10<00:43, 10.80s/it]\u001b[A[2024-07-23 13:39:59,364][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2891646\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 13:40:05,143][1383994605.py][line:1089][INFO] mean_spacing_error：52.34608，col=75，count=216，rate=34.72222%，jerk=0.12037，miniumu_ttc=56.23468\n\n 40%|████      | 2/5 [00:21<00:33, 11.02s/it]\u001b[A[2024-07-23 13:40:10,823][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3849926\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:40:16,362][1383994605.py][line:1089][INFO] mean_spacing_error：35.47903，col=35，count=216，rate=16.20370%，jerk=0.08897，miniumu_ttc=99.44851\n\n 60%|██████    | 3/5 [00:33<00:22, 11.11s/it]\u001b[A[2024-07-23 13:40:21,898][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2964164\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 13:40:27,617][1383994605.py][line:1089][INFO] mean_spacing_error：78.63140，col=10，count=216，rate=4.62963%，jerk=0.08753，miniumu_ttc=209.62885\n\n 80%|████████  | 4/5 [00:44<00:11, 11.17s/it]\u001b[A[2024-07-23 13:40:33,122][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2392180\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 13:40:38,737][1383994605.py][line:1089][INFO] mean_spacing_error：39.44754，col=35，count=216，rate=16.20370%，jerk=0.10482，miniumu_ttc=41.91365\n\n100%|██████████| 5/5 [00:55<00:00, 11.12s/it]\u001b[A\n 39%|███▉      | 196/500 [10:13:19<15:47:38, 187.03s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.71s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 13:42:16,230][1383994605.py][line:1670][INFO] 197/500. loss: 0.002797207795083523\n[2024-07-23 13:42:16,241][1383994605.py][line:1671][INFO] 197/500. mserror: 41.61851119995117  col: 38  count: 216  jerk: 0.07514921575784683  ttc: 131.47544860839844\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:42:21,725][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3014466\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 13:42:27,156][1383994605.py][line:1089][INFO] mean_spacing_error：52.88372，col=5，count=216，rate=2.31481%，jerk=0.14077，miniumu_ttc=253.53871\n\n 20%|██        | 1/5 [00:10<00:43, 10.90s/it]\u001b[A[2024-07-23 13:42:32,536][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2884891\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:42:38,094][1383994605.py][line:1089][INFO] mean_spacing_error：39.53404，col=47，count=216，rate=21.75926%，jerk=0.12254，miniumu_ttc=9.90410\n\n 40%|████      | 2/5 [00:21<00:32, 10.93s/it]\u001b[A[2024-07-23 13:42:43,478][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2721834\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 13:42:48,840][1383994605.py][line:1089][INFO] mean_spacing_error：44.39672，col=19，count=216，rate=8.79630%，jerk=0.09537，miniumu_ttc=169.15752\n\n 60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\u001b[A[2024-07-23 13:42:54,332][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2149316\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 13:42:59,776][1383994605.py][line:1089][INFO] mean_spacing_error：32.31851，col=27，count=216，rate=12.50000%，jerk=0.13293，miniumu_ttc=20.10001\n\n 80%|████████  | 4/5 [00:43<00:10, 10.88s/it]\u001b[A[2024-07-23 13:43:05,291][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2267594\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 13:43:10,781][1383994605.py][line:1089][INFO] mean_spacing_error：25.34879，col=13，count=216，rate=6.01852%，jerk=0.15890，miniumu_ttc=136.49870\n\n100%|██████████| 5/5 [00:54<00:00, 10.91s/it]\u001b[A\n 39%|███▉      | 197/500 [10:15:51<14:51:30, 176.54s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.73s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 13:45:29,385][1383994605.py][line:1670][INFO] 198/500. loss: 0.002518861088901758\n[2024-07-23 13:45:29,396][1383994605.py][line:1671][INFO] 198/500. mserror: 44.30802536010742  col: 34  count: 216  jerk: 0.07561855018138885  ttc: 142.94969177246094\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:45:34,597][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3120377\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:45:40,246][1383994605.py][line:1089][INFO] mean_spacing_error：45.14036，col=10，count=216，rate=4.62963%，jerk=0.11463，miniumu_ttc=196.55043\n\n 20%|██        | 1/5 [00:10<00:43, 10.84s/it]\u001b[A[2024-07-23 13:45:45,637][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2608128\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 13:45:51,023][1383994605.py][line:1089][INFO] mean_spacing_error：46.95337，col=68，count=216，rate=31.48148%，jerk=0.11129，miniumu_ttc=6.77942\n\n 40%|████      | 2/5 [00:21<00:32, 10.81s/it]\u001b[A[2024-07-23 13:45:56,474][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3674862\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 13:46:02,117][1383994605.py][line:1089][INFO] mean_spacing_error：31.98673，col=36，count=216，rate=16.66667%，jerk=0.09817，miniumu_ttc=90.31783\n\n 60%|██████    | 3/5 [00:32<00:21, 10.94s/it]\u001b[A[2024-07-23 13:46:07,680][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2771668\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 13:46:12,962][1383994605.py][line:1089][INFO] mean_spacing_error：62.83659，col=9，count=216，rate=4.16667%，jerk=0.10042，miniumu_ttc=215.49651\n\n 80%|████████  | 4/5 [00:43<00:10, 10.90s/it]\u001b[A[2024-07-23 13:46:18,706][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2273816\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:46:24,244][1383994605.py][line:1089][INFO] mean_spacing_error：36.29073，col=25，count=216，rate=11.57407%，jerk=0.11754，miniumu_ttc=89.14495\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 40%|███▉      | 198/500 [10:19:05<15:14:09, 181.62s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 13:48:50,033][1383994605.py][line:1670][INFO] 199/500. loss: 0.0013253944925963879\n[2024-07-23 13:48:50,049][1383994605.py][line:1671][INFO] 199/500. mserror: 48.09935760498047  col: 26  count: 216  jerk: 0.0777079239487648  ttc: 155.6889190673828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:48:55,320][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2947843\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:49:00,910][1383994605.py][line:1089][INFO] mean_spacing_error：49.89760，col=9，count=216，rate=4.16667%，jerk=0.10807，miniumu_ttc=227.80525\n\n 20%|██        | 1/5 [00:10<00:43, 10.85s/it]\u001b[A[2024-07-23 13:49:06,646][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2392294\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 13:49:12,344][1383994605.py][line:1089][INFO] mean_spacing_error：35.85854，col=19，count=216，rate=8.79630%，jerk=0.16797，miniumu_ttc=12.84681\n\n 40%|████      | 2/5 [00:22<00:33, 11.19s/it]\u001b[A[2024-07-23 13:49:17,881][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2409541\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 13:49:23,501][1383994605.py][line:1089][INFO] mean_spacing_error：24.80756，col=7，count=216，rate=3.24074%，jerk=0.17550，miniumu_ttc=149.89771\n\n 60%|██████    | 3/5 [00:33<00:22, 11.18s/it]\u001b[A[2024-07-23 13:49:29,010][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1998634\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 13:49:34,619][1383994605.py][line:1089][INFO] mean_spacing_error：27.27617，col=22，count=216，rate=10.18519%，jerk=0.12403，miniumu_ttc=98.15957\n\n 80%|████████  | 4/5 [00:44<00:11, 11.15s/it]\u001b[A[2024-07-23 13:49:40,238][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2034056\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 13:49:46,202][1383994605.py][line:1089][INFO] mean_spacing_error：24.25056，col=14，count=216，rate=6.48148%，jerk=0.13134，miniumu_ttc=138.66628\n\n100%|██████████| 5/5 [00:56<00:00, 11.23s/it]\u001b[A\n 40%|███▉      | 199/500 [10:22:27<15:41:42, 187.72s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.25s/it]\u001b[A\n2it [00:03,  1.80s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 13:52:20,773][1383994605.py][line:1670][INFO] 200/500. loss: 0.0025841202586889267\n[2024-07-23 13:52:20,786][1383994605.py][line:1671][INFO] 200/500. mserror: 46.533687591552734  col: 22  count: 216  jerk: 0.08275522291660309  ttc: 166.01373291015625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:52:26,548][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2928178\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:52:32,103][1383994605.py][line:1089][INFO] mean_spacing_error：50.70708，col=9，count=216，rate=4.16667%，jerk=0.10624，miniumu_ttc=239.54559\n\n 20%|██        | 1/5 [00:11<00:45, 11.31s/it]\u001b[A[2024-07-23 13:52:37,601][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2358179\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 13:52:43,151][1383994605.py][line:1089][INFO] mean_spacing_error：34.76758，col=19，count=216，rate=8.79630%，jerk=0.16374，miniumu_ttc=14.90729\n\n 40%|████      | 2/5 [00:22<00:33, 11.16s/it]\u001b[A[2024-07-23 13:52:48,839][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2314310\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 13:52:54,578][1383994605.py][line:1089][INFO] mean_spacing_error：25.08037，col=7，count=216，rate=3.24074%，jerk=0.18027，miniumu_ttc=136.97119\n\n 60%|██████    | 3/5 [00:33<00:22, 11.28s/it]\u001b[A[2024-07-23 13:53:00,051][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2009135\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:53:05,705][1383994605.py][line:1089][INFO] mean_spacing_error：23.38696，col=16，count=216，rate=7.40741%，jerk=0.13610，miniumu_ttc=106.94753\n\n 80%|████████  | 4/5 [00:44<00:11, 11.22s/it]\u001b[A[2024-07-23 13:53:11,196][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1966893\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:53:16,864][1383994605.py][line:1089][INFO] mean_spacing_error：25.42605，col=18，count=216，rate=8.33333%，jerk=0.12602，miniumu_ttc=104.94277\n\n100%|██████████| 5/5 [00:56<00:00, 11.21s/it]\u001b[A\n 40%|████      | 200/500 [10:25:57<16:13:00, 194.60s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.44s/it]\u001b[A\n2it [00:03,  1.90s/it]\u001b[A\n3it [00:05,  1.80s/it]\u001b[A\n[2024-07-23 13:55:05,749][1383994605.py][line:1670][INFO] 201/500. loss: 0.002865084446966648\n[2024-07-23 13:55:05,768][1383994605.py][line:1671][INFO] 201/500. mserror: 41.143070220947266  col: 23  count: 216  jerk: 0.08595316857099533  ttc: 148.1929931640625\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:55:10,920][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3147093\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 13:55:16,650][1383994605.py][line:1089][INFO] mean_spacing_error：50.14133，col=9，count=216，rate=4.16667%，jerk=0.10698，miniumu_ttc=211.26199\n\n 20%|██        | 1/5 [00:10<00:43, 10.88s/it]\u001b[A[2024-07-23 13:55:22,082][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2469300\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 13:55:27,993][1383994605.py][line:1089][INFO] mean_spacing_error：46.24366，col=66，count=216，rate=30.55556%，jerk=0.11797，miniumu_ttc=6.13343\n\n 40%|████      | 2/5 [00:22<00:33, 11.15s/it]\u001b[A[2024-07-23 13:55:33,757][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3325540\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 13:55:39,521][1383994605.py][line:1089][INFO] mean_spacing_error：29.09735，col=14，count=216，rate=6.48148%，jerk=0.11672，miniumu_ttc=140.87621\n\n 60%|██████    | 3/5 [00:33<00:22, 11.32s/it]\u001b[A[2024-07-23 13:55:45,192][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2757405\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.63s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 13:55:51,332][1383994605.py][line:1089][INFO] mean_spacing_error：63.55923，col=17，count=216，rate=7.87037%，jerk=0.08283，miniumu_ttc=188.42412\n\n 80%|████████  | 4/5 [00:45<00:11, 11.51s/it]\u001b[A[2024-07-23 13:55:56,958][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2355130\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 13:56:02,828][1383994605.py][line:1089][INFO] mean_spacing_error：36.98145，col=27，count=216，rate=12.50000%，jerk=0.10789，miniumu_ttc=99.05417\n\n100%|██████████| 5/5 [00:57<00:00, 11.41s/it]\u001b[A\n 40%|████      | 201/500 [10:28:43<15:26:58, 186.02s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 13:58:23,826][1383994605.py][line:1670][INFO] 202/500. loss: 0.0018855722931524117\n[2024-07-23 13:58:23,837][1383994605.py][line:1671][INFO] 202/500. mserror: 37.18439483642578  col: 27  count: 216  jerk: 0.08752954751253128  ttc: 159.30995178222656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 13:58:29,105][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3247488\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:58:34,634][1383994605.py][line:1089][INFO] mean_spacing_error：46.24494，col=6，count=216，rate=2.77778%，jerk=0.12970，miniumu_ttc=211.19456\n\n 20%|██        | 1/5 [00:10<00:43, 10.78s/it]\u001b[A[2024-07-23 13:58:40,116][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2456229\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.08s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 13:58:46,103][1383994605.py][line:1089][INFO] mean_spacing_error：43.68888，col=61，count=216，rate=28.24074%，jerk=0.11509，miniumu_ttc=7.61679\n\n 40%|████      | 2/5 [00:22<00:33, 11.19s/it]\u001b[A[2024-07-23 13:58:51,603][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2803070\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 13:58:57,150][1383994605.py][line:1089][INFO] mean_spacing_error：28.55771，col=11，count=216，rate=5.09259%，jerk=0.11972，miniumu_ttc=147.41275\n\n 60%|██████    | 3/5 [00:33<00:22, 11.12s/it]\u001b[A[2024-07-23 13:59:02,705][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1997245\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:59:08,383][1383994605.py][line:1089][INFO] mean_spacing_error：29.61495，col=24，count=216，rate=11.11111%，jerk=0.12796，miniumu_ttc=93.27123\n\n 80%|████████  | 4/5 [00:44<00:11, 11.17s/it]\u001b[A[2024-07-23 13:59:13,806][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2087921\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 13:59:19,476][1383994605.py][line:1089][INFO] mean_spacing_error：27.62109，col=14，count=216，rate=6.48148%，jerk=0.14847，miniumu_ttc=161.47200\n\n100%|██████████| 5/5 [00:55<00:00, 11.13s/it]\u001b[A\n 40%|████      | 202/500 [10:32:00<15:39:42, 189.20s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.67s/it]\u001b[A\n[2024-07-23 14:01:53,424][1383994605.py][line:1670][INFO] 203/500. loss: 0.0013121358739833038\n[2024-07-23 14:01:53,438][1383994605.py][line:1671][INFO] 203/500. mserror: 36.568878173828125  col: 34  count: 216  jerk: 0.08497998118400574  ttc: 117.33375549316406\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:01:58,945][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2998720\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 14:02:04,598][1383994605.py][line:1089][INFO] mean_spacing_error：53.04653，col=2，count=216，rate=0.92593%，jerk=0.15549，miniumu_ttc=243.80397\n\n 20%|██        | 1/5 [00:11<00:44, 11.15s/it]\u001b[A[2024-07-23 14:02:10,166][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2680725\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 14:02:15,951][1383994605.py][line:1089][INFO] mean_spacing_error：36.95520，col=44，count=216，rate=20.37037%，jerk=0.12601，miniumu_ttc=14.47365\n\n 40%|████      | 2/5 [00:22<00:33, 11.27s/it]\u001b[A[2024-07-23 14:02:21,625][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2434809\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.98s/it]\u001b[A\u001b[A\n[2024-07-23 14:02:27,854][1383994605.py][line:1089][INFO] mean_spacing_error：26.95431，col=11，count=216，rate=5.09259%，jerk=0.14785，miniumu_ttc=198.76018\n\n 60%|██████    | 3/5 [00:34<00:23, 11.56s/it]\u001b[A[2024-07-23 14:02:33,702][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2041975\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 14:02:39,438][1383994605.py][line:1089][INFO] mean_spacing_error：28.07123，col=20，count=216，rate=9.25926%，jerk=0.14860，miniumu_ttc=82.40174\n\n 80%|████████  | 4/5 [00:45<00:11, 11.57s/it]\u001b[A[2024-07-23 14:02:44,864][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2083797\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:02:50,537][1383994605.py][line:1089][INFO] mean_spacing_error：24.69593，col=9，count=216，rate=4.16667%，jerk=0.15295，miniumu_ttc=147.16934\n\n100%|██████████| 5/5 [00:57<00:00, 11.42s/it]\u001b[A\n 41%|████      | 203/500 [10:35:31<16:09:00, 195.76s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.20s/it]\u001b[A\n2it [00:03,  1.85s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 14:04:41,028][1383994605.py][line:1670][INFO] 204/500. loss: 0.002993559775253137\n[2024-07-23 14:04:41,048][1383994605.py][line:1671][INFO] 204/500. mserror: 37.56803894042969  col: 37  count: 216  jerk: 0.08214246481657028  ttc: 134.67686462402344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:04:46,279][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2967839\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 14:04:51,812][1383994605.py][line:1089][INFO] mean_spacing_error：47.42169，col=2，count=216，rate=0.92593%，jerk=0.15516，miniumu_ttc=247.01318\n\n 20%|██        | 1/5 [00:10<00:43, 10.76s/it]\u001b[A[2024-07-23 14:04:57,223][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2521788\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 14:05:02,930][1383994605.py][line:1089][INFO] mean_spacing_error：34.75253，col=25，count=216，rate=11.57407%，jerk=0.14691，miniumu_ttc=18.23057\n\n 40%|████      | 2/5 [00:21<00:32, 10.97s/it]\u001b[A[2024-07-23 14:05:08,842][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2221374\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 14:05:14,715][1383994605.py][line:1089][INFO] mean_spacing_error：28.98004，col=7，count=216，rate=3.24074%，jerk=0.21237，miniumu_ttc=130.50333\n\n 60%|██████    | 3/5 [00:33<00:22, 11.34s/it]\u001b[A[2024-07-23 14:05:20,494][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2429590\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:05:26,077][1383994605.py][line:1089][INFO] mean_spacing_error：26.37473，col=8，count=216，rate=3.70370%，jerk=0.21185，miniumu_ttc=125.71865\n\n 80%|████████  | 4/5 [00:45<00:11, 11.35s/it]\u001b[A[2024-07-23 14:05:31,782][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2436606\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 14:05:37,342][1383994605.py][line:1089][INFO] mean_spacing_error：21.62593，col=17，count=216，rate=7.87037%，jerk=0.15317，miniumu_ttc=101.44774\n\n100%|██████████| 5/5 [00:56<00:00, 11.26s/it]\u001b[A\n 41%|████      | 204/500 [10:38:18<15:22:54, 187.08s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 14:07:22,807][1383994605.py][line:1670][INFO] 205/500. loss: 0.003184161459406217\n[2024-07-23 14:07:22,810][1383994605.py][line:1671][INFO] 205/500. mserror: 39.36533737182617  col: 34  count: 216  jerk: 0.080496646463871  ttc: 157.142578125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:07:28,034][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3172841\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 14:07:33,463][1383994605.py][line:1089][INFO] mean_spacing_error：55.13623，col=5，count=216，rate=2.31481%，jerk=0.13240，miniumu_ttc=209.01839\n\n 20%|██        | 1/5 [00:10<00:42, 10.64s/it]\u001b[A[2024-07-23 14:07:39,046][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2735164\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 14:07:45,026][1383994605.py][line:1089][INFO] mean_spacing_error：52.99145，col=91，count=216，rate=42.12963%，jerk=0.10508，miniumu_ttc=4.91022\n\n 40%|████      | 2/5 [00:22<00:33, 11.18s/it]\u001b[A[2024-07-23 14:07:50,887][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3666703\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.71s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:07,  2.55s/it]\u001b[A\u001b[A\n[2024-07-23 14:07:58,902][1383994605.py][line:1089][INFO] mean_spacing_error：40.12180，col=33，count=216，rate=15.27778%，jerk=0.08733，miniumu_ttc=1087.14001\n\n 60%|██████    | 3/5 [00:36<00:24, 12.44s/it]\u001b[A[2024-07-23 14:08:06,422][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2851915\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.55s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 14:08:12,216][1383994605.py][line:1089][INFO] mean_spacing_error：68.86298，col=9，count=216，rate=4.16667%，jerk=0.09497，miniumu_ttc=214.37921\n\n 80%|████████  | 4/5 [00:49<00:12, 12.76s/it]\u001b[A[2024-07-23 14:08:18,028][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2306590\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:08:23,720][1383994605.py][line:1089][INFO] mean_spacing_error：37.97057，col=24，count=216，rate=11.11111%，jerk=0.11202，miniumu_ttc=81.79263\n\n100%|██████████| 5/5 [01:00<00:00, 12.18s/it]\u001b[A\n 41%|████      | 205/500 [10:41:04<14:49:14, 180.86s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.70s/it]\u001b[A\n3it [00:04,  1.62s/it]\u001b[A\n[2024-07-23 14:10:03,454][1383994605.py][line:1670][INFO] 206/500. loss: 0.0028745243325829506\n[2024-07-23 14:10:03,464][1383994605.py][line:1671][INFO] 206/500. mserror: 41.670921325683594  col: 25  count: 216  jerk: 0.08299834281206131  ttc: 145.77407836914062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:10:08,850][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3160586\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 14:10:14,326][1383994605.py][line:1089][INFO] mean_spacing_error：59.98724，col=8，count=216，rate=3.70370%，jerk=0.10705，miniumu_ttc=217.51453\n\n 20%|██        | 1/5 [00:10<00:43, 10.85s/it]\u001b[A[2024-07-23 14:10:20,143][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2677583\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 14:10:25,949][1383994605.py][line:1089][INFO] mean_spacing_error：51.96206，col=65，count=216，rate=30.09259%，jerk=0.12297，miniumu_ttc=13.04672\n\n 40%|████      | 2/5 [00:22<00:33, 11.31s/it]\u001b[A[2024-07-23 14:10:31,401][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3581962\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:10:36,658][1383994605.py][line:1089][INFO] mean_spacing_error：36.25883，col=26，count=216，rate=12.03704%，jerk=0.10164，miniumu_ttc=102.87457\n\n 60%|██████    | 3/5 [00:33<00:22, 11.03s/it]\u001b[A[2024-07-23 14:10:42,186][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2710822\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 14:10:47,577][1383994605.py][line:1089][INFO] mean_spacing_error：65.26131，col=10，count=216，rate=4.62963%，jerk=0.09331，miniumu_ttc=198.00726\n\n 80%|████████  | 4/5 [00:44<00:10, 10.99s/it]\u001b[A[2024-07-23 14:10:53,152][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2253806\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 14:10:58,962][1383994605.py][line:1089][INFO] mean_spacing_error：37.77423，col=26，count=216，rate=12.03704%，jerk=0.11049，miniumu_ttc=101.95130\n\n100%|██████████| 5/5 [00:55<00:00, 11.10s/it]\u001b[A\n 41%|████      | 206/500 [10:43:39<14:08:32, 173.17s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.22s/it]\u001b[A\n2it [00:03,  1.82s/it]\u001b[A\n3it [00:05,  1.73s/it]\u001b[A\n[2024-07-23 14:13:25,966][1383994605.py][line:1670][INFO] 207/500. loss: 0.002553562168031931\n[2024-07-23 14:13:25,980][1383994605.py][line:1671][INFO] 207/500. mserror: 38.24222946166992  col: 23  count: 216  jerk: 0.08824335038661957  ttc: 142.05563354492188\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:13:31,423][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3207008\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 14:13:37,217][1383994605.py][line:1089][INFO] mean_spacing_error：64.45899，col=7，count=216，rate=3.24074%，jerk=0.10775，miniumu_ttc=226.43114\n\n 20%|██        | 1/5 [00:11<00:44, 11.23s/it]\u001b[A[2024-07-23 14:13:43,122][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2718266\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.61s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.07s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.95s/it]\u001b[A\u001b[A\n[2024-07-23 14:13:49,248][1383994605.py][line:1089][INFO] mean_spacing_error：46.13523，col=56，count=216，rate=25.92593%，jerk=0.11709，miniumu_ttc=7.58238\n\n 40%|████      | 2/5 [00:23<00:35, 11.70s/it]\u001b[A[2024-07-23 14:13:55,046][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3400993\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 14:14:00,868][1383994605.py][line:1089][INFO] mean_spacing_error：37.30078，col=25，count=216，rate=11.57407%，jerk=0.09974，miniumu_ttc=103.08810\n\n 60%|██████    | 3/5 [00:34<00:23, 11.66s/it]\u001b[A[2024-07-23 14:14:07,176][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2697784\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 14:14:13,019][1383994605.py][line:1089][INFO] mean_spacing_error：65.57295，col=9，count=216，rate=4.16667%，jerk=0.09348，miniumu_ttc=907.17517\n\n 80%|████████  | 4/5 [00:47<00:11, 11.86s/it]\u001b[A[2024-07-23 14:14:19,021][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2249152\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 14:14:24,806][1383994605.py][line:1089][INFO] mean_spacing_error：37.53607，col=26，count=216，rate=12.03704%，jerk=0.11128，miniumu_ttc=88.15382\n\n100%|██████████| 5/5 [00:58<00:00, 11.76s/it]\u001b[A\n 41%|████▏     | 207/500 [10:47:05<14:53:31, 182.98s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.71s/it]\u001b[A\n2it [00:04,  2.04s/it]\u001b[A\n3it [00:05,  1.91s/it]\u001b[A\n[2024-07-23 14:16:50,587][1383994605.py][line:1670][INFO] 208/500. loss: 0.0014284684633215268\n[2024-07-23 14:16:50,602][1383994605.py][line:1671][INFO] 208/500. mserror: 33.50067901611328  col: 33  count: 216  jerk: 0.08857543021440506  ttc: 115.04485321044922\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:16:56,109][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3070915\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 14:17:01,733][1383994605.py][line:1089][INFO] mean_spacing_error：62.34380，col=3，count=216，rate=1.38889%，jerk=0.14507，miniumu_ttc=243.00508\n\n 20%|██        | 1/5 [00:11<00:44, 11.12s/it]\u001b[A[2024-07-23 14:17:07,480][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2927149\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:17:13,172][1383994605.py][line:1089][INFO] mean_spacing_error：31.66744，col=27，count=216，rate=12.50000%，jerk=0.12729，miniumu_ttc=81.88825\n\n 40%|████      | 2/5 [00:22<00:33, 11.31s/it]\u001b[A[2024-07-23 14:17:18,950][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2344697\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 14:17:24,506][1383994605.py][line:1089][INFO] mean_spacing_error：25.15303，col=12，count=216，rate=5.55556%，jerk=0.13656，miniumu_ttc=174.81166\n\n 60%|██████    | 3/5 [00:33<00:22, 11.32s/it]\u001b[A[2024-07-23 14:17:30,416][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2075676\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.50s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 14:17:36,280][1383994605.py][line:1089][INFO] mean_spacing_error：28.08118，col=17，count=216，rate=7.87037%，jerk=0.15587，miniumu_ttc=12.29766\n\n 80%|████████  | 4/5 [00:45<00:11, 11.50s/it]\u001b[A[2024-07-23 14:17:42,024][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2177142\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:17:47,705][1383994605.py][line:1089][INFO] mean_spacing_error：26.32332，col=7，count=216，rate=3.24074%，jerk=0.16057，miniumu_ttc=166.12514\n\n100%|██████████| 5/5 [00:57<00:00, 11.42s/it]\u001b[A\n 42%|████▏     | 208/500 [10:50:28<15:19:34, 188.95s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.84s/it]\u001b[A\n3it [00:05,  1.74s/it]\u001b[A\n[2024-07-23 14:19:39,178][1383994605.py][line:1670][INFO] 209/500. loss: 0.0028942767530679703\n[2024-07-23 14:19:39,198][1383994605.py][line:1671][INFO] 209/500. mserror: 33.59822082519531  col: 42  count: 216  jerk: 0.08818279206752777  ttc: 91.69092559814453\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:19:44,379][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2305572\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 14:19:50,204][1383994605.py][line:1089][INFO] mean_spacing_error：34.13995，col=12，count=216，rate=5.55556%，jerk=0.12637，miniumu_ttc=175.72202\n\n 20%|██        | 1/5 [00:11<00:44, 11.01s/it]\u001b[A[2024-07-23 14:19:55,773][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2181022\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.66s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.92s/it]\u001b[A\u001b[A\n[2024-07-23 14:20:01,844][1383994605.py][line:1089][INFO] mean_spacing_error：34.70529，col=11，count=216，rate=5.09259%，jerk=0.12088，miniumu_ttc=144.29221\n\n 40%|████      | 2/5 [00:22<00:34, 11.37s/it]\u001b[A[2024-07-23 14:20:07,498][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1932424\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 14:20:13,261][1383994605.py][line:1089][INFO] mean_spacing_error：26.04498，col=13，count=216，rate=6.01852%，jerk=0.15747，miniumu_ttc=116.64930\n\n 60%|██████    | 3/5 [00:34<00:22, 11.39s/it]\u001b[A[2024-07-23 14:20:18,755][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2026404\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 14:20:24,528][1383994605.py][line:1089][INFO] mean_spacing_error：31.80739，col=15，count=216，rate=6.94444%，jerk=0.13706，miniumu_ttc=115.93123\n\n 80%|████████  | 4/5 [00:45<00:11, 11.35s/it]\u001b[A[2024-07-23 14:20:30,293][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1917394\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 14:20:36,060][1383994605.py][line:1089][INFO] mean_spacing_error：34.02360，col=14，count=216，rate=6.48148%，jerk=0.13339，miniumu_ttc=122.43281\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 42%|████▏     | 209/500 [10:53:16<14:46:28, 182.78s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.24s/it]\u001b[A\n2it [00:03,  1.81s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 14:23:01,213][1383994605.py][line:1670][INFO] 210/500. loss: 0.0028807837516069412\n[2024-07-23 14:23:01,224][1383994605.py][line:1671][INFO] 210/500. mserror: 33.7727165222168  col: 38  count: 216  jerk: 0.08657696843147278  ttc: 93.20435333251953\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:23:06,766][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2881895\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.59s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.99s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.85s/it]\u001b[A\u001b[A\n[2024-07-23 14:23:12,619][1383994605.py][line:1089][INFO] mean_spacing_error：30.55062，col=11，count=216，rate=5.09259%，jerk=0.15118，miniumu_ttc=171.61909\n\n 20%|██        | 1/5 [00:11<00:45, 11.38s/it]\u001b[A[2024-07-23 14:23:18,337][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2334314\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:23:23,916][1383994605.py][line:1089][INFO] mean_spacing_error：25.71028，col=28，count=216，rate=12.96296%，jerk=0.11651，miniumu_ttc=41.75362\n\n 40%|████      | 2/5 [00:22<00:34, 11.34s/it]\u001b[A[2024-07-23 14:23:29,631][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2305114\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.37s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.90s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:23:35,215][1383994605.py][line:1089][INFO] mean_spacing_error：38.59871，col=22，count=216，rate=10.18519%，jerk=0.10008，miniumu_ttc=105.36127\n\n 60%|██████    | 3/5 [00:33<00:22, 11.32s/it]\u001b[A[2024-07-23 14:23:41,673][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2167341\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 14:23:47,425][1383994605.py][line:1089][INFO] mean_spacing_error：36.75917，col=13，count=216，rate=6.01852%，jerk=0.12981，miniumu_ttc=135.27849\n\n 80%|████████  | 4/5 [00:46<00:11, 11.67s/it]\u001b[A[2024-07-23 14:23:53,204][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2043486\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 14:23:59,065][1383994605.py][line:1089][INFO] mean_spacing_error：42.91386，col=16，count=216，rate=7.40741%，jerk=0.11300，miniumu_ttc=134.14713\n\n100%|██████████| 5/5 [00:57<00:00, 11.57s/it]\u001b[A\n 42%|████▏     | 210/500 [10:56:39<15:12:44, 188.84s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.14s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 14:25:47,144][1383994605.py][line:1670][INFO] 211/500. loss: 0.0016367146745324135\n[2024-07-23 14:25:47,161][1383994605.py][line:1671][INFO] 211/500. mserror: 36.078006744384766  col: 36  count: 216  jerk: 0.082884781062603  ttc: 106.25082397460938\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:25:52,396][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2901507\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 14:25:57,916][1383994605.py][line:1089][INFO] mean_spacing_error：26.63177，col=12，count=216，rate=5.55556%，jerk=0.14442，miniumu_ttc=196.19708\n\n 20%|██        | 1/5 [00:10<00:42, 10.75s/it]\u001b[A[2024-07-23 14:26:03,399][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2181581\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 14:26:09,056][1383994605.py][line:1089][INFO] mean_spacing_error：25.33502，col=12，count=216，rate=5.55556%，jerk=0.12707，miniumu_ttc=101.22206\n\n 40%|████      | 2/5 [00:21<00:32, 10.98s/it]\u001b[A[2024-07-23 14:26:14,428][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2182881\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.04s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 14:26:20,285][1383994605.py][line:1089][INFO] mean_spacing_error：40.56996，col=33，count=216，rate=15.27778%，jerk=0.09939，miniumu_ttc=106.95376\n\n 60%|██████    | 3/5 [00:33<00:22, 11.09s/it]\u001b[A[2024-07-23 14:26:25,694][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2148090\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 14:26:31,427][1383994605.py][line:1089][INFO] mean_spacing_error：40.49331，col=12，count=216，rate=5.55556%，jerk=0.12775，miniumu_ttc=145.77625\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 14:26:36,830][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1906102\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:26:42,414][1383994605.py][line:1089][INFO] mean_spacing_error：28.96250，col=14，count=216，rate=6.48148%，jerk=0.15898，miniumu_ttc=86.59602\n\n100%|██████████| 5/5 [00:55<00:00, 11.05s/it]\u001b[A\n 42%|████▏     | 211/500 [10:59:23<14:32:46, 181.20s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.72s/it]\u001b[A\n3it [00:04,  1.63s/it]\u001b[A\n[2024-07-23 14:28:23,108][1383994605.py][line:1670][INFO] 212/500. loss: 0.0032497324670354524\n[2024-07-23 14:28:23,115][1383994605.py][line:1671][INFO] 212/500. mserror: 39.32084274291992  col: 35  count: 216  jerk: 0.07908052206039429  ttc: 107.8280029296875\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:28:28,450][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2847890\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 14:28:33,937][1383994605.py][line:1089][INFO] mean_spacing_error：27.47913，col=13，count=216，rate=6.01852%，jerk=0.13923，miniumu_ttc=173.76430\n\n 20%|██        | 1/5 [00:10<00:43, 10.81s/it]\u001b[A[2024-07-23 14:28:39,444][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2144227\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 14:28:45,161][1383994605.py][line:1089][INFO] mean_spacing_error：26.04674，col=12，count=216，rate=5.55556%，jerk=0.13369，miniumu_ttc=119.79359\n\n 40%|████      | 2/5 [00:22<00:33, 11.06s/it]\u001b[A[2024-07-23 14:28:50,609][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2133236\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:28:55,949][1383994605.py][line:1089][INFO] mean_spacing_error：39.68940，col=36，count=216，rate=16.66667%，jerk=0.10245，miniumu_ttc=90.65462\n\n 60%|██████    | 3/5 [00:32<00:21, 10.93s/it]\u001b[A[2024-07-23 14:29:01,556][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2214485\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 14:29:07,073][1383994605.py][line:1089][INFO] mean_spacing_error：41.34404，col=13，count=216，rate=6.01852%，jerk=0.11840，miniumu_ttc=144.26463\n\n 80%|████████  | 4/5 [00:43<00:11, 11.01s/it]\u001b[A[2024-07-23 14:29:12,551][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1916956\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.25s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 14:29:17,985][1383994605.py][line:1089][INFO] mean_spacing_error：27.66182，col=13，count=216，rate=6.01852%，jerk=0.15881，miniumu_ttc=89.39626\n\n100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\u001b[A\n 42%|████▏     | 212/500 [11:01:58<13:52:48, 173.50s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.00s/it]\u001b[A\n2it [00:03,  1.63s/it]\u001b[A\n3it [00:04,  1.56s/it]\u001b[A\n[2024-07-23 14:30:56,019][1383994605.py][line:1670][INFO] 213/500. loss: 0.003425841530164083\n[2024-07-23 14:30:56,030][1383994605.py][line:1671][INFO] 213/500. mserror: 42.23727798461914  col: 35  count: 216  jerk: 0.07607286423444748  ttc: 116.3282470703125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:31:01,135][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2821093\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.40s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 14:31:06,877][1383994605.py][line:1089][INFO] mean_spacing_error：27.65204，col=13，count=216，rate=6.01852%，jerk=0.13530，miniumu_ttc=173.27222\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 14:31:12,359][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2219409\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.22s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:31:17,690][1383994605.py][line:1089][INFO] mean_spacing_error：24.04391，col=11，count=216，rate=5.09259%，jerk=0.14161，miniumu_ttc=95.77144\n\n 40%|████      | 2/5 [00:21<00:32, 10.82s/it]\u001b[A[2024-07-23 14:31:23,081][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2239203\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 14:31:28,443][1383994605.py][line:1089][INFO] mean_spacing_error：35.12443，col=21，count=216，rate=9.72222%，jerk=0.11285，miniumu_ttc=104.94843\n\n 60%|██████    | 3/5 [00:32<00:21, 10.79s/it]\u001b[A[2024-07-23 14:31:33,940][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2079537\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:31:39,616][1383994605.py][line:1089][INFO] mean_spacing_error：38.44096，col=12，count=216，rate=5.55556%，jerk=0.12717，miniumu_ttc=156.98575\n\n 80%|████████  | 4/5 [00:43<00:10, 10.94s/it]\u001b[A[2024-07-23 14:31:45,543][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1920636\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:31:50,918][1383994605.py][line:1089][INFO] mean_spacing_error：28.67553，col=14，count=216，rate=6.48148%，jerk=0.15655，miniumu_ttc=88.28764\n\n100%|██████████| 5/5 [00:54<00:00, 10.98s/it]\u001b[A\n 43%|████▎     | 213/500 [11:04:31<13:20:24, 167.33s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.75s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 14:34:13,034][1383994605.py][line:1670][INFO] 214/500. loss: 0.001340043731033802\n[2024-07-23 14:34:13,046][1383994605.py][line:1671][INFO] 214/500. mserror: 42.89919662475586  col: 40  count: 216  jerk: 0.07312561571598053  ttc: 111.16817474365234\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:34:18,349][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2847525\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 14:34:23,661][1383994605.py][line:1089][INFO] mean_spacing_error：28.11805，col=13，count=216，rate=6.01852%，jerk=0.13566，miniumu_ttc=175.26466\n\n 20%|██        | 1/5 [00:10<00:42, 10.61s/it]\u001b[A[2024-07-23 14:34:29,154][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2181629\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 14:34:34,680][1383994605.py][line:1089][INFO] mean_spacing_error：23.70831，col=12，count=216，rate=5.55556%，jerk=0.13875，miniumu_ttc=95.91255\n\n 40%|████      | 2/5 [00:21<00:32, 10.85s/it]\u001b[A[2024-07-23 14:34:40,260][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2264512\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 14:34:45,663][1383994605.py][line:1089][INFO] mean_spacing_error：35.97927，col=22，count=216，rate=10.18519%，jerk=0.11090，miniumu_ttc=109.80421\n\n 60%|██████    | 3/5 [00:32<00:21, 10.91s/it]\u001b[A[2024-07-23 14:34:51,469][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2092196\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:34:56,811][1383994605.py][line:1089][INFO] mean_spacing_error：39.52192，col=12，count=216，rate=5.55556%，jerk=0.12711，miniumu_ttc=152.10847\n\n 80%|████████  | 4/5 [00:43<00:11, 11.00s/it]\u001b[A[2024-07-23 14:35:02,221][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1917843\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 14:35:07,690][1383994605.py][line:1089][INFO] mean_spacing_error：29.08859，col=15，count=216，rate=6.94444%，jerk=0.15735，miniumu_ttc=86.98685\n\n100%|██████████| 5/5 [00:54<00:00, 10.93s/it]\u001b[A\n 43%|████▎     | 214/500 [11:07:48<13:59:43, 176.17s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.07s/it]\u001b[A\n2it [00:03,  1.67s/it]\u001b[A\n3it [00:04,  1.61s/it]\u001b[A\n[2024-07-23 14:36:46,225][1383994605.py][line:1670][INFO] 215/500. loss: 0.002912485972046852\n[2024-07-23 14:36:46,241][1383994605.py][line:1671][INFO] 215/500. mserror: 40.15491485595703  col: 41  count: 216  jerk: 0.07475236803293228  ttc: 110.02388000488281\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:36:51,450][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2882336\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.18s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 14:36:56,980][1383994605.py][line:1089][INFO] mean_spacing_error：25.98889，col=11，count=216，rate=5.09259%，jerk=0.14227，miniumu_ttc=175.50146\n\n 20%|██        | 1/5 [00:10<00:42, 10.74s/it]\u001b[A[2024-07-23 14:37:02,234][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2155659\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 14:37:07,640][1383994605.py][line:1089][INFO] mean_spacing_error：23.56078，col=11，count=216，rate=5.09259%，jerk=0.13952，miniumu_ttc=74.54744\n\n 40%|████      | 2/5 [00:21<00:32, 10.69s/it]\u001b[A[2024-07-23 14:37:12,900][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2128960\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 14:37:18,275][1383994605.py][line:1089][INFO] mean_spacing_error：41.14663，col=30，count=216，rate=13.88889%，jerk=0.10530，miniumu_ttc=98.14464\n\n 60%|██████    | 3/5 [00:32<00:21, 10.66s/it]\u001b[A[2024-07-23 14:37:23,776][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2170312\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 14:37:29,532][1383994605.py][line:1089][INFO] mean_spacing_error：43.12934，col=13，count=216，rate=6.01852%，jerk=0.12068，miniumu_ttc=219.09999\n\n 80%|████████  | 4/5 [00:43<00:10, 10.90s/it]\u001b[A[2024-07-23 14:37:34,949][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1937943\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 14:37:40,320][1383994605.py][line:1089][INFO] mean_spacing_error：27.90036，col=13，count=216，rate=6.01852%，jerk=0.15801，miniumu_ttc=99.29156\n\n100%|██████████| 5/5 [00:54<00:00, 10.82s/it]\u001b[A\n 43%|████▎     | 215/500 [11:10:21<13:23:15, 169.11s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.66s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 14:39:20,427][1383994605.py][line:1670][INFO] 216/500. loss: 0.002878601041932901\n[2024-07-23 14:39:20,437][1383994605.py][line:1671][INFO] 216/500. mserror: 35.6004638671875  col: 38  count: 216  jerk: 0.08018703013658524  ttc: 157.11512756347656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:39:25,973][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2971439\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.20s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.73s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 14:39:31,253][1383994605.py][line:1089][INFO] mean_spacing_error：36.73548，col=7，count=216，rate=3.24074%，jerk=0.14561，miniumu_ttc=215.58626\n\n 20%|██        | 1/5 [00:10<00:43, 10.81s/it]\u001b[A[2024-07-23 14:39:36,825][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2482325\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 14:39:42,345][1383994605.py][line:1089][INFO] mean_spacing_error：37.45442，col=23，count=216，rate=10.64815%，jerk=0.16939，miniumu_ttc=11.18143\n\n 40%|████      | 2/5 [00:21<00:32, 10.97s/it]\u001b[A[2024-07-23 14:39:47,899][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2453488\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 14:39:53,269][1383994605.py][line:1089][INFO] mean_spacing_error：24.20673，col=9，count=216，rate=4.16667%，jerk=0.16194，miniumu_ttc=172.86082\n\n 60%|██████    | 3/5 [00:32<00:21, 10.95s/it]\u001b[A[2024-07-23 14:39:58,889][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2076155\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.06s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 14:40:04,836][1383994605.py][line:1089][INFO] mean_spacing_error：31.01340，col=29，count=216，rate=13.42593%，jerk=0.13574，miniumu_ttc=30.85256\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 14:40:10,593][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2223305\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:40:16,193][1383994605.py][line:1089][INFO] mean_spacing_error：48.32579，col=7，count=216，rate=3.24074%，jerk=0.13082，miniumu_ttc=214.41568\n\n100%|██████████| 5/5 [00:55<00:00, 11.15s/it]\u001b[A\n 43%|████▎     | 216/500 [11:12:57<13:01:38, 165.13s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.06s/it]\u001b[A\n2it [00:03,  1.63s/it]\u001b[A\n3it [00:04,  1.56s/it]\u001b[A\n[2024-07-23 14:41:51,302][1383994605.py][line:1670][INFO] 217/500. loss: 0.002832703913251559\n[2024-07-23 14:41:51,314][1383994605.py][line:1671][INFO] 217/500. mserror: 30.46039581298828  col: 37  count: 216  jerk: 0.08802586048841476  ttc: 105.92278289794922\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:41:56,386][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3087847\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.67s/it]\u001b[A\u001b[A\n[2024-07-23 14:42:01,679][1383994605.py][line:1089][INFO] mean_spacing_error：47.33774，col=5，count=216，rate=2.31481%，jerk=0.14429，miniumu_ttc=212.17763\n\n 20%|██        | 1/5 [00:10<00:41, 10.36s/it]\u001b[A[2024-07-23 14:42:07,122][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2757853\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 14:42:12,549][1383994605.py][line:1089][INFO] mean_spacing_error：30.02786，col=37，count=216，rate=17.12963%，jerk=0.13459，miniumu_ttc=11.68669\n\n 40%|████      | 2/5 [00:21<00:31, 10.66s/it]\u001b[A[2024-07-23 14:42:18,089][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2400419\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:42:23,356][1383994605.py][line:1089][INFO] mean_spacing_error：26.33215，col=10，count=216，rate=4.62963%，jerk=0.14484，miniumu_ttc=178.64061\n\n 60%|██████    | 3/5 [00:32<00:21, 10.73s/it]\u001b[A[2024-07-23 14:42:28,823][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2122623\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 14:42:34,215][1383994605.py][line:1089][INFO] mean_spacing_error：31.52386，col=23，count=216，rate=10.64815%，jerk=0.16083，miniumu_ttc=10.01681\n\n 80%|████████  | 4/5 [00:42<00:10, 10.78s/it]\u001b[A[2024-07-23 14:42:39,592][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2308870\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.21s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:42:44,965][1383994605.py][line:1089][INFO] mean_spacing_error：31.91409，col=7，count=216，rate=3.24074%，jerk=0.14379，miniumu_ttc=240.65460\n\n100%|██████████| 5/5 [00:53<00:00, 10.73s/it]\u001b[A\n 43%|████▎     | 217/500 [11:15:25<12:35:43, 160.23s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.05s/it]\u001b[A\n2it [00:03,  1.67s/it]\u001b[A\n3it [00:04,  1.59s/it]\u001b[A\n[2024-07-23 14:44:17,620][1383994605.py][line:1670][INFO] 218/500. loss: 0.0033453783641258874\n[2024-07-23 14:44:17,633][1383994605.py][line:1671][INFO] 218/500. mserror: 27.52515411376953  col: 36  count: 216  jerk: 0.09447193890810013  ttc: 101.47106170654297\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:44:23,020][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3185847\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.09s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.71s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.63s/it]\u001b[A\u001b[A\n[2024-07-23 14:44:28,182][1383994605.py][line:1089][INFO] mean_spacing_error：42.43747，col=5，count=216，rate=2.31481%，jerk=0.14723，miniumu_ttc=218.88786\n\n 20%|██        | 1/5 [00:10<00:42, 10.54s/it]\u001b[A[2024-07-23 14:44:33,527][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2516426\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 14:44:38,737][1383994605.py][line:1089][INFO] mean_spacing_error：36.67274，col=25，count=216，rate=11.57407%，jerk=0.15807，miniumu_ttc=13.94671\n\n 40%|████      | 2/5 [00:21<00:31, 10.55s/it]\u001b[A[2024-07-23 14:44:44,087][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2227205\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.79s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:44:49,428][1383994605.py][line:1089][INFO] mean_spacing_error：25.03459，col=7，count=216，rate=3.24074%，jerk=0.20825，miniumu_ttc=136.19014\n\n 60%|██████    | 3/5 [00:31<00:21, 10.62s/it]\u001b[A[2024-07-23 14:44:55,009][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2228564\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.74s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:45:00,283][1383994605.py][line:1089][INFO] mean_spacing_error：21.27402，col=13，count=216，rate=6.01852%，jerk=0.18501，miniumu_ttc=111.59960\n\n 80%|████████  | 4/5 [00:42<00:10, 10.71s/it]\u001b[A[2024-07-23 14:45:05,847][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2057368\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.15s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.73s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.63s/it]\u001b[A\u001b[A\n[2024-07-23 14:45:11,013][1383994605.py][line:1089][INFO] mean_spacing_error：34.17261，col=36，count=216，rate=16.66667%，jerk=0.12424，miniumu_ttc=95.40206\n\n100%|██████████| 5/5 [00:53<00:00, 10.68s/it]\u001b[A\n 44%|████▎     | 218/500 [11:17:51<12:13:04, 155.97s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.02s/it]\u001b[A\n2it [00:03,  1.67s/it]\u001b[A\n3it [00:04,  1.58s/it]\u001b[A\n[2024-07-23 14:46:47,624][1383994605.py][line:1670][INFO] 219/500. loss: 0.0030017507572968802\n[2024-07-23 14:46:47,633][1383994605.py][line:1671][INFO] 219/500. mserror: 27.284177780151367  col: 36  count: 216  jerk: 0.09774621576070786  ttc: 96.8160629272461\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:46:53,086][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2810803\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.13s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.73s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:46:58,340][1383994605.py][line:1089][INFO] mean_spacing_error：36.01220，col=12，count=216，rate=5.55556%，jerk=0.13895，miniumu_ttc=248.87358\n\n 20%|██        | 1/5 [00:10<00:42, 10.70s/it]\u001b[A[2024-07-23 14:47:03,821][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2793661\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.12s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:47:09,070][1383994605.py][line:1089][INFO] mean_spacing_error：33.12683，col=52，count=216，rate=24.07407%，jerk=0.11887，miniumu_ttc=18.43759\n\n 40%|████      | 2/5 [00:21<00:32, 10.71s/it]\u001b[A[2024-07-23 14:47:14,370][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2434867\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.19s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.76s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.65s/it]\u001b[A\u001b[A\n[2024-07-23 14:47:19,599][1383994605.py][line:1089][INFO] mean_spacing_error：28.22372，col=9，count=216，rate=4.16667%，jerk=0.15076，miniumu_ttc=199.11273\n\n 60%|██████    | 3/5 [00:31<00:21, 10.63s/it]\u001b[A[2024-07-23 14:47:24,905][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2277741\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.17s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.72s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.64s/it]\u001b[A\u001b[A\n[2024-07-23 14:47:30,100][1383994605.py][line:1089][INFO] mean_spacing_error：36.78868，col=23，count=216，rate=10.64815%，jerk=0.20360，miniumu_ttc=15.16543\n\n 80%|████████  | 4/5 [00:42<00:10, 10.58s/it]\u001b[A[2024-07-23 14:47:35,569][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2581457\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 14:47:40,884][1383994605.py][line:1089][INFO] mean_spacing_error：26.40514，col=6，count=216，rate=2.77778%，jerk=0.17786，miniumu_ttc=187.24004\n\n100%|██████████| 5/5 [00:53<00:00, 10.65s/it]\u001b[A\n 44%|████▍     | 219/500 [11:20:21<12:01:53, 154.14s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.68s/it]\u001b[A\n3it [00:04,  1.59s/it]\u001b[A\n[2024-07-23 14:49:21,022][1383994605.py][line:1670][INFO] 220/500. loss: 0.003372818852464358\n[2024-07-23 14:49:21,035][1383994605.py][line:1671][INFO] 220/500. mserror: 27.1336669921875  col: 34  count: 216  jerk: 0.09748705476522446  ttc: 186.0290985107422\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:49:26,071][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.2898329\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.75s/it]\u001b[A\u001b[A\n\n3it [00:04,  1.66s/it]\u001b[A\u001b[A\n[2024-07-23 14:49:31,338][1383994605.py][line:1089][INFO] mean_spacing_error：30.07867，col=10，count=216，rate=4.62963%，jerk=0.13905，miniumu_ttc=190.46812\n\n 20%|██        | 1/5 [00:10<00:41, 10.30s/it]\u001b[A[2024-07-23 14:49:36,622][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2373779\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 14:49:42,132][1383994605.py][line:1089][INFO] mean_spacing_error：26.78843，col=35，count=216，rate=16.20370%，jerk=0.12146，miniumu_ttc=21.75930\n\n 40%|████      | 2/5 [00:21<00:31, 10.59s/it]\u001b[A[2024-07-23 14:49:47,488][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2240780\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.71s/it]\u001b[A\u001b[A\n[2024-07-23 14:49:52,919][1383994605.py][line:1089][INFO] mean_spacing_error：34.82019，col=12，count=216，rate=5.55556%，jerk=0.11922，miniumu_ttc=219.89792\n\n 60%|██████    | 3/5 [00:31<00:21, 10.68s/it]\u001b[A[2024-07-23 14:49:58,323][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2013709\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 14:50:03,762][1383994605.py][line:1089][INFO] mean_spacing_error：31.60903，col=14，count=216，rate=6.48148%，jerk=0.13206，miniumu_ttc=131.50034\n\n 80%|████████  | 4/5 [00:42<00:10, 10.74s/it]\u001b[A[2024-07-23 14:50:09,487][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1925875\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 14:50:14,838][1383994605.py][line:1089][INFO] mean_spacing_error：32.85141，col=17，count=216，rate=7.87037%，jerk=0.12817，miniumu_ttc=132.82642\n\n100%|██████████| 5/5 [00:53<00:00, 10.76s/it]\u001b[A\n 44%|████▍     | 220/500 [11:22:55<11:59:03, 154.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.62s/it]\u001b[A\n[2024-07-23 14:52:32,274][1383994605.py][line:1670][INFO] 221/500. loss: 0.0015780222602188587\n[2024-07-23 14:52:32,285][1383994605.py][line:1671][INFO] 221/500. mserror: 27.653539657592773  col: 29  count: 216  jerk: 0.09630273282527924  ttc: 129.39198303222656\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:52:37,561][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3674863\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 14:52:43,061][1383994605.py][line:1089][INFO] mean_spacing_error：44.23611，col=10，count=216，rate=4.62963%，jerk=0.11614，miniumu_ttc=197.43408\n\n 20%|██        | 1/5 [00:10<00:43, 10.77s/it]\u001b[A[2024-07-23 14:52:48,708][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2575022\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 14:52:54,373][1383994605.py][line:1089][INFO] mean_spacing_error：49.25945，col=86，count=216，rate=39.81481%，jerk=0.10430，miniumu_ttc=5.83031\n\n 40%|████      | 2/5 [00:22<00:33, 11.09s/it]\u001b[A[2024-07-23 14:52:59,894][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3553116\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 14:53:05,581][1383994605.py][line:1089][INFO] mean_spacing_error：28.33675，col=25，count=216，rate=11.57407%，jerk=0.10868，miniumu_ttc=120.63121\n\n 60%|██████    | 3/5 [00:33<00:22, 11.14s/it]\u001b[A[2024-07-23 14:53:11,143][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3122302\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 14:53:16,682][1383994605.py][line:1089][INFO] mean_spacing_error：71.55909，col=8，count=216，rate=3.70370%，jerk=0.09939，miniumu_ttc=224.91772\n\n 80%|████████  | 4/5 [00:44<00:11, 11.13s/it]\u001b[A[2024-07-23 14:53:22,248][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2387480\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 14:53:27,958][1383994605.py][line:1089][INFO] mean_spacing_error：39.92315，col=39，count=216，rate=18.05556%，jerk=0.10160，miniumu_ttc=84.70935\n\n100%|██████████| 5/5 [00:55<00:00, 11.13s/it]\u001b[A\n 44%|████▍     | 221/500 [11:26:08<12:50:57, 165.80s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.15s/it]\u001b[A\n2it [00:03,  1.74s/it]\u001b[A\n3it [00:04,  1.64s/it]\u001b[A\n[2024-07-23 14:55:51,957][1383994605.py][line:1670][INFO] 222/500. loss: 0.002940597633520762\n[2024-07-23 14:55:51,968][1383994605.py][line:1671][INFO] 222/500. mserror: 30.338768005371094  col: 21  count: 216  jerk: 0.09561219811439514  ttc: 144.81732177734375\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:55:57,151][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3440866\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.57s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 14:56:02,953][1383994605.py][line:1089][INFO] mean_spacing_error：49.34966，col=13，count=216，rate=6.01852%，jerk=0.10063，miniumu_ttc=233.97173\n\n 20%|██        | 1/5 [00:10<00:43, 10.98s/it]\u001b[A[2024-07-23 14:56:08,531][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2666015\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.89s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.77s/it]\u001b[A\u001b[A\n[2024-07-23 14:56:14,127][1383994605.py][line:1089][INFO] mean_spacing_error：47.66929，col=75，count=216，rate=34.72222%，jerk=0.10884，miniumu_ttc=6.86990\n\n 40%|████      | 2/5 [00:22<00:33, 11.09s/it]\u001b[A[2024-07-23 14:56:19,701][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3550247\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 14:56:25,250][1383994605.py][line:1089][INFO] mean_spacing_error：29.50246，col=34，count=216，rate=15.74074%，jerk=0.10603，miniumu_ttc=90.81621\n\n 60%|██████    | 3/5 [00:33<00:22, 11.11s/it]\u001b[A[2024-07-23 14:56:30,643][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2624455\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.61s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 14:56:36,589][1383994605.py][line:1089][INFO] mean_spacing_error：54.16172，col=8，count=216，rate=3.70370%，jerk=0.11361，miniumu_ttc=198.12202\n\n 80%|████████  | 4/5 [00:44<00:11, 11.20s/it]\u001b[A[2024-07-23 14:56:42,033][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2116931\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 14:56:47,648][1383994605.py][line:1089][INFO] mean_spacing_error：32.75952，col=23，count=216，rate=10.64815%，jerk=0.12569，miniumu_ttc=86.94660\n\n100%|██████████| 5/5 [00:55<00:00, 11.14s/it]\u001b[A\n 44%|████▍     | 222/500 [11:29:28<13:35:18, 175.96s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.11s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:04,  1.65s/it]\u001b[A\n[2024-07-23 14:59:18,460][1383994605.py][line:1670][INFO] 223/500. loss: 0.002467579518755277\n[2024-07-23 14:59:18,471][1383994605.py][line:1671][INFO] 223/500. mserror: 30.347505569458008  col: 21  count: 216  jerk: 0.09604240953922272  ttc: 144.83018493652344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 14:59:23,445][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3431038\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 14:59:28,903][1383994605.py][line:1089][INFO] mean_spacing_error：50.50378，col=9，count=216，rate=4.16667%，jerk=0.10387，miniumu_ttc=211.63922\n\n 20%|██        | 1/5 [00:10<00:41, 10.43s/it]\u001b[A[2024-07-23 14:59:34,309][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2724378\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.29s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 14:59:39,877][1383994605.py][line:1089][INFO] mean_spacing_error：52.67114，col=76，count=216，rate=35.18519%，jerk=0.11191，miniumu_ttc=5.32529\n\n 40%|████      | 2/5 [00:21<00:32, 10.75s/it]\u001b[A[2024-07-23 14:59:45,590][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3729758\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 14:59:51,301][1383994605.py][line:1089][INFO] mean_spacing_error：31.43235，col=39，count=216，rate=18.05556%，jerk=0.09782，miniumu_ttc=90.68881\n\n 60%|██████    | 3/5 [00:32<00:22, 11.06s/it]\u001b[A[2024-07-23 14:59:56,799][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2731486\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.28s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 15:00:02,338][1383994605.py][line:1089][INFO] mean_spacing_error：59.52092，col=8，count=216，rate=3.70370%，jerk=0.11080，miniumu_ttc=201.93394\n\n 80%|████████  | 4/5 [00:43<00:11, 11.05s/it]\u001b[A[2024-07-23 15:00:08,422][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2192227\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.83s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 15:00:13,931][1383994605.py][line:1089][INFO] mean_spacing_error：34.94204，col=24，count=216，rate=11.11111%，jerk=0.12693，miniumu_ttc=45.95664\n\n100%|██████████| 5/5 [00:55<00:00, 11.09s/it]\u001b[A\n 45%|████▍     | 223/500 [11:32:54<14:14:21, 185.06s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.92s/it]\u001b[A\n3it [00:05,  1.79s/it]\u001b[A\n[2024-07-23 15:02:48,116][1383994605.py][line:1670][INFO] 224/500. loss: 0.0014895681912700336\n[2024-07-23 15:02:48,127][1383994605.py][line:1671][INFO] 224/500. mserror: 31.69928741455078  col: 27  count: 216  jerk: 0.09123905748128891  ttc: 157.34298706054688\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:02:53,833][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3453399\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 15:02:59,523][1383994605.py][line:1089][INFO] mean_spacing_error：40.00963，col=7，count=216，rate=3.24074%，jerk=0.12946，miniumu_ttc=205.11562\n\n 20%|██        | 1/5 [00:11<00:45, 11.39s/it]\u001b[A[2024-07-23 15:03:05,386][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2703895\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 15:03:11,227][1383994605.py][line:1089][INFO] mean_spacing_error：47.54102，col=91，count=216，rate=42.12963%，jerk=0.10430，miniumu_ttc=5.71780\n\n 40%|████      | 2/5 [00:23<00:34, 11.57s/it]\u001b[A[2024-07-23 15:03:17,076][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3605261\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 15:03:22,605][1383994605.py][line:1089][INFO] mean_spacing_error：35.82692，col=29，count=216，rate=13.42593%，jerk=0.09171，miniumu_ttc=142.34035\n\n 60%|██████    | 3/5 [00:34<00:22, 11.49s/it]\u001b[A[2024-07-23 15:03:28,659][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.3030458\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 15:03:34,443][1383994605.py][line:1089][INFO] mean_spacing_error：72.52509，col=7，count=216，rate=3.24074%，jerk=0.10713，miniumu_ttc=213.18266\n\n 80%|████████  | 4/5 [00:46<00:11, 11.62s/it]\u001b[A[2024-07-23 15:03:40,363][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2241051\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:06,  2.00s/it]\u001b[A\u001b[A\n[2024-07-23 15:03:46,665][1383994605.py][line:1089][INFO] mean_spacing_error：39.20527，col=32，count=216，rate=14.81481%，jerk=0.11952，miniumu_ttc=26.49420\n\n100%|██████████| 5/5 [00:58<00:00, 11.71s/it]\u001b[A\n 45%|████▍     | 224/500 [11:36:27<14:49:29, 193.37s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.93s/it]\u001b[A\n3it [00:05,  1.80s/it]\u001b[A\n[2024-07-23 15:05:34,891][1383994605.py][line:1670][INFO] 225/500. loss: 0.003150432991484801\n[2024-07-23 15:05:34,904][1383994605.py][line:1671][INFO] 225/500. mserror: 33.77561950683594  col: 34  count: 216  jerk: 0.0872022956609726  ttc: 156.59976196289062\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:05:40,523][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3038373\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.80s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.72s/it]\u001b[A\u001b[A\n[2024-07-23 15:05:45,973][1383994605.py][line:1089][INFO] mean_spacing_error：23.75909，col=10，count=216，rate=4.62963%，jerk=0.14374，miniumu_ttc=183.84543\n\n 20%|██        | 1/5 [00:11<00:44, 11.06s/it]\u001b[A[2024-07-23 15:05:51,803][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2191859\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 15:05:57,374][1383994605.py][line:1089][INFO] mean_spacing_error：24.65155，col=21，count=216，rate=9.72222%，jerk=0.12896，miniumu_ttc=79.09475\n\n 40%|████      | 2/5 [00:22<00:33, 11.26s/it]\u001b[A[2024-07-23 15:06:02,951][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1957417\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.91s/it]\u001b[A\u001b[A\n[2024-07-23 15:06:08,954][1383994605.py][line:1089][INFO] mean_spacing_error：26.70679，col=12，count=216，rate=5.55556%，jerk=0.17576，miniumu_ttc=123.72785\n\n 60%|██████    | 3/5 [00:34<00:22, 11.41s/it]\u001b[A[2024-07-23 15:06:14,600][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1967116\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 15:06:20,290][1383994605.py][line:1089][INFO] mean_spacing_error：22.85353，col=11，count=216，rate=5.09259%，jerk=0.18478，miniumu_ttc=95.37236\n\n 80%|████████  | 4/5 [00:45<00:11, 11.38s/it]\u001b[A[2024-07-23 15:06:26,002][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1938487\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 15:06:31,467][1383994605.py][line:1089][INFO] mean_spacing_error：23.27877，col=23，count=216，rate=10.64815%，jerk=0.14455，miniumu_ttc=86.79842\n\n100%|██████████| 5/5 [00:56<00:00, 11.31s/it]\u001b[A\n 45%|████▌     | 225/500 [11:39:12<14:06:59, 184.80s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.23s/it]\u001b[A\n2it [00:03,  1.77s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 15:08:16,190][1383994605.py][line:1670][INFO] 226/500. loss: 0.002837499293188254\n[2024-07-23 15:08:16,203][1383994605.py][line:1671][INFO] 226/500. mserror: 34.79426574707031  col: 33  count: 216  jerk: 0.0866413563489914  ttc: 140.40365600585938\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:08:21,365][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3039974\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.36s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 15:08:26,973][1383994605.py][line:1089][INFO] mean_spacing_error：23.13409，col=12，count=216，rate=5.55556%，jerk=0.14730，miniumu_ttc=173.44987\n\n 20%|██        | 1/5 [00:10<00:43, 10.77s/it]\u001b[A[2024-07-23 15:08:32,646][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2154237\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.34s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.75s/it]\u001b[A\u001b[A\n[2024-07-23 15:08:38,196][1383994605.py][line:1089][INFO] mean_spacing_error：22.63181，col=19，count=216，rate=8.79630%，jerk=0.13243，miniumu_ttc=44.52984\n\n 40%|████      | 2/5 [00:21<00:33, 11.03s/it]\u001b[A[2024-07-23 15:08:44,368][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1917421\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.60s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 15:08:50,346][1383994605.py][line:1089][INFO] mean_spacing_error：28.97730，col=13，count=216，rate=6.01852%，jerk=0.18209，miniumu_ttc=51.08654\n\n 60%|██████    | 3/5 [00:34<00:23, 11.55s/it]\u001b[A[2024-07-23 15:08:56,177][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2054395\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 15:09:01,970][1383994605.py][line:1089][INFO] mean_spacing_error：24.74059，col=8，count=216，rate=3.70370%，jerk=0.19049，miniumu_ttc=134.70627\n\n 80%|████████  | 4/5 [00:45<00:11, 11.57s/it]\u001b[A[2024-07-23 15:09:07,900][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1883894\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.47s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.91s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 15:09:13,598][1383994605.py][line:1089][INFO] mean_spacing_error：23.16602，col=22，count=216，rate=10.18519%，jerk=0.13804，miniumu_ttc=100.48990\n\n100%|██████████| 5/5 [00:57<00:00, 11.48s/it]\u001b[A\n 45%|████▌     | 226/500 [11:41:54<13:32:49, 177.99s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.09s/it]\u001b[A\n2it [00:03,  1.67s/it]\u001b[A\n3it [00:04,  1.60s/it]\u001b[A\n[2024-07-23 15:10:55,177][1383994605.py][line:1670][INFO] 227/500. loss: 0.001678568311035633\n[2024-07-23 15:10:55,189][1383994605.py][line:1671][INFO] 227/500. mserror: 35.99855041503906  col: 32  count: 216  jerk: 0.08562116324901581  ttc: 134.22731018066406\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:11:00,866][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3087472\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.27s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.77s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.69s/it]\u001b[A\u001b[A\n[2024-07-23 15:11:06,213][1383994605.py][line:1089][INFO] mean_spacing_error：30.43926，col=7，count=216，rate=3.24074%，jerk=0.14510，miniumu_ttc=238.78049\n\n 20%|██        | 1/5 [00:11<00:44, 11.02s/it]\u001b[A[2024-07-23 15:11:11,679][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2220227\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.81s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.70s/it]\u001b[A\u001b[A\n[2024-07-23 15:11:17,070][1383994605.py][line:1089][INFO] mean_spacing_error：20.86089，col=14，count=216，rate=6.48148%，jerk=0.13721，miniumu_ttc=95.06859\n\n 40%|████      | 2/5 [00:21<00:32, 10.93s/it]\u001b[A[2024-07-23 15:11:22,771][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1944037\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 15:11:28,447][1383994605.py][line:1089][INFO] mean_spacing_error：31.52738，col=21，count=216，rate=9.72222%，jerk=0.13215，miniumu_ttc=88.88153\n\n 60%|██████    | 3/5 [00:33<00:22, 11.13s/it]\u001b[A[2024-07-23 15:11:33,944][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2122160\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.23s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.79s/it]\u001b[A\u001b[A\n[2024-07-23 15:11:39,589][1383994605.py][line:1089][INFO] mean_spacing_error：25.71460，col=15，count=216，rate=6.94444%，jerk=0.14282，miniumu_ttc=124.16988\n\n 80%|████████  | 4/5 [00:44<00:11, 11.14s/it]\u001b[A[2024-07-23 15:11:45,641][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1818796\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 15:11:51,389][1383994605.py][line:1089][INFO] mean_spacing_error：23.67612，col=16，count=216，rate=7.40741%，jerk=0.14511，miniumu_ttc=86.70703\n\n100%|██████████| 5/5 [00:56<00:00, 11.24s/it]\u001b[A\n 45%|████▌     | 227/500 [11:44:32<13:02:16, 171.93s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.12s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 15:13:39,546][1383994605.py][line:1670][INFO] 228/500. loss: 0.0032987516994277635\n[2024-07-23 15:13:39,557][1383994605.py][line:1671][INFO] 228/500. mserror: 36.857276916503906  col: 28  count: 216  jerk: 0.08629169315099716  ttc: 146.1342315673828\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:13:44,781][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3118157\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.31s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.85s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 15:13:50,399][1383994605.py][line:1089][INFO] mean_spacing_error：35.85987，col=7，count=216，rate=3.24074%，jerk=0.13612，miniumu_ttc=428.69046\n\n 20%|██        | 1/5 [00:10<00:43, 10.83s/it]\u001b[A[2024-07-23 15:13:55,985][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2251931\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.33s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.87s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 15:14:01,731][1383994605.py][line:1089][INFO] mean_spacing_error：25.54104，col=16，count=216，rate=7.40741%，jerk=0.16276，miniumu_ttc=14.08841\n\n 40%|████      | 2/5 [00:22<00:33, 11.12s/it]\u001b[A[2024-07-23 15:14:07,435][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2075477\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.88s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.78s/it]\u001b[A\u001b[A\n[2024-07-23 15:14:13,080][1383994605.py][line:1089][INFO] mean_spacing_error：23.28046，col=12，count=216，rate=5.55556%，jerk=0.16696，miniumu_ttc=134.25594\n\n 60%|██████    | 3/5 [00:33<00:22, 11.23s/it]\u001b[A[2024-07-23 15:14:18,686][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1911559\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.24s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.78s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.68s/it]\u001b[A\u001b[A\n[2024-07-23 15:14:24,010][1383994605.py][line:1089][INFO] mean_spacing_error：21.34996，col=14，count=216，rate=6.48148%，jerk=0.15813，miniumu_ttc=108.66782\n\n 80%|████████  | 4/5 [00:44<00:11, 11.11s/it]\u001b[A[2024-07-23 15:14:29,493][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1891605\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.54s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 15:14:35,314][1383994605.py][line:1089][INFO] mean_spacing_error：23.76740，col=17，count=216，rate=7.87037%，jerk=0.13458，miniumu_ttc=108.52523\n\n100%|██████████| 5/5 [00:55<00:00, 11.15s/it]\u001b[A\n 46%|████▌     | 228/500 [11:47:16<12:48:31, 169.53s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.18s/it]\u001b[A\n2it [00:03,  1.79s/it]\u001b[A\n3it [00:05,  1.70s/it]\u001b[A\n[2024-07-23 15:16:56,620][1383994605.py][line:1670][INFO] 229/500. loss: 0.0013068572928508122\n[2024-07-23 15:16:56,627][1383994605.py][line:1671][INFO] 229/500. mserror: 37.939266204833984  col: 27  count: 216  jerk: 0.08531270176172256  ttc: 156.8971710205078\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:17:02,187][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3291759\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.48s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.89s/it]\u001b[A\u001b[A\n[2024-07-23 15:17:08,136][1383994605.py][line:1089][INFO] mean_spacing_error：52.00896，col=7，count=216，rate=3.24074%，jerk=0.12340，miniumu_ttc=262.75928\n\n 20%|██        | 1/5 [00:11<00:45, 11.49s/it]\u001b[A[2024-07-23 15:17:14,077][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2846426\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.44s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 15:17:19,849][1383994605.py][line:1089][INFO] mean_spacing_error：42.50113，col=72，count=216，rate=33.33333%，jerk=0.12671，miniumu_ttc=9.37105\n\n 40%|████      | 2/5 [00:23<00:34, 11.62s/it]\u001b[A[2024-07-23 15:17:25,799][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.3703014\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.41s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.00s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 15:17:31,729][1383994605.py][line:1089][INFO] mean_spacing_error：30.65183，col=34，count=216，rate=15.74074%，jerk=0.09758，miniumu_ttc=101.94769\n\n 60%|██████    | 3/5 [00:35<00:23, 11.74s/it]\u001b[A[2024-07-23 15:17:37,760][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2748048\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.03s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.94s/it]\u001b[A\u001b[A\n[2024-07-23 15:17:43,882][1383994605.py][line:1089][INFO] mean_spacing_error：64.12301，col=10，count=216，rate=4.62963%，jerk=0.09383，miniumu_ttc=205.71548\n\n 80%|████████  | 4/5 [00:47<00:11, 11.90s/it]\u001b[A[2024-07-23 15:17:49,525][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.2259284\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.49s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.96s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 15:17:55,292][1383994605.py][line:1089][INFO] mean_spacing_error：34.26062，col=30，count=216，rate=13.88889%，jerk=0.11753，miniumu_ttc=125.62077\n\n100%|██████████| 5/5 [00:58<00:00, 11.73s/it]\u001b[A\n 46%|████▌     | 229/500 [11:50:36<13:26:58, 178.67s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.21s/it]\u001b[A\n2it [00:03,  1.78s/it]\u001b[A\n3it [00:05,  1.69s/it]\u001b[A\n[2024-07-23 15:19:42,037][1383994605.py][line:1670][INFO] 230/500. loss: 0.0016985135152935982\n[2024-07-23 15:19:42,061][1383994605.py][line:1671][INFO] 230/500. mserror: 37.222286224365234  col: 32  count: 216  jerk: 0.08338842540979385  ttc: 173.34161376953125\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:19:47,193][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3025167\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.02s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 15:19:53,063][1383994605.py][line:1089][INFO] mean_spacing_error：25.34562，col=15，count=216，rate=6.94444%，jerk=0.14163，miniumu_ttc=174.94571\n\n 20%|██        | 1/5 [00:11<00:44, 11.00s/it]\u001b[A[2024-07-23 15:19:58,508][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2100083\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.38s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.97s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.88s/it]\u001b[A\u001b[A\n[2024-07-23 15:20:04,434][1383994605.py][line:1089][INFO] mean_spacing_error：20.94764，col=14，count=216，rate=6.48148%，jerk=0.13890，miniumu_ttc=89.06863\n\n 40%|████      | 2/5 [00:22<00:33, 11.22s/it]\u001b[A[2024-07-23 15:20:10,009][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1926565\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.86s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.76s/it]\u001b[A\u001b[A\n[2024-07-23 15:20:15,593][1383994605.py][line:1089][INFO] mean_spacing_error：29.84496，col=14，count=216，rate=6.48148%，jerk=0.17435，miniumu_ttc=32.92203\n\n 60%|██████    | 3/5 [00:33<00:22, 11.19s/it]\u001b[A[2024-07-23 15:20:21,219][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.1988073\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.67s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.05s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.93s/it]\u001b[A\u001b[A\n[2024-07-23 15:20:27,343][1383994605.py][line:1089][INFO] mean_spacing_error：24.08148，col=7，count=216，rate=3.24074%，jerk=0.17775，miniumu_ttc=141.07275\n\n 80%|████████  | 4/5 [00:45<00:11, 11.41s/it]\u001b[A[2024-07-23 15:20:32,948][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1840919\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.42s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.82s/it]\u001b[A\u001b[A\n[2024-07-23 15:20:38,694][1383994605.py][line:1089][INFO] mean_spacing_error：23.02540，col=16，count=216，rate=7.40741%，jerk=0.13758，miniumu_ttc=115.05210\n\n100%|██████████| 5/5 [00:56<00:00, 11.33s/it]\u001b[A\n 46%|████▌     | 230/500 [11:53:19<13:03:23, 174.09s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.19s/it]\u001b[A\n2it [00:03,  1.76s/it]\u001b[A\n3it [00:04,  1.66s/it]\u001b[A\n[2024-07-23 15:22:20,045][1383994605.py][line:1670][INFO] 231/500. loss: 0.0016877295759816964\n[2024-07-23 15:22:20,053][1383994605.py][line:1671][INFO] 231/500. mserror: 36.638729095458984  col: 38  count: 216  jerk: 0.08172610402107239  ttc: 160.46690368652344\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:22:25,126][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3074769\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.32s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.93s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.80s/it]\u001b[A\u001b[A\n[2024-07-23 15:22:30,800][1383994605.py][line:1089][INFO] mean_spacing_error：29.25174，col=16，count=216，rate=7.40741%，jerk=0.13844，miniumu_ttc=166.58456\n\n 20%|██        | 1/5 [00:10<00:42, 10.73s/it]\u001b[A[2024-07-23 15:22:36,303][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2254859\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.30s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.82s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.73s/it]\u001b[A\u001b[A\n[2024-07-23 15:22:41,769][1383994605.py][line:1089][INFO] mean_spacing_error：23.60141，col=11，count=216，rate=5.09259%，jerk=0.13081，miniumu_ttc=123.80253\n\n 40%|████      | 2/5 [00:21<00:32, 10.87s/it]\u001b[A[2024-07-23 15:22:47,270][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.2191729\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.39s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 15:22:52,974][1383994605.py][line:1089][INFO] mean_spacing_error：41.72968，col=38，count=216，rate=17.59259%，jerk=0.09414，miniumu_ttc=101.30380\n\n 60%|██████    | 3/5 [00:32<00:22, 11.03s/it]\u001b[A[2024-07-23 15:22:58,613][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2292789\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.51s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.98s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.86s/it]\u001b[A\u001b[A\n[2024-07-23 15:23:04,470][1383994605.py][line:1089][INFO] mean_spacing_error：42.98482，col=14，count=216，rate=6.48148%，jerk=0.11217，miniumu_ttc=145.44264\n\n 80%|████████  | 4/5 [00:44<00:11, 11.21s/it]\u001b[A[2024-07-23 15:23:09,895][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1901751\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.26s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.84s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.74s/it]\u001b[A\u001b[A\n[2024-07-23 15:23:15,401][1383994605.py][line:1089][INFO] mean_spacing_error：27.17446，col=11，count=216，rate=5.09259%，jerk=0.16664，miniumu_ttc=94.07383\n\n100%|██████████| 5/5 [00:55<00:00, 11.07s/it]\u001b[A\n 46%|████▌     | 231/500 [11:55:56<12:37:07, 168.88s/it]\n0it [00:00, ?it/s]\u001b[A\n1it [00:02,  2.31s/it]\u001b[A\n2it [00:03,  1.89s/it]\u001b[A\n3it [00:05,  1.75s/it]\u001b[A\n[2024-07-23 15:25:38,141][1383994605.py][line:1670][INFO] 232/500. loss: 0.0013466461872061093\n[2024-07-23 15:25:38,153][1383994605.py][line:1671][INFO] 232/500. mserror: 35.75787353515625  col: 38  count: 216  jerk: 0.08153414726257324  ttc: 137.81654357910156\n\n  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A[2024-07-23 15:25:43,531][1383994605.py][line:1086][INFO] Epoch: 1| Train Loss: 0.3105780\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.92s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.83s/it]\u001b[A\u001b[A\n[2024-07-23 15:25:49,290][1383994605.py][line:1089][INFO] mean_spacing_error：27.86020，col=16，count=216，rate=7.40741%，jerk=0.13800，miniumu_ttc=166.73274\n\n 20%|██        | 1/5 [00:11<00:44, 11.13s/it]\u001b[A[2024-07-23 15:25:54,797][1383994605.py][line:1086][INFO] Epoch: 2| Train Loss: 0.2191590\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.52s/it]\u001b[A\u001b[A\n\n2it [00:04,  2.01s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.87s/it]\u001b[A\u001b[A\n[2024-07-23 15:26:00,679][1383994605.py][line:1089][INFO] mean_spacing_error：21.68600，col=14，count=216，rate=6.48148%，jerk=0.12922，miniumu_ttc=104.21233\n\n 40%|████      | 2/5 [00:22<00:33, 11.28s/it]\u001b[A[2024-07-23 15:26:06,495][1383994605.py][line:1086][INFO] Epoch: 3| Train Loss: 0.1978812\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.46s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.90s/it]\u001b[A\u001b[A\n[2024-07-23 15:26:12,470][1383994605.py][line:1089][INFO] mean_spacing_error：32.62822，col=24，count=216，rate=11.11111%，jerk=0.12346，miniumu_ttc=104.78403\n\n 60%|██████    | 3/5 [00:34<00:23, 11.51s/it]\u001b[A[2024-07-23 15:26:18,051][1383994605.py][line:1086][INFO] Epoch: 4| Train Loss: 0.2229111\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.43s/it]\u001b[A\u001b[A\n\n2it [00:04,  1.95s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.84s/it]\u001b[A\u001b[A\n[2024-07-23 15:26:23,833][1383994605.py][line:1089][INFO] mean_spacing_error：29.82151，col=15，count=216，rate=6.94444%，jerk=0.13491，miniumu_ttc=123.37307\n\n 80%|████████  | 4/5 [00:45<00:11, 11.46s/it]\u001b[A[2024-07-23 15:26:29,354][1383994605.py][line:1086][INFO] Epoch: 5| Train Loss: 0.1870215\n\n\n0it [00:00, ?it/s]\u001b[A\u001b[A\n\n1it [00:02,  2.35s/it]\u001b[A\u001b[A\n\n2it [00:03,  1.94s/it]\u001b[A\u001b[A\n\n3it [00:05,  1.81s/it]\u001b[A\u001b[A\n[2024-07-23 15:26:35,080][1383994605.py][line:1089][INFO] mean_spacing_error：23.30736，col=13，count=216，rate=6.01852%，jerk=0.15399，miniumu_ttc=94.01125\n\n100%|██████████| 5/5 [00:56<00:00, 11.38s/it]\u001b[A\n 46%|████▋     | 232/500 [11:59:15<13:15:34, 178.11s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nfrom torch import _weight_norm\nimport numpy as np\nfrom math import sqrt\n\n\nclass TemporalAttention(nn.Module):\n    def __init__(self, input_dim):\n        super(TemporalAttention, self).__init__()\n        self.query = nn.Linear(input_dim, input_dim)\n        self.key = nn.Linear(input_dim, input_dim)\n        self.value = nn.Linear(input_dim, input_dim)\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x, param=None):\n        if param == None:\n            # mask = np.array([[1 if i>j else 0 for i in range(x.size(2))] for j in range(x.size(2))])\n            # if x.is_cuda:\n            #     mask = torch.ByteTensor(mask).cuda(x.get_device())\n            # else:\n            #     mask = torch.ByteTensor(mask)\n            q = self.query(x)\n            k = self.key(x)\n            v = self.value(x)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1])\n            k = F.linear(x, weight=param[2], bias=param[3])\n            v = F.linear(x, weight=param[4], bias=param[5])\n\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (x.shape[-1] ** 0.5)\n        # attn_scores.data.masked_fill_(mask, -float('inf'))\n        attn_weights = self.softmax(attn_scores)\n        attended_values = torch.matmul(attn_weights, v)\n        return attended_values\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim_in = input_dim\n        self.dim_k = input_dim//2\n        self.dim_v = input_dim//2\n        self.num_heads = num_heads\n        self.linear_q = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_k = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_v = nn.Linear(input_dim, input_dim, bias=True)\n        self._norm_fact = 1 / sqrt((input_dim//2) // num_heads)\n\n    def forward(self, x, param=None):\n        batch, n, dim_in = x.shape\n        assert dim_in == self.dim_in\n\n        nh = self.num_heads\n        dk = self.dim_k // nh  # dim_k of each head\n        dv = self.dim_v // nh  # dim_v of each head\n        if param == None:\n            q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            v = self.linear_v(x).reshape(batch, n, nh, dv*2).transpose(1, 2)  # (batch, nh, n, dv)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1]).reshape(batch, n, nh, dk).transpose(1, 2)\n            k = F.linear(x, weight=param[2], bias=param[3]).reshape(batch, n, nh, dk).transpose(1, 2)\n            v = F.linear(x, weight=param[4], bias=param[5]).reshape(batch, n, nh, dv*2).transpose(1, 2)\n        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact  # batch, nh, n, n\n        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n\n        att = torch.matmul(dist, v)  # batch, nh, n, dv\n        out = att.transpose(1, 2).reshape(batch, n, self.dim_in)  # batch, n, dim_v\n        \n        # out = att.reshape(att.shape[0], -1)\n\n        return out\n    \n\nclass conv1d(nn.Module):\n    def _init_(self):\n        super(conv1d, self).__init__()\n    \n    def forward(self, x, weight, bias, stride, padding, dilation):\n        return F.conv1d(x, weight, bias=bias, stride=stride, padding=padding, dilation=dilation)\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        # self.relu1 = nn.Tanh()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        # self.relu2 = nn.Tanh()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        # self.relu = nn.Tanh()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        # num_levels = num_channels\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n        self.token = nn.Parameter(torch.ones(1, num_channels[-1]))\n        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=10, nhead= 5,\\\n        #                     dim_feedforward=10, batch_first=True)\n        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n        # self.features = nn.LSTM(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.features = nn.GRU(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.attention = TemporalAttention(num_channels[-1])\n        self.attention = MultiHeadSelfAttention(num_channels[-1])\n        # self.features = nn.Linear(num_channels[-1], num_channels[-1])\n        # self.decoder = nn.Linear(num_channels[-1], 1)\n        self.head = nn.Sequential(\n            nn.Linear(num_channels[-1], num_channels[-1]),\n            nn.ReLU(),\n            nn.Linear(num_channels[-1], 1))\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=num_channels[i-1], nhead= 4,\\\n        #                     dim_feedforward=num_channels[i-1], batch_first=False)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        # self.tanh = nn.Tanh()\n\n    def forward(self, x, weights=None):\n        if weights == None:\n            # transformer output:\n            # x = self.transformer_encoder(x) + x\n            \n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n            # output = self.attention(output)\n            # linear output:\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n            # h_n = self.features(output)\n            \n            # LSTM output:\n            # enc_x, (h_n, c_n) = self.features(output)\n            \n            # GRU output:\n            # enc_x, h_n = self.features(output)\n\n            # if len(h_n.shape) == 3:\n            #     h_n = h_n[-1] # (32,16)\n            \n            # 通过线性层和激活函数得到最终输出\n            # out = self.decoder(F.relu(h_n))\n            out = self.head(output)\n        else:\n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n\n            # output = output.transpose(1, 2)\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n\n            out = F.relu(F.linear(output, weight=weights[0] , bias=weights[1]))\n            out = F.linear(out, weight=weights[2], bias=weights[3])\n\n        return out\n\nimport numbers\nfrom copy import copy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport random\n\n\ndef extract_top_level_dict(current_dict):\n    \"\"\"\n    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n    :param value: Param value\n    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n    :return: A dictionary graph of the params already added to the graph.\n    \"\"\"\n    output_dict = dict()\n    for key in current_dict.keys():\n        name = key.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"block_dict.\", \"\")\n        name = name.replace(\"module-\", \"\")\n        top_level = name.split(\".\")[0]\n        sub_level = \".\".join(name.split(\".\")[1:])\n\n        if top_level not in output_dict:\n            if sub_level == \"\":\n                output_dict[top_level] = current_dict[key]\n            else:\n                output_dict[top_level] = {sub_level: current_dict[key]}\n        else:\n            new_item = {key: value for key, value in output_dict[top_level].items()}\n            new_item[sub_level] = current_dict[key]\n            output_dict[top_level] = new_item\n\n    #print(current_dict.keys(), output_dict.keys())\n    return output_dict\n\n\nclass MetaLinearLayer(nn.Module):\n    def __init__(self, input_shape, num_filters, use_bias):\n        \"\"\"\n        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n        learning setting.\n        :param input_shape: The shape of the input data, in the form (b, f)\n        :param num_filters: Number of output filters\n        :param use_bias: Whether to use biases or not.\n        \"\"\"\n        super(MetaLinearLayer, self).__init__()\n        b, c = input_shape\n\n        self.use_bias = use_bias\n        self.weights = nn.Parameter(torch.ones(num_filters, c))\n        nn.init.xavier_uniform_(self.weights)\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.zeros(num_filters))\n\n    def forward(self, x, params=None):\n        \"\"\"\n        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n        Otherwise passed params will be used to execute the function.\n        :param x: Input data batch, in the form (b, f)\n        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n        Otherwise the external are used.\n        :return: The result of the linear function.\n        \"\"\"\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n            if self.use_bias:\n                (weight, bias) = params[\"weights\"], params[\"bias\"]\n            else:\n                (weight) = params[\"weights\"]\n                bias = None\n        else:\n            pass\n            #print('no inner loop params', self)\n\n            if self.use_bias:\n                weight, bias = self.weights, self.bias\n            else:\n                weight = self.weights\n                bias = None\n        # print(x.shape)\n        out = F.linear(input=x, weight=weight, bias=bias)\n        return out\n\n\nclass MetaStepLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        super(MetaStepLossNetwork, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        out = x\n\n        self.linear1 = MetaLinearLayer(input_shape=self.input_shape,\n                                                    num_filters=self.input_dim, use_bias=True)\n\n        self.linear2 = MetaLinearLayer(input_shape=(1, self.input_dim),\n                                                    num_filters=1, use_bias=True)\n\n\n        out = self.linear1(out)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n    def forward(self, x, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n\n        linear1_params = None\n        linear2_params = None\n\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n\n            linear1_params = params['linear1']\n            linear2_params = params['linear2']\n\n        out = x\n        \n        out = self.linear1(out, linear1_params)\n        out = F.relu_(out)\n        out = self.linear2(out, linear2_params)\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass MetaLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        \"\"\"\n        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n        used at inference time. Enables inner loop optimization readily.\n        :param im_shape: The input image batch shape.\n        :param num_output_classes: The number of output classes of the network.\n        :param args: A named tuple containing the system's hyperparameters.\n        :param device: The device to run this on.\n        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n        be enabled. \n        \"\"\"\n        super(MetaLossNetwork, self).__init__()\n        \n        self.device = device\n        # self.args = args\n        self.notspi = notspi\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        self.layer_dict = nn.ModuleDict()\n        # ф\n        for i in range(self.num_steps): \n            self.layer_dict['step{}'.format(i)] = MetaStepLossNetwork(self.input_dim, notspi=self.notspi, device=self.device)\n\n            out = self.layer_dict['step{}'.format(i)](x)\n\n    def forward(self, x, num_step, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n        param_dict = dict()\n\n        if params is not None: \n            # params = {key: value[0] for key, value in params.items()}\n            param_dict = extract_top_level_dict(current_dict=params)\n            \n        for name, param in self.layer_dict.named_parameters():\n            path_bits = name.split(\".\")\n            layer_name = path_bits[0]\n            if layer_name not in param_dict:\n                param_dict[layer_name] = None\n\n            \n        out = x\n        \n        out = self.layer_dict['step{}'.format(num_step)](out, param_dict['step{}'.format(num_step)])\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass StepLossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(StepLossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.output_dim = num_loss_net_layers * 2 * 2 # 2 for weight and bias, another 2 for multiplier and offset\n\n        self.linear1 = nn.Linear(input_dim, input_dim)\n        self.activation = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(input_dim, self.output_dim)\n\n        self.multiplier_bias = nn.Parameter(torch.zeros(self.output_dim // 2))\n        self.offset_bias = nn.Parameter(torch.zeros(self.output_dim // 2))\n\n    def forward(self, task_state, num_step, loss_params):\n        # ψ\n        out = self.linear1(task_state)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n        generated_multiplier, generated_offset = torch.chunk(out, chunks=2, dim=-1)\n        # generated_multiplier, generated_offset = torch.split(out, split_size_or_sections=4)\n\n        i = 0\n        updated_loss_weights = dict()\n        for key, val in loss_params.items():\n            if 'step{}'.format(num_step) in key:\n                updated_loss_weights[key] = (1 + self.multiplier_bias[i] * generated_multiplier[i]) * val + \\\n                                             self.offset_bias[i] * generated_offset[i]\n                # updated_loss_weights[key] = generated_multiplier[i] * val + generated_offset[i]\n                i+=1\n\n        return updated_loss_weights\n\n\nclass LossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(LossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.loss_adapter = nn.ModuleList()\n        for i in range(self.num_steps): \n            self.loss_adapter.append(StepLossAdapter(input_dim, num_loss_net_layers, notspi=notspi, device=device))\n\n    def forward(self, task_state, num_step, loss_params):\n        return self.loss_adapter[num_step](task_state, num_step, loss_params)\n\n# Anil-跟车  version1\n# %matplotlib inline\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport copy\nimport random\nimport logging\n# from torch.utils.tensorboard import SummaryWriter\n# from Anil_TCN import TemporalConvNet\n# from net.meta_neural_network_architectures import MetaLossNetwork, LossAdapter\n\n\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/Anil_LSTM_log\")\n\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\n# 保存日志\ndef get_logger(filename, verbosity=1, name=None):\n    # 设置不同verbosity对应的日志级别\n    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n    # 设置日志输出格式\n    formatter = logging.Formatter(\n        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level_dict[verbosity])\n    # 创建文件处理器，将日志写入文件\n    fh = logging.FileHandler(filename, \"w\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # 创建控制台处理器，将日志输出到控制台\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n# 日志保存路径\ndataset = 'Waymo'\nlogger = get_logger(f'/kaggle/working/MeTAL_Anil_{device}_a0.01_b0.01_{dataset}_tcn.log')\n\nif dataset == 'HighD':\n    Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\n    max_len = 375 # for HighD dataset is 375 for others are 150\nelse:\n    Ts = 0.1\n    max_len = 150\nhis_horizon = 10\n\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len, Ts = Ts):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n        self.Ts = Ts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / self.Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(nn.Linear(input_size,256),\n                nn.ReLU(),\n                nn.Linear(256,256),\n                nn.ReLU())\n        self.head=nn.Sequential(nn.Linear(256,1))\n        self.input_size = input_size\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights=None): \n        if weights == None:\n            out = self.net(x)\n            out = self.head(out)\n        else:\n            out = self.net(x)\n            out = F.linear(out,weights[0],weights[1])\n        return out\n\n\n# 计算LSTM输出\nclass out_lstm(nn.Module):\n    def __init__(self):\n        super(out_lstm, self).__init__()\n\n    def forward(self, inputs):\n        encx, (hn, cn) = inputs\n        if len(hn.shape) == 3:\n            hn = hn[-1] # (32,16)\n        return hn\n\n# 计算GRU输出\nclass out_gru(nn.Module):\n    def __init__(self):\n        super(out_gru, self).__init__()\n\n    def forward(self, inputs):\n        encx, hn = inputs\n        if len(hn.shape) == 3:\n            hn = hn[-1] # (32,16)\n        return hn\n\n# LSTM模型\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        self.net = nn.Sequential(\n            nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout),\n            out_lstm(),\n        )\n        self.head = nn.Sequential(\n            nn.Linear(hidden_size,hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1)\n        )\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.lstm_layers = lstm_layers\n        self.dropout = dropout\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    def forward(self, x, weights=None):\n        if weights == None:\n            out = self.net(x)\n            out = self.head(out)\n        else:\n            out = self.net(x)\n            out = F.linear(out,weights[0],weights[1])\n            out = F.relu(out)\n            out = F.linear(out,weights[2],weights[3])\n        return out\n\n# GRU模型\nclass GRU(nn.Module):\n    def __init__(self, input_size=3, hidden_size=16, dropout=0.4):\n        super(GRU, self).__init__()\n        self.net = nn.Sequential(\n            nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=False),\n            out_gru(),\n            nn.Dropout(dropout),\n        )\n        # self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n        self.head = nn.Sequential(\n            nn.Linear(hidden_size, 8),\n            # nn.ReLU(), \n            # nn.Linear(8, 8),\n            # nn.ReLU(),\n            nn.Tanh(),\n            nn.Linear(8, 1)\n        )\n    def forward(self, x, weights=None):\n        if weights == None:\n            out = self.net(x)\n            out = self.head(out)\n        else:\n            out = self.net(x)\n            out = F.linear(out,weights[0],weights[1])\n            # out = F.relu(out)\n            out = F.tanh(out)\n            out = F.linear(out,weights[2],weights[3])\n            # out = F.relu(out)\n            # out = F.linear(out,weights[4],weights[5])\n        return out\n    # def forward(self, input):\n    #     output, h_n = self.gru(input, None)# output:(seq_len, batch_size, hidden_size)，h0可以直接None\n    #     # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n    #     if len(h_n.shape) == 3:\n    #         h_n = h_n[-1] # (32,16)\n    #         # h_n = h_n[-1, :, :]# output:(batch_size, hidden_size)\n    #     out = self.mlp(h_n)# 进过一个多层感知机，也就是全连接层，output:(batch_size, output_size)\n    #     return out\n\n\n# 计算ttc\ndef calculate_safety(ttc):\n    minimum_ttc = min(ttc)\n    return minimum_ttc\n# 多数据同时验证\ndef model_evaluate(model,his_horizon,testdata):\n    # random.seed(SEED)\n    # np.random.seed(SEED)\n    # torch.manual_seed(SEED)\n    model.eval()\n\n    jerk_set = torch.empty(0).to(device)\n    error_set = torch.empty(0).to(device)\n    minimum_ttc_set = torch.empty(0).to(device)\n\n    criterion = nn.MSELoss()\n    count = 0\n    col = 0\n    Ts = 0.1\n    max_len = 150\n    if dataset == 'HighD':\n        Ts = 0.04\n        max_len = 375\n    for i, item in tqdm(enumerate(testdata)):#每个样本挨个出来\n        \n        x_data, y_data = item['inputs'], item['label']\n        x_data = torch.stack(x_data)#3x149\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        count += B\n        x_data_orig = x_data.clone().detach()#提前克隆的副本\n        \n        col_list = torch.full((B,), -1).to(device)\n        acc_batch = torch.empty(B,0).to(device)\n        ttc_batch = torch.empty(B,0).to(device)\n\n        \n        for frame in range(his_horizon, T):#对32个样本检测\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]   # (20, 3, 10)\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            #x = x.transpose(0,1).reshape(B, -1)\n            acc_pre = model(x).squeeze()\n            if acc_pre.dim() == 0:\n                acc_pre = acc_pre.unsqueeze(0)\n            acc_batch = torch.cat((acc_batch, acc_pre.unsqueeze(1)), dim=1)#记录所有acc     \n            if model_type == 'cnn1d' or model_type == 'tcn':\n                if frame < T-1:\n                    # 根据当前速度和加速度计算下一时间速度\n                    sv_spd_ = x_data[:, 1, frame] + acc_pre*Ts # (32)\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n                    # 计算下一时间速度的相对速度\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n                    # 该时刻真实相对速度\n                    delta_v = x_data[:, -1, frame] # (32)\n                    # 通过两车车距加上相对位移得到下一时间段车距 ？\n                    spacing_ = x_data[:, 0, frame] + Ts*(delta_v + delta_v_)/2 # (32)\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    # update 根据计算得到的值，更新下一时间的值\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[:, :, frame + 1] = next_frame_data\n            else:        \n                if frame < T-1:#开环训练，不断依照预测值更新下一个时间节点的数据\n                    sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts#新的自车速度\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)#保证速度不小于等于0\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_#vt临时计算的\n                    delta_v = x_data[frame, :, -1]#v0原有的\n                    spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2#新的距离\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[frame + 1] = next_frame_data\n            if spacing_.dim() == 0:\n                ttc_batch_ = (-spacing_ / delta_v_).unsqueeze(0)\n            else:\n                ttc_batch_ = (-spacing_ / delta_v_)\n                \n            ttc_batch = torch.cat((ttc_batch, ttc_batch_.unsqueeze(1)), dim=1)\n        for i in range(B):#对32个样本统一处理\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                spacing_obs = x_data_orig[i,0,...]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[i, 0, :col_list[i]]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[i, 0, :]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            else:\n                spacing_obs = x_data_orig[...,i,0]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[:col_list[i],i, 0]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[:,i, 0]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            acc_batch_ = acc_batch_.cpu().detach().numpy()\n            jerk_single = np.mean(np.abs(np.diff(acc_batch_)/Ts))\n            TTC_single = [x for x in ttc_batch_ if x >= 0]#除去其中TTC小于0的\n            if len(TTC_single) > 0:\n                minimum_ttc_single = calculate_safety(TTC_single)#该样本的最小TTC\n                minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)      \n            jerk_set= torch.cat((jerk_set,torch.tensor(jerk_single).unsqueeze(0).to(device)), dim=0)\n            error_set= torch.cat((error_set,torch.tensor(error_single).unsqueeze(0).to(device)), dim=0)\n        col = col + torch.sum(col_list != -1).item()\n    if len(minimum_ttc_set) == 0:\n        ttc = 0\n    else:\n        ttc = sum(minimum_ttc_set)/len(minimum_ttc_set)\n    error = sum(error_set)/len(error_set)\n    return error,col,count,sum(jerk_set)/len(jerk_set),ttc#还需修改（以MSE为基准）\n\ndef finetuning(best_mse_state):\n    # random.seed(SEED)\n    # np.random.seed(SEED)\n    # torch.manual_seed(SEED)\n\n    # test_epoch += 1\n    # logger.info(\"MAML_Epoch: {0}\".format(epoch))\n    # state_path = os.path.join(directory, file_name)\n    # Train\n    Ts = 0.1\n    max_len = 150\n    if dataset == 'HighD':\n        Ts = 0.04\n        max_len = 375\n    og_net = anil.net\n    # 创建一个与原始网络结构相同的虚拟网络\n    if model_type == 'nn':\n        dummy_net = nn_model(input_size = his_horizon*3)\n    elif model_type == 'lstm':\n        dummy_net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n    elif model_type == 'gru':\n        dummy_net = GRU(input_size = 3).to(device) # single layer lstm\n    elif model_type == 'tcn':\n        dummy_net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\n    dummy_net=dummy_net.to(device)\n    # 加载原始网络的权重\n    # weight_path = torch.load(state_path)\n    # dummy_net.load_state_dict(weight_path.state_dict())\n    dummy_net.load_state_dict(og_net.state_dict())\n    # 进行迭代，每次更新虚拟网络的参数\n    num_shots=5\n    lr = 0.01\n    loss_fn=nn.MSELoss()\n    optim=torch.optim.Adam\n    opt=optim(dummy_net.parameters(),lr=lr, weight_decay=3e-4)\n\n\n    # 数据集划分\n    def split_data(data,data_ratio):\n        # random.seed(SEED)\n        # np.random.seed(SEED)\n        # torch.manual_seed(SEED)\n        if data_ratio > 0 and data_ratio <= 1:\n            # 如果小于等于1，根据输入百分比计算获取数据集的数量\n            data_size=int(len(data)*data_ratio)\n        else:\n            # 如果大于1，则data_ratio为获取数据集中的数量\n            data_size = data_ratio\n        return data[:data_size]\n    # np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n    # 获取数据集的数量\n    K=40\n    dataset_train = split_data(train_data, K)\n    dataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len, Ts=Ts)\n    train_loader = DataLoader(\n                dataset_loader_train,\n                batch_size=10,\n                shuffle=True,\n                num_workers=1,\n                drop_last=True)\n    dataset_test = test_data\n    dataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len, Ts=Ts)\n    test_loader = DataLoader(\n                dataset_loader_test,\n                batch_size=256,\n                shuffle=False,\n                num_workers=1,\n                drop_last=False)\n    # dataset_val = NGSIM_val\n    # dataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len, Ts=Ts)\n    # val_loader = DataLoader(\n    #             dataset_loader_val,\n    #             batch_size=10,\n    #             shuffle=True,\n    #             num_workers=1,\n    #             drop_last=True)\n\n\n    # 初始化变量\n    train_loss_his = [] # 训练损失\n\n    # 训练过程\n    best_error = 10000\n    for epoch in tqdm(range(num_shots)):\n        train_losses = [] # 记录每个epoch的训练损失\n        dummy_net.train()\n        # 遍历数据集\n        for i, item in enumerate(train_loader):\n            # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n            # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n            x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n            # Put T into the first dimension, B, T, d -> T, B, d\n            # 将x_data中3个(32,374)连接，转换成(3,32,374)\n            x_data = torch.stack(x_data)\n            # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                # x->(bs,3,149)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n                B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            else:\n                # x->(149,bs,3)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n                T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            # print(T,B,d)  # 149  20  3\n                \n            y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n            y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n            # 从历史数据时间步开始遍历\n            for frame in range(his_horizon, T):\n                if model_type == 'cnn1d' or model_type == 'tcn':\n                    x= x_data[:, :, frame-his_horizon:frame]\n                else:\n                    x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n                if model_type == 'nn':\n                    x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n                # 根据his_horizon个数据预测加速度\n                acc_pre = dummy_net(x).squeeze() # (32)\n                y_pre[frame - his_horizon] = acc_pre\n            #计算损失并进行反传及优化\n            loss = loss_fn(y_pre, y_label)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n            opt.step()\n\n            train_losses.append(loss.item())\n        # 计算本轮平均损失\n        train_loss = np.mean(train_losses)\n\n        train_loss_his.append(train_loss)\n        print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n        logger.info(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n        mean_spacing_error,col,count,jerk,miniumu_ttc = model_evaluate(dummy_net,his_horizon,test_loader)\n        logger.info(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n        if epoch == 0:\n            first_step_error = mean_spacing_error\n        if mean_spacing_error < best_error:\n            best_error = mean_spacing_error\n        if mean_spacing_error < best_mse_state:\n            best_mse_state = mean_spacing_error\n            # save the best model\n            with open(save_path, 'wb') as f:\n                torch.save(dummy_net, f)\n    return best_error, first_step_error, best_mse_state\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion, metal=None):\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x, temp_weights).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        if metal == None:\n            return loss\n        else:\n            return loss, y_pre\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            # Ts = 0.1\n            # max_len = 150\n            Ts = 0.04\n            max_len = 375\n            # print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n            # print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD1_data\n            Ts = 0.1\n            max_len = 150\n            # print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD2_data\n            Ts = 0.1\n            max_len = 150\n            # print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n            # print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len, Ts = Ts)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len, Ts = Ts)\n        return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self, epoch):\n        # 从1到5中随机选择3个不重复的数\n        rng = np.random.RandomState(epoch)\n        data_choice = rng.choice(self.data_range, size=self.n, replace=False) + 1\n        # data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\ndef get_inner_loop_parameter_dict(params):\n    \"\"\"\n    Returns a dictionary with the parameters to use for inner loop updates.\n    :param params: A dictionary of the network's parameters.\n    :return: A dictionary of the parameters to use for the inner loop optimization process.\n    \"\"\"\n    param_dict = dict()\n    for name, param in params:\n        if param.requires_grad:\n            param_dict[name] = param.to(device)\n\n    return param_dict\n\ndef get_per_step_loss_importance_vector(inner_step, current_epoch):\n        \"\"\"\n        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n        loss towards the optimization loss.\n        :return: A tensor to be used to compute the weighted average of the loss, useful for\n        the MSL (Multi Step Loss) mechanism.\n        \"\"\"\n        multi_step_loss_num_epochs = 20\n        loss_weights = np.ones(shape=(inner_step)) * (\n                1.0 / inner_step)\n        decay_rate = 1.0 / inner_step / multi_step_loss_num_epochs\n        min_value_for_non_final_losses = 0.03 / inner_step\n        for i in range(len(loss_weights) - 1):\n            curr_value = np.maximum(loss_weights[i] - (current_epoch * decay_rate), min_value_for_non_final_losses)\n            loss_weights[i] = curr_value\n\n        curr_value = np.minimum(\n            loss_weights[-1] + (current_epoch * (inner_step - 1) * decay_rate),\n            1.0 - ((inner_step - 1) * min_value_for_non_final_losses))\n        loss_weights[-1] = curr_value\n        loss_weights = torch.Tensor(loss_weights).to(device)\n        return loss_weights\n\n\n# Anil元学习训练过程\nclass Anil():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=150)\n        # 初始化 Anil 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.head = net.head\n        self.weights = list(net.head.parameters())  # 获取神经网络的参数\n        self.all_parameters = [net.token] + list(net.network.parameters()) + list(net.attention.parameters()) + list(net.head.parameters())\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        # self.meta_optimiser = torch.optim.Adam(self.all_parameters, self.beta, weight_decay=1e-4)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.meta_test = []\n        self.finetuning_error = []\n        self.plot_every = 10  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = int(self.k/2)\n        self.all_best_state = 10000 # 用于存储微调后最好mse时的模型权重\n        self.inner_step = 1\n        params = [{'params': self.all_parameters}]\n        if MeTAL:\n            base_learner_num_layers = len(self.weights)\n            support_meta_loss_num_dim = base_learner_num_layers + 2 + 1\n            support_adapter_num_dim = base_learner_num_layers + 1\n\n            self.meta_loss = MetaLossNetwork(support_meta_loss_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_loss_adapter = LossAdapter(support_adapter_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_loss_adapter.parameters()}\n            params.append(new_params)\n\n            if semi_supervised:\n                query_num_dim = base_learner_num_layers + 1 + 1\n                self.meta_query_loss = MetaLossNetwork(query_num_dim, notspi=self.inner_step, device=device).to(device=device)\n                self.meta_query_loss_adapter = LossAdapter(query_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n                new_params = {'params': self.meta_query_loss.parameters()}\n                params.append(new_params)\n                new_params = {'params': self.meta_query_loss_adapter.parameters()}\n                params.append(new_params)\n        self.meta_optimiser = torch.optim.Adam(params, self.beta, weight_decay=1e-4)\n\n    def inner_loop(self, task):\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        for step in range(self.inner_step):\n            if MeTAL:\n                support_task_state = []\n                # 获取支持集 Loss 和 预测值\n                support_loss, support_preds = inner_train(dataloader1, self.net, temp_weights, self.criterion, metal=1)\n                support_loss /= int(self.batch_size)\n                support_task_state.append(support_loss)\n\n                # 将每层模型权重的平均值添加到task_state中\n                for v in temp_weights:\n                    support_task_state.append(v.mean())\n\n                support_task_state = torch.stack(support_task_state)\n                # 归一化\n                adapt_support_task_state = (support_task_state - support_task_state.mean())/(support_task_state.std() + 1e-12)                                                 \n                # 损失函数网络参数更新\n                updated_meta_loss_weights = self.meta_loss_adapter(adapt_support_task_state, step, self.names_loss_weights_copy)\n                for i, item in enumerate(dataloader1):\n                    acceleration = item['label']\n                # 将预测值添加到task_state\n                support_task_state = torch.cat((\n                    support_task_state.view(1, -1).expand(support_preds.size(1), -1),\n                    support_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                    acceleration.mean(dim=1, keepdim=True).float().to(device)\n                ), -1)\n\n                support_task_state = (support_task_state - support_task_state.mean()) / (support_task_state.std() + 1e-12)\n                # 计算support_loss\n                meta_support_loss = self.meta_loss(support_task_state, step, params=updated_meta_loss_weights).mean().squeeze()\n                if semi_supervised:\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    query_loss, query_preds = inner_train(dataloader2, self.net, temp_weights, self.criterion, metal=1)\n                    query_loss /= int(self.batch_size)\n                    query_task_state = []\n                    \n                    for i, item in enumerate(dataloader2):\n                        acceleration = item['label']\n                    # query_task_state.append(query_loss)\n\n                    for v in temp_weights:\n                        query_task_state.append(v.mean())\n                    query_task_state = torch.stack(query_task_state)\n                    query_task_state = torch.cat((\n                                query_task_state.view(1, -1).expand(query_preds.size(1), -1),\n                                query_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                                acceleration.mean(dim=1, keepdim=True).float().to(device)\n                    ), -1)\n\n                    query_task_state = (query_task_state - query_task_state.mean())/(query_task_state.std() + 1e-12)\n                    updated_meta_query_loss_weights = self.meta_query_loss_adapter(query_task_state.mean(0), step, self.names_query_loss_weights_copy)\n\n                    meta_query_loss = self.meta_query_loss(query_task_state, step, params=updated_meta_query_loss_weights).mean().squeeze()\n\n                    loss = support_loss + meta_support_loss + meta_query_loss\n                else:\n                    loss = support_loss + meta_support_loss\n            else:\n                # 训练过程\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n            grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n        temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # 临时参数更新 梯度下降\n        dataloader2 = DataLoader(\n                dataset_loader2,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n        return metaloss\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        total_loss = 0\n        best_loss = 10000\n        best_first_step_error = 10000\n        best_index = 0\n        for epoch in tqdm(range(1, num_epochs + 1)):\n            if MeTAL:\n                self.names_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_loss.named_parameters())\n                if semi_supervised:\n                    self.names_query_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_query_loss.named_parameters())\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task(epoch)\n            for i in tasks:\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss.backward(retain_graph=True)\n                metaloss_sum += metaloss  # mete_loss求和\n            # metagrads = torch.autograd.grad(metaloss_sum, self.weights)  # 计算元学习损失对参数的梯度\n            # 重要步骤：使用元学习梯度更新网络参数\n            # for w, g in zip(self.weights, metagrads):\n            #     w.grad = g\n            if MeTAL:\n                meta_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss.parameters(), meta_loss_grads):\n                    w.grad = g\n\n                meta_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss_adapter.parameters(), meta_loss_adapter_grads):\n                    w.grad = g\n\n                if semi_supervised:\n                    meta_query_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss.parameters(), meta_query_loss_grads):\n                        w.grad = g\n\n                    meta_query_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss_adapter.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss_adapter.parameters(), meta_query_loss_adapter_grads):\n                        w.grad = g\n            for p in self.all_parameters:\n                p.grad.data.mul_(1.0 / len(tasks))\n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            mserror,col,count,jerk,ttc = model_evaluate(self.net,his_horizon,meta_loader)\n            if epoch % self.print_every == 0:\n                logger.info(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                logger.info(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n                if total_loss < best_loss:\n                    best_loss = total_loss\n                    best_index = epoch\n                    # with open(file_path, 'wb') as f:\n                    #     torch.save(self.net, f)\n                self.meta_losses.append(total_loss / self.print_every)\n                self.meta_test.append(mserror.cpu().detach().numpy() / self.print_every)\n            # if epoch % self.plot_every == 0:\n            total_loss = 0\n            fine_error, first_step_error, best_mse_state = finetuning(self.all_best_state)\n            self.all_best_state = best_mse_state\n            if first_step_error < best_first_step_error:\n                best_first_step_epoch = epoch\n                best_first_step_error = first_step_error\n            self.finetuning_error.append(fine_error.cpu().detach().numpy())\n        print(\"(\", best_index, \")\",\"best_loss:\", best_loss)\n        print(\"(\", best_first_step_epoch, \")\", \"best_first_step_error:\", best_first_step_error)\n\n# dataset = 'NGSIM'\nmodel_type = 'tcn'\nMeTAL = True   # 任务自适应loss函数\nsemi_supervised = True  # 使用查询集 作为半监督\nif dataset == 'SPMD1':\n    train_data = SPMD1_train\n    test_data = SPMD1_test\nelif dataset == 'SPMD2':\n    train_data = SPMD2_train\n    test_data = SPMD2_test\nelif dataset == 'Waymo':\n    train_data = Waymo_train\n    test_data = Waymo_test\nelif dataset == 'NGSIM':\n    train_data = NGSIM_train\n    test_data = NGSIM_test\nelif dataset == 'Lyft':\n    train_data = Lyft_train\n    test_data = Lyft_test\nelif dataset == 'HighD':\n    train_data = HighD_train\n    test_data = HighD_test\nmeta_test = test_data\nmeta_loader_test = ImitationCarFolData(meta_test, max_len = max_len, Ts=Ts)\nmeta_loader = DataLoader(\n            meta_loader_test,\n            batch_size=256,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n# 定义保存文件的文件夹路径\nsave_folder = '/kaggle/working'\nsave = f'MeTAL_Anil_{device}_a0.01_b0.01_{dataset}_{model_type}.pt' # 保存模型文件\n# file_path = \"/home/ubuntu/fedavg/FollowNet-car/Anil_state/Anil_nn_a0.01_b0.0001_k300_E10000.pt\"\n# 确保保存文件的文件夹存在，如果不存在，则创建\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n# 定义文件保存路径\nsave_path = os.path.join(save_folder, save)\n# 模型初始化\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\nelif model_type == 'gru':\n    net = GRU(input_size = 3).to(device) # single layer lstm\nelif model_type == 'tcn':\n    net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\ndata_tasks=DataDistribution(data_range=5, n=3)\nanil=Anil(net,alpha=0.01,beta=0.01,tasks=data_tasks,k=200)\nanil.outer_loop(num_epochs=500)\n\nplt_path = \"/kaggle/working\"\nplt.plot(anil.meta_losses)\nplt.savefig(os.path.join(plt_path, f'MeTAL_Anil_{device}_a0.01_b0.01_{dataset}_Loss_tcn.png'))\nplt.show()\nplt.close()\n\nplt.plot(anil.meta_test)\nplt.savefig(os.path.join(plt_path, f'MeTAL_Anil_{device}_a0.01_b0.01_{dataset}_mserror_tcn.png'))\nplt.show()\nplt.close()\n\nplt.plot(anil.finetuning_error)\nplt.savefig(os.path.join(plt_path, f'MeTAL_Anil_{device}_a0.01_b0.01_{dataset}_finetuning_mserror_tcn.png'))\nplt.show()\nplt.close()\n\n# 将数据保存到文件中\nnp.save(f'/kaggle/working/MeTAL_Anil_{device}_tcnks5_1632_a0.01_b0.01_step1_k200_K40_Mese_acc_meta_losses.npy', np.array(anil.meta_losses))\nnp.save(f'/kaggle/working/MeTAL_Anil_{device}_tcnks5_1632_a0.01_b0.01_step1_k200_K40_Mese_acc_meta_test.npy', np.array(anil.meta_test))\nnp.save(f'/kaggle/working/MeTAL_Anil_{device}_tcnks5_1632_a0.01_b0.01_step1_k200_K40_Mese_acc_finetuning_error.npy', np.array(anil.finetuning_error))\n\nprint('----------------------')\nlogger.info(\"--------------------------\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:01:31.014453Z","iopub.execute_input":"2024-07-24T02:01:31.014867Z"},"trusted":true},"execution_count":null,"outputs":[]}]}