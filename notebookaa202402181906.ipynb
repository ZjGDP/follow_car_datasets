{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7439658,"sourceType":"datasetVersion","datasetId":4330046},{"sourceId":7476209,"sourceType":"datasetVersion","datasetId":4351914},{"sourceId":7512101,"sourceType":"datasetVersion","datasetId":4375343},{"sourceId":7661903,"sourceType":"datasetVersion","datasetId":4467677},{"sourceId":7987664,"sourceType":"datasetVersion","datasetId":4702006}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 原跟车训练测试过程\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nwaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# car_following_data = HighD_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # HighD(12541, 4, 375)\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts  # (max_len - 1)\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# Define data-driven car-following models\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3, hidden_size = 256):\n        super(nn_model, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n            nn.Tanh(),\n            )\n\n    def forward(self, x):\n        out = ACC_LIMIT*self.encoder(x)\n        return out\n\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nnn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n        nn.init.normal_(self.linear1.weight, 0, 0.02)\n        nn.init.constant_(self.linear1.bias, 0.0)\n        nn.init.normal_(self.linear2.weight, 0, 0.02)\n        nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\n\n# Train\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = split_train(car_following_data,0.8,0.1)\n# train_data = Lyft_train\n# test_data = Lyft_test\n# val_data = Lyft_val\ntrain_data = NGSIM_train\ntest_data = NGSIM_test\nval_data = NGSIM_val\nprint(train_data.shape, test_data.shape, val_data.shape)\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nbatch_size = 32\ntotal_epochs = 30\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\n# lr = 0.01\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\n# 根据名称定义模型\nif model_type == 'nn':\n    model = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    model = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\nmodel_state = list(model.parameters())\nprint(model_state)\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\nprint(\"----\")\n# 训练过程\nfor epoch in tqdm(range(total_epochs)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    model.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = model(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        model_optim.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    model.eval()\n    error_list = []\n    for i, item in enumerate(validation_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n            acc_pre = model(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n        model_optim.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n        with open(save, 'wb') as f:\n            torch.save(model, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# 用该数据集训练得到的模型去对其他数据集进行测试\n# Ts = 0.04\n# max_len = 375\n# # car_following_data = NGSIM_data\n# # print(car_following_data.shape)\n# # train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# test_data = HighD_test\n# # 创建测试集 DataLoader\n# test_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\n# test_loader = DataLoader(\n#         test_dataset,\n#         batch_size=batch_size,\n#         shuffle=False,\n#         num_workers=1,\n#         drop_last=True)\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.04\n# max_len = 375\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n#             MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 只含有test和指标计算代码部分，用于使用在不同数据集下训练的模型用在其他数据集上测试\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nwaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\n# car_following_data = NGSIM_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # (12541, 4, 375)\n\n# # 除HighD的数据集需要进行连接\n# def car_stack(data):\n#     # 创建一个空的目标数组，形状为 (1930, 4, 600)\n#     target_shape = (data.shape[0], data.shape[1], data[0, 0].shape[0])\n#     target_array = np.empty(target_shape)\n\n#     # 遍历原始数组，将每个元素转换为 (4, 600) 的子数组，并放入目标数组\n#     for i in range(data.shape[0]):\n#         for j in range(data.shape[1]):\n#             target_array[i, j, :] = data[i, j]\n#     # 最终 target_array 的形状为 (1930, 4, 600)\n#     return target_array\n\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的除最后一项的前 max_len-1 个时间步\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# Define data-driven car-following models\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3, hidden_size = 256):\n        super(nn_model, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1),\n            nn.Tanh(),\n            )\n\n    def forward(self, x):\n        out = ACC_LIMIT*self.encoder(x)\n        return out\n\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nnn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n        nn.init.normal_(self.linear1.weight, 0, 0.02)\n        nn.init.constant_(self.linear1.bias, 0.0)\n        nn.init.normal_(self.linear2.weight, 0, 0.02)\n        nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\n\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = Lyft_train, Lyft_test, Lyft_val\ntrain_data = NGSIM_train\ntest_data = NGSIM_test\nval_data = NGSIM_val\nprint(test_data.shape)\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nbatch_size = 32\ntotal_epochs = 20\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\n# 根据名称定义模型\nif model_type == 'nn':\n    model = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    model = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=MyDevice)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_)\n\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 求一些值用\nfrom torch import nn, optim\nimport torch\nimport torch.nn as nnn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s   15/150s\n# max_len = 375 # for HighD dataset is 375 for others are 150\nTs = 0.1\nmax_len = 150\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 获取每个跟车事件的时间步长\ndef DataMin(dataset):\n    data_min = []\n    for i in range(dataset.shape[0]):\n        data_min.append(dataset[i][0].size)\n    return data_min\n\ndatamin = DataMin(NGSIM_data)\n# print(datamin)\nprint(\"最小时间步：\", np.min(datamin))\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n# 求原测试集中的平均jerk\nTs = 0.1\nmax_len = 150\ndataset_test = Lyft_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\nhis_horizon = 10\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n#     x_data, y_data = x_data.transpose(0, 2).float(), y_data.transpose(0, 1).float()\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B = y_data.shape # (total steps, batch_size, d) as the shape of the data\n#     print(T,B)\n    a = np.diff(y_data)/Ts\n    b = np.abs(a)\n    jerk =np.mean(b)\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"jerk\", np.mean(jerk_set))","metadata":{"execution":{"iopub.status.busy":"2024-02-19T01:21:49.760181Z","iopub.execute_input":"2024-02-19T01:21:49.760557Z","iopub.status.idle":"2024-02-19T01:22:00.390929Z","shell.execute_reply.started":"2024-02-19T01:21:49.760523Z","shell.execute_reply":"2024-02-19T01:22:00.389827Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"cuda:0\n最小时间步： 150\n","output_type":"stream"},{"name":"stderr","text":"2240it [00:07, 288.08it/s]","output_type":"stream"},{"name":"stdout","text":"jerk 0.7705402415347494\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车  version1\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        # 根据索引划分数据集\n        data_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]\n        data_indices2 = shuffled_indices[data_num:data_ratio]\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# 定义模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(input_size,256)),\n            ('relu1',nn.ReLU()),\n            ('l2',nn.Linear(256,256)),\n            ('relu2',nn.ReLU()),\n            ('l3',nn.Linear(256,1))\n        ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def argforward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net.argforward(x, temp_weights).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len)\n        return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = int(self.k/2)\n\n    def inner_loop(self, task):\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程\n        loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n        grads = torch.autograd.grad(loss, temp_weights)  # 计算损失对参数的梯度\n        temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # 临时参数更新 梯度下降\n        dataloader2 = DataLoader(\n                dataset_loader2,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n        return metaloss\n\n    def outer_loop(self, num_epochs):  # epoch 500\n        total_loss = 0\n        for epoch in range(1, num_epochs + 1):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()\n            for i in tasks:\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights)  # 计算元学习损失对参数的梯度\n            # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'nn'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=400)\n\n\nmaml.outer_loop(num_epochs=10000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn.Sequential(OrderedDict([\n        ('l1', nn.Linear(his_horizon*3,256)),\n        ('relu1', nn.ReLU()),\n        ('l2', nn.Linear(256,256)),\n        ('relu2', nn.ReLU()),\n        ('l3', nn.Linear(256,1))\n    ]))\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=5\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.SGD\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n#         opt.zero_grad()\n#         torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n#         opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T01:31:20.594435Z","iopub.execute_input":"2024-03-08T01:31:20.594910Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cpu\n1/10000. loss: 0.030780471861362457\n2/10000. loss: 0.04516826073328654\n3/10000. loss: 0.3769957224527995\n4/10000. loss: 0.3960939645767212\n5/10000. loss: 0.34749897321065265\n6/10000. loss: 0.03875022133191427\n7/10000. loss: 0.6340972979863485\n8/10000. loss: 1.103097677230835\n9/10000. loss: 0.7345333099365234\n10/10000. loss: 0.8408072789510092\n11/10000. loss: 0.4542378584543864\n12/10000. loss: 0.1158681611220042\n13/10000. loss: 0.06863734622796376\n14/10000. loss: 0.7657466729482015\n15/10000. loss: 0.19987616936365762\n16/10000. loss: 1.2442971070607503\n17/10000. loss: 0.14375762144724527\n18/10000. loss: 0.98065718015035\n19/10000. loss: 1.2417702674865723\n20/10000. loss: 0.20725069443384805\n21/10000. loss: 0.11335354050000508\n22/10000. loss: 0.15266165137290955\n23/10000. loss: 1.0211774508158367\n24/10000. loss: 1.0980748335520427\n25/10000. loss: 0.8192783196767172\n26/10000. loss: 0.6264430284500122\n27/10000. loss: 0.504382848739624\n28/10000. loss: 0.38451504707336426\n29/10000. loss: 0.1360906958580017\n30/10000. loss: 0.12474276622136433\n31/10000. loss: 0.12997093796730042\n32/10000. loss: 0.3321178952852885\n33/10000. loss: 0.3175383408864339\n34/10000. loss: 0.2705320517222087\n35/10000. loss: 0.3122217655181885\n36/10000. loss: 0.18445678551991782\n37/10000. loss: 0.27397119998931885\n38/10000. loss: 0.243138055006663\n39/10000. loss: 0.22698434193929037\n40/10000. loss: 0.18202861150105795\n41/10000. loss: 0.2219420870145162\n42/10000. loss: 0.19615912437438965\n43/10000. loss: 0.288410743077596\n44/10000. loss: 0.10028127829233806\n45/10000. loss: 0.10038862625757854\n46/10000. loss: 0.12347403168678284\n47/10000. loss: 0.15024463335673013\n48/10000. loss: 0.12395741542180379\n49/10000. loss: 0.12782380978266397\n50/10000. loss: 0.15330479542414346\n51/10000. loss: 0.12315595149993896\n52/10000. loss: 0.18812302748362222\n53/10000. loss: 0.1263024906317393\n54/10000. loss: 0.12220177054405212\n55/10000. loss: 0.10237505038579305\n56/10000. loss: 0.13946601748466492\n57/10000. loss: 0.1746010184288025\n58/10000. loss: 0.1360509196917216\n59/10000. loss: 0.09431007504463196\n60/10000. loss: 0.05752186973889669\n61/10000. loss: 0.13718934853871664\n62/10000. loss: 0.13804481426874796\n63/10000. loss: 0.11205055316289265\n64/10000. loss: 0.10126383105913798\n65/10000. loss: 0.1109372079372406\n66/10000. loss: 0.08317519724369049\n67/10000. loss: 0.0831543505191803\n68/10000. loss: 0.0633540153503418\n69/10000. loss: 0.09294744332631429\n70/10000. loss: 0.08310100436210632\n71/10000. loss: 0.06207134326299032\n72/10000. loss: 0.06151878833770752\n73/10000. loss: 0.10621529817581177\n74/10000. loss: 0.037563701470692955\n75/10000. loss: 0.05839581787586212\n76/10000. loss: 0.06452501813570659\n77/10000. loss: 0.04233876864115397\n78/10000. loss: 0.0547116349140803\n79/10000. loss: 0.03507260729869207\n80/10000. loss: 0.037377183636029564\n81/10000. loss: 0.036514535546302795\n82/10000. loss: 0.04188318053881327\n83/10000. loss: 0.027873230477174122\n84/10000. loss: 0.02856268733739853\n85/10000. loss: 0.02346936861673991\n86/10000. loss: 0.0228446622689565\n87/10000. loss: 0.020900989572207134\n88/10000. loss: 0.016984352221091587\n89/10000. loss: 0.02153753737608592\n90/10000. loss: 0.019228039930264156\n91/10000. loss: 0.029676832258701324\n92/10000. loss: 0.012274066607157389\n93/10000. loss: 0.021771493057409923\n94/10000. loss: 0.02576011170943578\n95/10000. loss: 0.01716308792432149\n96/10000. loss: 0.013868536800146103\n97/10000. loss: 0.010404416670401892\n98/10000. loss: 0.011926131943861643\n99/10000. loss: 0.012373042603333792\n100/10000. loss: 0.014536798000335693\n101/10000. loss: 0.013183513035376867\n102/10000. loss: 0.01110409696896871\n103/10000. loss: 0.011629564066727957\n104/10000. loss: 0.010401589795947075\n105/10000. loss: 0.009749958912531534\n106/10000. loss: 0.012787846227486929\n107/10000. loss: 0.011129585405190786\n108/10000. loss: 0.00829542800784111\n109/10000. loss: 0.007229112088680267\n110/10000. loss: 0.011319290846586227\n111/10000. loss: 0.015557395915190378\n112/10000. loss: 0.008968010544776917\n113/10000. loss: 0.0144721120595932\n114/10000. loss: 0.013522079835335413\n115/10000. loss: 0.010947869469722113\n116/10000. loss: 0.009269607563813528\n117/10000. loss: 0.008470458288987478\n118/10000. loss: 0.007664629568656285\n119/10000. loss: 0.0071311021844546\n120/10000. loss: 0.005792911474903424\n121/10000. loss: 0.008416699866453806\n122/10000. loss: 0.007151767611503601\n123/10000. loss: 0.0069066789001226425\n124/10000. loss: 0.007444720094402631\n125/10000. loss: 0.00927892824014028\n126/10000. loss: 0.011488984028498331\n127/10000. loss: 0.00811196118593216\n128/10000. loss: 0.009725059072176615\n129/10000. loss: 0.006300296013553937\n130/10000. loss: 0.004587997371951739\n131/10000. loss: 0.008583924422661463\n132/10000. loss: 0.007284481078386307\n133/10000. loss: 0.006219994276762009\n134/10000. loss: 0.005828443914651871\n135/10000. loss: 0.0067285578697919846\n136/10000. loss: 0.006125390529632568\n137/10000. loss: 0.004259626691540082\n138/10000. loss: 0.005872676149010658\n139/10000. loss: 0.0061950286229451495\n140/10000. loss: 0.008766892676552137\n141/10000. loss: 0.005286309868097305\n142/10000. loss: 0.005281340330839157\n143/10000. loss: 0.0062134650846322375\n144/10000. loss: 0.005885706593592961\n145/10000. loss: 0.00492656013617913\n146/10000. loss: 0.007272087037563324\n147/10000. loss: 0.004539353772997856\n148/10000. loss: 0.003115115687251091\n149/10000. loss: 0.005752951527635257\n150/10000. loss: 0.002841624431312084\n151/10000. loss: 0.003096065173546473\n152/10000. loss: 0.004322901057700316\n153/10000. loss: 0.0038682265828053155\n154/10000. loss: 0.004648126972218354\n155/10000. loss: 0.005397118007143338\n156/10000. loss: 0.003999367045859496\n157/10000. loss: 0.00470776700725158\n158/10000. loss: 0.003550206000606219\n159/10000. loss: 0.002860461672147115\n160/10000. loss: 0.003065311349928379\n161/10000. loss: 0.0020579053089022636\n162/10000. loss: 0.0025554153447349868\n163/10000. loss: 0.002373156137764454\n164/10000. loss: 0.0019953028919796147\n165/10000. loss: 0.005494608233372371\n166/10000. loss: 0.0029520258928338685\n167/10000. loss: 0.0028984639793634415\n168/10000. loss: 0.002502742223441601\n169/10000. loss: 0.0035937512293457985\n170/10000. loss: 0.003047395497560501\n171/10000. loss: 0.001690679540236791\n172/10000. loss: 0.0035089440643787384\n173/10000. loss: 0.0025982285539309182\n174/10000. loss: 0.0035415968547264733\n175/10000. loss: 0.002674019274612268\n176/10000. loss: 0.0019512055441737175\n177/10000. loss: 0.002147058335443338\n178/10000. loss: 0.0023837070912122726\n179/10000. loss: 0.00273426187535127\n180/10000. loss: 0.00294740063448747\n181/10000. loss: 0.0027571329846978188\n182/10000. loss: 0.002830350771546364\n183/10000. loss: 0.0017937988353272278\n184/10000. loss: 0.001557646319270134\n185/10000. loss: 0.0016003701214989026\n186/10000. loss: 0.0014499199266235034\n187/10000. loss: 0.002012966355929772\n188/10000. loss: 0.0018954221159219742\n189/10000. loss: 0.0017705896558860938\n190/10000. loss: 0.0015144259668886662\n191/10000. loss: 0.00238934438675642\n192/10000. loss: 0.0030811280012130737\n193/10000. loss: 0.003122964253028234\n194/10000. loss: 0.002331335097551346\n195/10000. loss: 0.002232526894658804\n196/10000. loss: 0.0013243692616621654\n197/10000. loss: 0.0017432126527031262\n198/10000. loss: 0.002161927210787932\n199/10000. loss: 0.0012569059617817402\n200/10000. loss: 0.002322730297843615\n201/10000. loss: 0.002163671267529329\n202/10000. loss: 0.0025973329320549965\n203/10000. loss: 0.0014890365613003571\n204/10000. loss: 0.001788125994304816\n205/10000. loss: 0.0020764564784864583\n206/10000. loss: 0.0012455872880915801\n207/10000. loss: 0.001959351201852163\n208/10000. loss: 0.0024563043067852655\n209/10000. loss: 0.0015092094739278157\n210/10000. loss: 0.0015846178866922855\n211/10000. loss: 0.0021899652977784476\n212/10000. loss: 0.0018817273279031117\n213/10000. loss: 0.0016632756839195888\n214/10000. loss: 0.0016879883284370105\n215/10000. loss: 0.002323427858452002\n216/10000. loss: 0.0019664911863704524\n217/10000. loss: 0.0011976505629718304\n218/10000. loss: 0.0017336327582597733\n219/10000. loss: 0.002171479475994905\n220/10000. loss: 0.0017174432675043743\n221/10000. loss: 0.001870622547964255\n222/10000. loss: 0.001024694259588917\n223/10000. loss: 0.002766987308859825\n224/10000. loss: 0.0011873421414444845\n225/10000. loss: 0.0012539434246718884\n226/10000. loss: 0.001303812178472678\n227/10000. loss: 0.0020703126986821494\n228/10000. loss: 0.0021612485870718956\n229/10000. loss: 0.0009874793856094282\n230/10000. loss: 0.0019298708066344261\n231/10000. loss: 0.0017987356210748355\n232/10000. loss: 0.002377110533416271\n233/10000. loss: 0.0018941421682635944\n234/10000. loss: 0.0018805544823408127\n235/10000. loss: 0.0015136331009368102\n236/10000. loss: 0.002071139713128408\n237/10000. loss: 0.001507008137802283\n238/10000. loss: 0.0018153226313491662\n239/10000. loss: 0.002248848012338082\n240/10000. loss: 0.0014505207849045594\n241/10000. loss: 0.0020189622106651464\n242/10000. loss: 0.0011042901314795017\n243/10000. loss: 0.0016733685818811257\n244/10000. loss: 0.0013014387028912704\n245/10000. loss: 0.0011063284861544769\n246/10000. loss: 0.002268776452789704\n247/10000. loss: 0.0018428539236386616\n248/10000. loss: 0.0016893820526699226\n249/10000. loss: 0.0014297782133022945\n250/10000. loss: 0.00173859562103947\n251/10000. loss: 0.0016450360417366028\n252/10000. loss: 0.0010114157727609079\n253/10000. loss: 0.0013474894997974236\n254/10000. loss: 0.0011785272508859634\n255/10000. loss: 0.001061013899743557\n256/10000. loss: 0.001240200052658717\n257/10000. loss: 0.0013101914276679356\n258/10000. loss: 0.00239397119730711\n259/10000. loss: 0.002226792275905609\n260/10000. loss: 0.0019531560440858207\n261/10000. loss: 0.0013387979318698247\n262/10000. loss: 0.002660832367837429\n263/10000. loss: 0.001090029642606775\n264/10000. loss: 0.0016610678285360336\n265/10000. loss: 0.0009932967368513346\n266/10000. loss: 0.0017092159638802211\n267/10000. loss: 0.001658118950823943\n268/10000. loss: 0.0010034890534977119\n269/10000. loss: 0.0014842478558421135\n270/10000. loss: 0.0015050203849871953\n271/10000. loss: 0.001656437913576762\n272/10000. loss: 0.001244541723281145\n273/10000. loss: 0.0016619718323151271\n274/10000. loss: 0.0015225362343092759\n275/10000. loss: 0.0013210158795118332\n276/10000. loss: 0.0016362393895785015\n277/10000. loss: 0.0008395101564625899\n278/10000. loss: 0.0015725092962384224\n279/10000. loss: 0.0013586211328705151\n280/10000. loss: 0.001216491839538018\n281/10000. loss: 0.001353134245922168\n282/10000. loss: 0.002105241951843103\n283/10000. loss: 0.0017428947612643242\n284/10000. loss: 0.001810968853533268\n285/10000. loss: 0.0014504107336203258\n286/10000. loss: 0.0015976813932259877\n287/10000. loss: 0.0013616204572220643\n288/10000. loss: 0.001551586203277111\n289/10000. loss: 0.0016194071310261886\n290/10000. loss: 0.001890324056148529\n291/10000. loss: 0.0014062200983365376\n292/10000. loss: 0.0012724646367132664\n293/10000. loss: 0.0009842850267887115\n294/10000. loss: 0.0009767028192679088\n295/10000. loss: 0.0013194374429682891\n296/10000. loss: 0.0008573657833039761\n297/10000. loss: 0.0008888500742614269\n298/10000. loss: 0.0010864267436166604\n299/10000. loss: 0.0013898983597755432\n300/10000. loss: 0.0013675357525547345\n301/10000. loss: 0.0016118007091184456\n302/10000. loss: 0.0022553357606132827\n303/10000. loss: 0.0007949825376272202\n304/10000. loss: 0.0010437427554279566\n305/10000. loss: 0.0007384871132671833\n306/10000. loss: 0.001468454332401355\n307/10000. loss: 0.001565091156711181\n308/10000. loss: 0.0009189618285745382\n309/10000. loss: 0.0008708262660851082\n310/10000. loss: 0.0017213428703447182\n311/10000. loss: 0.0012413025833666325\n312/10000. loss: 0.0009992675234874089\n313/10000. loss: 0.000702517805621028\n314/10000. loss: 0.0010124669255067904\n315/10000. loss: 0.0010871371099104483\n316/10000. loss: 0.0011032276476422946\n317/10000. loss: 0.0007691738040496906\n318/10000. loss: 0.00125398983558019\n319/10000. loss: 0.001938657679905494\n320/10000. loss: 0.0010859320561091106\n321/10000. loss: 0.001641155065347751\n322/10000. loss: 0.0012626979344834883\n323/10000. loss: 0.0009815750333170097\n324/10000. loss: 0.0010029035620391369\n325/10000. loss: 0.0014502075500786304\n326/10000. loss: 0.0009997113763044279\n327/10000. loss: 0.0013723969459533691\n328/10000. loss: 0.0013056847577293713\n329/10000. loss: 0.0011348103483517964\n330/10000. loss: 0.0009706157725304365\n331/10000. loss: 0.0009639174677431583\n332/10000. loss: 0.0013965005055069923\n333/10000. loss: 0.0014222973647216957\n334/10000. loss: 0.001101169114311536\n335/10000. loss: 0.0016991542652249336\n336/10000. loss: 0.0016003741572300594\n337/10000. loss: 0.0009051500043521324\n338/10000. loss: 0.001379136461764574\n339/10000. loss: 0.0011216052807867527\n340/10000. loss: 0.0011821261917551358\n341/10000. loss: 0.001295106407875816\n342/10000. loss: 0.0013054635686179001\n343/10000. loss: 0.0015076071334381898\n344/10000. loss: 0.00120585427309076\n345/10000. loss: 0.0017652617146571477\n346/10000. loss: 0.0013363974479337533\n347/10000. loss: 0.0011508118671675522\n348/10000. loss: 0.001661204732954502\n349/10000. loss: 0.001809803768992424\n350/10000. loss: 0.0015775275727113087\n351/10000. loss: 0.001966670465966066\n352/10000. loss: 0.0012625088760008414\n353/10000. loss: 0.001056994078680873\n354/10000. loss: 0.0010532958743472893\n355/10000. loss: 0.0012616094512244065\n356/10000. loss: 0.0013969217737515767\n357/10000. loss: 0.0014495295472443104\n358/10000. loss: 0.0021014466571311155\n359/10000. loss: 0.0009249305973450342\n360/10000. loss: 0.0012870017283906539\n361/10000. loss: 0.0011156902182847261\n362/10000. loss: 0.0018827235326170921\n363/10000. loss: 0.0008180040555695692\n364/10000. loss: 0.0009777368977665901\n365/10000. loss: 0.0018047560006380081\n366/10000. loss: 0.0011820630946507056\n367/10000. loss: 0.0017396692807475727\n368/10000. loss: 0.0012828251346945763\n369/10000. loss: 0.0013854031761487324\n370/10000. loss: 0.0011211501744886239\n371/10000. loss: 0.0018690256401896477\n372/10000. loss: 0.0016547680522004764\n373/10000. loss: 0.001420949740956227\n374/10000. loss: 0.0016861225788791974\n375/10000. loss: 0.0020289594928423562\n376/10000. loss: 0.0011137464704612892\n377/10000. loss: 0.001303676205376784\n378/10000. loss: 0.0016214102506637573\n379/10000. loss: 0.0010001350504656632\n380/10000. loss: 0.0014012505610783894\n381/10000. loss: 0.001351978008945783\n382/10000. loss: 0.001305945838491122\n383/10000. loss: 0.0012790953430036704\n384/10000. loss: 0.001562219113111496\n385/10000. loss: 0.0007328620801369349\n386/10000. loss: 0.0009054255982240041\n387/10000. loss: 0.0009531867690384388\n388/10000. loss: 0.0010909334135552247\n389/10000. loss: 0.0010889661498367786\n390/10000. loss: 0.000904894977187117\n391/10000. loss: 0.0013285147336622078\n392/10000. loss: 0.0013514285286267598\n393/10000. loss: 0.0012055146507918835\n394/10000. loss: 0.0007156785577535629\n395/10000. loss: 0.0014222245663404465\n396/10000. loss: 0.0010322478289405506\n397/10000. loss: 0.001207747186223666\n398/10000. loss: 0.0016112032656868298\n399/10000. loss: 0.0012914206211765606\n400/10000. loss: 0.0010235062800347805\n401/10000. loss: 0.0016467319801449776\n402/10000. loss: 0.0010247087726990383\n403/10000. loss: 0.0012997832770148914\n404/10000. loss: 0.001146679666514198\n405/10000. loss: 0.0017248308286070824\n406/10000. loss: 0.0009777309217800696\n407/10000. loss: 0.0015093252683679264\n408/10000. loss: 0.0017077942999700706\n409/10000. loss: 0.0017773006111383438\n410/10000. loss: 0.0012921444916476805\n411/10000. loss: 0.0013272132103641827\n412/10000. loss: 0.0018644418256978195\n413/10000. loss: 0.0014248501198987167\n414/10000. loss: 0.0014187654790778954\n415/10000. loss: 0.0015507318700353305\n416/10000. loss: 0.0016765566542744637\n417/10000. loss: 0.001740686905880769\n418/10000. loss: 0.0021426724269986153\n419/10000. loss: 0.002968675767381986\n420/10000. loss: 0.0017405042114357154\n421/10000. loss: 0.0013150510688622792\n422/10000. loss: 0.0015322898204127948\n423/10000. loss: 0.001946317342420419\n424/10000. loss: 0.0022915921484430632\n425/10000. loss: 0.001671702756235997\n426/10000. loss: 0.002005509721736113\n427/10000. loss: 0.0016148836972812812\n428/10000. loss: 0.0014630967440704505\n429/10000. loss: 0.0017144063798089821\n430/10000. loss: 0.001839626890917619\n431/10000. loss: 0.0011879249941557646\n432/10000. loss: 0.0013359198346734047\n433/10000. loss: 0.0017879341418544452\n434/10000. loss: 0.0014315253744522731\n435/10000. loss: 0.002919707757731279\n436/10000. loss: 0.0014760545454919338\n437/10000. loss: 0.0015135191691418488\n438/10000. loss: 0.0017959281491736572\n439/10000. loss: 0.001696366195877393\n440/10000. loss: 0.0015113487218817074\n441/10000. loss: 0.0021822828178604445\n442/10000. loss: 0.0015277994486192863\n443/10000. loss: 0.0013332761203249295\n444/10000. loss: 0.0015748776495456696\n445/10000. loss: 0.0015227772916356723\n446/10000. loss: 0.0012924432133634884\n447/10000. loss: 0.0010400842875242233\n448/10000. loss: 0.0015041666726271312\n449/10000. loss: 0.0016326559707522392\n450/10000. loss: 0.001563016635676225\n451/10000. loss: 0.0016515182020763557\n452/10000. loss: 0.0014394714186588924\n453/10000. loss: 0.001959062647074461\n454/10000. loss: 0.0009424023640652498\n455/10000. loss: 0.0009199882236619791\n456/10000. loss: 0.0009575539734214544\n457/10000. loss: 0.0015897209135194619\n458/10000. loss: 0.001151939621195197\n459/10000. loss: 0.0015084993404646714\n460/10000. loss: 0.0008679155725985765\n461/10000. loss: 0.0010870158051451047\n462/10000. loss: 0.001091527131696542\n463/10000. loss: 0.0008645475221176943\n464/10000. loss: 0.0017167550201217334\n465/10000. loss: 0.0014899475499987602\n466/10000. loss: 0.0008585931888471047\n467/10000. loss: 0.0013296438070635002\n468/10000. loss: 0.0013091669728358586\n469/10000. loss: 0.0008358028717339039\n470/10000. loss: 0.0016564105947812398\n471/10000. loss: 0.0013803900219500065\n472/10000. loss: 0.0017463552455107372\n473/10000. loss: 0.0011937576346099377\n474/10000. loss: 0.0022702814700702825\n475/10000. loss: 0.0011280692803363006\n476/10000. loss: 0.0008452643329898516\n477/10000. loss: 0.0016717324033379555\n478/10000. loss: 0.0013946965336799622\n479/10000. loss: 0.0007448570492366949\n480/10000. loss: 0.0016481041287382443\n481/10000. loss: 0.001034894570087393\n482/10000. loss: 0.0008948406515022119\n483/10000. loss: 0.0013803352291385333\n484/10000. loss: 0.0008085422838727633\n485/10000. loss: 0.0012994525022804737\n486/10000. loss: 0.0008163327972094218\n487/10000. loss: 0.0008821548738827308\n488/10000. loss: 0.0013413617076973121\n489/10000. loss: 0.000730741381024321\n490/10000. loss: 0.0008746442229797443\n491/10000. loss: 0.0014740102924406528\n492/10000. loss: 0.0008902829140424728\n493/10000. loss: 0.001263656032582124\n494/10000. loss: 0.0015860886002580326\n495/10000. loss: 0.0017817461242278416\n496/10000. loss: 0.0014205037926634152\n497/10000. loss: 0.0007856468049188455\n498/10000. loss: 0.0009168963103244702\n499/10000. loss: 0.001614417415112257\n500/10000. loss: 0.0013513220474123955\n501/10000. loss: 0.0009298312943428755\n502/10000. loss: 0.0016582848814626534\n503/10000. loss: 0.0015856496368845303\n504/10000. loss: 0.001196338174243768\n505/10000. loss: 0.0014300905168056488\n506/10000. loss: 0.001966124555716912\n507/10000. loss: 0.001138548832386732\n508/10000. loss: 0.0016139065846800804\n509/10000. loss: 0.0013079003741343815\n510/10000. loss: 0.0018095462583005428\n511/10000. loss: 0.0014030088980992634\n512/10000. loss: 0.0011370470747351646\n513/10000. loss: 0.0017478208368023236\n514/10000. loss: 0.0013602420998116334\n515/10000. loss: 0.0015415670350193977\n516/10000. loss: 0.0012229007358352344\n517/10000. loss: 0.0013500115213294823\n518/10000. loss: 0.0018994590888420741\n519/10000. loss: 0.0018325097238024075\n520/10000. loss: 0.0011263963921616476\n521/10000. loss: 0.0013295464838544528\n522/10000. loss: 0.0024132486432790756\n523/10000. loss: 0.001467456730703513\n524/10000. loss: 0.0018186659241716068\n525/10000. loss: 0.0023473730931679406\n526/10000. loss: 0.001873043520996968\n527/10000. loss: 0.0014337160003681977\n528/10000. loss: 0.0026022111997008324\n529/10000. loss: 0.002392615812520186\n530/10000. loss: 0.0014575696550309658\n531/10000. loss: 0.0018876800313591957\n532/10000. loss: 0.0025660350608328977\n533/10000. loss: 0.0018662144429981709\n534/10000. loss: 0.0017873561009764671\n535/10000. loss: 0.0026432760059833527\n536/10000. loss: 0.0023342398926615715\n537/10000. loss: 0.0021615315539141497\n538/10000. loss: 0.0015997765585780144\n539/10000. loss: 0.0016247519912819068\n540/10000. loss: 0.001590301903585593\n541/10000. loss: 0.002465828787535429\n542/10000. loss: 0.002757192278901736\n543/10000. loss: 0.001794988289475441\n544/10000. loss: 0.0012589394270131986\n545/10000. loss: 0.0014650688196221988\n546/10000. loss: 0.0010570645487556856\n547/10000. loss: 0.002323993636916081\n548/10000. loss: 0.0009876621576646964\n549/10000. loss: 0.0016869034928580124\n550/10000. loss: 0.0016260227809349697\n551/10000. loss: 0.002728956751525402\n552/10000. loss: 0.002270412320892016\n553/10000. loss: 0.001815443392843008\n554/10000. loss: 0.001312298234552145\n555/10000. loss: 0.0017525323977073033\n556/10000. loss: 0.0014688942270974319\n557/10000. loss: 0.0011009750111649434\n558/10000. loss: 0.0017046357194582622\n559/10000. loss: 0.001073997700586915\n560/10000. loss: 0.0015163449570536613\n561/10000. loss: 0.0008129005630811056\n562/10000. loss: 0.0019023797164360683\n563/10000. loss: 0.0021954143109420934\n564/10000. loss: 0.001334750869621833\n565/10000. loss: 0.0013948396469155948\n566/10000. loss: 0.0009438399380693833\n567/10000. loss: 0.001283796348919471\n568/10000. loss: 0.001138037924344341\n569/10000. loss: 0.001054992511247595\n570/10000. loss: 0.0016742820541063945\n571/10000. loss: 0.002184485550969839\n572/10000. loss: 0.0017874498541156452\n573/10000. loss: 0.0021408433094620705\n574/10000. loss: 0.0013794703409075737\n575/10000. loss: 0.0011274789770444233\n576/10000. loss: 0.0011561845894902945\n577/10000. loss: 0.0022928748900691667\n578/10000. loss: 0.0022778501734137535\n579/10000. loss: 0.0011937703626851242\n580/10000. loss: 0.0033440223584572473\n581/10000. loss: 0.001878170296549797\n582/10000. loss: 0.0019887425005435944\n583/10000. loss: 0.0017156144604086876\n584/10000. loss: 0.0019942489452660084\n585/10000. loss: 0.001115532514328758\n586/10000. loss: 0.001745760440826416\n587/10000. loss: 0.0016012735043962796\n588/10000. loss: 0.0016027626891930897\n589/10000. loss: 0.0014444258995354176\n590/10000. loss: 0.0011748091783374548\n591/10000. loss: 0.0013209835936625798\n592/10000. loss: 0.0020387753223379454\n593/10000. loss: 0.001006936344007651\n594/10000. loss: 0.0012424790766090155\n595/10000. loss: 0.0010341086114446323\n596/10000. loss: 0.0011431040863196056\n597/10000. loss: 0.0009199910952399174\n598/10000. loss: 0.0017649633809924126\n599/10000. loss: 0.001196684471021096\n600/10000. loss: 0.0011566579341888428\n601/10000. loss: 0.0011831348141034443\n602/10000. loss: 0.0014059773335854213\n603/10000. loss: 0.001292875036597252\n604/10000. loss: 0.001521825169523557\n605/10000. loss: 0.0009133889495084683\n606/10000. loss: 0.0011783037334680557\n607/10000. loss: 0.0009306586192299923\n608/10000. loss: 0.0008009937591850758\n609/10000. loss: 0.001196497119963169\n610/10000. loss: 0.0007378526497632265\n611/10000. loss: 0.0007604834778855244\n612/10000. loss: 0.0009222840890288353\n613/10000. loss: 0.0017227202964325745\n614/10000. loss: 0.0014465499358872573\n615/10000. loss: 0.0008659789649148782\n616/10000. loss: 0.0008868536291023096\n617/10000. loss: 0.0012634132678310077\n618/10000. loss: 0.0008670284102360407\n619/10000. loss: 0.001021612746020158\n620/10000. loss: 0.0009938508737832308\n621/10000. loss: 0.0015344951922694843\n622/10000. loss: 0.0012882623511056106\n623/10000. loss: 0.001163244480267167\n624/10000. loss: 0.001292237313464284\n625/10000. loss: 0.0014426466077566147\n626/10000. loss: 0.0007882803523292145\n627/10000. loss: 0.0008594619575887918\n628/10000. loss: 0.0016057377991576989\n629/10000. loss: 0.0007752040401101112\n630/10000. loss: 0.0013510431163012981\n631/10000. loss: 0.001163690195729335\n632/10000. loss: 0.0012937259549895923\n633/10000. loss: 0.0010110050595055025\n634/10000. loss: 0.0011444004097332556\n635/10000. loss: 0.0008754781447350979\n636/10000. loss: 0.0010706436975548665\n637/10000. loss: 0.0011162512625257175\n638/10000. loss: 0.0009904907395442326\n639/10000. loss: 0.0012216650259991486\n640/10000. loss: 0.0007222668112566074\n641/10000. loss: 0.0007833410054445267\n642/10000. loss: 0.0008967613490919272\n643/10000. loss: 0.0008358875444779793\n644/10000. loss: 0.0010318254741529624\n645/10000. loss: 0.0009498302824795246\n646/10000. loss: 0.0009171892888844013\n647/10000. loss: 0.0008345659201343855\n648/10000. loss: 0.0011564469120154779\n649/10000. loss: 0.0010193038421372573\n650/10000. loss: 0.0009689095119635264\n651/10000. loss: 0.001064819594224294\n652/10000. loss: 0.0013389438390731812\n653/10000. loss: 0.0007794084182629982\n654/10000. loss: 0.0012656017982711394\n655/10000. loss: 0.0008903818670660257\n656/10000. loss: 0.0007700185912350813\n657/10000. loss: 0.0012549621363480885\n658/10000. loss: 0.0009517906388888756\n659/10000. loss: 0.0009914613328874111\n660/10000. loss: 0.000706506660208106\n661/10000. loss: 0.001278515284260114\n662/10000. loss: 0.0007436717860400677\n663/10000. loss: 0.0015622883414228756\n664/10000. loss: 0.0010338789628197749\n665/10000. loss: 0.0010497687229265769\n666/10000. loss: 0.0011070136291285355\n667/10000. loss: 0.0008529399055987597\n668/10000. loss: 0.0010039873110751312\n669/10000. loss: 0.0011811487687130768\n670/10000. loss: 0.0015150238759815693\n671/10000. loss: 0.0012509638133148353\n672/10000. loss: 0.0011493571413060029\n673/10000. loss: 0.0016124256265660126\n674/10000. loss: 0.0015384986375768979\n675/10000. loss: 0.0014186700185139973\n676/10000. loss: 0.0017290421140690644\n677/10000. loss: 0.0011723126905659835\n678/10000. loss: 0.0014869463630020618\n679/10000. loss: 0.0010007841823001702\n680/10000. loss: 0.0015933145768940449\n681/10000. loss: 0.0010327065829187632\n682/10000. loss: 0.001353889238089323\n683/10000. loss: 0.0013544425989190738\n684/10000. loss: 0.001613718302299579\n685/10000. loss: 0.0014402777887880802\n686/10000. loss: 0.0014727301895618439\n687/10000. loss: 0.0011748004083832104\n688/10000. loss: 0.0014113377158840497\n689/10000. loss: 0.0011190840353568394\n690/10000. loss: 0.0008227881044149399\n691/10000. loss: 0.001103185738126437\n692/10000. loss: 0.0010289275863518317\n693/10000. loss: 0.0013706229316691558\n694/10000. loss: 0.0010813544504344463\n695/10000. loss: 0.0010766243406881888\n696/10000. loss: 0.001332754734903574\n697/10000. loss: 0.0014695703672866027\n698/10000. loss: 0.001102620658154289\n699/10000. loss: 0.0008785165846347809\n700/10000. loss: 0.0007533878864099582\n701/10000. loss: 0.0009653021891911825\n702/10000. loss: 0.0012187456401685874\n703/10000. loss: 0.0009211273863911629\n704/10000. loss: 0.0011344643620153267\n705/10000. loss: 0.0012356343989570935\n706/10000. loss: 0.0010824644317229588\n707/10000. loss: 0.0012775171392907698\n708/10000. loss: 0.0007090657794227203\n709/10000. loss: 0.001595042645931244\n710/10000. loss: 0.0012744572789718707\n711/10000. loss: 0.0012079610799749692\n712/10000. loss: 0.0007905747431019942\n713/10000. loss: 0.0010327940496305625\n714/10000. loss: 0.0015687684838970501\n715/10000. loss: 0.0011686836369335651\n716/10000. loss: 0.0010815751738846302\n717/10000. loss: 0.001138766606648763\n718/10000. loss: 0.001471366888533036\n719/10000. loss: 0.0013687096846600373\n720/10000. loss: 0.0010520354844629765\n721/10000. loss: 0.001680439958969752\n722/10000. loss: 0.0007608801436920961\n723/10000. loss: 0.0010585327787945669\n724/10000. loss: 0.0010848807481427987\n725/10000. loss: 0.0009356249744693438\n726/10000. loss: 0.0007984418577204148\n727/10000. loss: 0.000731330830603838\n728/10000. loss: 0.0010564043962707121\n729/10000. loss: 0.0010279160148153703\n730/10000. loss: 0.0007424419745802879\n731/10000. loss: 0.0009482183959335089\n732/10000. loss: 0.0012810400997598965\n733/10000. loss: 0.0011336659081280231\n734/10000. loss: 0.00079571851529181\n735/10000. loss: 0.0007532041830321153\n736/10000. loss: 0.0007019340991973877\n737/10000. loss: 0.0007204127808411916\n738/10000. loss: 0.001517689786851406\n739/10000. loss: 0.0013999234264095624\n740/10000. loss: 0.0011994844923416774\n741/10000. loss: 0.0013897271516422431\n742/10000. loss: 0.0008271062591423591\n743/10000. loss: 0.0009060822582493225\n744/10000. loss: 0.001116798259317875\n745/10000. loss: 0.0008412258078654607\n746/10000. loss: 0.0015436064762373765\n747/10000. loss: 0.0009332293023665746\n748/10000. loss: 0.000727982105066379\n749/10000. loss: 0.000615113570044438\n750/10000. loss: 0.0010406003954509895\n751/10000. loss: 0.0008387874501446883\n752/10000. loss: 0.0007057761152585348\n753/10000. loss: 0.001122086929778258\n754/10000. loss: 0.0010862274405856927\n755/10000. loss: 0.0011724679886053007\n756/10000. loss: 0.0007496663214017948\n757/10000. loss: 0.0012274134593705337\n758/10000. loss: 0.000651847260693709\n759/10000. loss: 0.0012441505677998066\n760/10000. loss: 0.0009528730685512224\n761/10000. loss: 0.0007049546887477239\n762/10000. loss: 0.0007293061353266239\n763/10000. loss: 0.0010638490008811157\n764/10000. loss: 0.0010409172003467877\n765/10000. loss: 0.000920896806443731\n766/10000. loss: 0.0006708800792694092\n767/10000. loss: 0.0007690828448782364\n768/10000. loss: 0.0008813537036379179\n769/10000. loss: 0.0008672209611783425\n770/10000. loss: 0.0008644863652686278\n771/10000. loss: 0.0010259314440190792\n772/10000. loss: 0.0006699057606359323\n773/10000. loss: 0.0006913170218467712\n774/10000. loss: 0.001103327376767993\n775/10000. loss: 0.0009098521744211515\n776/10000. loss: 0.0011214592183629672\n777/10000. loss: 0.0012478836191197236\n778/10000. loss: 0.001313239336013794\n779/10000. loss: 0.0010557068356623252\n780/10000. loss: 0.0010611210018396378\n781/10000. loss: 0.0014155103514591854\n782/10000. loss: 0.0008733329207946857\n783/10000. loss: 0.0012009614147245884\n784/10000. loss: 0.0009544049389660358\n785/10000. loss: 0.0013030000651876132\n786/10000. loss: 0.0010822468126813571\n787/10000. loss: 0.0008226054875801007\n788/10000. loss: 0.0012092154162625472\n789/10000. loss: 0.0012373058125376701\n790/10000. loss: 0.000725005908558766\n791/10000. loss: 0.0007469007590164741\n792/10000. loss: 0.0007344088517129421\n793/10000. loss: 0.001199032257621487\n794/10000. loss: 0.0010806812594334285\n795/10000. loss: 0.0007134169961015383\n796/10000. loss: 0.0006661584290365378\n797/10000. loss: 0.0012941401607046525\n798/10000. loss: 0.0011469824239611626\n799/10000. loss: 0.001136778388172388\n800/10000. loss: 0.000896446251620849\n801/10000. loss: 0.0009960029274225235\n802/10000. loss: 0.0006963772854457299\n803/10000. loss: 0.0011410638689994812\n804/10000. loss: 0.0007107183337211609\n805/10000. loss: 0.0010051655893524487\n806/10000. loss: 0.0010939705340812604\n807/10000. loss: 0.0006810243551929792\n808/10000. loss: 0.0008892626501619816\n809/10000. loss: 0.0012515163980424404\n810/10000. loss: 0.0011622091600050528\n811/10000. loss: 0.0009316598686079184\n812/10000. loss: 0.0009302631951868534\n813/10000. loss: 0.0010296618565917015\n814/10000. loss: 0.0011130243850251038\n815/10000. loss: 0.0007395460270345211\n816/10000. loss: 0.0010334116717179616\n817/10000. loss: 0.0007120898614327112\n818/10000. loss: 0.0006513257976621389\n819/10000. loss: 0.0008376174761603276\n820/10000. loss: 0.0013634702190756798\n821/10000. loss: 0.0011882716789841652\n822/10000. loss: 0.0013869342704614003\n823/10000. loss: 0.0011504351471861203\n824/10000. loss: 0.001094409419844548\n825/10000. loss: 0.0007529574601600567\n826/10000. loss: 0.0006628893315792084\n827/10000. loss: 0.0008999500423669815\n828/10000. loss: 0.0009426751639693975\n829/10000. loss: 0.0010699174211670954\n830/10000. loss: 0.0006609821381668249\n831/10000. loss: 0.000998347532004118\n832/10000. loss: 0.00084148277528584\n833/10000. loss: 0.0010686983975271385\n834/10000. loss: 0.0006497519401212534\n835/10000. loss: 0.0007501681490490834\n836/10000. loss: 0.0007386427217473587\n837/10000. loss: 0.0011135757279892762\n838/10000. loss: 0.0009917157391707103\n839/10000. loss: 0.0010368066529432933\n840/10000. loss: 0.0013634286200006802\n841/10000. loss: 0.0009640627540647984\n842/10000. loss: 0.0010321013008554776\n843/10000. loss: 0.0008542196204264959\n844/10000. loss: 0.0009925926569849253\n845/10000. loss: 0.0009578632501264414\n846/10000. loss: 0.0013510682620108128\n847/10000. loss: 0.0013915070643027623\n848/10000. loss: 0.0012371149690200884\n849/10000. loss: 0.0014589478572209675\n850/10000. loss: 0.0006962654491265615\n851/10000. loss: 0.0011068303138017654\n852/10000. loss: 0.0009151749933759371\n853/10000. loss: 0.0008671021399398645\n854/10000. loss: 0.0015570345955590408\n855/10000. loss: 0.0011903670771668355\n856/10000. loss: 0.000998536745707194\n857/10000. loss: 0.0008458453230559826\n858/10000. loss: 0.0007340725666532913\n859/10000. loss: 0.001113510923460126\n860/10000. loss: 0.0011012229758004348\n861/10000. loss: 0.0009831392671912909\n862/10000. loss: 0.000710073004787167\n863/10000. loss: 0.000766785970578591\n864/10000. loss: 0.0007690345713247856\n865/10000. loss: 0.0008902715829511484\n866/10000. loss: 0.0008268458768725395\n867/10000. loss: 0.001013701471189658\n868/10000. loss: 0.0007090588721136252\n869/10000. loss: 0.0011142461250225704\n870/10000. loss: 0.0006850722711533308\n871/10000. loss: 0.0011154965807994206\n872/10000. loss: 0.0007004082823793093\n873/10000. loss: 0.0010724180998901527\n874/10000. loss: 0.0006621825353552898\n875/10000. loss: 0.0010093123031159241\n876/10000. loss: 0.0008488826764126619\n877/10000. loss: 0.0008334641655286154\n878/10000. loss: 0.0010589763211707275\n879/10000. loss: 0.0011515567700068157\n880/10000. loss: 0.0009241014098127683\n881/10000. loss: 0.0007183868438005447\n882/10000. loss: 0.0006531088147312403\n883/10000. loss: 0.0009430385349939266\n884/10000. loss: 0.0009396900422871113\n885/10000. loss: 0.0010657695432504017\n886/10000. loss: 0.0010189935564994812\n887/10000. loss: 0.0011977691513796647\n888/10000. loss: 0.0009558695989350477\n889/10000. loss: 0.0012809406034648418\n890/10000. loss: 0.0010926829030116398\n891/10000. loss: 0.0019039842300117016\n892/10000. loss: 0.0012857736243555944\n893/10000. loss: 0.0010713588756819565\n894/10000. loss: 0.0010562443640083075\n895/10000. loss: 0.0013007161517937977\n896/10000. loss: 0.001146759216984113\n897/10000. loss: 0.0012629026702294748\n898/10000. loss: 0.0012344086232284706\n899/10000. loss: 0.002042769609640042\n900/10000. loss: 0.002401590347290039\n901/10000. loss: 0.001744188057879607\n902/10000. loss: 0.001558183381954829\n903/10000. loss: 0.002414791223903497\n904/10000. loss: 0.002027482104798158\n905/10000. loss: 0.0026631044844786325\n906/10000. loss: 0.0013370304368436337\n907/10000. loss: 0.0014150887727737427\n908/10000. loss: 0.002270200445006291\n909/10000. loss: 0.002724994905292988\n910/10000. loss: 0.0022655779806276164\n911/10000. loss: 0.002285315034290155\n912/10000. loss: 0.0024647669245799384\n913/10000. loss: 0.0014471506389478843\n914/10000. loss: 0.002267514200260242\n915/10000. loss: 0.002820677434404691\n916/10000. loss: 0.002455874035755793\n917/10000. loss: 0.0024262008567651114\n918/10000. loss: 0.0026084966957569122\n919/10000. loss: 0.00206682737916708\n920/10000. loss: 0.002709982916712761\n921/10000. loss: 0.0027662826081116996\n922/10000. loss: 0.0033806941161553064\n923/10000. loss: 0.002511189008752505\n924/10000. loss: 0.00205352995544672\n925/10000. loss: 0.002454129047691822\n926/10000. loss: 0.0028263855104645095\n927/10000. loss: 0.0029741904387871423\n928/10000. loss: 0.0038370871916413307\n929/10000. loss: 0.0034941742196679115\n930/10000. loss: 0.0028142749021450677\n931/10000. loss: 0.0019694495325287185\n932/10000. loss: 0.0032270302375157676\n933/10000. loss: 0.003199638177951177\n934/10000. loss: 0.0027038843060533204\n935/10000. loss: 0.003086839492122332\n936/10000. loss: 0.0033210869878530502\n937/10000. loss: 0.003339232876896858\n938/10000. loss: 0.003263902540008227\n939/10000. loss: 0.004058142192661762\n940/10000. loss: 0.0031249293436606727\n941/10000. loss: 0.0022079674527049065\n942/10000. loss: 0.0023928314136962094\n943/10000. loss: 0.0018416144885122776\n944/10000. loss: 0.003010471041003863\n945/10000. loss: 0.0016132667660713196\n946/10000. loss: 0.0031403138612707457\n947/10000. loss: 0.002712842387457689\n948/10000. loss: 0.002154387223223845\n949/10000. loss: 0.0019647623412311077\n950/10000. loss: 0.00340752179423968\n951/10000. loss: 0.0022613598654667535\n952/10000. loss: 0.0027617579326033592\n953/10000. loss: 0.002522882384558519\n954/10000. loss: 0.0023198891431093216\n955/10000. loss: 0.0022785146720707417\n956/10000. loss: 0.0021578053322931132\n957/10000. loss: 0.002812842217584451\n958/10000. loss: 0.0014263334063192208\n959/10000. loss: 0.0013162163086235523\n960/10000. loss: 0.0019524435823162396\n961/10000. loss: 0.0014172530112167199\n962/10000. loss: 0.001499911459783713\n963/10000. loss: 0.0015537912646929424\n964/10000. loss: 0.0013107027237613995\n965/10000. loss: 0.001581259227047364\n966/10000. loss: 0.0016052362819512684\n967/10000. loss: 0.0016794954426586628\n968/10000. loss: 0.00164827440554897\n969/10000. loss: 0.0011967494307706754\n970/10000. loss: 0.0015020024341841538\n971/10000. loss: 0.0016397808988889058\n972/10000. loss: 0.001533213226745526\n973/10000. loss: 0.0008990616382410129\n974/10000. loss: 0.0013626657115916412\n975/10000. loss: 0.0018536679757138093\n976/10000. loss: 0.0015467782504856586\n977/10000. loss: 0.001342254380385081\n978/10000. loss: 0.0014445744454860687\n979/10000. loss: 0.001302502118051052\n980/10000. loss: 0.000777561217546463\n981/10000. loss: 0.0008054698506991068\n982/10000. loss: 0.0011332939223696787\n983/10000. loss: 0.0008058432334413131\n984/10000. loss: 0.0014936705119907856\n985/10000. loss: 0.0008984596158067385\n986/10000. loss: 0.0012161604439218838\n987/10000. loss: 0.001182319363579154\n988/10000. loss: 0.0011177370324730873\n989/10000. loss: 0.0012018132644395034\n990/10000. loss: 0.0008458836625019709\n991/10000. loss: 0.001038034213706851\n992/10000. loss: 0.0007310575650384029\n993/10000. loss: 0.001502409887810548\n994/10000. loss: 0.001538132627805074\n995/10000. loss: 0.0011225300841033459\n996/10000. loss: 0.0010084548654655616\n997/10000. loss: 0.0007666651314745346\n998/10000. loss: 0.0010076228839655716\n999/10000. loss: 0.0013095447793602943\n1000/10000. loss: 0.0009382712499548992\n1001/10000. loss: 0.0007437064001957575\n1002/10000. loss: 0.0011015857259432475\n1003/10000. loss: 0.0010099511127918959\n1004/10000. loss: 0.0012666276500870783\n1005/10000. loss: 0.0010721548460423946\n1006/10000. loss: 0.0009131296537816525\n1007/10000. loss: 0.001002433088918527\n1008/10000. loss: 0.001278747261191408\n1009/10000. loss: 0.0007642236693451802\n1010/10000. loss: 0.0007793067488819361\n1011/10000. loss: 0.0008741985075175762\n1012/10000. loss: 0.0010322417753438156\n1013/10000. loss: 0.0008834201532105604\n1014/10000. loss: 0.0008351909151921669\n1015/10000. loss: 0.0007506647768119971\n1016/10000. loss: 0.0010454166525353987\n1017/10000. loss: 0.0006802322653432687\n1018/10000. loss: 0.0007016840390861034\n1019/10000. loss: 0.0014199800789356232\n1020/10000. loss: 0.001345919445157051\n1021/10000. loss: 0.0012960822011033695\n1022/10000. loss: 0.0010364831735690434\n1023/10000. loss: 0.0008802007262905439\n1024/10000. loss: 0.00101687580657502\n1025/10000. loss: 0.0008320799097418785\n1026/10000. loss: 0.001054618585233887\n1027/10000. loss: 0.0008700063141683737\n1028/10000. loss: 0.0014594805737336476\n1029/10000. loss: 0.000694722713281711\n1030/10000. loss: 0.0008471477000663677\n1031/10000. loss: 0.0006735564675182104\n1032/10000. loss: 0.0010473895817995071\n1033/10000. loss: 0.0008813319727778435\n1034/10000. loss: 0.0010084187767157953\n1035/10000. loss: 0.0006914973103751739\n1036/10000. loss: 0.0005716986488550901\n1037/10000. loss: 0.0006644489088406166\n1038/10000. loss: 0.0005932893448819717\n1039/10000. loss: 0.0008691802310446898\n1040/10000. loss: 0.0009169937111437321\n1041/10000. loss: 0.0008559202154477438\n1042/10000. loss: 0.0005802885474016269\n1043/10000. loss: 0.0010471000180890162\n1044/10000. loss: 0.0014270152896642685\n1045/10000. loss: 0.0005809296077738205\n1046/10000. loss: 0.0007918556220829487\n1047/10000. loss: 0.000792675573999683\n1048/10000. loss: 0.0008462954623003801\n1049/10000. loss: 0.0008628383123626312\n1050/10000. loss: 0.0005777743256961306\n1051/10000. loss: 0.0009436630643904209\n1052/10000. loss: 0.0012205940050383408\n1053/10000. loss: 0.0008199665074547132\n1054/10000. loss: 0.0006046261017521223\n1055/10000. loss: 0.0006095658366878828\n1056/10000. loss: 0.0009215283207595348\n1057/10000. loss: 0.0006892033852636814\n1058/10000. loss: 0.0009335811094691356\n1059/10000. loss: 0.0007571599756677946\n1060/10000. loss: 0.0008437333938976129\n1061/10000. loss: 0.0009526953411599001\n1062/10000. loss: 0.0005400585165868202\n1063/10000. loss: 0.0009606854679683844\n1064/10000. loss: 0.0006282350514084101\n1065/10000. loss: 0.0010310492167870204\n1066/10000. loss: 0.0008922898365805546\n1067/10000. loss: 0.000954273467262586\n1068/10000. loss: 0.0008221800283839306\n1069/10000. loss: 0.0005672949676712354\n1070/10000. loss: 0.001067595633988579\n1071/10000. loss: 0.0008285189978778362\n1072/10000. loss: 0.0008975386153906584\n1073/10000. loss: 0.0006466197470823923\n1074/10000. loss: 0.0008279494165132443\n1075/10000. loss: 0.0007889422898491224\n1076/10000. loss: 0.0006185084348544478\n1077/10000. loss: 0.0012832448507348697\n1078/10000. loss: 0.0013123160849014919\n1079/10000. loss: 0.0006360103531430165\n1080/10000. loss: 0.0008840411125371853\n1081/10000. loss: 0.0006763976998627186\n1082/10000. loss: 0.0005590757355093956\n1083/10000. loss: 0.0006961394101381302\n1084/10000. loss: 0.0005543947142238418\n1085/10000. loss: 0.0009963922202587128\n1086/10000. loss: 0.0008146805533518394\n1087/10000. loss: 0.0006835584839185079\n1088/10000. loss: 0.0009228100534528494\n1089/10000. loss: 0.0008256489721437296\n1090/10000. loss: 0.0010278810126086075\n1091/10000. loss: 0.0011596417364974816\n1092/10000. loss: 0.0010049797128885984\n1093/10000. loss: 0.0006806187642117342\n1094/10000. loss: 0.0008342453899482886\n1095/10000. loss: 0.0005625313691173991\n1096/10000. loss: 0.0008047387624780337\n1097/10000. loss: 0.0009504319168627262\n1098/10000. loss: 0.0006305309555803736\n1099/10000. loss: 0.0005922178970649838\n1100/10000. loss: 0.0010485649884988864\n1101/10000. loss: 0.0006686076521873474\n1102/10000. loss: 0.0006173238313446442\n1103/10000. loss: 0.0007546793203800917\n1104/10000. loss: 0.0007798438891768456\n1105/10000. loss: 0.000987701816484332\n1106/10000. loss: 0.0005786638163651029\n1107/10000. loss: 0.000728108532105883\n1108/10000. loss: 0.0010508066043257713\n1109/10000. loss: 0.0009940162611504395\n1110/10000. loss: 0.0009065386839210987\n1111/10000. loss: 0.0011359748896211386\n1112/10000. loss: 0.0007711906606952349\n1113/10000. loss: 0.0006379621724287669\n1114/10000. loss: 0.000748098362237215\n1115/10000. loss: 0.0009654266759753227\n1116/10000. loss: 0.0009127329879750808\n1117/10000. loss: 0.0008478705616046985\n1118/10000. loss: 0.0006795548833906651\n1119/10000. loss: 0.000538336462341249\n1120/10000. loss: 0.0009285258129239082\n1121/10000. loss: 0.0005326518245662252\n1122/10000. loss: 0.0007057522113124529\n1123/10000. loss: 0.0010095970549931128\n1124/10000. loss: 0.0009097141834596792\n1125/10000. loss: 0.000757504099359115\n1126/10000. loss: 0.0005035276214281718\n1127/10000. loss: 0.0006292715358237425\n1128/10000. loss: 0.0009172225836664438\n1129/10000. loss: 0.0004717198899015784\n1130/10000. loss: 0.0010441153620680173\n1131/10000. loss: 0.000789102011670669\n1132/10000. loss: 0.0007735069375485182\n1133/10000. loss: 0.0007292095882197221\n1134/10000. loss: 0.0007926667264352242\n1135/10000. loss: 0.0009298294316977262\n1136/10000. loss: 0.0005709644950305423\n1137/10000. loss: 0.000575797283090651\n1138/10000. loss: 0.0010354200688501198\n1139/10000. loss: 0.0010587765524784725\n1140/10000. loss: 0.0008454367828865846\n1141/10000. loss: 0.0011756789560119312\n1142/10000. loss: 0.0008918653863171736\n1143/10000. loss: 0.001044188781330983\n1144/10000. loss: 0.000918978825211525\n1145/10000. loss: 0.0010219245838622253\n1146/10000. loss: 0.0006117148247237006\n1147/10000. loss: 0.0006636053634186586\n1148/10000. loss: 0.0005636306401963035\n1149/10000. loss: 0.0008575016787896553\n1150/10000. loss: 0.0007158075459301472\n1151/10000. loss: 0.0011174116904536884\n1152/10000. loss: 0.0011586561643828948\n1153/10000. loss: 0.0005061962098504106\n1154/10000. loss: 0.0006772427198787531\n1155/10000. loss: 0.000552037808423241\n1156/10000. loss: 0.0007707232919832071\n1157/10000. loss: 0.0005519726934532324\n1158/10000. loss: 0.0006914110078165928\n1159/10000. loss: 0.00058559017876784\n1160/10000. loss: 0.0008306185094018778\n1161/10000. loss: 0.0008954708464443684\n1162/10000. loss: 0.0010132680957516034\n1163/10000. loss: 0.0005147873889654875\n1164/10000. loss: 0.00072662935902675\n1165/10000. loss: 0.0008418946526944637\n1166/10000. loss: 0.000580940512008965\n1167/10000. loss: 0.0008090783376246691\n1168/10000. loss: 0.0007190430381645759\n1169/10000. loss: 0.00048671259234348935\n1170/10000. loss: 0.0010288663518925507\n1171/10000. loss: 0.0006711936245361964\n1172/10000. loss: 0.0005088032533725103\n1173/10000. loss: 0.0005264135543256998\n1174/10000. loss: 0.000937154982239008\n1175/10000. loss: 0.0006344718082497517\n1176/10000. loss: 0.0007834343705326319\n1177/10000. loss: 0.0004692124202847481\n1178/10000. loss: 0.0008206313941627741\n1179/10000. loss: 0.0007316571039458116\n1180/10000. loss: 0.0005357212309415141\n1181/10000. loss: 0.0007419111207127571\n1182/10000. loss: 0.0011135791428387165\n1183/10000. loss: 0.000515864153082172\n1184/10000. loss: 0.0007921805760512749\n1185/10000. loss: 0.0008839584576586882\n1186/10000. loss: 0.0008067210825781027\n1187/10000. loss: 0.0007456648163497448\n1188/10000. loss: 0.0007062433287501335\n1189/10000. loss: 0.0007649162628998359\n1190/10000. loss: 0.001021070871502161\n1191/10000. loss: 0.0008163037709891796\n1192/10000. loss: 0.0007672406112154325\n1193/10000. loss: 0.0006848759949207306\n1194/10000. loss: 0.0005955634017785391\n1195/10000. loss: 0.0012333701209475596\n1196/10000. loss: 0.0013355597232778866\n1197/10000. loss: 0.0004841665892551343\n1198/10000. loss: 0.0008995635434985161\n1199/10000. loss: 0.0006996607407927513\n1200/10000. loss: 0.0009097338964541753\n1201/10000. loss: 0.0010123023142417271\n1202/10000. loss: 0.0005822020660464963\n1203/10000. loss: 0.0005679447979976734\n1204/10000. loss: 0.0008936238009482622\n1205/10000. loss: 0.000789708225056529\n1206/10000. loss: 0.0011023723054677248\n1207/10000. loss: 0.0010613173556824524\n1208/10000. loss: 0.0008730808428178231\n1209/10000. loss: 0.0012293473506967227\n1210/10000. loss: 0.0007115496943394343\n1211/10000. loss: 0.001347240371008714\n1212/10000. loss: 0.0010383497768392165\n1213/10000. loss: 0.0006861986281971136\n1214/10000. loss: 0.0006779802497476339\n1215/10000. loss: 0.0014253056918581326\n1216/10000. loss: 0.0006023054787268242\n1217/10000. loss: 0.00068878677363197\n1218/10000. loss: 0.001001686944315831\n1219/10000. loss: 0.001005291473120451\n1220/10000. loss: 0.0006210632467021545\n1221/10000. loss: 0.0009549041278660297\n1222/10000. loss: 0.0008530714549124241\n1223/10000. loss: 0.0006127293066432079\n1224/10000. loss: 0.0006751518230885267\n1225/10000. loss: 0.0007016495801508427\n1226/10000. loss: 0.0006778423363963763\n1227/10000. loss: 0.0007801991887390614\n1228/10000. loss: 0.0010805570830901463\n1229/10000. loss: 0.0006328186330695947\n1230/10000. loss: 0.0007303863919029633\n1231/10000. loss: 0.0010475833745052416\n1232/10000. loss: 0.0011116570482651393\n1233/10000. loss: 0.0010105065690974395\n1234/10000. loss: 0.0009133926747987667\n1235/10000. loss: 0.0011565927416086197\n1236/10000. loss: 0.0007637756255765756\n1237/10000. loss: 0.0012360143785675366\n1238/10000. loss: 0.0013061710633337498\n1239/10000. loss: 0.0015871223683158557\n1240/10000. loss: 0.001462882695098718\n1241/10000. loss: 0.00082135076324145\n1242/10000. loss: 0.0015580362329880397\n1243/10000. loss: 0.0013532270677387714\n1244/10000. loss: 0.0015753939126928647\n1245/10000. loss: 0.0011298762013514836\n1246/10000. loss: 0.0015094482029477756\n1247/10000. loss: 0.0010147732682526112\n1248/10000. loss: 0.0009099649420628945\n1249/10000. loss: 0.0007970331547160944\n1250/10000. loss: 0.000833376698816816\n1251/10000. loss: 0.0011445178339878719\n1252/10000. loss: 0.001189605953792731\n1253/10000. loss: 0.00099923446153601\n1254/10000. loss: 0.0006927596405148506\n1255/10000. loss: 0.0010130535811185837\n1256/10000. loss: 0.0010458565472314756\n1257/10000. loss: 0.0006611102726310492\n1258/10000. loss: 0.0009461863276859125\n1259/10000. loss: 0.0009702055249363184\n1260/10000. loss: 0.0008780043572187424\n1261/10000. loss: 0.0007464016477266947\n1262/10000. loss: 0.0011978964321315289\n1263/10000. loss: 0.0007170333216587702\n1264/10000. loss: 0.0009418266514937083\n1265/10000. loss: 0.000807689968496561\n1266/10000. loss: 0.0009617316536605358\n1267/10000. loss: 0.0006440178646395603\n1268/10000. loss: 0.0006596008315682411\n1269/10000. loss: 0.0008611985637495915\n1270/10000. loss: 0.0009096119708071152\n1271/10000. loss: 0.0009348096015552679\n1272/10000. loss: 0.0012958050550272067\n1273/10000. loss: 0.000773953894774119\n1274/10000. loss: 0.0008744988590478897\n1275/10000. loss: 0.000826110364869237\n1276/10000. loss: 0.0008556459409495195\n1277/10000. loss: 0.0007216883823275566\n1278/10000. loss: 0.0008874175449212393\n1279/10000. loss: 0.0011642752215266228\n1280/10000. loss: 0.000681719354664286\n1281/10000. loss: 0.0007976608661313852\n1282/10000. loss: 0.001241672628869613\n1283/10000. loss: 0.0008776127360761166\n1284/10000. loss: 0.000809732669343551\n1285/10000. loss: 0.0010728253982961178\n1286/10000. loss: 0.0009988481178879738\n1287/10000. loss: 0.0005497564561665058\n1288/10000. loss: 0.001031944217781226\n1289/10000. loss: 0.0006936602294445038\n1290/10000. loss: 0.0009215189299235741\n1291/10000. loss: 0.000973459721232454\n1292/10000. loss: 0.001251151707644264\n1293/10000. loss: 0.0008606708919008573\n1294/10000. loss: 0.0009772854391485453\n1295/10000. loss: 0.0006405141903087497\n1296/10000. loss: 0.0011454927735030651\n1297/10000. loss: 0.0009396678457657496\n1298/10000. loss: 0.0007458798742542664\n1299/10000. loss: 0.001052070486669739\n1300/10000. loss: 0.0007770949353774389\n1301/10000. loss: 0.0007298480098446211\n1302/10000. loss: 0.0005854920794566473\n1303/10000. loss: 0.0007736127202709516\n1304/10000. loss: 0.0008305910353859266\n1305/10000. loss: 0.0007951539009809494\n1306/10000. loss: 0.0010087005017946165\n1307/10000. loss: 0.0010215668007731438\n1308/10000. loss: 0.0008866233595957359\n1309/10000. loss: 0.0010757970158010721\n1310/10000. loss: 0.000989519835760196\n1311/10000. loss: 0.0009954679602136214\n1312/10000. loss: 0.0008563464507460594\n1313/10000. loss: 0.0008406082633882761\n1314/10000. loss: 0.001152900590871771\n1315/10000. loss: 0.0013014900032430887\n1316/10000. loss: 0.0010613281435022752\n1317/10000. loss: 0.000812049334247907\n1318/10000. loss: 0.0012018847434471052\n1319/10000. loss: 0.001027432270348072\n1320/10000. loss: 0.0006841137850036224\n1321/10000. loss: 0.0008943841482202212\n1322/10000. loss: 0.0007115465899308523\n1323/10000. loss: 0.0010151177023847897\n1324/10000. loss: 0.0007314023872216543\n1325/10000. loss: 0.0009701968325922886\n1326/10000. loss: 0.0010001221671700478\n1327/10000. loss: 0.0006332140571127335\n1328/10000. loss: 0.0008963251020759344\n1329/10000. loss: 0.0009453198872506618\n1330/10000. loss: 0.0008231879522403082\n1331/10000. loss: 0.0005544848584880432\n1332/10000. loss: 0.0009074361684421698\n1333/10000. loss: 0.0010112229113777478\n1334/10000. loss: 0.0007630611459414164\n1335/10000. loss: 0.0005543774071459969\n1336/10000. loss: 0.0006056080649917325\n1337/10000. loss: 0.0009441045112907887\n1338/10000. loss: 0.0006998046301305294\n1339/10000. loss: 0.0008703280861179034\n1340/10000. loss: 0.0010094215006877978\n1341/10000. loss: 0.0005404708984618386\n1342/10000. loss: 0.0007944646446655194\n1343/10000. loss: 0.000581619911827147\n1344/10000. loss: 0.000781543863316377\n1345/10000. loss: 0.0008018087440480789\n1346/10000. loss: 0.0007560945426424345\n1347/10000. loss: 0.000816570594906807\n1348/10000. loss: 0.0008031482187410196\n1349/10000. loss: 0.000660687064131101\n1350/10000. loss: 0.0008329905879994234\n1351/10000. loss: 0.0008018425044914087\n1352/10000. loss: 0.0009426930919289589\n1353/10000. loss: 0.0008306521146247784\n1354/10000. loss: 0.0012866702551643054\n1355/10000. loss: 0.0007813865474114815\n1356/10000. loss: 0.0009476508324344953\n1357/10000. loss: 0.001504549290984869\n1358/10000. loss: 0.0011492001358419657\n1359/10000. loss: 0.001069035225858291\n1360/10000. loss: 0.0010135009263952572\n1361/10000. loss: 0.0013009928322086732\n1362/10000. loss: 0.001258459718277057\n1363/10000. loss: 0.001119726647933324\n1364/10000. loss: 0.0014113144328196843\n1365/10000. loss: 0.001448180681715409\n1366/10000. loss: 0.001338376197963953\n1367/10000. loss: 0.001455234984556834\n1368/10000. loss: 0.0010899129944543044\n1369/10000. loss: 0.0015398052831490834\n1370/10000. loss: 0.0015933516745766003\n1371/10000. loss: 0.001361946730564038\n1372/10000. loss: 0.0014744391664862633\n1373/10000. loss: 0.001375445630401373\n1374/10000. loss: 0.0009219284014155468\n1375/10000. loss: 0.0008433670736849308\n1376/10000. loss: 0.0011998345144093037\n1377/10000. loss: 0.0008575709847112497\n1378/10000. loss: 0.0012997474210957687\n1379/10000. loss: 0.0009956121599922578\n1380/10000. loss: 0.001172035001218319\n1381/10000. loss: 0.0011095303731660049\n1382/10000. loss: 0.0016631679609417915\n1383/10000. loss: 0.0006760486091176668\n1384/10000. loss: 0.0015204766144355137\n1385/10000. loss: 0.001419634868701299\n1386/10000. loss: 0.0014631644201775391\n1387/10000. loss: 0.0012215026654303074\n1388/10000. loss: 0.0015493097404638927\n1389/10000. loss: 0.0012818717708190281\n1390/10000. loss: 0.0011607541237026453\n1391/10000. loss: 0.0012573345253864925\n1392/10000. loss: 0.0019479583327968915\n1393/10000. loss: 0.0011398571853836377\n1394/10000. loss: 0.001216256758198142\n1395/10000. loss: 0.0011235765026261408\n1396/10000. loss: 0.0012900617439299822\n1397/10000. loss: 0.0009745640369753042\n1398/10000. loss: 0.0013998357268671195\n1399/10000. loss: 0.0011030676153798897\n1400/10000. loss: 0.001332309873153766\n1401/10000. loss: 0.0009289919398725033\n1402/10000. loss: 0.0006666156308104595\n1403/10000. loss: 0.0008210733843346437\n1404/10000. loss: 0.0006552710353086392\n1405/10000. loss: 0.0012331274338066578\n1406/10000. loss: 0.001087567846601208\n1407/10000. loss: 0.000859173092370232\n1408/10000. loss: 0.00110063833805422\n1409/10000. loss: 0.0008108386149009069\n1410/10000. loss: 0.0006544158483544985\n1411/10000. loss: 0.0007312096810589234\n1412/10000. loss: 0.0010163990470270317\n1413/10000. loss: 0.0005934792570769787\n1414/10000. loss: 0.0008706835409005483\n1415/10000. loss: 0.001199066328505675\n1416/10000. loss: 0.0009065473762651285\n1417/10000. loss: 0.00090990603591005\n1418/10000. loss: 0.0006577588307360808\n1419/10000. loss: 0.0009817984731247027\n1420/10000. loss: 0.0008143001856903235\n1421/10000. loss: 0.001212194561958313\n1422/10000. loss: 0.0009409958341469368\n1423/10000. loss: 0.0008817593722293774\n1424/10000. loss: 0.001007729209959507\n1425/10000. loss: 0.0008461784260968367\n1426/10000. loss: 0.0008437937746445338\n1427/10000. loss: 0.0008737208942572275\n1428/10000. loss: 0.0007126405059049526\n1429/10000. loss: 0.000927246098096172\n1430/10000. loss: 0.0011038055332998435\n1431/10000. loss: 0.0007703745892892281\n1432/10000. loss: 0.0009072034154087305\n1433/10000. loss: 0.000636604924996694\n1434/10000. loss: 0.0007517222935954729\n1435/10000. loss: 0.000857981542746226\n1436/10000. loss: 0.0009598820470273495\n1437/10000. loss: 0.0007020829555888971\n1438/10000. loss: 0.0006875175361831983\n1439/10000. loss: 0.0008674048197766145\n1440/10000. loss: 0.00047951098531484604\n1441/10000. loss: 0.0008030662623544534\n1442/10000. loss: 0.0008992554309467474\n1443/10000. loss: 0.0008497283949206272\n1444/10000. loss: 0.000968738691881299\n1445/10000. loss: 0.0008459568489342928\n1446/10000. loss: 0.0009636137789736191\n1447/10000. loss: 0.0005486217560246587\n1448/10000. loss: 0.0008858332100013891\n1449/10000. loss: 0.0007859633769840002\n1450/10000. loss: 0.0009830837758878868\n1451/10000. loss: 0.0008777346617231766\n1452/10000. loss: 0.0007878495380282402\n1453/10000. loss: 0.0010996080624560516\n1454/10000. loss: 0.0007234451671441396\n1455/10000. loss: 0.0006923467541734377\n1456/10000. loss: 0.0009169050802787145\n1457/10000. loss: 0.0006491344732542833\n1458/10000. loss: 0.0006798718435068926\n1459/10000. loss: 0.0005631238066901764\n1460/10000. loss: 0.001050337605799238\n1461/10000. loss: 0.0010487592468659084\n1462/10000. loss: 0.0011489244643598795\n1463/10000. loss: 0.0014387539898355801\n1464/10000. loss: 0.0006669051945209503\n1465/10000. loss: 0.0010434419382363558\n1466/10000. loss: 0.001015420847882827\n1467/10000. loss: 0.0009519299492239952\n1468/10000. loss: 0.0008802083320915699\n1469/10000. loss: 0.0007800799794495106\n1470/10000. loss: 0.0006342027336359024\n1471/10000. loss: 0.0007388472246627013\n1472/10000. loss: 0.0009766477936257918\n1473/10000. loss: 0.001004772571225961\n1474/10000. loss: 0.001088073942810297\n1475/10000. loss: 0.0010540150105953217\n1476/10000. loss: 0.0016603730618953705\n1477/10000. loss: 0.0006628520010660092\n1478/10000. loss: 0.0011632378833989303\n1479/10000. loss: 0.0008019307473053535\n1480/10000. loss: 0.0010500775339702766\n1481/10000. loss: 0.0012297766904036205\n1482/10000. loss: 0.000987195565054814\n1483/10000. loss: 0.0008267678786069155\n1484/10000. loss: 0.0012489000024894874\n1485/10000. loss: 0.0008260776133586963\n1486/10000. loss: 0.0006433515421425303\n1487/10000. loss: 0.0009480046574026346\n1488/10000. loss: 0.001202336357285579\n1489/10000. loss: 0.0008363809126118819\n1490/10000. loss: 0.0008261518863340219\n1491/10000. loss: 0.0009790816499541204\n1492/10000. loss: 0.0009423003842433294\n1493/10000. loss: 0.0008499551719675461\n1494/10000. loss: 0.0009387037716805935\n1495/10000. loss: 0.0009692676831036806\n1496/10000. loss: 0.000852604474251469\n1497/10000. loss: 0.0009752856567502022\n1498/10000. loss: 0.0012192366023858388\n1499/10000. loss: 0.0011428707124044497\n1500/10000. loss: 0.0009072400474299988\n1501/10000. loss: 0.0006185867823660374\n1502/10000. loss: 0.0009632030657182137\n1503/10000. loss: 0.000820852272833387\n1504/10000. loss: 0.0009252976936598619\n1505/10000. loss: 0.0008873601133624712\n1506/10000. loss: 0.0011233182934423287\n1507/10000. loss: 0.000750393761942784\n1508/10000. loss: 0.0010272586562981207\n1509/10000. loss: 0.0007266670775910219\n1510/10000. loss: 0.0009565929261346658\n1511/10000. loss: 0.0007423927697042624\n1512/10000. loss: 0.0011802089090148609\n1513/10000. loss: 0.0008712383763243755\n1514/10000. loss: 0.0011331684266527493\n1515/10000. loss: 0.0005228253624712428\n1516/10000. loss: 0.0012322345282882452\n1517/10000. loss: 0.0008912064755956332\n1518/10000. loss: 0.0008473582565784454\n1519/10000. loss: 0.0011166627518832684\n1520/10000. loss: 0.0012026389595121145\n1521/10000. loss: 0.0007267810093859831\n1522/10000. loss: 0.0007297095532218615\n1523/10000. loss: 0.0013088107419510682\n1524/10000. loss: 0.0008502323180437088\n1525/10000. loss: 0.0012103430926799774\n1526/10000. loss: 0.0012683042635520299\n1527/10000. loss: 0.0007864529422173897\n1528/10000. loss: 0.001992313346515099\n1529/10000. loss: 0.0010932930745184422\n1530/10000. loss: 0.0012404965236783028\n1531/10000. loss: 0.0015087869639197986\n1532/10000. loss: 0.0009678429923951626\n1533/10000. loss: 0.0015631926556428273\n1534/10000. loss: 0.0010620987353225548\n1535/10000. loss: 0.0012330257644255955\n1536/10000. loss: 0.002024364657700062\n1537/10000. loss: 0.0010248368295530479\n1538/10000. loss: 0.002300417826821407\n1539/10000. loss: 0.001667261899759372\n1540/10000. loss: 0.00110049305173258\n1541/10000. loss: 0.0015010670758783817\n1542/10000. loss: 0.001362727799763282\n1543/10000. loss: 0.0011624897985408704\n1544/10000. loss: 0.0012043213937431574\n1545/10000. loss: 0.001994913754363855\n1546/10000. loss: 0.001790929740915696\n1547/10000. loss: 0.0016721471523245175\n1548/10000. loss: 0.0010411683470010757\n1549/10000. loss: 0.0009125911941130956\n1550/10000. loss: 0.001984446464727322\n1551/10000. loss: 0.001012023848791917\n1552/10000. loss: 0.0012248487522204716\n1553/10000. loss: 0.0013788724318146706\n1554/10000. loss: 0.0007650047385444244\n1555/10000. loss: 0.000979671875635783\n1556/10000. loss: 0.0010374061142404873\n1557/10000. loss: 0.001172407219807307\n1558/10000. loss: 0.000970358494669199\n1559/10000. loss: 0.0010836923029273748\n1560/10000. loss: 0.0007195296542098125\n1561/10000. loss: 0.0011233972230305274\n1562/10000. loss: 0.0007133789670964082\n1563/10000. loss: 0.0010863204176227252\n1564/10000. loss: 0.000633798500833412\n1565/10000. loss: 0.001076305905977885\n1566/10000. loss: 0.0009789603451887767\n1567/10000. loss: 0.0006814231940855583\n1568/10000. loss: 0.0007432803201178709\n1569/10000. loss: 0.0005601282852391402\n1570/10000. loss: 0.0010509970597922802\n1571/10000. loss: 0.000871188472956419\n1572/10000. loss: 0.0006216098554432392\n1573/10000. loss: 0.0007530401926487684\n1574/10000. loss: 0.000764498021453619\n1575/10000. loss: 0.0008497933546702067\n1576/10000. loss: 0.000847154917816321\n1577/10000. loss: 0.0005435465524593989\n1578/10000. loss: 0.0005613271690284213\n1579/10000. loss: 0.0008193943649530411\n1580/10000. loss: 0.000490365355896453\n1581/10000. loss: 0.0005682541135077676\n1582/10000. loss: 0.0007647136226296425\n1583/10000. loss: 0.000524998603699108\n1584/10000. loss: 0.0005556699664642414\n1585/10000. loss: 0.0007995980946967999\n1586/10000. loss: 0.000513662351295352\n1587/10000. loss: 0.0006316234357655048\n1588/10000. loss: 0.0004928521035859982\n1589/10000. loss: 0.00063107597331206\n1590/10000. loss: 0.0007073871480921904\n1591/10000. loss: 0.0006914257537573576\n1592/10000. loss: 0.0005924647363523642\n1593/10000. loss: 0.000693304929882288\n1594/10000. loss: 0.0007844827293107907\n1595/10000. loss: 0.00044429391467322904\n1596/10000. loss: 0.0005135110501820842\n1597/10000. loss: 0.0005766248796135187\n1598/10000. loss: 0.0005193661587933699\n1599/10000. loss: 0.0006884032239516576\n1600/10000. loss: 0.0009665406929949919\n1601/10000. loss: 0.0006575748945275942\n1602/10000. loss: 0.0009738883624474207\n1603/10000. loss: 0.0006816376311083635\n1604/10000. loss: 0.0007926063456883033\n1605/10000. loss: 0.0007365723140537739\n1606/10000. loss: 0.0010621725426365931\n1607/10000. loss: 0.0005428282699237267\n1608/10000. loss: 0.0005637968424707651\n1609/10000. loss: 0.0009889306190113227\n1610/10000. loss: 0.0009311660348127285\n1611/10000. loss: 0.0007207357169439396\n1612/10000. loss: 0.000527472235262394\n1613/10000. loss: 0.0008875133935362101\n1614/10000. loss: 0.0009065011205772558\n1615/10000. loss: 0.0007280687956760327\n1616/10000. loss: 0.0005608704717208942\n1617/10000. loss: 0.0008624426554888487\n1618/10000. loss: 0.0007962331486244997\n1619/10000. loss: 0.0006175498322894176\n1620/10000. loss: 0.0007199665221075217\n1621/10000. loss: 0.0010036146268248558\n1622/10000. loss: 0.0005182123277336359\n1623/10000. loss: 0.0008534938873102268\n1624/10000. loss: 0.0009927861392498016\n1625/10000. loss: 0.0009287178205947081\n1626/10000. loss: 0.0007824741769582033\n1627/10000. loss: 0.0007751921657472849\n1628/10000. loss: 0.0007135296085228523\n1629/10000. loss: 0.0009414814412593842\n1630/10000. loss: 0.0008315970189869404\n1631/10000. loss: 0.0005090580477068821\n1632/10000. loss: 0.0007458791757623354\n1633/10000. loss: 0.0013340686758359273\n1634/10000. loss: 0.0008530540702243646\n1635/10000. loss: 0.0007654337678104639\n1636/10000. loss: 0.0006010609989364942\n1637/10000. loss: 0.0005863115657120943\n1638/10000. loss: 0.001060276214654247\n1639/10000. loss: 0.0010744125271836917\n1640/10000. loss: 0.0009862839554746945\n1641/10000. loss: 0.0008289283917595943\n1642/10000. loss: 0.0008471597296496233\n1643/10000. loss: 0.0008496093408515056\n1644/10000. loss: 0.0008214562355230252\n1645/10000. loss: 0.0006250295943270127\n1646/10000. loss: 0.0009856410324573517\n1647/10000. loss: 0.0010052483218411605\n1648/10000. loss: 0.0007632989436388016\n1649/10000. loss: 0.0007482892833650112\n1650/10000. loss: 0.000840153389920791\n1651/10000. loss: 0.0007623727433383465\n1652/10000. loss: 0.0008451766334474087\n1653/10000. loss: 0.0006997372644642988\n1654/10000. loss: 0.0009521141182631254\n1655/10000. loss: 0.0006918845853457848\n1656/10000. loss: 0.0011997038188079994\n1657/10000. loss: 0.0007874518632888794\n1658/10000. loss: 0.0008404111334433159\n1659/10000. loss: 0.0010108303589125474\n1660/10000. loss: 0.0012006943579763174\n1661/10000. loss: 0.0010526844610770543\n1662/10000. loss: 0.0012895791636159022\n1663/10000. loss: 0.0011281038168817759\n1664/10000. loss: 0.001822172353665034\n1665/10000. loss: 0.0010285164850453536\n1666/10000. loss: 0.0008825061377137899\n1667/10000. loss: 0.0007564874831587076\n1668/10000. loss: 0.0013058266292015712\n1669/10000. loss: 0.0007982008003940185\n1670/10000. loss: 0.0011952387479444344\n1671/10000. loss: 0.0015003141015768051\n1672/10000. loss: 0.000820936169475317\n1673/10000. loss: 0.0010299246447781722\n1674/10000. loss: 0.0010852542084952195\n1675/10000. loss: 0.0010134355785946052\n1676/10000. loss: 0.0009289953547219435\n1677/10000. loss: 0.0009166393429040909\n1678/10000. loss: 0.0009082495234906673\n1679/10000. loss: 0.0008468820403019587\n1680/10000. loss: 0.0008320444418738285\n1681/10000. loss: 0.0007848657357196013\n1682/10000. loss: 0.0011028296624620755\n1683/10000. loss: 0.000726397925366958\n1684/10000. loss: 0.0005273433635011315\n1685/10000. loss: 0.0008041605663796266\n1686/10000. loss: 0.0004920516706382235\n1687/10000. loss: 0.0004655094041178624\n1688/10000. loss: 0.000631688085074226\n1689/10000. loss: 0.0005201149033382535\n1690/10000. loss: 0.0006081287283450365\n1691/10000. loss: 0.0008068324532359838\n1692/10000. loss: 0.0006815927724043528\n1693/10000. loss: 0.0009877447349329789\n1694/10000. loss: 0.0006444312554473678\n1695/10000. loss: 0.0006535024537394444\n1696/10000. loss: 0.0010108364125092824\n1697/10000. loss: 0.0007515940815210342\n1698/10000. loss: 0.0006842856916288534\n1699/10000. loss: 0.0007857431968053182\n1700/10000. loss: 0.0006127280260746678\n1701/10000. loss: 0.0008216130081564188\n1702/10000. loss: 0.0008596304493645827\n1703/10000. loss: 0.0008202493190765381\n1704/10000. loss: 0.000541545256661872\n1705/10000. loss: 0.0008162010150651137\n1706/10000. loss: 0.0008439410788317522\n1707/10000. loss: 0.000540447731812795\n1708/10000. loss: 0.0005643948291738828\n1709/10000. loss: 0.0011108348456521828\n1710/10000. loss: 0.0008454231234888235\n1711/10000. loss: 0.000930788383508722\n1712/10000. loss: 0.0011011365180214245\n1713/10000. loss: 0.0010012430138885975\n1714/10000. loss: 0.000982549972832203\n1715/10000. loss: 0.0008382350982477268\n1716/10000. loss: 0.0008583657909184694\n1717/10000. loss: 0.0009540635316322247\n1718/10000. loss: 0.0009722732162723938\n1719/10000. loss: 0.0006818460921446482\n1720/10000. loss: 0.0006215136963874102\n1721/10000. loss: 0.0006538755260407925\n1722/10000. loss: 0.0005488917231559753\n1723/10000. loss: 0.0008252267725765705\n1724/10000. loss: 0.0008581675744305054\n1725/10000. loss: 0.0009806666833659012\n1726/10000. loss: 0.0010173437961687644\n1727/10000. loss: 0.0007832216409345468\n1728/10000. loss: 0.001011899517228206\n1729/10000. loss: 0.0006038098751256863\n1730/10000. loss: 0.0008217928310235342\n1731/10000. loss: 0.0008378901208440462\n1732/10000. loss: 0.000563665060326457\n1733/10000. loss: 0.0004963458050042391\n1734/10000. loss: 0.000770378935461243\n1735/10000. loss: 0.000476314725043873\n1736/10000. loss: 0.0006228812271729112\n1737/10000. loss: 0.0008379475524028143\n1738/10000. loss: 0.0006906700630982717\n1739/10000. loss: 0.0008662727195769548\n1740/10000. loss: 0.0008451420969019333\n1741/10000. loss: 0.0007254338512818018\n1742/10000. loss: 0.0006115467598040899\n1743/10000. loss: 0.0007082358933985233\n1744/10000. loss: 0.0009227113332599401\n1745/10000. loss: 0.0009792052830259006\n1746/10000. loss: 0.0009872089140117168\n1747/10000. loss: 0.0007915111103405555\n1748/10000. loss: 0.0007499211933463812\n1749/10000. loss: 0.0009433524683117867\n1750/10000. loss: 0.0008824251126497984\n1751/10000. loss: 0.0009595568602283796\n1752/10000. loss: 0.0009617134928703308\n1753/10000. loss: 0.0017020649587114651\n1754/10000. loss: 0.0006905103412767252\n1755/10000. loss: 0.001108865796898802\n1756/10000. loss: 0.0008994905898968378\n1757/10000. loss: 0.0011144871823489666\n1758/10000. loss: 0.001185617409646511\n1759/10000. loss: 0.0008252399663130442\n1760/10000. loss: 0.0009680466415981451\n1761/10000. loss: 0.0007979422807693481\n1762/10000. loss: 0.0010500021744519472\n1763/10000. loss: 0.0012121043788890045\n1764/10000. loss: 0.001147353866448005\n1765/10000. loss: 0.0012751591857522726\n1766/10000. loss: 0.0009472763631492853\n1767/10000. loss: 0.0010274560190737247\n1768/10000. loss: 0.0011138958701243002\n1769/10000. loss: 0.001385323703289032\n1770/10000. loss: 0.001053201189885537\n1771/10000. loss: 0.0006521986797451973\n1772/10000. loss: 0.0013475753366947174\n1773/10000. loss: 0.0009631146676838398\n1774/10000. loss: 0.0008695160504430532\n1775/10000. loss: 0.001201887692635258\n1776/10000. loss: 0.0010873669137557347\n1777/10000. loss: 0.001044635350505511\n1778/10000. loss: 0.00125163064027826\n1779/10000. loss: 0.0005992670388271412\n1780/10000. loss: 0.000774990456799666\n1781/10000. loss: 0.0007700632947186629\n1782/10000. loss: 0.001073744148015976\n1783/10000. loss: 0.0008499966158221165\n1784/10000. loss: 0.0007706099810699621\n1785/10000. loss: 0.0008464610824982325\n1786/10000. loss: 0.0007161917164921761\n1787/10000. loss: 0.0004717849660664797\n1788/10000. loss: 0.0006721578538417816\n1789/10000. loss: 0.0006206427545597156\n1790/10000. loss: 0.00048694278423984844\n1791/10000. loss: 0.0007043201476335526\n1792/10000. loss: 0.0006197992867479721\n1793/10000. loss: 0.0005505488564570745\n1794/10000. loss: 0.0007490315474569798\n1795/10000. loss: 0.0005508228593195478\n1796/10000. loss: 0.0007494670959810416\n1797/10000. loss: 0.0003908673922220866\n1798/10000. loss: 0.0005834285402670503\n1799/10000. loss: 0.0006755911745131016\n1800/10000. loss: 0.0006247664568945765\n1801/10000. loss: 0.00047492833497623604\n1802/10000. loss: 0.0007919122775395712\n1803/10000. loss: 0.0005475421591351429\n1804/10000. loss: 0.00048433834065993625\n1805/10000. loss: 0.0008752281622340282\n1806/10000. loss: 0.0006043583465119203\n1807/10000. loss: 0.0007006924909849962\n1808/10000. loss: 0.0006043595882753531\n1809/10000. loss: 0.0005799318508555492\n1810/10000. loss: 0.0006389152258634567\n1811/10000. loss: 0.0005030203998709718\n1812/10000. loss: 0.0006998491783936819\n1813/10000. loss: 0.0006439910115053257\n1814/10000. loss: 0.0007170888129621744\n1815/10000. loss: 0.0006149680896972617\n1816/10000. loss: 0.000887915181616942\n1817/10000. loss: 0.0006953349026540915\n1818/10000. loss: 0.0005947661120444536\n1819/10000. loss: 0.000585460647319754\n1820/10000. loss: 0.0004384277466063698\n1821/10000. loss: 0.0007320489579190811\n1822/10000. loss: 0.0003891679225489497\n1823/10000. loss: 0.0005589483383422097\n1824/10000. loss: 0.0008178567513823509\n1825/10000. loss: 0.0007124952971935272\n1826/10000. loss: 0.0006285374208043019\n1827/10000. loss: 0.0006837942637503147\n1828/10000. loss: 0.0004538372935106357\n1829/10000. loss: 0.000733744353055954\n1830/10000. loss: 0.0006523507957657179\n1831/10000. loss: 0.0005225942780574163\n1832/10000. loss: 0.000801882396141688\n1833/10000. loss: 0.0008941865526139736\n1834/10000. loss: 0.0005022546586891016\n1835/10000. loss: 0.0006935256533324718\n1836/10000. loss: 0.000776924037684997\n1837/10000. loss: 0.0011213870408634345\n1838/10000. loss: 0.001166400033980608\n1839/10000. loss: 0.0011168099008500576\n1840/10000. loss: 0.0013978139807780583\n1841/10000. loss: 0.0016912791567544143\n1842/10000. loss: 0.0012045088224112988\n1843/10000. loss: 0.0019927301133672395\n1844/10000. loss: 0.001968946928779284\n1845/10000. loss: 0.0026737969989577928\n1846/10000. loss: 0.0020901855702201524\n1847/10000. loss: 0.0024378842984636626\n1848/10000. loss: 0.0022964890425403914\n1849/10000. loss: 0.0027567679062485695\n1850/10000. loss: 0.0018967160334189732\n1851/10000. loss: 0.002589244395494461\n1852/10000. loss: 0.0022198321918646493\n1853/10000. loss: 0.004886403679847717\n1854/10000. loss: 0.0032852167884508767\n1855/10000. loss: 0.003568477307756742\n1856/10000. loss: 0.0019471334914366405\n1857/10000. loss: 0.0042574026932319\n1858/10000. loss: 0.0037098477284113565\n1859/10000. loss: 0.005627749487757683\n1860/10000. loss: 0.003962029392520587\n1861/10000. loss: 0.003070713331302007\n1862/10000. loss: 0.002993525005877018\n1863/10000. loss: 0.0024665341091652713\n1864/10000. loss: 0.00398127796749274\n1865/10000. loss: 0.002615782432258129\n1866/10000. loss: 0.0019543777840832868\n1867/10000. loss: 0.0029459670186042786\n1868/10000. loss: 0.001920985213170449\n1869/10000. loss: 0.0025252578780055046\n1870/10000. loss: 0.0031644701957702637\n1871/10000. loss: 0.003147571347653866\n1872/10000. loss: 0.0009209899387011925\n1873/10000. loss: 0.003956368503471215\n1874/10000. loss: 0.0014646452230711777\n1875/10000. loss: 0.0020519538472096124\n1876/10000. loss: 0.001564509856204192\n1877/10000. loss: 0.003275500610470772\n1878/10000. loss: 0.0014312372853358586\n1879/10000. loss: 0.0018217408408721287\n1880/10000. loss: 0.0034191819528738656\n1881/10000. loss: 0.0021853315023084483\n1882/10000. loss: 0.0014867903664708138\n1883/10000. loss: 0.002177793998271227\n1884/10000. loss: 0.0010145321333160002\n1885/10000. loss: 0.002984104057153066\n1886/10000. loss: 0.0008306676366676887\n1887/10000. loss: 0.002002923438946406\n1888/10000. loss: 0.0014405843491355579\n1889/10000. loss: 0.001490384650727113\n1890/10000. loss: 0.0019738453750809035\n1891/10000. loss: 0.0013188842373589675\n1892/10000. loss: 0.001780882788201173\n1893/10000. loss: 0.002323268291850885\n1894/10000. loss: 0.001009929149101178\n1895/10000. loss: 0.0015623876824975014\n1896/10000. loss: 0.0011819556045035522\n1897/10000. loss: 0.0013391111666957538\n1898/10000. loss: 0.0013576298952102661\n1899/10000. loss: 0.0008461504088093837\n1900/10000. loss: 0.0012112832628190517\n1901/10000. loss: 0.001318288656572501\n1902/10000. loss: 0.0011388286172101896\n1903/10000. loss: 0.0009920171772440274\n1904/10000. loss: 0.000863305681074659\n1905/10000. loss: 0.0013656864563624065\n1906/10000. loss: 0.0005571461127450069\n1907/10000. loss: 0.000563224001477162\n1908/10000. loss: 0.000625317256587247\n1909/10000. loss: 0.000590093278636535\n1910/10000. loss: 0.0005346031005804738\n1911/10000. loss: 0.000788034638389945\n1912/10000. loss: 0.0008256652702887853\n1913/10000. loss: 0.0010926949325948954\n1914/10000. loss: 0.0007611731222520272\n1915/10000. loss: 0.0008163206124057373\n1916/10000. loss: 0.0007220411983629068\n1917/10000. loss: 0.0004972237705563506\n1918/10000. loss: 0.00039433167936901253\n1919/10000. loss: 0.0005032907938584685\n1920/10000. loss: 0.0007348171590516964\n1921/10000. loss: 0.0006097735992322365\n1922/10000. loss: 0.00045899185352027416\n1923/10000. loss: 0.0005826013317952553\n1924/10000. loss: 0.0008577781263738871\n1925/10000. loss: 0.0006497585369894902\n1926/10000. loss: 0.00044538868435968954\n1927/10000. loss: 0.000857859073827664\n1928/10000. loss: 0.0006099046828846136\n1929/10000. loss: 0.000787489814683795\n1930/10000. loss: 0.0009173985260228316\n1931/10000. loss: 0.0007268374320119619\n1932/10000. loss: 0.0006478856860970458\n1933/10000. loss: 0.000732342324530085\n1934/10000. loss: 0.0007108518232901891\n1935/10000. loss: 0.0007343994608769814\n1936/10000. loss: 0.0004741491284221411\n1937/10000. loss: 0.0009558376235266527\n1938/10000. loss: 0.0005804200191050768\n1939/10000. loss: 0.000504025723785162\n1940/10000. loss: 0.0007593377182881037\n1941/10000. loss: 0.0006338047484556834\n1942/10000. loss: 0.0004902887934197983\n1943/10000. loss: 0.0007496234029531479\n1944/10000. loss: 0.0008701916473607222\n1945/10000. loss: 0.0013319108014305432\n1946/10000. loss: 0.001194689500456055\n1947/10000. loss: 0.0007727335517605146\n1948/10000. loss: 0.001123211346566677\n1949/10000. loss: 0.0014561149291694164\n1950/10000. loss: 0.0013357941061258316\n1951/10000. loss: 0.0012209087920685608\n1952/10000. loss: 0.0017871870659291744\n1953/10000. loss: 0.0016037003758052986\n1954/10000. loss: 0.0017962045967578888\n1955/10000. loss: 0.00196809321641922\n1956/10000. loss: 0.0012243237967292468\n1957/10000. loss: 0.0015456564724445343\n1958/10000. loss: 0.0015381999934713046\n1959/10000. loss: 0.001348123885691166\n1960/10000. loss: 0.0012857296193639438\n1961/10000. loss: 0.0017653259759147961\n1962/10000. loss: 0.0011098656492928665\n1963/10000. loss: 0.0011749060358852148\n1964/10000. loss: 0.0012141931025932233\n1965/10000. loss: 0.0007832891618212064\n1966/10000. loss: 0.0007754174682001272\n1967/10000. loss: 0.0011528381922592719\n1968/10000. loss: 0.0012283246808995802\n1969/10000. loss: 0.0012128058200081189\n1970/10000. loss: 0.0016415738500654697\n1971/10000. loss: 0.0014944834013779957\n1972/10000. loss: 0.0015243932915230591\n1973/10000. loss: 0.0012529326292375724\n1974/10000. loss: 0.0013400791212916374\n1975/10000. loss: 0.00159030066182216\n1976/10000. loss: 0.0012927451947083075\n1977/10000. loss: 0.0011119211558252573\n1978/10000. loss: 0.0008926374527315298\n1979/10000. loss: 0.0012187076111634572\n1980/10000. loss: 0.0009056444590290388\n1981/10000. loss: 0.000835108570754528\n1982/10000. loss: 0.0011280061056216557\n1983/10000. loss: 0.0012687849036107461\n1984/10000. loss: 0.0014801753374437492\n1985/10000. loss: 0.0009484759842356046\n1986/10000. loss: 0.0011698782133559387\n1987/10000. loss: 0.0008009173131237427\n1988/10000. loss: 0.001352575607597828\n1989/10000. loss: 0.0013277814723551273\n1990/10000. loss: 0.0009644266683608294\n1991/10000. loss: 0.0010664381552487612\n1992/10000. loss: 0.0012723551287005346\n1993/10000. loss: 0.0010441504418849945\n1994/10000. loss: 0.0010677760777374108\n1995/10000. loss: 0.0010778950527310371\n1996/10000. loss: 0.0010129425209015608\n1997/10000. loss: 0.0011184186053772767\n1998/10000. loss: 0.0005941293202340603\n1999/10000. loss: 0.000853301336367925\n2000/10000. loss: 0.0008993972248087326\n2001/10000. loss: 0.0010292250663042068\n2002/10000. loss: 0.0007226463252057632\n2003/10000. loss: 0.000574796344153583\n2004/10000. loss: 0.0009726833862562975\n2005/10000. loss: 0.0007881637041767439\n2006/10000. loss: 0.0005343375572313865\n2007/10000. loss: 0.0009244030807167292\n2008/10000. loss: 0.0007739200567205747\n2009/10000. loss: 0.0007623963368435701\n2010/10000. loss: 0.0005414586824675401\n2011/10000. loss: 0.0007204689706365267\n2012/10000. loss: 0.000513191451318562\n2013/10000. loss: 0.0007748839755853018\n2014/10000. loss: 0.0006830457908411821\n2015/10000. loss: 0.0006888469991584619\n2016/10000. loss: 0.0004930043360218406\n2017/10000. loss: 0.0006302421679720283\n2018/10000. loss: 0.00042380877615263063\n2019/10000. loss: 0.000691990756119291\n2020/10000. loss: 0.0004642390413209796\n2021/10000. loss: 0.0005908245220780373\n2022/10000. loss: 0.0005684227605039874\n2023/10000. loss: 0.0005666966705272595\n2024/10000. loss: 0.00042595221505810815\n2025/10000. loss: 0.00043501487622658413\n2026/10000. loss: 0.000659677820901076\n2027/10000. loss: 0.0006494917130718628\n2028/10000. loss: 0.0005919133157779773\n2029/10000. loss: 0.00043393217492848635\n2030/10000. loss: 0.00041892608472456533\n2031/10000. loss: 0.00040751575337102014\n2032/10000. loss: 0.00045326123169312876\n2033/10000. loss: 0.0004495036943505208\n2034/10000. loss: 0.0005670280661433935\n2035/10000. loss: 0.00040784943848848343\n2036/10000. loss: 0.0006876251815507809\n2037/10000. loss: 0.0006737857280919949\n2038/10000. loss: 0.00047892886990060407\n2039/10000. loss: 0.0005415078873435656\n2040/10000. loss: 0.0005648416311790546\n2041/10000. loss: 0.0006589153005431095\n2042/10000. loss: 0.0006130707915872335\n2043/10000. loss: 0.0004336803685873747\n2044/10000. loss: 0.0004556251224130392\n2045/10000. loss: 0.00043567964651932317\n2046/10000. loss: 0.0006821005760381619\n2047/10000. loss: 0.000820056302472949\n2048/10000. loss: 0.0005363471573218703\n2049/10000. loss: 0.0005976633013536533\n2050/10000. loss: 0.0007116203972448906\n2051/10000. loss: 0.0006616193180282911\n2052/10000. loss: 0.0004688296855116884\n2053/10000. loss: 0.0006438246928155422\n2054/10000. loss: 0.0007629917624096075\n2055/10000. loss: 0.00040150759741663933\n2056/10000. loss: 0.00037307416399319965\n2057/10000. loss: 0.00048252327057222527\n2058/10000. loss: 0.001149371654416124\n2059/10000. loss: 0.0007063124018410841\n2060/10000. loss: 0.0006315373272324601\n2061/10000. loss: 0.0006369552963102857\n2062/10000. loss: 0.0006211938646932443\n2063/10000. loss: 0.000683656350399057\n2064/10000. loss: 0.00045016908552497625\n2065/10000. loss: 0.0008156356246521076\n2066/10000. loss: 0.0007071887763837973\n2067/10000. loss: 0.00048350108166535694\n2068/10000. loss: 0.00046642924038072425\n2069/10000. loss: 0.0008622980676591396\n2070/10000. loss: 0.0007382575422525406\n2071/10000. loss: 0.0009721750393509865\n2072/10000. loss: 0.0006053901355092725\n2073/10000. loss: 0.000661865808069706\n2074/10000. loss: 0.0009276968582222859\n2075/10000. loss: 0.0008583220187574625\n2076/10000. loss: 0.0008517235983163118\n2077/10000. loss: 0.0009166189314176639\n2078/10000. loss: 0.0005642447698240479\n2079/10000. loss: 0.0009946311668803294\n2080/10000. loss: 0.0009845173917710781\n2081/10000. loss: 0.0012558494539310534\n2082/10000. loss: 0.0009220139278719822\n2083/10000. loss: 0.0008647572249174118\n2084/10000. loss: 0.0012120947552224\n2085/10000. loss: 0.001032697968184948\n2086/10000. loss: 0.0007161852748443683\n2087/10000. loss: 0.0009693495618800322\n2088/10000. loss: 0.001390875472376744\n2089/10000. loss: 0.0008786926046013832\n2090/10000. loss: 0.0008369753292451302\n2091/10000. loss: 0.0009521905643244585\n2092/10000. loss: 0.0006878006582458814\n2093/10000. loss: 0.001583777057627837\n2094/10000. loss: 0.0007439488545060158\n2095/10000. loss: 0.0011672119920452435\n2096/10000. loss: 0.0008226568655421337\n2097/10000. loss: 0.000945497847472628\n2098/10000. loss: 0.0010521189930538337\n2099/10000. loss: 0.001104260018716256\n2100/10000. loss: 0.001169744102905194\n2101/10000. loss: 0.0008862426814933618\n2102/10000. loss: 0.0009936906086901824\n2103/10000. loss: 0.0006750853111346563\n2104/10000. loss: 0.0007598018273711205\n2105/10000. loss: 0.0013616890646517277\n2106/10000. loss: 0.0008913044972966114\n2107/10000. loss: 0.0009284412177900473\n2108/10000. loss: 0.0006544069231798252\n2109/10000. loss: 0.0007354302797466516\n2110/10000. loss: 0.0005251707043498755\n2111/10000. loss: 0.000776216775799791\n2112/10000. loss: 0.0008467829320579767\n2113/10000. loss: 0.0006664303752283255\n2114/10000. loss: 0.0008149133839954933\n2115/10000. loss: 0.0005785004856685797\n2116/10000. loss: 0.0009515282387534777\n2117/10000. loss: 0.0005172172095626593\n2118/10000. loss: 0.0008139694885661205\n2119/10000. loss: 0.0007288060151040554\n2120/10000. loss: 0.0006134983850643039\n2121/10000. loss: 0.0005557923577725887\n2122/10000. loss: 0.0005180245110144218\n2123/10000. loss: 0.0004862489877268672\n2124/10000. loss: 0.0006922983253995577\n2125/10000. loss: 0.0007073658828934034\n2126/10000. loss: 0.0007777352972577015\n2127/10000. loss: 0.00039896062420060235\n2128/10000. loss: 0.00047484308015555143\n2129/10000. loss: 0.0005366869736462831\n2130/10000. loss: 0.0005490621939922372\n2131/10000. loss: 0.0005366234884907802\n2132/10000. loss: 0.000420111115090549\n2133/10000. loss: 0.0003672651946544647\n2134/10000. loss: 0.0006043758864204088\n2135/10000. loss: 0.0006724779959768057\n2136/10000. loss: 0.0006658080189178387\n2137/10000. loss: 0.0007362300530076027\n2138/10000. loss: 0.00047068325026581687\n2139/10000. loss: 0.0004908182503034672\n2140/10000. loss: 0.0007374685568114122\n2141/10000. loss: 0.0004079292993992567\n2142/10000. loss: 0.0005055759490157167\n2143/10000. loss: 0.0006358934721599022\n2144/10000. loss: 0.0005344646827628216\n2145/10000. loss: 0.0005583086749538779\n2146/10000. loss: 0.0004029685320953528\n2147/10000. loss: 0.0006806477904319763\n2148/10000. loss: 0.00038205556726704043\n2149/10000. loss: 0.0005740435250724355\n2150/10000. loss: 0.0005560856079682708\n2151/10000. loss: 0.0006556952527413765\n2152/10000. loss: 0.0006343214772641659\n2153/10000. loss: 0.0007259747168670098\n2154/10000. loss: 0.0006486785520489017\n2155/10000. loss: 0.0005662035352240006\n2156/10000. loss: 0.00056785197618107\n2157/10000. loss: 0.0006356129888445139\n2158/10000. loss: 0.00046058449273308116\n2159/10000. loss: 0.000661635926614205\n2160/10000. loss: 0.000734519058217605\n2161/10000. loss: 0.0008257037649552027\n2162/10000. loss: 0.0005201286791513363\n2163/10000. loss: 0.0005085010003919402\n2164/10000. loss: 0.0006557328160852194\n2165/10000. loss: 0.0005226443366458019\n2166/10000. loss: 0.0006973034081359705\n2167/10000. loss: 0.0006721824562797943\n2168/10000. loss: 0.00044081592932343483\n2169/10000. loss: 0.0005670962079117695\n2170/10000. loss: 0.0007459383147458235\n2171/10000. loss: 0.0011716440009574096\n2172/10000. loss: 0.0006005970450739065\n2173/10000. loss: 0.00048580742441117764\n2174/10000. loss: 0.0006170115666463971\n2175/10000. loss: 0.0007292193671067556\n2176/10000. loss: 0.0006526481204976639\n2177/10000. loss: 0.0006231265142560005\n2178/10000. loss: 0.0007802022931476434\n2179/10000. loss: 0.0005295024408648411\n2180/10000. loss: 0.0008511785417795181\n2181/10000. loss: 0.0008908757784714302\n2182/10000. loss: 0.0017811104965706666\n2183/10000. loss: 0.0010775839909911156\n2184/10000. loss: 0.0010371770864973466\n2185/10000. loss: 0.0008826428093016148\n2186/10000. loss: 0.0008926752489060163\n2187/10000. loss: 0.0011278286886711915\n2188/10000. loss: 0.000788580005367597\n2189/10000. loss: 0.0009211609916140636\n2190/10000. loss: 0.0008030550864835581\n2191/10000. loss: 0.0009628282859921455\n2192/10000. loss: 0.0006337645851696531\n2193/10000. loss: 0.0010250476965059836\n2194/10000. loss: 0.0008870614692568779\n2195/10000. loss: 0.0007744692265987396\n2196/10000. loss: 0.0007287313540776571\n2197/10000. loss: 0.0005609577832122644\n2198/10000. loss: 0.0008337101899087429\n2199/10000. loss: 0.000506668584421277\n2200/10000. loss: 0.00047532162473847467\n2201/10000. loss: 0.0006608698361863693\n2202/10000. loss: 0.0009292379642526308\n2203/10000. loss: 0.0009311207880576452\n2204/10000. loss: 0.0007686327832440535\n2205/10000. loss: 0.0004325838914761941\n2206/10000. loss: 0.0009422679431736469\n2207/10000. loss: 0.0005583722765247027\n2208/10000. loss: 0.0006661431398242712\n2209/10000. loss: 0.000620492889235417\n2210/10000. loss: 0.0006734064469734827\n2211/10000. loss: 0.0005372068844735622\n2212/10000. loss: 0.0007095950034757456\n2213/10000. loss: 0.0007127307665844759\n2214/10000. loss: 0.0007567202361921469\n2215/10000. loss: 0.0008175622982283434\n2216/10000. loss: 0.0005715780425816774\n2217/10000. loss: 0.0006822647216419379\n2218/10000. loss: 0.0005713478894904256\n2219/10000. loss: 0.0008093488092223803\n2220/10000. loss: 0.0007769227959215641\n2221/10000. loss: 0.00047539896331727505\n2222/10000. loss: 0.0009289271353433529\n2223/10000. loss: 0.0007578656077384949\n2224/10000. loss: 0.0005367280682548881\n2225/10000. loss: 0.0005865689599886537\n2226/10000. loss: 0.0006096985889598727\n2227/10000. loss: 0.0005529146486272415\n2228/10000. loss: 0.0007770551213373741\n2229/10000. loss: 0.0006303256377577782\n2230/10000. loss: 0.0005505015530313054\n2231/10000. loss: 0.0006854900469382604\n2232/10000. loss: 0.000979326122129957\n2233/10000. loss: 0.0010889143062134583\n2234/10000. loss: 0.000720596561829249\n2235/10000. loss: 0.0004910617911567291\n2236/10000. loss: 0.0007376242429018021\n2237/10000. loss: 0.0005108184413984418\n2238/10000. loss: 0.0009409294774134954\n2239/10000. loss: 0.00046707588868836564\n2240/10000. loss: 0.00046891594926516217\n2241/10000. loss: 0.00041888429162402946\n2242/10000. loss: 0.00042533529146264\n2243/10000. loss: 0.0007937146971623102\n2244/10000. loss: 0.000436968596962591\n2245/10000. loss: 0.0008386386713633934\n2246/10000. loss: 0.0004817829467356205\n2247/10000. loss: 0.0006013199842224518\n2248/10000. loss: 0.0006092166683326165\n2249/10000. loss: 0.0006418936730672916\n2250/10000. loss: 0.0005783813151841363\n2251/10000. loss: 0.0005610402828703324\n2252/10000. loss: 0.0006204016972333193\n2253/10000. loss: 0.00037147845917691785\n2254/10000. loss: 0.000702819088473916\n2255/10000. loss: 0.0005983294686302543\n2256/10000. loss: 0.0007811688507596651\n2257/10000. loss: 0.0007291947646687428\n2258/10000. loss: 0.0004551306677361329\n2259/10000. loss: 0.0006907963349173466\n2260/10000. loss: 0.0007538010831922293\n2261/10000. loss: 0.0005777704839905103\n2262/10000. loss: 0.0009884972435732682\n2263/10000. loss: 0.000521564157679677\n2264/10000. loss: 0.0007316382446636757\n2265/10000. loss: 0.0005543986723447839\n2266/10000. loss: 0.0006466763637339076\n2267/10000. loss: 0.0007709832862019539\n2268/10000. loss: 0.0009288632621367773\n2269/10000. loss: 0.000739505048841238\n2270/10000. loss: 0.0009284534025937319\n2271/10000. loss: 0.0011113887497534354\n2272/10000. loss: 0.0011369353160262108\n2273/10000. loss: 0.0009988310436407726\n2274/10000. loss: 0.00127407427256306\n2275/10000. loss: 0.0011395427864044905\n2276/10000. loss: 0.0011775832002361615\n2277/10000. loss: 0.00132315078129371\n2278/10000. loss: 0.0015353248454630375\n2279/10000. loss: 0.001081410174568494\n2280/10000. loss: 0.0012682500916222732\n2281/10000. loss: 0.0015024932411809762\n2282/10000. loss: 0.0008901966114838918\n2283/10000. loss: 0.001090517733246088\n2284/10000. loss: 0.0017034436265627544\n2285/10000. loss: 0.001214518832663695\n2286/10000. loss: 0.0015223704588909943\n2287/10000. loss: 0.001144272896150748\n2288/10000. loss: 0.0013787765055894852\n2289/10000. loss: 0.0013206377625465393\n2290/10000. loss: 0.0013622439776857693\n2291/10000. loss: 0.0017146444879472256\n2292/10000. loss: 0.0010474738664925098\n2293/10000. loss: 0.0008387196188171705\n2294/10000. loss: 0.000877312229325374\n2295/10000. loss: 0.0011083162389695644\n2296/10000. loss: 0.0006745898475249609\n2297/10000. loss: 0.0009717932747056087\n2298/10000. loss: 0.00059671300308158\n2299/10000. loss: 0.0007617202742646138\n2300/10000. loss: 0.0007251889910548925\n2301/10000. loss: 0.000590045975210766\n2302/10000. loss: 0.0005335969229539236\n2303/10000. loss: 0.0007212907851984104\n2304/10000. loss: 0.0008638429765899976\n2305/10000. loss: 0.0006385863913844029\n2306/10000. loss: 0.0006093757304673394\n2307/10000. loss: 0.000649351510219276\n2308/10000. loss: 0.00041503490259250003\n2309/10000. loss: 0.0007566655986011028\n2310/10000. loss: 0.0006979873093465964\n2311/10000. loss: 0.0008627166971564293\n2312/10000. loss: 0.0009050468603769938\n2313/10000. loss: 0.0009241290390491486\n2314/10000. loss: 0.0006241579540073872\n2315/10000. loss: 0.0013561715992788474\n2316/10000. loss: 0.0015062489546835423\n2317/10000. loss: 0.0011883920524269342\n2318/10000. loss: 0.0012532342225313187\n2319/10000. loss: 0.0010132977428535621\n2320/10000. loss: 0.001324605041493972\n2321/10000. loss: 0.0015184061291317146\n2322/10000. loss: 0.0007656848368545374\n2323/10000. loss: 0.0015805546815196674\n2324/10000. loss: 0.0012042639621843894\n2325/10000. loss: 0.0012309527955949306\n2326/10000. loss: 0.0008788059931248426\n2327/10000. loss: 0.0011745290830731392\n2328/10000. loss: 0.001046231792618831\n2329/10000. loss: 0.0015142705912391345\n2330/10000. loss: 0.0016488227993249893\n2331/10000. loss: 0.0023152129724621773\n2332/10000. loss: 0.0015552394712964694\n2333/10000. loss: 0.001811949536204338\n2334/10000. loss: 0.0017236250763138135\n2335/10000. loss: 0.0015196882498761017\n2336/10000. loss: 0.0018531270325183868\n2337/10000. loss: 0.0011313677144547303\n2338/10000. loss: 0.0015272432938218117\n2339/10000. loss: 0.0012560657535990079\n2340/10000. loss: 0.0012816324985275667\n2341/10000. loss: 0.0011923981364816427\n2342/10000. loss: 0.001128341614579161\n2343/10000. loss: 0.0012848332989960909\n2344/10000. loss: 0.0016703565294543903\n2345/10000. loss: 0.001213253941386938\n2346/10000. loss: 0.001011884305626154\n2347/10000. loss: 0.0007138399717708429\n2348/10000. loss: 0.0009379625941316286\n2349/10000. loss: 0.0008514531267186006\n2350/10000. loss: 0.0010754365163544815\n2351/10000. loss: 0.000981278174246351\n2352/10000. loss: 0.0006476662044102947\n2353/10000. loss: 0.0008959342570354542\n2354/10000. loss: 0.0005449651507660747\n2355/10000. loss: 0.0004899952715883652\n2356/10000. loss: 0.0004992280155420303\n2357/10000. loss: 0.0005945820594206452\n2358/10000. loss: 0.0006625169577697912\n2359/10000. loss: 0.0006034205046792825\n2360/10000. loss: 0.0006925615016371012\n2361/10000. loss: 0.0009126327931880951\n2362/10000. loss: 0.0008515859954059124\n2363/10000. loss: 0.0005065985023975372\n2364/10000. loss: 0.0005847321978459755\n2365/10000. loss: 0.0005551246382916967\n2366/10000. loss: 0.0007067445355157057\n2367/10000. loss: 0.0004507938089470069\n2368/10000. loss: 0.0007307621805618206\n2369/10000. loss: 0.0007659100616971651\n2370/10000. loss: 0.0010393221552173297\n2371/10000. loss: 0.0007154491419593493\n2372/10000. loss: 0.0008963168753931919\n2373/10000. loss: 0.0006946693174540997\n2374/10000. loss: 0.0005013343955700597\n2375/10000. loss: 0.0009507607513417801\n2376/10000. loss: 0.0008032443777968487\n2377/10000. loss: 0.0006541040105124315\n2378/10000. loss: 0.0008751373582830032\n2379/10000. loss: 0.0007463924121111631\n2380/10000. loss: 0.0009098677740742763\n2381/10000. loss: 0.0005900495452806354\n2382/10000. loss: 0.0005373410725345215\n2383/10000. loss: 0.0006903348645816246\n2384/10000. loss: 0.0007630807037154833\n2385/10000. loss: 0.0004951560404151678\n2386/10000. loss: 0.0005328400681416193\n2387/10000. loss: 0.0006162615027278662\n2388/10000. loss: 0.0006768199770400921\n2389/10000. loss: 0.000504946568980813\n2390/10000. loss: 0.0007004969132443269\n2391/10000. loss: 0.000597339045877258\n2392/10000. loss: 0.000731464009732008\n2393/10000. loss: 0.00042025318058828515\n2394/10000. loss: 0.0006851486396044493\n2395/10000. loss: 0.0005814847536385059\n2396/10000. loss: 0.0006302533826480309\n2397/10000. loss: 0.000839451172699531\n2398/10000. loss: 0.0009367625849942366\n2399/10000. loss: 0.0009205762762576342\n2400/10000. loss: 0.0006317362034072479\n2401/10000. loss: 0.0004411247791722417\n2402/10000. loss: 0.0007292854910095533\n2403/10000. loss: 0.00043501212106396753\n2404/10000. loss: 0.000681789048636953\n2405/10000. loss: 0.0006195385164270798\n2406/10000. loss: 0.0004640331802268823\n2407/10000. loss: 0.0006667664274573326\n2408/10000. loss: 0.0004915999403844277\n2409/10000. loss: 0.0008348192398746809\n2410/10000. loss: 0.0007292166507492462\n2411/10000. loss: 0.0005495828421165546\n2412/10000. loss: 0.0007360164696971575\n2413/10000. loss: 0.0005166437476873398\n2414/10000. loss: 0.0005464861945559581\n2415/10000. loss: 0.0005822821209828059\n2416/10000. loss: 0.0007662465795874596\n2417/10000. loss: 0.0007224065096427997\n2418/10000. loss: 0.0004850642678017418\n2419/10000. loss: 0.0006834239854166905\n2420/10000. loss: 0.0008355366686979929\n2421/10000. loss: 0.0007501382691164812\n2422/10000. loss: 0.0007299322169274092\n2423/10000. loss: 0.000993456148232023\n2424/10000. loss: 0.0007082742328445116\n2425/10000. loss: 0.0007814858884861072\n2426/10000. loss: 0.0006782697358479103\n2427/10000. loss: 0.000674636879314979\n2428/10000. loss: 0.0005154525861144066\n2429/10000. loss: 0.0009170073705414931\n2430/10000. loss: 0.0006332687335088849\n2431/10000. loss: 0.0005915191334982713\n2432/10000. loss: 0.0008837122780581316\n2433/10000. loss: 0.000897691740343968\n2434/10000. loss: 0.001091813047726949\n2435/10000. loss: 0.0009029921299467484\n2436/10000. loss: 0.0009651080084343752\n2437/10000. loss: 0.0008723780823250612\n2438/10000. loss: 0.0006946784754594167\n2439/10000. loss: 0.0009794185558954875\n2440/10000. loss: 0.0009852851120134194\n2441/10000. loss: 0.0009910969529300928\n2442/10000. loss: 0.00106561246017615\n2443/10000. loss: 0.0009842823880414169\n2444/10000. loss: 0.0009410170217355093\n2445/10000. loss: 0.0005874281438688437\n2446/10000. loss: 0.0011436173226684332\n2447/10000. loss: 0.0008517100941389799\n2448/10000. loss: 0.0008508459820101658\n2449/10000. loss: 0.0006934101693332195\n2450/10000. loss: 0.001035647854829828\n2451/10000. loss: 0.0006450437164554993\n2452/10000. loss: 0.0006238415371626616\n2453/10000. loss: 0.0005047812592238188\n2454/10000. loss: 0.0006640106439590454\n2455/10000. loss: 0.0008621628706653913\n2456/10000. loss: 0.0005759946070611477\n2457/10000. loss: 0.0009409671183675528\n2458/10000. loss: 0.000589944616270562\n2459/10000. loss: 0.0006127504166215658\n2460/10000. loss: 0.0008817417547106743\n2461/10000. loss: 0.0005580793755749861\n2462/10000. loss: 0.00046810414642095566\n2463/10000. loss: 0.0005019340120876828\n2464/10000. loss: 0.0006532427699615558\n2465/10000. loss: 0.00044444610830396414\n2466/10000. loss: 0.000691176780189077\n2467/10000. loss: 0.00042740934683630866\n2468/10000. loss: 0.0006058962705234686\n2469/10000. loss: 0.00040356023237109184\n2470/10000. loss: 0.0004914057596276203\n2471/10000. loss: 0.0005114254308864474\n2472/10000. loss: 0.00040999415796250105\n2473/10000. loss: 0.000514098055039843\n2474/10000. loss: 0.00038206348350892466\n2475/10000. loss: 0.0006269811419770122\n2476/10000. loss: 0.00038322356219093007\n2477/10000. loss: 0.000566092940668265\n2478/10000. loss: 0.0005703879287466407\n2479/10000. loss: 0.0005145169949779908\n2480/10000. loss: 0.000435587833635509\n2481/10000. loss: 0.0004042360621194045\n2482/10000. loss: 0.0006269065973659357\n2483/10000. loss: 0.00041460664942860603\n2484/10000. loss: 0.0005290814830611149\n2485/10000. loss: 0.00034843206716080505\n2486/10000. loss: 0.0006174189814676841\n2487/10000. loss: 0.0004612087116887172\n2488/10000. loss: 0.0003870203314969937\n2489/10000. loss: 0.00035630667116492987\n2490/10000. loss: 0.00033917716549088556\n2491/10000. loss: 0.00036953703965991735\n2492/10000. loss: 0.00033286035371323425\n2493/10000. loss: 0.0004807447548955679\n2494/10000. loss: 0.0005007416087513169\n2495/10000. loss: 0.00048132155401011306\n2496/10000. loss: 0.0005847392603754997\n2497/10000. loss: 0.0005984095623716712\n2498/10000. loss: 0.00047758598035822314\n2499/10000. loss: 0.0004796102487792571\n2500/10000. loss: 0.00040713546331971884\n2501/10000. loss: 0.0004359372736265262\n2502/10000. loss: 0.000465411227196455\n2503/10000. loss: 0.0008313550303379694\n2504/10000. loss: 0.0004977213684469461\n2505/10000. loss: 0.0006088094475368658\n2506/10000. loss: 0.000380665451909105\n2507/10000. loss: 0.0005735998274758458\n2508/10000. loss: 0.0007955019827932119\n2509/10000. loss: 0.0005747608374804258\n2510/10000. loss: 0.0004283336456865072\n2511/10000. loss: 0.0005227984317267934\n2512/10000. loss: 0.0006416659646977981\n2513/10000. loss: 0.0005450206032643715\n2514/10000. loss: 0.0006913289738198122\n2515/10000. loss: 0.0006313679041340947\n2516/10000. loss: 0.0007063328133275112\n2517/10000. loss: 0.0005160433162624637\n2518/10000. loss: 0.0005621690070256591\n2519/10000. loss: 0.0008026007562875748\n2520/10000. loss: 0.0005667653555671374\n2521/10000. loss: 0.0005720792105421424\n2522/10000. loss: 0.00045488491499175626\n2523/10000. loss: 0.0006345127476379275\n2524/10000. loss: 0.000517426097455124\n2525/10000. loss: 0.0005913105172415575\n2526/10000. loss: 0.0004781484603881836\n2527/10000. loss: 0.00043294023877630633\n2528/10000. loss: 0.0008144055803616842\n2529/10000. loss: 0.000545969232916832\n2530/10000. loss: 0.000378206605091691\n2531/10000. loss: 0.0005173524065564076\n2532/10000. loss: 0.0005313855363056064\n2533/10000. loss: 0.0008136366959661245\n2534/10000. loss: 0.00039301603101193905\n2535/10000. loss: 0.0005618830521901449\n2536/10000. loss: 0.0006180008252461752\n2537/10000. loss: 0.0007353493322928747\n2538/10000. loss: 0.0003984242600078384\n2539/10000. loss: 0.0004142624093219638\n2540/10000. loss: 0.0006732571249206861\n2541/10000. loss: 0.0008116367583473524\n2542/10000. loss: 0.0005896312262242039\n2543/10000. loss: 0.0004910378096004327\n2544/10000. loss: 0.0006139935770382484\n2545/10000. loss: 0.00036400552683820325\n2546/10000. loss: 0.0004267058102414012\n2547/10000. loss: 0.0003905211730549733\n2548/10000. loss: 0.0004864446818828583\n2549/10000. loss: 0.0005346461354444424\n2550/10000. loss: 0.0005285150449102124\n2551/10000. loss: 0.00034961069468408823\n2552/10000. loss: 0.00035861670039594173\n2553/10000. loss: 0.00034251995384693146\n2554/10000. loss: 0.0005848292882243792\n2555/10000. loss: 0.00035199569538235664\n2556/10000. loss: 0.0005089232387642065\n2557/10000. loss: 0.0006175790913403034\n2558/10000. loss: 0.00035453801198552054\n2559/10000. loss: 0.0004458657543485363\n2560/10000. loss: 0.00039139964307347935\n2561/10000. loss: 0.00044197387372454006\n2562/10000. loss: 0.0006569089212765297\n2563/10000. loss: 0.00047296262346208096\n2564/10000. loss: 0.0007891835023959478\n2565/10000. loss: 0.0005836165122066935\n2566/10000. loss: 0.0005250232061371207\n2567/10000. loss: 0.0005034496231625477\n2568/10000. loss: 0.0003977710148319602\n2569/10000. loss: 0.0005508287188907465\n2570/10000. loss: 0.0004005752271041274\n2571/10000. loss: 0.0006160359674443802\n2572/10000. loss: 0.000913974829018116\n2573/10000. loss: 0.0008138801592091719\n2574/10000. loss: 0.0010959018642703693\n2575/10000. loss: 0.0010479451157152653\n2576/10000. loss: 0.0009958473965525627\n2577/10000. loss: 0.0010309226345270872\n2578/10000. loss: 0.0019224191394944985\n2579/10000. loss: 0.0016159545630216599\n2580/10000. loss: 0.0016818239043156307\n2581/10000. loss: 0.0018925340846180916\n2582/10000. loss: 0.002145198949923118\n2583/10000. loss: 0.0016015991568565369\n2584/10000. loss: 0.00237819692119956\n2585/10000. loss: 0.0015995061645905178\n2586/10000. loss: 0.0012209002549449603\n2587/10000. loss: 0.0018560293441017468\n2588/10000. loss: 0.0014870893210172653\n2589/10000. loss: 0.0016039128725727398\n2590/10000. loss: 0.0013249260373413563\n2591/10000. loss: 0.0019526478524009387\n2592/10000. loss: 0.0017125485464930534\n2593/10000. loss: 0.0007891419809311628\n2594/10000. loss: 0.0011470925528556108\n2595/10000. loss: 0.0014969940918187301\n2596/10000. loss: 0.0009942653123289347\n2597/10000. loss: 0.0010235737233112256\n2598/10000. loss: 0.001311068267871936\n2599/10000. loss: 0.0010806573554873466\n2600/10000. loss: 0.0006891942272583643\n2601/10000. loss: 0.0012377092304329078\n2602/10000. loss: 0.0009443671442568302\n2603/10000. loss: 0.000602867299069961\n2604/10000. loss: 0.0008632914784053961\n2605/10000. loss: 0.0007230075231442848\n2606/10000. loss: 0.0008721513828883568\n2607/10000. loss: 0.0009586618592341741\n2608/10000. loss: 0.0007318491892268261\n2609/10000. loss: 0.0006534846810003122\n2610/10000. loss: 0.0008043155539780855\n2611/10000. loss: 0.00042430458900829154\n2612/10000. loss: 0.001055365117887656\n2613/10000. loss: 0.0006725528898338476\n2614/10000. loss: 0.0007618017649898926\n2615/10000. loss: 0.0004410669207572937\n2616/10000. loss: 0.0007356344722211361\n2617/10000. loss: 0.0007645175792276859\n2618/10000. loss: 0.0007206518979122242\n2619/10000. loss: 0.0006616875374068817\n2620/10000. loss: 0.0007725687076648077\n2621/10000. loss: 0.0006482565077021718\n2622/10000. loss: 0.0006596144909660021\n2623/10000. loss: 0.0003779575539131959\n2624/10000. loss: 0.0006582013641794523\n2625/10000. loss: 0.0005360788200050592\n2626/10000. loss: 0.0003993265563622117\n2627/10000. loss: 0.0007343267401059469\n2628/10000. loss: 0.0005354929016903043\n2629/10000. loss: 0.0003905919147655368\n2630/10000. loss: 0.0005421448343743881\n2631/10000. loss: 0.0004911799139032761\n2632/10000. loss: 0.0005542420937369267\n2633/10000. loss: 0.0005967502171794573\n2634/10000. loss: 0.0003709011944010854\n2635/10000. loss: 0.0004979055374860764\n2636/10000. loss: 0.0007109826741119226\n2637/10000. loss: 0.0004128910368308425\n2638/10000. loss: 0.0007721235354741415\n2639/10000. loss: 0.0005244207180415591\n2640/10000. loss: 0.0006647299354275068\n2641/10000. loss: 0.000658712505052487\n2642/10000. loss: 0.0005855385291700562\n2643/10000. loss: 0.000624528193535904\n2644/10000. loss: 0.0006163640258212885\n2645/10000. loss: 0.0006536459550261497\n2646/10000. loss: 0.0006820034856597582\n2647/10000. loss: 0.0006071087749054035\n2648/10000. loss: 0.0007618033948043982\n2649/10000. loss: 0.0007467029305795828\n2650/10000. loss: 0.0008092517964541912\n2651/10000. loss: 0.000593893618012468\n2652/10000. loss: 0.0010576279213031132\n2653/10000. loss: 0.0012758105682830017\n2654/10000. loss: 0.0010106642730534077\n2655/10000. loss: 0.0012711674595872562\n2656/10000. loss: 0.000925703284641107\n2657/10000. loss: 0.0007164418542136749\n2658/10000. loss: 0.0013801315799355507\n2659/10000. loss: 0.0009327547159045935\n2660/10000. loss: 0.001148736880471309\n2661/10000. loss: 0.0006626650380591551\n2662/10000. loss: 0.0006102904832611481\n2663/10000. loss: 0.0007508418057113886\n2664/10000. loss: 0.001108801846082012\n2665/10000. loss: 0.001005161941672365\n2666/10000. loss: 0.0005845850488791863\n2667/10000. loss: 0.0005581203537682692\n2668/10000. loss: 0.0006505211349576712\n2669/10000. loss: 0.000914490781724453\n2670/10000. loss: 0.000616653123870492\n2671/10000. loss: 0.0005349612717206279\n2672/10000. loss: 0.0007785237394273281\n2673/10000. loss: 0.0006782428051034609\n2674/10000. loss: 0.00044878863263875246\n2675/10000. loss: 0.0004638441993544499\n2676/10000. loss: 0.0007187378748009602\n2677/10000. loss: 0.0005340120599915584\n2678/10000. loss: 0.0007558586852004131\n2679/10000. loss: 0.0007187728770077229\n2680/10000. loss: 0.0006651845760643482\n2681/10000. loss: 0.000484160923709472\n2682/10000. loss: 0.0005232048376152912\n2683/10000. loss: 0.0010577511663238208\n2684/10000. loss: 0.0006743543005237976\n2685/10000. loss: 0.0007803709401438633\n2686/10000. loss: 0.0006535237965484461\n2687/10000. loss: 0.0007784860984732708\n2688/10000. loss: 0.0006491764603803555\n2689/10000. loss: 0.0006461974699050188\n2690/10000. loss: 0.0007629707300414642\n2691/10000. loss: 0.0007573654875159264\n2692/10000. loss: 0.00044695047351221245\n2693/10000. loss: 0.0006522391146669785\n2694/10000. loss: 0.0006531399364272753\n2695/10000. loss: 0.0006102234280357758\n2696/10000. loss: 0.0006256090321888527\n2697/10000. loss: 0.0008105603822817405\n2698/10000. loss: 0.0006467982505758604\n2699/10000. loss: 0.0008874657408644756\n2700/10000. loss: 0.0006615328602492809\n2701/10000. loss: 0.0008918091965218385\n2702/10000. loss: 0.001496768556535244\n2703/10000. loss: 0.0010821990047891934\n2704/10000. loss: 0.0015995434174935024\n2705/10000. loss: 0.0012291868527730305\n2706/10000. loss: 0.0012054528730611007\n2707/10000. loss: 0.0009635578220089277\n2708/10000. loss: 0.0010766141737500827\n2709/10000. loss: 0.0011907946318387985\n2710/10000. loss: 0.0013696419385572274\n2711/10000. loss: 0.0008483524434268475\n2712/10000. loss: 0.0010540061630308628\n2713/10000. loss: 0.000744806524987022\n2714/10000. loss: 0.0006954584581156572\n2715/10000. loss: 0.0007847272014866272\n2716/10000. loss: 0.0009132753281543652\n2717/10000. loss: 0.001054683079322179\n2718/10000. loss: 0.0008420767262578011\n2719/10000. loss: 0.0007611056789755821\n2720/10000. loss: 0.0007438145888348421\n2721/10000. loss: 0.0006993965556224188\n2722/10000. loss: 0.00073886647199591\n2723/10000. loss: 0.0008652654166022936\n2724/10000. loss: 0.0008403336784491936\n2725/10000. loss: 0.0006311197066679597\n2726/10000. loss: 0.000690908869728446\n2727/10000. loss: 0.0005136392234514157\n2728/10000. loss: 0.00041554806133111316\n2729/10000. loss: 0.0004925192333757877\n2730/10000. loss: 0.0003651537699624896\n2731/10000. loss: 0.00045790174044668674\n2732/10000. loss: 0.0003554834208140771\n2733/10000. loss: 0.0006866746892531713\n2734/10000. loss: 0.0005972976408277949\n2735/10000. loss: 0.0006791899601618449\n2736/10000. loss: 0.00038233834008375805\n2737/10000. loss: 0.00044911416868368786\n2738/10000. loss: 0.0006390652464081844\n2739/10000. loss: 0.0006079824330906073\n2740/10000. loss: 0.0008458172281583151\n2741/10000. loss: 0.0005939860905831059\n2742/10000. loss: 0.0008732386243840059\n2743/10000. loss: 0.0015836916863918304\n2744/10000. loss: 0.0012946752831339836\n2745/10000. loss: 0.002178429625928402\n2746/10000. loss: 0.0012087938375771046\n2747/10000. loss: 0.0013128956779837608\n2748/10000. loss: 0.0025498544176419577\n2749/10000. loss: 0.002281320902208487\n2750/10000. loss: 0.003821627547343572\n2751/10000. loss: 0.0016510958472887676\n2752/10000. loss: 0.0026148324832320213\n2753/10000. loss: 0.0021015064169963202\n2754/10000. loss: 0.0032114780818422637\n2755/10000. loss: 0.0027201902121305466\n2756/10000. loss: 0.001971643573294083\n2757/10000. loss: 0.0018025552853941917\n2758/10000. loss: 0.0019429891059796016\n2759/10000. loss: 0.0017834353881577651\n2760/10000. loss: 0.0016238826016585033\n2761/10000. loss: 0.000818661026035746\n2762/10000. loss: 0.00107578095048666\n2763/10000. loss: 0.001558190832535426\n2764/10000. loss: 0.0007134008531769117\n2765/10000. loss: 0.0010766723038007815\n2766/10000. loss: 0.0012931481469422579\n2767/10000. loss: 0.0008680425429095825\n2768/10000. loss: 0.000982998947923382\n2769/10000. loss: 0.0011712233535945415\n2770/10000. loss: 0.0009195863579710325\n2771/10000. loss: 0.0010106673774619896\n2772/10000. loss: 0.0007427883489678303\n2773/10000. loss: 0.0009638511886199316\n2774/10000. loss: 0.0007537941758831342\n2775/10000. loss: 0.000695432381083568\n2776/10000. loss: 0.0007361150346696377\n2777/10000. loss: 0.0006578261964023113\n2778/10000. loss: 0.0005185953341424465\n2779/10000. loss: 0.0007391710144778093\n2780/10000. loss: 0.0006010612317671379\n2781/10000. loss: 0.0008901233474413554\n2782/10000. loss: 0.0008570431576420864\n2783/10000. loss: 0.0010911600353817146\n2784/10000. loss: 0.000898396751532952\n2785/10000. loss: 0.0013984969506661098\n2786/10000. loss: 0.001676291072120269\n2787/10000. loss: 0.0014446931891143322\n2788/10000. loss: 0.0016084082114199798\n2789/10000. loss: 0.0015970336583753426\n2790/10000. loss: 0.0014898621787627537\n2791/10000. loss: 0.0013841496159633\n2792/10000. loss: 0.0011386337379614513\n2793/10000. loss: 0.0012405199619630973\n2794/10000. loss: 0.0010930844582617283\n2795/10000. loss: 0.0008314762574930986\n2796/10000. loss: 0.0013115281860033672\n2797/10000. loss: 0.0008514618190626303\n2798/10000. loss: 0.0006287203480799993\n2799/10000. loss: 0.0007398754047850767\n2800/10000. loss: 0.0007719709537923336\n2801/10000. loss: 0.0006699988152831793\n2802/10000. loss: 0.0005079800418267647\n2803/10000. loss: 0.0005672841022411982\n2804/10000. loss: 0.0007030735723674297\n2805/10000. loss: 0.00042678900839140016\n2806/10000. loss: 0.0008176942355930805\n2807/10000. loss: 0.0005121314898133278\n2808/10000. loss: 0.0006734627143790325\n2809/10000. loss: 0.0006007623936360081\n2810/10000. loss: 0.000887998224546512\n2811/10000. loss: 0.0005218049045652151\n2812/10000. loss: 0.0005138174165040255\n2813/10000. loss: 0.0006634663635243973\n2814/10000. loss: 0.0006273305043578148\n2815/10000. loss: 0.0005566816544160247\n2816/10000. loss: 0.0004863354843109846\n2817/10000. loss: 0.0004437232855707407\n2818/10000. loss: 0.0006219888649260005\n2819/10000. loss: 0.0009286353985468546\n2820/10000. loss: 0.0009397272951900959\n2821/10000. loss: 0.001050422511373957\n2822/10000. loss: 0.0005493453160549203\n2823/10000. loss: 0.0009941225871443748\n2824/10000. loss: 0.0007981122471392155\n2825/10000. loss: 0.0009505719256897768\n2826/10000. loss: 0.0008877701281259457\n2827/10000. loss: 0.0013819169253110886\n2828/10000. loss: 0.0013020435192932684\n2829/10000. loss: 0.001093644027908643\n2830/10000. loss: 0.0008242593612521887\n2831/10000. loss: 0.0014208282033602397\n2832/10000. loss: 0.0014466112479567528\n2833/10000. loss: 0.0010560163451979558\n2834/10000. loss: 0.001647042731444041\n2835/10000. loss: 0.00130050303414464\n2836/10000. loss: 0.0009030324096481005\n2837/10000. loss: 0.0012507160815099876\n2838/10000. loss: 0.0010489022048811119\n2839/10000. loss: 0.0014360272325575352\n2840/10000. loss: 0.0011216962399582069\n2841/10000. loss: 0.0013468852266669273\n2842/10000. loss: 0.0011357169908781846\n2843/10000. loss: 0.0009973074775189161\n2844/10000. loss: 0.001649715006351471\n2845/10000. loss: 0.0016550238554676373\n2846/10000. loss: 0.00118586840108037\n2847/10000. loss: 0.001677941686163346\n2848/10000. loss: 0.0011320173895607393\n2849/10000. loss: 0.0012259486441810925\n2850/10000. loss: 0.0012529673210034769\n2851/10000. loss: 0.001217885718991359\n2852/10000. loss: 0.0012763917135695617\n2853/10000. loss: 0.001076893803353111\n2854/10000. loss: 0.0010904520750045776\n2855/10000. loss: 0.0009158661123365164\n2856/10000. loss: 0.001023102318868041\n2857/10000. loss: 0.0008673383854329586\n2858/10000. loss: 0.000782013793165485\n2859/10000. loss: 0.001060033372292916\n2860/10000. loss: 0.0010054619051516056\n2861/10000. loss: 0.0005672643892467022\n2862/10000. loss: 0.0011027168948203325\n2863/10000. loss: 0.0008747850855191549\n2864/10000. loss: 0.001252726496507724\n2865/10000. loss: 0.0011403387567649286\n2866/10000. loss: 0.0007766198832541704\n2867/10000. loss: 0.0005335309154664477\n2868/10000. loss: 0.0006772482302039862\n2869/10000. loss: 0.000722743726025025\n2870/10000. loss: 0.0008571435076495012\n2871/10000. loss: 0.0004511749915157755\n2872/10000. loss: 0.0005937076639384031\n2873/10000. loss: 0.0006811672355979681\n2874/10000. loss: 0.000683580602829655\n2875/10000. loss: 0.00043236146060129005\n2876/10000. loss: 0.000415107817389071\n2877/10000. loss: 0.0006142274166146914\n2878/10000. loss: 0.000604476702089111\n2879/10000. loss: 0.0004470179943988721\n2880/10000. loss: 0.0004336320562288165\n2881/10000. loss: 0.0004769958322867751\n2882/10000. loss: 0.0006379803720240792\n2883/10000. loss: 0.00035052459376553696\n2884/10000. loss: 0.0006926302642871937\n2885/10000. loss: 0.00036354685047020513\n2886/10000. loss: 0.0004766915614406268\n2887/10000. loss: 0.0004196712203944723\n2888/10000. loss: 0.00033673946745693684\n2889/10000. loss: 0.0004992480777824918\n2890/10000. loss: 0.0004892748935769001\n2891/10000. loss: 0.0002826808486133814\n2892/10000. loss: 0.0005461434678484997\n2893/10000. loss: 0.0003236307529732585\n2894/10000. loss: 0.0006409071308250228\n2895/10000. loss: 0.00035937662081172067\n2896/10000. loss: 0.000670853303745389\n2897/10000. loss: 0.00029514054767787457\n2898/10000. loss: 0.0006723636761307716\n2899/10000. loss: 0.0003701574169099331\n2900/10000. loss: 0.00033517930811891955\n2901/10000. loss: 0.0005283172164733211\n2902/10000. loss: 0.00041051243897527456\n2903/10000. loss: 0.00034066922186563414\n2904/10000. loss: 0.0006695838334659735\n2905/10000. loss: 0.0006280819264551004\n2906/10000. loss: 0.00045201811008155346\n2907/10000. loss: 0.00044729017342130345\n2908/10000. loss: 0.0005983815838893255\n2909/10000. loss: 0.0005737190755705038\n2910/10000. loss: 0.0005018620286136866\n2911/10000. loss: 0.0004462234986325105\n2912/10000. loss: 0.0004537922019759814\n2913/10000. loss: 0.0005144924313450853\n2914/10000. loss: 0.0004405038586507241\n2915/10000. loss: 0.0005134115926921368\n2916/10000. loss: 0.0005300599926461776\n2917/10000. loss: 0.0003474556530515353\n2918/10000. loss: 0.0005580368839825193\n2919/10000. loss: 0.0003492582666998108\n2920/10000. loss: 0.00035691373826315004\n2921/10000. loss: 0.0004946814151480794\n2922/10000. loss: 0.0003688265181457003\n2923/10000. loss: 0.00044052380447586376\n2924/10000. loss: 0.0004431960017730792\n2925/10000. loss: 0.00036570724720756215\n2926/10000. loss: 0.0004533214960247278\n2927/10000. loss: 0.00040148078308751184\n2928/10000. loss: 0.0006387365671495596\n2929/10000. loss: 0.0003740347844238083\n2930/10000. loss: 0.0005992845787356297\n2931/10000. loss: 0.0006781659709910551\n2932/10000. loss: 0.000358275487087667\n2933/10000. loss: 0.000610126725708445\n2934/10000. loss: 0.0006295601294065515\n2935/10000. loss: 0.000672149316718181\n2936/10000. loss: 0.0007128122573097547\n2937/10000. loss: 0.0007174942487229904\n2938/10000. loss: 0.0007558845294018587\n2939/10000. loss: 0.0007904150988906622\n2940/10000. loss: 0.0008963570774843296\n2941/10000. loss: 0.001112579678495725\n2942/10000. loss: 0.0012359350609282653\n2943/10000. loss: 0.000986057488868634\n2944/10000. loss: 0.0019765604908267656\n2945/10000. loss: 0.00242452509701252\n2946/10000. loss: 0.002187915767232577\n2947/10000. loss: 0.004036931941906611\n2948/10000. loss: 0.003375910222530365\n2949/10000. loss: 0.0027284277603030205\n2950/10000. loss: 0.0036830871055523553\n2951/10000. loss: 0.003082015241185824\n2952/10000. loss: 0.0023102867417037487\n2953/10000. loss: 0.0027981450160344443\n2954/10000. loss: 0.0032496576507886252\n2955/10000. loss: 0.0023907835905750594\n2956/10000. loss: 0.0016476611296335857\n2957/10000. loss: 0.0018262251590689023\n2958/10000. loss: 0.0009079258888959885\n2959/10000. loss: 0.0014505060389637947\n2960/10000. loss: 0.0016403133049607277\n2961/10000. loss: 0.0015421914868056774\n2962/10000. loss: 0.0009052573392788569\n2963/10000. loss: 0.000772267347201705\n2964/10000. loss: 0.0014226604253053665\n2965/10000. loss: 0.0009197765029966831\n2966/10000. loss: 0.0012315756175667048\n2967/10000. loss: 0.0010364822422464688\n2968/10000. loss: 0.0007832784516115984\n2969/10000. loss: 0.0008867058592538039\n2970/10000. loss: 0.000964911732201775\n2971/10000. loss: 0.0010960674068580072\n2972/10000. loss: 0.0008269610504309336\n2973/10000. loss: 0.000649942783638835\n2974/10000. loss: 0.0006394078178952137\n2975/10000. loss: 0.0007736238185316324\n2976/10000. loss: 0.0007173064320037762\n2977/10000. loss: 0.0005938092557092508\n2978/10000. loss: 0.0005402465661366781\n2979/10000. loss: 0.00040815487348785\n2980/10000. loss: 0.0004379325934375326\n2981/10000. loss: 0.0007401642700036367\n2982/10000. loss: 0.0004709604351470868\n2983/10000. loss: 0.0006213472224771976\n2984/10000. loss: 0.00035845080856233835\n2985/10000. loss: 0.00043195039809991914\n2986/10000. loss: 0.0005230498500168324\n2987/10000. loss: 0.0006016306579113007\n2988/10000. loss: 0.0004243190245081981\n2989/10000. loss: 0.0005187136121094227\n2990/10000. loss: 0.0004854979148755471\n2991/10000. loss: 0.00041741908838351566\n2992/10000. loss: 0.0007587408957382044\n2993/10000. loss: 0.0004916831385344267\n2994/10000. loss: 0.0003589144131789605\n2995/10000. loss: 0.00031924445647746325\n2996/10000. loss: 0.0006937387709816297\n2997/10000. loss: 0.00032894328857461613\n2998/10000. loss: 0.0005647627792010704\n2999/10000. loss: 0.0004685478052124381\n3000/10000. loss: 0.00042716953127334517\n3001/10000. loss: 0.0003496416223545869\n3002/10000. loss: 0.0006280165786544482\n3003/10000. loss: 0.0005383063107728958\n3004/10000. loss: 0.00034077721647918224\n3005/10000. loss: 0.0006579600740224123\n3006/10000. loss: 0.0004407025796050827\n3007/10000. loss: 0.0004358483711257577\n3008/10000. loss: 0.00048079947009682655\n3009/10000. loss: 0.0004542372577513258\n3010/10000. loss: 0.0005181825254112482\n3011/10000. loss: 0.000543953695644935\n3012/10000. loss: 0.00033317071696122486\n3013/10000. loss: 0.0006085417699068785\n3014/10000. loss: 0.0005380545432368914\n3015/10000. loss: 0.0003589602808157603\n3016/10000. loss: 0.00046805703702072304\n3017/10000. loss: 0.0006767603723953167\n3018/10000. loss: 0.000390696261699001\n3019/10000. loss: 0.000605485945319136\n3020/10000. loss: 0.0005565180520837506\n3021/10000. loss: 0.00045315657431880635\n3022/10000. loss: 0.00032445408093432587\n3023/10000. loss: 0.00034467507308969897\n3024/10000. loss: 0.0005819416837766767\n3025/10000. loss: 0.00035433308221399784\n3026/10000. loss: 0.0005284537716458241\n3027/10000. loss: 0.0005551446617270509\n3028/10000. loss: 0.0003296514041721821\n3029/10000. loss: 0.00037119459981719655\n3030/10000. loss: 0.0004491427292426427\n3031/10000. loss: 0.0006273276327798764\n3032/10000. loss: 0.0003646857027585308\n3033/10000. loss: 0.0003091603284701705\n3034/10000. loss: 0.0003379311723013719\n3035/10000. loss: 0.0004491768777370453\n3036/10000. loss: 0.0004609785197923581\n3037/10000. loss: 0.0003564482710013787\n3038/10000. loss: 0.00040105726414670545\n3039/10000. loss: 0.0006405596310893694\n3040/10000. loss: 0.0003342649433761835\n3041/10000. loss: 0.00037543712339053553\n3042/10000. loss: 0.00045449985191226006\n3043/10000. loss: 0.0005773321414987246\n3044/10000. loss: 0.00041223041868458193\n3045/10000. loss: 0.0005121404925982157\n3046/10000. loss: 0.0004790444703151782\n3047/10000. loss: 0.0003276893791432182\n3048/10000. loss: 0.0004141691606491804\n3049/10000. loss: 0.0004458165494725108\n3050/10000. loss: 0.0005976456838349501\n3051/10000. loss: 0.00044452639607091743\n3052/10000. loss: 0.0005209378432482481\n3053/10000. loss: 0.0005380620714277029\n3054/10000. loss: 0.0005361551108459631\n3055/10000. loss: 0.0003702969600756963\n3056/10000. loss: 0.0005872843321412802\n3057/10000. loss: 0.0005515454104170203\n3058/10000. loss: 0.0005600378305340806\n3059/10000. loss: 0.0005759565780560175\n3060/10000. loss: 0.0007285509879390398\n3061/10000. loss: 0.000839421913648645\n3062/10000. loss: 0.0009618850114444891\n3063/10000. loss: 0.0009332935636242231\n3064/10000. loss: 0.0010735941274712484\n3065/10000. loss: 0.0010981899686157703\n3066/10000. loss: 0.0013192311550180118\n3067/10000. loss: 0.001859446366628011\n3068/10000. loss: 0.004252451782425244\n3069/10000. loss: 0.0029123717298110328\n3070/10000. loss: 0.0015595111375053723\n3071/10000. loss: 0.003058680333197117\n3072/10000. loss: 0.00751689945658048\n3073/10000. loss: 0.0038550527145465216\n3074/10000. loss: 0.007132863005002339\n3075/10000. loss: 0.006542171041170756\n3076/10000. loss: 0.005253046130140622\n3077/10000. loss: 0.006045779834191005\n3078/10000. loss: 0.006273384516437848\n3079/10000. loss: 0.005554795265197754\n3080/10000. loss: 0.004012036758164565\n3081/10000. loss: 0.0037018867830435434\n3082/10000. loss: 0.002627173438668251\n3083/10000. loss: 0.0020939799336095652\n3084/10000. loss: 0.0024256535495320954\n3085/10000. loss: 0.002382038626819849\n3086/10000. loss: 0.001971871592104435\n3087/10000. loss: 0.0016785521681110065\n3088/10000. loss: 0.0012476606449733179\n3089/10000. loss: 0.0011154734529554844\n3090/10000. loss: 0.0011275198000172775\n3091/10000. loss: 0.001604878343641758\n3092/10000. loss: 0.0013394087242583435\n3093/10000. loss: 0.0011529583328713973\n3094/10000. loss: 0.001512799256791671\n3095/10000. loss: 0.0010954516474157572\n3096/10000. loss: 0.0012636508326977491\n3097/10000. loss: 0.0010916837491095066\n3098/10000. loss: 0.0007734323541323344\n3099/10000. loss: 0.0006613382138311863\n3100/10000. loss: 0.0007508180569857359\n3101/10000. loss: 0.0007865655546387037\n3102/10000. loss: 0.0007280511781573296\n3103/10000. loss: 0.0006889865423242251\n3104/10000. loss: 0.000414177270916601\n3105/10000. loss: 0.0006603910587728024\n3106/10000. loss: 0.00045512445891896885\n3107/10000. loss: 0.0004414548942198356\n3108/10000. loss: 0.0004969748357931773\n3109/10000. loss: 0.0007076220742116371\n3110/10000. loss: 0.00046013617732872564\n3111/10000. loss: 0.000567687830577294\n3112/10000. loss: 0.0005492270380879442\n3113/10000. loss: 0.0006991025681296984\n3114/10000. loss: 0.0005773794061193863\n3115/10000. loss: 0.00043199929253508645\n3116/10000. loss: 0.0005421869767208894\n3117/10000. loss: 0.0006272345005224148\n3118/10000. loss: 0.00043420109432190657\n3119/10000. loss: 0.0004295728091771404\n3120/10000. loss: 0.0006396073537568251\n3121/10000. loss: 0.00042878413417687017\n3122/10000. loss: 0.0006121807576467594\n3123/10000. loss: 0.00043295502352217835\n3124/10000. loss: 0.000462791610819598\n3125/10000. loss: 0.0005415775425111254\n3126/10000. loss: 0.0004656415743132432\n3127/10000. loss: 0.0006749369204044342\n3128/10000. loss: 0.0004740285997589429\n3129/10000. loss: 0.0004350259356821577\n3130/10000. loss: 0.0005156801004583637\n3131/10000. loss: 0.0004176554890970389\n3132/10000. loss: 0.00036100259361167747\n3133/10000. loss: 0.00041314233870555955\n3134/10000. loss: 0.00040562838936845463\n3135/10000. loss: 0.00047115546961625415\n3136/10000. loss: 0.0005157874741901954\n3137/10000. loss: 0.0007692635990679264\n3138/10000. loss: 0.0003526032281418641\n3139/10000. loss: 0.0005491964984685183\n3140/10000. loss: 0.00032621963570515317\n3141/10000. loss: 0.0005197541322559118\n3142/10000. loss: 0.00043856888078153133\n3143/10000. loss: 0.0003168086016861101\n3144/10000. loss: 0.0003950160462409258\n3145/10000. loss: 0.00031651688429216546\n3146/10000. loss: 0.0004431129976486166\n3147/10000. loss: 0.0003674769541248679\n3148/10000. loss: 0.0004960945419346293\n3149/10000. loss: 0.0006955275312066078\n3150/10000. loss: 0.0003098895346435408\n3151/10000. loss: 0.0004710485227406025\n3152/10000. loss: 0.0005559892548869053\n3153/10000. loss: 0.00043454021215438843\n3154/10000. loss: 0.0005111630307510495\n3155/10000. loss: 0.0003269740069905917\n3156/10000. loss: 0.0004392430807153384\n3157/10000. loss: 0.0006256163275490204\n3158/10000. loss: 0.00033389396655062836\n3159/10000. loss: 0.000563628583525618\n3160/10000. loss: 0.0005419421941041946\n3161/10000. loss: 0.000507438788190484\n3162/10000. loss: 0.0005772218573838472\n3163/10000. loss: 0.0005087497799346844\n3164/10000. loss: 0.0005551399663090706\n3165/10000. loss: 0.0004392109500865142\n3166/10000. loss: 0.0006620960775762796\n3167/10000. loss: 0.0004507958268125852\n3168/10000. loss: 0.0005040682153776288\n3169/10000. loss: 0.0005474089023967584\n3170/10000. loss: 0.0005894151593868931\n3171/10000. loss: 0.0004430572735145688\n3172/10000. loss: 0.0006316829239949584\n3173/10000. loss: 0.0005042787330845991\n3174/10000. loss: 0.0005229490343481302\n3175/10000. loss: 0.0003893208922818303\n3176/10000. loss: 0.00048594312587132055\n3177/10000. loss: 0.0003849949377278487\n3178/10000. loss: 0.00044885731767863035\n3179/10000. loss: 0.00034105606998006505\n3180/10000. loss: 0.000447435537353158\n3181/10000. loss: 0.00047177262604236603\n3182/10000. loss: 0.00032129996300985414\n3183/10000. loss: 0.0004133978703369697\n3184/10000. loss: 0.0003298761633535226\n3185/10000. loss: 0.0004199279937893152\n3186/10000. loss: 0.0004309345968067646\n3187/10000. loss: 0.0004934032137195269\n3188/10000. loss: 0.0004571587002525727\n3189/10000. loss: 0.0003287194607158502\n3190/10000. loss: 0.0005055897248287996\n3191/10000. loss: 0.00044848397374153137\n3192/10000. loss: 0.00039404948862890404\n3193/10000. loss: 0.0004161090279618899\n3194/10000. loss: 0.0005168279555315772\n3195/10000. loss: 0.0004025607680281003\n3196/10000. loss: 0.00034380482975393534\n3197/10000. loss: 0.0003893437484900157\n3198/10000. loss: 0.0005743970395997167\n3199/10000. loss: 0.0004141350121547778\n3200/10000. loss: 0.0003788076573982835\n3201/10000. loss: 0.00040717814893772203\n3202/10000. loss: 0.00031733747649316985\n3203/10000. loss: 0.00040698501591881114\n3204/10000. loss: 0.0004895056287447611\n3205/10000. loss: 0.0003337043259913723\n3206/10000. loss: 0.00039272864038745564\n3207/10000. loss: 0.0005023465491831303\n3208/10000. loss: 0.0005927220142136017\n3209/10000. loss: 0.00032989059885342914\n3210/10000. loss: 0.000306446027631561\n3211/10000. loss: 0.0004338830476626754\n3212/10000. loss: 0.00047501723747700453\n3213/10000. loss: 0.0006384362156192461\n3214/10000. loss: 0.000405952800065279\n3215/10000. loss: 0.0005315660964697599\n3216/10000. loss: 0.0002997041253062586\n3217/10000. loss: 0.0002850018596897523\n3218/10000. loss: 0.0002737604857732852\n3219/10000. loss: 0.0004832940176129341\n3220/10000. loss: 0.0005870001235355934\n3221/10000. loss: 0.0004719930002465844\n3222/10000. loss: 0.0003098380984738469\n3223/10000. loss: 0.000538382368783156\n3224/10000. loss: 0.0005394043400883675\n3225/10000. loss: 0.0006528453280528387\n3226/10000. loss: 0.0005123143394788107\n3227/10000. loss: 0.0007783458568155766\n3228/10000. loss: 0.0005605508728573719\n3229/10000. loss: 0.000333588570356369\n3230/10000. loss: 0.0004414098026851813\n3231/10000. loss: 0.00034819267845402163\n3232/10000. loss: 0.0006307928512493769\n3233/10000. loss: 0.00044007420850296813\n3234/10000. loss: 0.0004833228498076399\n3235/10000. loss: 0.0003790719589839379\n3236/10000. loss: 0.0005802915741999944\n3237/10000. loss: 0.000320822698995471\n3238/10000. loss: 0.00037353454778591794\n3239/10000. loss: 0.0005547418259084225\n3240/10000. loss: 0.0004224070192625125\n3241/10000. loss: 0.0005734141062324246\n3242/10000. loss: 0.000493337904723982\n3243/10000. loss: 0.000510736679037412\n3244/10000. loss: 0.000450719924022754\n3245/10000. loss: 0.0005937283082554737\n3246/10000. loss: 0.0006441283815850815\n3247/10000. loss: 0.0005138398846611381\n3248/10000. loss: 0.0006012637168169022\n3249/10000. loss: 0.00040604119809965294\n3250/10000. loss: 0.0008084777897844712\n3251/10000. loss: 0.0006376661670704683\n3252/10000. loss: 0.0005648080647612611\n3253/10000. loss: 0.00047253720307101804\n3254/10000. loss: 0.0006703776307404041\n3255/10000. loss: 0.0005411109110961357\n3256/10000. loss: 0.0005341497405121723\n3257/10000. loss: 0.0006205784933020672\n3258/10000. loss: 0.0003932241428022583\n3259/10000. loss: 0.0003707448098187645\n3260/10000. loss: 0.000668348278850317\n3261/10000. loss: 0.000516511577491959\n3262/10000. loss: 0.0006214946818848451\n3263/10000. loss: 0.0007910708275934061\n3264/10000. loss: 0.0004597151031096776\n3265/10000. loss: 0.0005937636985133091\n3266/10000. loss: 0.00041691819205880165\n3267/10000. loss: 0.00046768734076370794\n3268/10000. loss: 0.0006296159699559212\n3269/10000. loss: 0.0006126205359275142\n3270/10000. loss: 0.0004924040598173937\n3271/10000. loss: 0.0003723927851145466\n3272/10000. loss: 0.000553936775152882\n3273/10000. loss: 0.00041089676475773257\n3274/10000. loss: 0.0005693869510044655\n3275/10000. loss: 0.0003487979993224144\n3276/10000. loss: 0.0003936207698037227\n3277/10000. loss: 0.0003291652149831255\n3278/10000. loss: 0.0006429880935077866\n3279/10000. loss: 0.0004972965301324924\n3280/10000. loss: 0.0006053824909031391\n3281/10000. loss: 0.0005305659336348375\n3282/10000. loss: 0.0005297161017855009\n3283/10000. loss: 0.0006688958965241909\n3284/10000. loss: 0.0004908561240881681\n3285/10000. loss: 0.0003350697613010804\n3286/10000. loss: 0.00048448956416298944\n3287/10000. loss: 0.0005007351282984018\n3288/10000. loss: 0.0005434913715968529\n3289/10000. loss: 0.000609979343911012\n3290/10000. loss: 0.00031310141397019226\n3291/10000. loss: 0.00046452623791992664\n3292/10000. loss: 0.0005207719126095375\n3293/10000. loss: 0.000597043273349603\n3294/10000. loss: 0.0005864289899667104\n3295/10000. loss: 0.0003531165032957991\n3296/10000. loss: 0.0005009438609704375\n3297/10000. loss: 0.0005338240492468079\n3298/10000. loss: 0.00044101391298075515\n3299/10000. loss: 0.00033070030622184277\n3300/10000. loss: 0.0004934016615152359\n3301/10000. loss: 0.0005253037282576164\n3302/10000. loss: 0.0004407213224718968\n3303/10000. loss: 0.0006743733150263628\n3304/10000. loss: 0.00041755835991352797\n3305/10000. loss: 0.00032278464641422033\n3306/10000. loss: 0.00035153057736655075\n3307/10000. loss: 0.00042643118649721146\n3308/10000. loss: 0.0004907997014621893\n3309/10000. loss: 0.0005409706694384416\n3310/10000. loss: 0.0005993257509544492\n3311/10000. loss: 0.0005476399480054776\n3312/10000. loss: 0.0005021352165689071\n3313/10000. loss: 0.00042321377744277317\n3314/10000. loss: 0.0005617965168009201\n3315/10000. loss: 0.0005872085457667708\n3316/10000. loss: 0.0006326754810288548\n3317/10000. loss: 0.00039754357809821766\n3318/10000. loss: 0.000742162267367045\n3319/10000. loss: 0.0007425270353754362\n3320/10000. loss: 0.0006112009286880493\n3321/10000. loss: 0.0005182455061003566\n3322/10000. loss: 0.0006387494116400679\n3323/10000. loss: 0.0007146922095368305\n3324/10000. loss: 0.00037131311061481637\n3325/10000. loss: 0.00047502949989090365\n3326/10000. loss: 0.0005511864243696133\n3327/10000. loss: 0.000401109845067064\n3328/10000. loss: 0.0007470023507873217\n3329/10000. loss: 0.0005547467153519392\n3330/10000. loss: 0.000677044503390789\n3331/10000. loss: 0.0004428151684502761\n3332/10000. loss: 0.0005762891378253698\n3333/10000. loss: 0.0003664641020198663\n3334/10000. loss: 0.00036199286114424467\n3335/10000. loss: 0.0004086424208556612\n3336/10000. loss: 0.00067277648486197\n3337/10000. loss: 0.0005749837340166172\n3338/10000. loss: 0.0006830600711206595\n3339/10000. loss: 0.0007054023444652557\n3340/10000. loss: 0.0005655240577956041\n3341/10000. loss: 0.00041205273009836674\n3342/10000. loss: 0.0008275506552308798\n3343/10000. loss: 0.0007278413977473974\n3344/10000. loss: 0.00086578579309086\n3345/10000. loss: 0.0009634893697996935\n3346/10000. loss: 0.0007698205299675465\n3347/10000. loss: 0.000659811000029246\n3348/10000. loss: 0.0006897001682470242\n3349/10000. loss: 0.0006744170095771551\n3350/10000. loss: 0.0006015043084820112\n3351/10000. loss: 0.0007557996238271395\n3352/10000. loss: 0.0006014884759982427\n3353/10000. loss: 0.0008729791734367609\n3354/10000. loss: 0.0007307416914651791\n3355/10000. loss: 0.0008786767721176147\n3356/10000. loss: 0.0006277774227783084\n3357/10000. loss: 0.001070479319120447\n3358/10000. loss: 0.0006424907284478346\n3359/10000. loss: 0.0007040625593314568\n3360/10000. loss: 0.0007473056515057882\n3361/10000. loss: 0.000649480459590753\n3362/10000. loss: 0.0006360109740247329\n3363/10000. loss: 0.0006548361852765083\n3364/10000. loss: 0.000507430755533278\n3365/10000. loss: 0.00048513244837522507\n3366/10000. loss: 0.0006467473382751147\n3367/10000. loss: 0.0004498645042379697\n3368/10000. loss: 0.0004763250860075156\n3369/10000. loss: 0.0005383489963908991\n3370/10000. loss: 0.0004548939565817515\n3371/10000. loss: 0.0005081117463608583\n3372/10000. loss: 0.0006554694070170323\n3373/10000. loss: 0.0005138168732325236\n3374/10000. loss: 0.0002944376125621299\n3375/10000. loss: 0.0005058591874937216\n3376/10000. loss: 0.00044113028949747485\n3377/10000. loss: 0.0003197712746138374\n3378/10000. loss: 0.0004062241253753503\n3379/10000. loss: 0.0003299982442210118\n3380/10000. loss: 0.0005392367796351513\n3381/10000. loss: 0.00032522880549853045\n3382/10000. loss: 0.0005743166742225488\n3383/10000. loss: 0.0003947937705864509\n3384/10000. loss: 0.0004302427793542544\n3385/10000. loss: 0.0005314087805648645\n3386/10000. loss: 0.000575962980898718\n3387/10000. loss: 0.0005675212402517597\n3388/10000. loss: 0.0006707770129044851\n3389/10000. loss: 0.0004301259759813547\n3390/10000. loss: 0.0006929110580434402\n3391/10000. loss: 0.000475324767952164\n3392/10000. loss: 0.0003871713997796178\n3393/10000. loss: 0.00048174639232456684\n3394/10000. loss: 0.00044427210620294016\n3395/10000. loss: 0.0003734846444179614\n3396/10000. loss: 0.0005136883507172266\n3397/10000. loss: 0.0002887114533223212\n3398/10000. loss: 0.00044678054594745237\n3399/10000. loss: 0.00036208308301866055\n3400/10000. loss: 0.00041156836474935216\n3401/10000. loss: 0.00029179989360272884\n3402/10000. loss: 0.0004899192911883196\n3403/10000. loss: 0.0004007481038570404\n3404/10000. loss: 0.00030069398538519937\n3405/10000. loss: 0.0004202369212483366\n3406/10000. loss: 0.00027313818767045933\n3407/10000. loss: 0.00043006955335537594\n3408/10000. loss: 0.00030994795573254425\n3409/10000. loss: 0.0004390263929963112\n3410/10000. loss: 0.00040213100146502256\n3411/10000. loss: 0.000398595390530924\n3412/10000. loss: 0.00037481232235829037\n3413/10000. loss: 0.0003208944690413773\n3414/10000. loss: 0.0005117745604366064\n3415/10000. loss: 0.00031874656754856306\n3416/10000. loss: 0.0004060703795403242\n3417/10000. loss: 0.0005064996269841989\n3418/10000. loss: 0.00039012070434788865\n3419/10000. loss: 0.00031213031616061926\n3420/10000. loss: 0.00039836418970177573\n3421/10000. loss: 0.000464737454118828\n3422/10000. loss: 0.0004265316141148408\n3423/10000. loss: 0.00035368348471820354\n3424/10000. loss: 0.0002829674437331657\n3425/10000. loss: 0.00038883741945028305\n3426/10000. loss: 0.000438014161773026\n3427/10000. loss: 0.0003458186984062195\n3428/10000. loss: 0.000509008183144033\n3429/10000. loss: 0.00026923239541550476\n3430/10000. loss: 0.0004701250387976567\n3431/10000. loss: 0.0004015484591946006\n3432/10000. loss: 0.00043024316740532714\n3433/10000. loss: 0.0005171726612995068\n3434/10000. loss: 0.00035647380476196605\n3435/10000. loss: 0.0003213776120295127\n3436/10000. loss: 0.0006667516039063534\n3437/10000. loss: 0.0007711136713624001\n3438/10000. loss: 0.00036496649651477736\n3439/10000. loss: 0.0007649392355233431\n3440/10000. loss: 0.0014553672323624294\n3441/10000. loss: 0.001837249690045913\n3442/10000. loss: 0.001507103443145752\n3443/10000. loss: 0.0025389070312182107\n3444/10000. loss: 0.0026079720507065454\n3445/10000. loss: 0.0030497449139753976\n3446/10000. loss: 0.003571245508889357\n3447/10000. loss: 0.004732480583091577\n3448/10000. loss: 0.005024531856179237\n3449/10000. loss: 0.004876987387736638\n3450/10000. loss: 0.00509389986594518\n3451/10000. loss: 0.0030401935800909996\n3452/10000. loss: 0.004119862491885821\n3453/10000. loss: 0.0021651160592834153\n3454/10000. loss: 0.0014876641022662322\n3455/10000. loss: 0.0023089988778034845\n3456/10000. loss: 0.003045010690887769\n3457/10000. loss: 0.0019919400413831076\n3458/10000. loss: 0.0018219323828816414\n3459/10000. loss: 0.0018865404029687245\n3460/10000. loss: 0.0020935569579402604\n3461/10000. loss: 0.0017832277032236259\n3462/10000. loss: 0.001401428443690141\n3463/10000. loss: 0.001119704373801748\n3464/10000. loss: 0.0007324561011046171\n3465/10000. loss: 0.0007111420854926109\n3466/10000. loss: 0.0007490306937446197\n3467/10000. loss: 0.0008706047665327787\n3468/10000. loss: 0.0008984293478230635\n3469/10000. loss: 0.0009001725508521\n3470/10000. loss: 0.00100941591275235\n3471/10000. loss: 0.0009168756660073996\n3472/10000. loss: 0.0009949412196874619\n3473/10000. loss: 0.0008173695920656124\n3474/10000. loss: 0.0006998892252643903\n3475/10000. loss: 0.0007584770210087299\n3476/10000. loss: 0.0007098858865598837\n3477/10000. loss: 0.0004952086601406336\n3478/10000. loss: 0.00041587103623896837\n3479/10000. loss: 0.0005268095216403405\n3480/10000. loss: 0.0006504539245118698\n3481/10000. loss: 0.0003234604567599793\n3482/10000. loss: 0.000521407462656498\n3483/10000. loss: 0.0005424727375308672\n3484/10000. loss: 0.0005786188800508777\n3485/10000. loss: 0.0007378991382817427\n3486/10000. loss: 0.0005612447081754605\n3487/10000. loss: 0.0005821466523533066\n3488/10000. loss: 0.0008703609928488731\n3489/10000. loss: 0.0006577752064913511\n3490/10000. loss: 0.000786196750899156\n3491/10000. loss: 0.0005764250721161565\n3492/10000. loss: 0.0005108579061925411\n3493/10000. loss: 0.000755366093168656\n3494/10000. loss: 0.0009405545424669981\n3495/10000. loss: 0.0006073537903527418\n3496/10000. loss: 0.0007303178620835146\n3497/10000. loss: 0.0008266716419408718\n3498/10000. loss: 0.0005969259267052015\n3499/10000. loss: 0.0005237148919453224\n3500/10000. loss: 0.0005346538964658976\n3501/10000. loss: 0.0006387553876265883\n3502/10000. loss: 0.00047240111355980236\n3503/10000. loss: 0.0004245118470862508\n3504/10000. loss: 0.0004549067622671525\n3505/10000. loss: 0.0005151438914860288\n3506/10000. loss: 0.0004935732576996088\n3507/10000. loss: 0.0004188469999159376\n3508/10000. loss: 0.0005245067877694964\n3509/10000. loss: 0.0006290250457823277\n3510/10000. loss: 0.000493502321963509\n3511/10000. loss: 0.0004196402927239736\n3512/10000. loss: 0.0005126665346324444\n3513/10000. loss: 0.00041656335815787315\n3514/10000. loss: 0.000464865006506443\n3515/10000. loss: 0.0005514305084943771\n3516/10000. loss: 0.00048721255734562874\n3517/10000. loss: 0.00032950867898762226\n3518/10000. loss: 0.0003266342294712861\n3519/10000. loss: 0.0003058319174063702\n3520/10000. loss: 0.00033393362537026405\n3521/10000. loss: 0.000509131078918775\n3522/10000. loss: 0.00044636680589367944\n3523/10000. loss: 0.0005319297779351473\n3524/10000. loss: 0.00034560907321671647\n3525/10000. loss: 0.0005079921878253421\n3526/10000. loss: 0.00031047880960007507\n3527/10000. loss: 0.0004209458129480481\n3528/10000. loss: 0.00041377202918132144\n3529/10000. loss: 0.0005212747491896152\n3530/10000. loss: 0.0005765441649903854\n3531/10000. loss: 0.00031177839264273643\n3532/10000. loss: 0.00034791692936172086\n3533/10000. loss: 0.00031478318851441145\n3534/10000. loss: 0.0004078704708566268\n3535/10000. loss: 0.00030810948616514605\n3536/10000. loss: 0.00043425068724900484\n3537/10000. loss: 0.00028527213726192713\n3538/10000. loss: 0.00031637448895101744\n3539/10000. loss: 0.0004988329795499643\n3540/10000. loss: 0.0003785123893370231\n3541/10000. loss: 0.0005024056881666183\n3542/10000. loss: 0.00048506702296435833\n3543/10000. loss: 0.0006113864947110415\n3544/10000. loss: 0.00026915040022383135\n3545/10000. loss: 0.0002640004192168514\n3546/10000. loss: 0.0003521257701019446\n3547/10000. loss: 0.0005437759682536125\n3548/10000. loss: 0.0004566581143687169\n3549/10000. loss: 0.0004243201110512018\n3550/10000. loss: 0.000279522268101573\n3551/10000. loss: 0.0004585655794168512\n3552/10000. loss: 0.0002998279912086825\n3553/10000. loss: 0.00033042180196692544\n3554/10000. loss: 0.000490642113921543\n3555/10000. loss: 0.0006279309357826909\n3556/10000. loss: 0.0004142071896543105\n3557/10000. loss: 0.00046830981348951656\n3558/10000. loss: 0.0005875454129030307\n3559/10000. loss: 0.0005409084260463715\n3560/10000. loss: 0.0005624094434703389\n3561/10000. loss: 0.00032462594875444967\n3562/10000. loss: 0.0003792049052814643\n3563/10000. loss: 0.0005477847686658303\n3564/10000. loss: 0.000548584774757425\n3565/10000. loss: 0.0003458948340266943\n3566/10000. loss: 0.0005749770595381657\n3567/10000. loss: 0.0003367609654863675\n3568/10000. loss: 0.00047289781893293065\n3569/10000. loss: 0.0004618747237448891\n3570/10000. loss: 0.00046464428305625916\n3571/10000. loss: 0.0004046813119202852\n3572/10000. loss: 0.00033804471604526043\n3573/10000. loss: 0.0004875774805744489\n3574/10000. loss: 0.000565089751034975\n3575/10000. loss: 0.0004045576012382905\n3576/10000. loss: 0.0006157391083737215\n3577/10000. loss: 0.0003839889153217276\n3578/10000. loss: 0.00038972427137196064\n3579/10000. loss: 0.0004781521080682675\n3580/10000. loss: 0.0005269846490894755\n3581/10000. loss: 0.0002599244083588322\n3582/10000. loss: 0.00033049875249465305\n3583/10000. loss: 0.000334586133249104\n3584/10000. loss: 0.0005550251031915346\n3585/10000. loss: 0.0004751207306981087\n3586/10000. loss: 0.0004075874264041583\n3587/10000. loss: 0.0004772906346867482\n3588/10000. loss: 0.0004879578482359648\n3589/10000. loss: 0.00029822248810281354\n3590/10000. loss: 0.0002924912647965054\n3591/10000. loss: 0.0005571073076377312\n3592/10000. loss: 0.0003290286598106225\n3593/10000. loss: 0.00042790376270810765\n3594/10000. loss: 0.0004711281896258394\n3595/10000. loss: 0.000341782346367836\n3596/10000. loss: 0.0003392184929301341\n3597/10000. loss: 0.00047905626706779003\n3598/10000. loss: 0.0003170455650736888\n3599/10000. loss: 0.0003293203966071208\n3600/10000. loss: 0.0004642509932940205\n3601/10000. loss: 0.00047923453773061436\n3602/10000. loss: 0.000333149335347116\n3603/10000. loss: 0.000528299754175047\n3604/10000. loss: 0.0004753101384267211\n3605/10000. loss: 0.0004995485845332345\n3606/10000. loss: 0.0003588015291218956\n3607/10000. loss: 0.0006394986994564533\n3608/10000. loss: 0.0006168342661112547\n3609/10000. loss: 0.0009366879239678383\n3610/10000. loss: 0.0006399831424156824\n3611/10000. loss: 0.0008015042791763941\n3612/10000. loss: 0.0011679714856048424\n3613/10000. loss: 0.0014917518322666485\n3614/10000. loss: 0.001956363053371509\n3615/10000. loss: 0.0021286408106486\n3616/10000. loss: 0.0029781749472022057\n3617/10000. loss: 0.002832124630610148\n3618/10000. loss: 0.0012563841106990974\n3619/10000. loss: 0.0030920496210455894\n3620/10000. loss: 0.0021866345778107643\n3621/10000. loss: 0.002255777052293221\n3622/10000. loss: 0.0016207477698723476\n3623/10000. loss: 0.00220171253507336\n3624/10000. loss: 0.0012545986101031303\n3625/10000. loss: 0.001692581611375014\n3626/10000. loss: 0.001190406425545613\n3627/10000. loss: 0.001318270651002725\n3628/10000. loss: 0.0011302786879241467\n3629/10000. loss: 0.0009734709747135639\n3630/10000. loss: 0.0010526568318406742\n3631/10000. loss: 0.0011846187214056652\n3632/10000. loss: 0.001088822027668357\n3633/10000. loss: 0.0007795326722164949\n3634/10000. loss: 0.0008918289095163345\n3635/10000. loss: 0.0005767557304352522\n3636/10000. loss: 0.0004987090360373259\n3637/10000. loss: 0.0008108046992371479\n3638/10000. loss: 0.0006035736296325922\n3639/10000. loss: 0.0007593765233953794\n3640/10000. loss: 0.0007824740993479887\n3641/10000. loss: 0.0007224861377229294\n3642/10000. loss: 0.0008058589107046524\n3643/10000. loss: 0.0006576554539302985\n3644/10000. loss: 0.0005652696127071977\n3645/10000. loss: 0.0006152320032318433\n3646/10000. loss: 0.0006242637755349278\n3647/10000. loss: 0.00048245303332805634\n3648/10000. loss: 0.0004091671435162425\n3649/10000. loss: 0.0006717280484735966\n3650/10000. loss: 0.0005089369369670749\n3651/10000. loss: 0.00036057468969374895\n3652/10000. loss: 0.00039859899940590066\n3653/10000. loss: 0.0005691980477422476\n3654/10000. loss: 0.00045250907229880494\n3655/10000. loss: 0.00035037705674767494\n3656/10000. loss: 0.0003853099575887124\n3657/10000. loss: 0.0003513807508473595\n3658/10000. loss: 0.0007024786124626795\n3659/10000. loss: 0.0005573151477922996\n3660/10000. loss: 0.00035344444525738555\n3661/10000. loss: 0.0004118389915674925\n3662/10000. loss: 0.0003375538702433308\n3663/10000. loss: 0.0005150739258776108\n3664/10000. loss: 0.0006505554386725029\n3665/10000. loss: 0.0006179258149738113\n3666/10000. loss: 0.0004846282924214999\n3667/10000. loss: 0.0003350681314865748\n3668/10000. loss: 0.0004981277355303367\n3669/10000. loss: 0.0005357969009007016\n3670/10000. loss: 0.0004132482378433148\n3671/10000. loss: 0.0004639908050497373\n3672/10000. loss: 0.0003249043559965988\n3673/10000. loss: 0.00043397063078979653\n3674/10000. loss: 0.0005347081848109762\n3675/10000. loss: 0.0003036736937550207\n3676/10000. loss: 0.0005803471819187204\n3677/10000. loss: 0.0006220946088433266\n3678/10000. loss: 0.0002887280813107888\n3679/10000. loss: 0.0003053063410334289\n3680/10000. loss: 0.00040314757886032265\n3681/10000. loss: 0.00039918324910104275\n3682/10000. loss: 0.00041959139828880626\n3683/10000. loss: 0.0003749524476006627\n3684/10000. loss: 0.0003916197456419468\n3685/10000. loss: 0.000364727689884603\n3686/10000. loss: 0.0003271196425581972\n3687/10000. loss: 0.00042356861134370166\n3688/10000. loss: 0.0004824113566428423\n3689/10000. loss: 0.00043969644078363973\n3690/10000. loss: 0.0005021744873374701\n3691/10000. loss: 0.0004207779420539737\n3692/10000. loss: 0.00029886665288358927\n3693/10000. loss: 0.00033974942440787953\n3694/10000. loss: 0.00047312785560886067\n3695/10000. loss: 0.00028453530588497716\n3696/10000. loss: 0.00031094941853856045\n3697/10000. loss: 0.0003243856675301989\n3698/10000. loss: 0.0002962942235171795\n3699/10000. loss: 0.00028413332377870876\n3700/10000. loss: 0.0002781742950901389\n3701/10000. loss: 0.0003611657302826643\n3702/10000. loss: 0.0005643313440183798\n3703/10000. loss: 0.0003288286194826166\n3704/10000. loss: 0.0003018274049585064\n3705/10000. loss: 0.0002996069185125331\n3706/10000. loss: 0.0005540311491737763\n3707/10000. loss: 0.000530014435450236\n3708/10000. loss: 0.000485782278701663\n3709/10000. loss: 0.0004146466962993145\n3710/10000. loss: 0.0005801903704802195\n3711/10000. loss: 0.00038965993250409764\n3712/10000. loss: 0.0005599448923021555\n3713/10000. loss: 0.00038660472879807156\n3714/10000. loss: 0.0007098804538448652\n3715/10000. loss: 0.00045564611597607535\n3716/10000. loss: 0.00035904782513777417\n3717/10000. loss: 0.00036943843588232994\n3718/10000. loss: 0.000484062513957421\n3719/10000. loss: 0.0004097783239558339\n3720/10000. loss: 0.0003253367419044177\n3721/10000. loss: 0.000420698585609595\n3722/10000. loss: 0.00028505350928753614\n3723/10000. loss: 0.0005608752835541964\n3724/10000. loss: 0.00033550899631033343\n3725/10000. loss: 0.0004730599466711283\n3726/10000. loss: 0.00047302090873320896\n3727/10000. loss: 0.0004811022663488984\n3728/10000. loss: 0.0003940712970991929\n3729/10000. loss: 0.00038761079000929993\n3730/10000. loss: 0.0004891934028516213\n3731/10000. loss: 0.0002995178025836746\n3732/10000. loss: 0.0002827176552576323\n3733/10000. loss: 0.00035640554657826823\n3734/10000. loss: 0.0005100208412234982\n3735/10000. loss: 0.00043583308191349107\n3736/10000. loss: 0.0004877669271081686\n3737/10000. loss: 0.00044246964777509373\n3738/10000. loss: 0.0004607667215168476\n3739/10000. loss: 0.0006269008542100588\n3740/10000. loss: 0.0003396743753304084\n3741/10000. loss: 0.00040394595513741177\n3742/10000. loss: 0.00041782316596557695\n3743/10000. loss: 0.0005519821618994077\n3744/10000. loss: 0.0006365506754567226\n3745/10000. loss: 0.0005350850988179445\n3746/10000. loss: 0.0005152734229341149\n3747/10000. loss: 0.000535317036944131\n3748/10000. loss: 0.0003727363267292579\n3749/10000. loss: 0.0007308862016846737\n3750/10000. loss: 0.0004829787261163195\n3751/10000. loss: 0.00047494253764549893\n3752/10000. loss: 0.0006826600680748621\n3753/10000. loss: 0.0004191584885120392\n3754/10000. loss: 0.000673922710120678\n3755/10000. loss: 0.0009034674925108751\n3756/10000. loss: 0.0010798271590222914\n3757/10000. loss: 0.0007193750546624263\n3758/10000. loss: 0.001352783137311538\n3759/10000. loss: 0.0009724561435480913\n3760/10000. loss: 0.0013299142010509968\n3761/10000. loss: 0.0011468551432092984\n3762/10000. loss: 0.0013097502912084262\n3763/10000. loss: 0.0010919299287100632\n3764/10000. loss: 0.0009511110838502645\n3765/10000. loss: 0.0013519374964137871\n3766/10000. loss: 0.0009043041306237379\n3767/10000. loss: 0.0010167257860302925\n3768/10000. loss: 0.0008622008220603069\n3769/10000. loss: 0.0006836980270842711\n3770/10000. loss: 0.0011027591147770484\n3771/10000. loss: 0.0006476764101535082\n3772/10000. loss: 0.0006167443546776971\n3773/10000. loss: 0.000878477313866218\n3774/10000. loss: 0.0009581128445764383\n3775/10000. loss: 0.0006267193627233306\n3776/10000. loss: 0.000774116488173604\n3777/10000. loss: 0.0006134463474154472\n3778/10000. loss: 0.0006555796135216951\n3779/10000. loss: 0.0007116192330916723\n3780/10000. loss: 0.0005716830880070726\n3781/10000. loss: 0.0004579521094759305\n3782/10000. loss: 0.000588075335447987\n3783/10000. loss: 0.00043760395298401516\n3784/10000. loss: 0.0006202229609092077\n3785/10000. loss: 0.0003874542114014427\n3786/10000. loss: 0.0006221085010717312\n3787/10000. loss: 0.0005172502327089509\n3788/10000. loss: 0.0004569875697294871\n3789/10000. loss: 0.000581262051127851\n3790/10000. loss: 0.00040280733567972976\n3791/10000. loss: 0.0006048704963177443\n3792/10000. loss: 0.00040183755724380415\n3793/10000. loss: 0.0004921200064321359\n3794/10000. loss: 0.0003399467095732689\n3795/10000. loss: 0.0006565943670769533\n3796/10000. loss: 0.0005654057022184134\n3797/10000. loss: 0.0005370902363210917\n3798/10000. loss: 0.00036464860507597524\n3799/10000. loss: 0.0005303564636657635\n3800/10000. loss: 0.0005142396160711845\n3801/10000. loss: 0.00039354199543595314\n3802/10000. loss: 0.0003314315884684523\n3803/10000. loss: 0.0003792975718776385\n3804/10000. loss: 0.0002802082259828846\n3805/10000. loss: 0.00040277924078206223\n3806/10000. loss: 0.00027825834695249796\n3807/10000. loss: 0.0002619463096683224\n3808/10000. loss: 0.00038570080262919265\n3809/10000. loss: 0.0003120967885479331\n3810/10000. loss: 0.00028858077712357044\n3811/10000. loss: 0.0002641026706745227\n3812/10000. loss: 0.00044084157949934405\n3813/10000. loss: 0.0004252790240570903\n3814/10000. loss: 0.0005526806538303693\n3815/10000. loss: 0.00034729969532539445\n3816/10000. loss: 0.0004421401924143235\n3817/10000. loss: 0.00027487547292063635\n3818/10000. loss: 0.00044408537602672976\n3819/10000. loss: 0.00042514239127437275\n3820/10000. loss: 0.00043361967739959556\n3821/10000. loss: 0.0003835206540922324\n3822/10000. loss: 0.0004361203173175454\n3823/10000. loss: 0.00041382193254927796\n3824/10000. loss: 0.00047392118722200394\n3825/10000. loss: 0.00027533967901642126\n3826/10000. loss: 0.0004319441116725405\n3827/10000. loss: 0.00031043950002640486\n3828/10000. loss: 0.00043814706926544506\n3829/10000. loss: 0.0004748001229017973\n3830/10000. loss: 0.0003344772073129813\n3831/10000. loss: 0.00042131892405450344\n3832/10000. loss: 0.0003347796155139804\n3833/10000. loss: 0.0004947840934619308\n3834/10000. loss: 0.000399164001767834\n3835/10000. loss: 0.00029096752405166626\n3836/10000. loss: 0.0004336549124370019\n3837/10000. loss: 0.0003607522618646423\n3838/10000. loss: 0.00044852402061223984\n3839/10000. loss: 0.00025812685877705616\n3840/10000. loss: 0.0005335287423804402\n3841/10000. loss: 0.0004525598681842287\n3842/10000. loss: 0.0004579797387123108\n3843/10000. loss: 0.0004996375646442175\n3844/10000. loss: 0.0005920635691533486\n3845/10000. loss: 0.0004889988728488485\n3846/10000. loss: 0.0005413539474830031\n3847/10000. loss: 0.0014856602065265179\n3848/10000. loss: 0.0005310042373215159\n3849/10000. loss: 0.001414484033981959\n3850/10000. loss: 0.0019276700913906097\n3851/10000. loss: 0.0014520871142546337\n3852/10000. loss: 0.0013283311078945796\n3853/10000. loss: 0.0015734750777482986\n3854/10000. loss: 0.0018330393359065056\n3855/10000. loss: 0.0012474344111979008\n3856/10000. loss: 0.002073512257387241\n3857/10000. loss: 0.0021621044725179672\n3858/10000. loss: 0.0019813448501129947\n3859/10000. loss: 0.0018746598313252132\n3860/10000. loss: 0.0019856269160906472\n3861/10000. loss: 0.001379974652081728\n3862/10000. loss: 0.0019225506111979485\n3863/10000. loss: 0.001528997750331958\n3864/10000. loss: 0.0009088651277124882\n3865/10000. loss: 0.0010970402508974075\n3866/10000. loss: 0.0017175738078852494\n3867/10000. loss: 0.0008727253880351782\n3868/10000. loss: 0.0007737802031139532\n3869/10000. loss: 0.00045772800998141366\n3870/10000. loss: 0.0004317760467529297\n3871/10000. loss: 0.000470790973243614\n3872/10000. loss: 0.0004903411803146204\n3873/10000. loss: 0.0005171208952864011\n3874/10000. loss: 0.0006206689092020193\n3875/10000. loss: 0.0005128102687497934\n3876/10000. loss: 0.000506856944411993\n3877/10000. loss: 0.0004105563275516033\n3878/10000. loss: 0.0005051275948062539\n3879/10000. loss: 0.0005537700684120258\n3880/10000. loss: 0.000621649669483304\n3881/10000. loss: 0.0004922625763962666\n3882/10000. loss: 0.0006668847830345234\n3883/10000. loss: 0.0005773938416192929\n3884/10000. loss: 0.0005634749929110209\n3885/10000. loss: 0.0005167134416600069\n3886/10000. loss: 0.0005382565626253685\n3887/10000. loss: 0.0006901877156148354\n3888/10000. loss: 0.0003702046039203803\n3889/10000. loss: 0.000331172797208031\n3890/10000. loss: 0.000437974464148283\n3891/10000. loss: 0.0004362412728369236\n3892/10000. loss: 0.0005744116303200523\n3893/10000. loss: 0.0004035158781334758\n3894/10000. loss: 0.00032360289090623456\n3895/10000. loss: 0.0002938673909132679\n3896/10000. loss: 0.0002980582454862694\n3897/10000. loss: 0.00035421647286663455\n3898/10000. loss: 0.00038630197135110694\n3899/10000. loss: 0.0002904026381050547\n3900/10000. loss: 0.00030246219830587506\n3901/10000. loss: 0.0005198140473415455\n3902/10000. loss: 0.0004376999956245224\n3903/10000. loss: 0.00037582296257217723\n3904/10000. loss: 0.0003292692902808388\n3905/10000. loss: 0.0003294031290958325\n3906/10000. loss: 0.00045300348817060393\n3907/10000. loss: 0.00037457840517163277\n3908/10000. loss: 0.000295005738735199\n3909/10000. loss: 0.00035111869995792705\n3910/10000. loss: 0.0003631753691782554\n3911/10000. loss: 0.0003917706587041418\n3912/10000. loss: 0.00042109296191483736\n3913/10000. loss: 0.0005375407248114547\n3914/10000. loss: 0.000429417472332716\n3915/10000. loss: 0.00041345879435539246\n3916/10000. loss: 0.00044852351614584524\n3917/10000. loss: 0.0003712392256905635\n3918/10000. loss: 0.0002789529971778393\n3919/10000. loss: 0.00046018076439698535\n3920/10000. loss: 0.0002890702065390845\n3921/10000. loss: 0.00030380239089330036\n3922/10000. loss: 0.0009888785425573587\n3923/10000. loss: 0.0004383970517665148\n3924/10000. loss: 0.00035035858551661175\n3925/10000. loss: 0.0003272262401878834\n3926/10000. loss: 0.0003045525712271531\n3927/10000. loss: 0.00034085148945450783\n3928/10000. loss: 0.00045625326068451005\n3929/10000. loss: 0.0003067399569166203\n3930/10000. loss: 0.0002693198621273041\n3931/10000. loss: 0.00043593847658485174\n3932/10000. loss: 0.0003506878080467383\n3933/10000. loss: 0.0003416225081309676\n3934/10000. loss: 0.0005199279015262922\n3935/10000. loss: 0.00027764291735365987\n3936/10000. loss: 0.00036733058126022417\n3937/10000. loss: 0.0003191763535141945\n3938/10000. loss: 0.0004577499736721317\n3939/10000. loss: 0.000459773155550162\n3940/10000. loss: 0.0003476600783566634\n3941/10000. loss: 0.0003466487784559528\n3942/10000. loss: 0.0004571051492045323\n3943/10000. loss: 0.0003197278322962423\n3944/10000. loss: 0.000431641082589825\n3945/10000. loss: 0.0003610218021397789\n3946/10000. loss: 0.00037596859813978273\n3947/10000. loss: 0.0004804017177472512\n3948/10000. loss: 0.00038699495295683545\n3949/10000. loss: 0.00028858079652612406\n3950/10000. loss: 0.0003100595010134081\n3951/10000. loss: 0.0004029685320953528\n3952/10000. loss: 0.0004318456631153822\n3953/10000. loss: 0.00045776343904435635\n3954/10000. loss: 0.0006169631766776243\n3955/10000. loss: 0.0005450276657938957\n3956/10000. loss: 0.0004616673104465008\n3957/10000. loss: 0.00031663574433575076\n3958/10000. loss: 0.00043825642205774784\n3959/10000. loss: 0.0006138380461682876\n3960/10000. loss: 0.00040950820160408813\n3961/10000. loss: 0.00045997191530962783\n3962/10000. loss: 0.0005605998449027538\n3963/10000. loss: 0.0007638156724472841\n3964/10000. loss: 0.0005683590425178409\n3965/10000. loss: 0.0009048706851899624\n3966/10000. loss: 0.0012487849841515224\n3967/10000. loss: 0.0008074945459763209\n3968/10000. loss: 0.000984412928422292\n3969/10000. loss: 0.0017216258371869724\n3970/10000. loss: 0.0020409459248185158\n3971/10000. loss: 0.0024623608527084193\n3972/10000. loss: 0.00250883586704731\n3973/10000. loss: 0.0023050600041945777\n3974/10000. loss: 0.002132576579848925\n3975/10000. loss: 0.00196661613881588\n3976/10000. loss: 0.0020906906574964523\n3977/10000. loss: 0.0019425659750898678\n3978/10000. loss: 0.002284691669046879\n3979/10000. loss: 0.0012319519494970639\n3980/10000. loss: 0.0009392128946880499\n3981/10000. loss: 0.001991447682181994\n3982/10000. loss: 0.0012851835538943608\n3983/10000. loss: 0.0016044772540529568\n3984/10000. loss: 0.0012865327298641205\n3985/10000. loss: 0.0008563500984261433\n3986/10000. loss: 0.0010162409550199907\n3987/10000. loss: 0.001095312802741925\n3988/10000. loss: 0.0009990727218488853\n3989/10000. loss: 0.0008816181216388941\n3990/10000. loss: 0.0008498195093125105\n3991/10000. loss: 0.0006051899399608374\n3992/10000. loss: 0.0004142005927860737\n3993/10000. loss: 0.0008293624656895796\n3994/10000. loss: 0.0008178579155355692\n3995/10000. loss: 0.0007475367747247219\n3996/10000. loss: 0.0005733619521682461\n3997/10000. loss: 0.0007679646369069815\n3998/10000. loss: 0.0007833894342184067\n3999/10000. loss: 0.000571280368603766\n4000/10000. loss: 0.0006366142382224401\n4001/10000. loss: 0.0006279986506948868\n4002/10000. loss: 0.0005114733551939329\n4003/10000. loss: 0.00047780720827480155\n4004/10000. loss: 0.0003995477066685756\n4005/10000. loss: 0.0004079574719071388\n4006/10000. loss: 0.0005354102468118072\n4007/10000. loss: 0.0003521739272400737\n4008/10000. loss: 0.0004114379019786914\n4009/10000. loss: 0.00040391723935802776\n4010/10000. loss: 0.00034055672585964203\n4011/10000. loss: 0.0005005750572308898\n4012/10000. loss: 0.0002827735152095556\n4013/10000. loss: 0.0003636677283793688\n4014/10000. loss: 0.0003268887521699071\n4015/10000. loss: 0.000535513274371624\n4016/10000. loss: 0.0004935547088583311\n4017/10000. loss: 0.0005208572838455439\n4018/10000. loss: 0.0005075817074005803\n4019/10000. loss: 0.00045315657431880635\n4020/10000. loss: 0.0004812179443736871\n4021/10000. loss: 0.0004122363170608878\n4022/10000. loss: 0.00034469785168766975\n4023/10000. loss: 0.0005391143107165893\n4024/10000. loss: 0.00033982043775419396\n4025/10000. loss: 0.00029677472775802016\n4026/10000. loss: 0.000459324490899841\n4027/10000. loss: 0.0004957551524663965\n4028/10000. loss: 0.00038041314110159874\n4029/10000. loss: 0.0002947759348899126\n4030/10000. loss: 0.0002989789160589377\n4031/10000. loss: 0.0005637979678188761\n4032/10000. loss: 0.00037240853998810053\n4033/10000. loss: 0.00043407427923132974\n4034/10000. loss: 0.00042951332094768685\n4035/10000. loss: 0.00045410737705727416\n4036/10000. loss: 0.00036489909204343957\n4037/10000. loss: 0.00038952256242434186\n4038/10000. loss: 0.00045034960688402254\n4039/10000. loss: 0.0004630193579941988\n4040/10000. loss: 0.00030366422530884546\n4041/10000. loss: 0.00046063947957009077\n4042/10000. loss: 0.00029340857872739434\n4043/10000. loss: 0.00048277627987166244\n4044/10000. loss: 0.0004807313283284505\n4045/10000. loss: 0.00045974296517670155\n4046/10000. loss: 0.0002971159992739558\n4047/10000. loss: 0.0003913279700403412\n4048/10000. loss: 0.0004200648982077837\n4049/10000. loss: 0.00043357737983266514\n4050/10000. loss: 0.00035924076413114864\n4051/10000. loss: 0.0003748995950445533\n4052/10000. loss: 0.0004825121723115444\n4053/10000. loss: 0.00033001252450048923\n4054/10000. loss: 0.0003480231777454416\n4055/10000. loss: 0.0003389860891426603\n4056/10000. loss: 0.00026279938174411654\n4057/10000. loss: 0.0005161525526394447\n4058/10000. loss: 0.0003057217691093683\n4059/10000. loss: 0.00035234664877255756\n4060/10000. loss: 0.00035800303642948467\n4061/10000. loss: 0.000409606300915281\n4062/10000. loss: 0.0004298643519481023\n4063/10000. loss: 0.00026571940785894793\n4064/10000. loss: 0.00037808444661398727\n4065/10000. loss: 0.0002813499498491486\n4066/10000. loss: 0.0004785988324632247\n4067/10000. loss: 0.00028273680557807285\n4068/10000. loss: 0.0003776857629418373\n4069/10000. loss: 0.00044386919277409714\n4070/10000. loss: 0.000365740736015141\n4071/10000. loss: 0.0005151292231554786\n4072/10000. loss: 0.0003240725491195917\n4073/10000. loss: 0.0004495329534014066\n4074/10000. loss: 0.0002752905711531639\n4075/10000. loss: 0.00047057824364552897\n4076/10000. loss: 0.000490664504468441\n4077/10000. loss: 0.00040300912223756313\n4078/10000. loss: 0.00039025159397472936\n4079/10000. loss: 0.00045539656033118564\n4080/10000. loss: 0.00033220998011529446\n4081/10000. loss: 0.0005509655845041076\n4082/10000. loss: 0.00034322581874827546\n4083/10000. loss: 0.0004895709765454134\n4084/10000. loss: 0.0005152498294288913\n4085/10000. loss: 0.0006472116413836678\n4086/10000. loss: 0.0006107761679838101\n4087/10000. loss: 0.000706306037803491\n4088/10000. loss: 0.000658125306169192\n4089/10000. loss: 0.0007458574449022611\n4090/10000. loss: 0.0008306738454848528\n4091/10000. loss: 0.0009612347930669785\n4092/10000. loss: 0.0010214541107416153\n4093/10000. loss: 0.0008768129628151655\n4094/10000. loss: 0.001291401218622923\n4095/10000. loss: 0.0007584378278503815\n4096/10000. loss: 0.0018896072482069333\n4097/10000. loss: 0.001102898425112168\n4098/10000. loss: 0.0012766245442132156\n4099/10000. loss: 0.0021728910505771637\n4100/10000. loss: 0.003291879780590534\n4101/10000. loss: 0.0009018414032955965\n4102/10000. loss: 0.0013375038591523964\n4103/10000. loss: 0.0012144528639813263\n4104/10000. loss: 0.0008290585440893968\n4105/10000. loss: 0.0006815027445554733\n4106/10000. loss: 0.0005543624671796957\n4107/10000. loss: 0.0010395366698503494\n4108/10000. loss: 0.0007048491388559341\n4109/10000. loss: 0.0007207375019788742\n4110/10000. loss: 0.000556171095619599\n4111/10000. loss: 0.0008285647879044215\n4112/10000. loss: 0.0006619219978650411\n4113/10000. loss: 0.0006490443677951893\n4114/10000. loss: 0.0004810433990011613\n4115/10000. loss: 0.0004595603483418624\n4116/10000. loss: 0.0005713808350265026\n4117/10000. loss: 0.000579703482799232\n4118/10000. loss: 0.0005316774671276411\n4119/10000. loss: 0.0005797749618068337\n4120/10000. loss: 0.0005097031050051252\n4121/10000. loss: 0.000565798138268292\n4122/10000. loss: 0.00047886363851527375\n4123/10000. loss: 0.0005072461208328605\n4124/10000. loss: 0.00044141259665290516\n4125/10000. loss: 0.0004028615852197011\n4126/10000. loss: 0.0004313486473013957\n4127/10000. loss: 0.0003230537986382842\n4128/10000. loss: 0.00041716021951287985\n4129/10000. loss: 0.00040620320942252874\n4130/10000. loss: 0.0005439004938428601\n4131/10000. loss: 0.0004564432504897316\n4132/10000. loss: 0.0005365286876137058\n4133/10000. loss: 0.00042762502562254667\n4134/10000. loss: 0.000385417602956295\n4135/10000. loss: 0.0005118335441996654\n4136/10000. loss: 0.000603529391810298\n4137/10000. loss: 0.00039220772062738735\n4138/10000. loss: 0.00045170898859699565\n4139/10000. loss: 0.00038634395847717923\n4140/10000. loss: 0.00038320183133085567\n4141/10000. loss: 0.0004067909127722184\n4142/10000. loss: 0.0003012413702284296\n4143/10000. loss: 0.00028166105039417744\n4144/10000. loss: 0.0002676779016231497\n4145/10000. loss: 0.0002704792811224858\n4146/10000. loss: 0.0005033844305823246\n4147/10000. loss: 0.0004483700031414628\n4148/10000. loss: 0.0004491851820300023\n4149/10000. loss: 0.000429219954336683\n4150/10000. loss: 0.00028568665341784555\n4151/10000. loss: 0.0005119996300588051\n4152/10000. loss: 0.000340308955249687\n4153/10000. loss: 0.00025971113548924524\n4154/10000. loss: 0.0005042017437517643\n4155/10000. loss: 0.00033380265813320875\n4156/10000. loss: 0.00041953427717089653\n4157/10000. loss: 0.0003148573062693079\n4158/10000. loss: 0.00033631637537231046\n4159/10000. loss: 0.0004744878193984429\n4160/10000. loss: 0.0002604302523347239\n4161/10000. loss: 0.0002202376490458846\n4162/10000. loss: 0.0003799501961718003\n4163/10000. loss: 0.00024704926181584597\n4164/10000. loss: 0.0003099648553567628\n4165/10000. loss: 0.00032440510888894397\n4166/10000. loss: 0.00025252351770177484\n4167/10000. loss: 0.0003525728825479746\n4168/10000. loss: 0.00032500375527888536\n4169/10000. loss: 0.0002746672835201025\n4170/10000. loss: 0.0003878128870079915\n4171/10000. loss: 0.00044197123497724533\n4172/10000. loss: 0.000403617974370718\n4173/10000. loss: 0.0004250347459067901\n4174/10000. loss: 0.0003373953513801098\n4175/10000. loss: 0.0005118925279627243\n4176/10000. loss: 0.0003641864750534296\n4177/10000. loss: 0.00032458788094421226\n4178/10000. loss: 0.00030338003610571224\n4179/10000. loss: 0.0003203360247425735\n4180/10000. loss: 0.0005771104091157516\n4181/10000. loss: 0.0002863421104848385\n4182/10000. loss: 0.0004570059633503358\n4183/10000. loss: 0.0005862294929102063\n4184/10000. loss: 0.0004955777355159322\n4185/10000. loss: 0.0007764040492475033\n4186/10000. loss: 0.0010146024481703837\n4187/10000. loss: 0.0008212129275004069\n4188/10000. loss: 0.0006560709637900194\n4189/10000. loss: 0.001111609861254692\n4190/10000. loss: 0.0010408434706429641\n4191/10000. loss: 0.001354852846513192\n4192/10000. loss: 0.0016114201086262863\n4193/10000. loss: 0.0017089969478547573\n4194/10000. loss: 0.001108319265767932\n4195/10000. loss: 0.0013706531996528308\n4196/10000. loss: 0.001270999200642109\n4197/10000. loss: 0.001102705408508579\n4198/10000. loss: 0.0007450821188588937\n4199/10000. loss: 0.0010623175961275895\n4200/10000. loss: 0.0011328435502946377\n4201/10000. loss: 0.0014596986584365368\n4202/10000. loss: 0.00048017269000411034\n4203/10000. loss: 0.0006350486073642969\n4204/10000. loss: 0.0005488336707154909\n4205/10000. loss: 0.0008324162724117438\n4206/10000. loss: 0.0006074000072355071\n4207/10000. loss: 0.00041162633957962197\n4208/10000. loss: 0.0004023573516557614\n4209/10000. loss: 0.0004979485335449377\n4210/10000. loss: 0.0005775958610077699\n4211/10000. loss: 0.0009178648858020703\n4212/10000. loss: 0.00040606708110620576\n4213/10000. loss: 0.0005046415608376265\n4214/10000. loss: 0.0005915688040355841\n4215/10000. loss: 0.0005738061154261231\n4216/10000. loss: 0.0003925286776696642\n4217/10000. loss: 0.0004916319157928228\n4218/10000. loss: 0.0004258553187052409\n4219/10000. loss: 0.0004242016390586893\n4220/10000. loss: 0.00029663640695313614\n4221/10000. loss: 0.0004473707328240077\n4222/10000. loss: 0.0003805192730699976\n4223/10000. loss: 0.00030427412517989677\n4224/10000. loss: 0.0005246287134165565\n4225/10000. loss: 0.00038961767374227446\n4226/10000. loss: 0.0007129260338842869\n4227/10000. loss: 0.00047437241300940514\n4228/10000. loss: 0.0005232268789162239\n4229/10000. loss: 0.0003558796209593614\n4230/10000. loss: 0.00043968499327699345\n4231/10000. loss: 0.0004718715014557044\n4232/10000. loss: 0.0004969004075974226\n4233/10000. loss: 0.0002902340687190493\n4234/10000. loss: 0.00026300254588325817\n4235/10000. loss: 0.00036379299126565456\n4236/10000. loss: 0.00028105433254192275\n4237/10000. loss: 0.0005005747855951389\n4238/10000. loss: 0.0004695858806371689\n4239/10000. loss: 0.0003292728215456009\n4240/10000. loss: 0.00034742076726009447\n4241/10000. loss: 0.0005311554608245691\n4242/10000. loss: 0.00023764530972888073\n4243/10000. loss: 0.0004204104964931806\n4244/10000. loss: 0.000338367884978652\n4245/10000. loss: 0.0003226038340168695\n4246/10000. loss: 0.0004013835374886791\n4247/10000. loss: 0.00029317660179610056\n4248/10000. loss: 0.00034400859537223977\n4249/10000. loss: 0.0003797749135022362\n4250/10000. loss: 0.00031923184481759864\n4251/10000. loss: 0.0004901681871463855\n4252/10000. loss: 0.00034774358694752056\n4253/10000. loss: 0.0005197484667102495\n4254/10000. loss: 0.00033117306884378195\n4255/10000. loss: 0.00035803578794002533\n4256/10000. loss: 0.0003256401202330987\n4257/10000. loss: 0.0004570404998958111\n4258/10000. loss: 0.0004907050946106514\n4259/10000. loss: 0.00028835262249534327\n4260/10000. loss: 0.00028111952512214583\n4261/10000. loss: 0.00026639719726517797\n4262/10000. loss: 0.00040192909849186737\n4263/10000. loss: 0.0002830104009869198\n4264/10000. loss: 0.00035725142030666274\n4265/10000. loss: 0.00030588139391814667\n4266/10000. loss: 0.0003005813729638855\n4267/10000. loss: 0.0004843637580052018\n4268/10000. loss: 0.00026187108596786857\n4269/10000. loss: 0.00034260125054667395\n4270/10000. loss: 0.00026418416139980155\n4271/10000. loss: 0.00042477925308048725\n4272/10000. loss: 0.00027452494638661545\n4273/10000. loss: 0.0003940376142660777\n4274/10000. loss: 0.0003976391162723303\n4275/10000. loss: 0.0004938977460066477\n4276/10000. loss: 0.0005500577390193939\n4277/10000. loss: 0.0005259315560882291\n4278/10000. loss: 0.0005695946359386047\n4279/10000. loss: 0.0007915738193939129\n4280/10000. loss: 0.0004703007483234008\n4281/10000. loss: 0.0008352008492996296\n4282/10000. loss: 0.0020863182532290616\n4283/10000. loss: 0.0016130444904168446\n4284/10000. loss: 0.0014010622786978881\n4285/10000. loss: 0.0011906976190706093\n4286/10000. loss: 0.002427582008143266\n4287/10000. loss: 0.002435084277143081\n4288/10000. loss: 0.0017676472974320252\n4289/10000. loss: 0.0017295473565657933\n4290/10000. loss: 0.0025475891306996346\n4291/10000. loss: 0.002148375070343415\n4292/10000. loss: 0.0018685953691601753\n4293/10000. loss: 0.0012880428694188595\n4294/10000. loss: 0.0011776751683404048\n4295/10000. loss: 0.0017948004727562268\n4296/10000. loss: 0.0012293930631130934\n4297/10000. loss: 0.0009176225091020266\n4298/10000. loss: 0.000807661097496748\n4299/10000. loss: 0.0007544211888064941\n4300/10000. loss: 0.0003861059279491504\n4301/10000. loss: 0.0008337704154352347\n4302/10000. loss: 0.0006106945996483167\n4303/10000. loss: 0.0006065765240540107\n4304/10000. loss: 0.0005899882332111398\n4305/10000. loss: 0.0006584740864733855\n4306/10000. loss: 0.0005038660019636154\n4307/10000. loss: 0.0005109080423911413\n4308/10000. loss: 0.00037764959658185643\n4309/10000. loss: 0.00041913101449608803\n4310/10000. loss: 0.00045788664525995654\n4311/10000. loss: 0.0003820544807240367\n4312/10000. loss: 0.00043681333772838116\n4313/10000. loss: 0.00043518437693516415\n4314/10000. loss: 0.0005564372598504027\n4315/10000. loss: 0.00043966295197606087\n4316/10000. loss: 0.0004303195746615529\n4317/10000. loss: 0.0004501756823932131\n4318/10000. loss: 0.0005201965104788542\n4319/10000. loss: 0.0004709452235450347\n4320/10000. loss: 0.000548328428218762\n4321/10000. loss: 0.00034848309587687254\n4322/10000. loss: 0.0003672998088101546\n4323/10000. loss: 0.0003536465422560771\n4324/10000. loss: 0.0004561230695496003\n4325/10000. loss: 0.0005767281788090864\n4326/10000. loss: 0.0003719209150100748\n4327/10000. loss: 0.0004014494673659404\n4328/10000. loss: 0.00039246841333806515\n4329/10000. loss: 0.0004577438812702894\n4330/10000. loss: 0.0003584959389020999\n4331/10000. loss: 0.0003136306380232175\n4332/10000. loss: 0.00045307865366339684\n4333/10000. loss: 0.000327436796699961\n4334/10000. loss: 0.00032607380611201126\n4335/10000. loss: 0.0004286711337044835\n4336/10000. loss: 0.0003394947076837222\n4337/10000. loss: 0.0002852109222051998\n4338/10000. loss: 0.0002510913764126599\n4339/10000. loss: 0.0004124909173697233\n4340/10000. loss: 0.0004959778937821587\n4341/10000. loss: 0.0004017944059645136\n4342/10000. loss: 0.0004680210258811712\n4343/10000. loss: 0.00032565432290236157\n4344/10000. loss: 0.00032878465329607326\n4345/10000. loss: 0.0004010536940768361\n4346/10000. loss: 0.00045927229803055525\n4347/10000. loss: 0.00040736918648084003\n4348/10000. loss: 0.0002707579600003858\n4349/10000. loss: 0.0004048994742333889\n4350/10000. loss: 0.00041973233843843144\n4351/10000. loss: 0.00025195562435934943\n4352/10000. loss: 0.00030875904485583305\n4353/10000. loss: 0.0004080943763256073\n4354/10000. loss: 0.00046491432779779035\n4355/10000. loss: 0.00028720366147657234\n4356/10000. loss: 0.0004124159070973595\n4357/10000. loss: 0.00037078986254831153\n4358/10000. loss: 0.00039476214442402124\n4359/10000. loss: 0.00038574961945414543\n4360/10000. loss: 0.0005339424436291059\n4361/10000. loss: 0.00043890920157233876\n4362/10000. loss: 0.00033156387507915497\n4363/10000. loss: 0.0002593829412944615\n4364/10000. loss: 0.0003248957800678909\n4365/10000. loss: 0.0002866243206275006\n4366/10000. loss: 0.00039253934907416504\n4367/10000. loss: 0.0003502418597539266\n4368/10000. loss: 0.00029081238123277825\n4369/10000. loss: 0.00038874149322509766\n4370/10000. loss: 0.0002939357267071803\n4371/10000. loss: 0.0003142297306718926\n4372/10000. loss: 0.00032121455296874046\n4373/10000. loss: 0.0003385917516425252\n4374/10000. loss: 0.0002744309992219011\n4375/10000. loss: 0.0002530723965416352\n4376/10000. loss: 0.0004708346289892991\n4377/10000. loss: 0.00027851849639167386\n4378/10000. loss: 0.00044618026974300545\n4379/10000. loss: 0.0004322342186545332\n4380/10000. loss: 0.0004366671588892738\n4381/10000. loss: 0.0002756453662489851\n4382/10000. loss: 0.0003975794728224476\n4383/10000. loss: 0.0004041568997005622\n4384/10000. loss: 0.00033216058121373254\n4385/10000. loss: 0.00025978940539062023\n4386/10000. loss: 0.0003825386520475149\n4387/10000. loss: 0.00019188943163802227\n4388/10000. loss: 0.00031324118996659916\n4389/10000. loss: 0.0003047465191533168\n4390/10000. loss: 0.0004455241917942961\n4391/10000. loss: 0.00029582066539054114\n4392/10000. loss: 0.00033686434229214984\n4393/10000. loss: 0.0003700058829660217\n4394/10000. loss: 0.000507585626716415\n4395/10000. loss: 0.0006111681771775087\n4396/10000. loss: 0.00030680142420654494\n4397/10000. loss: 0.0006763979326933622\n4398/10000. loss: 0.0005736548531179627\n4399/10000. loss: 0.0009638645375768343\n4400/10000. loss: 0.0013975131635864575\n4401/10000. loss: 0.0020590401254594326\n4402/10000. loss: 0.0012857597321271896\n4403/10000. loss: 0.002940350522597631\n4404/10000. loss: 0.0026793675497174263\n4405/10000. loss: 0.0024720363629360995\n4406/10000. loss: 0.004278292879462242\n4407/10000. loss: 0.002897240842382113\n4408/10000. loss: 0.00561505804459254\n4409/10000. loss: 0.00285097553084294\n4410/10000. loss: 0.002695972720781962\n4411/10000. loss: 0.002391902885089318\n4412/10000. loss: 0.001997309271246195\n4413/10000. loss: 0.0021466556936502457\n4414/10000. loss: 0.0015573686299224694\n4415/10000. loss: 0.001888105645775795\n4416/10000. loss: 0.00140823470428586\n4417/10000. loss: 0.0015516048297286034\n4418/10000. loss: 0.001207785913720727\n4419/10000. loss: 0.001214179831246535\n4420/10000. loss: 0.0009843500641485055\n4421/10000. loss: 0.0005971038481220603\n4422/10000. loss: 0.0007171325851231813\n4423/10000. loss: 0.0005428136792033911\n4424/10000. loss: 0.0006522089242935181\n4425/10000. loss: 0.0007353364489972591\n4426/10000. loss: 0.0009954643125335376\n4427/10000. loss: 0.0007003193410734335\n4428/10000. loss: 0.0008283613715320826\n4429/10000. loss: 0.0007540440807739893\n4430/10000. loss: 0.0006321780383586884\n4431/10000. loss: 0.0006370228560020527\n4432/10000. loss: 0.000488962202022473\n4433/10000. loss: 0.0006042375850180784\n4434/10000. loss: 0.00045838626101613045\n4435/10000. loss: 0.000456932505282263\n4436/10000. loss: 0.0004885493932912747\n4437/10000. loss: 0.00041189002028356\n4438/10000. loss: 0.0004936013525972763\n4439/10000. loss: 0.0007413740580280622\n4440/10000. loss: 0.00033458974212408066\n4441/10000. loss: 0.0005217901586244503\n4442/10000. loss: 0.0003820353886112571\n4443/10000. loss: 0.000265319113774846\n4444/10000. loss: 0.00029743420115361613\n4445/10000. loss: 0.0005743417423218489\n4446/10000. loss: 0.000516061515857776\n4447/10000. loss: 0.0003252161550335586\n4448/10000. loss: 0.00047927928001930314\n4449/10000. loss: 0.00033099751453846693\n4450/10000. loss: 0.0005269310204312205\n4451/10000. loss: 0.00035777556089063484\n4452/10000. loss: 0.0004624206339940429\n4453/10000. loss: 0.00033707839126388234\n4454/10000. loss: 0.00034425178697953623\n4455/10000. loss: 0.00033193132063994807\n4456/10000. loss: 0.0002895625851427515\n4457/10000. loss: 0.00035770652660479146\n4458/10000. loss: 0.00046432654683788616\n4459/10000. loss: 0.00037068889165918034\n4460/10000. loss: 0.00037251490478714305\n4461/10000. loss: 0.0003469182411208749\n4462/10000. loss: 0.000568962306715548\n4463/10000. loss: 0.0004162186135848363\n4464/10000. loss: 0.00030208483804017305\n4465/10000. loss: 0.0004426312322417895\n4466/10000. loss: 0.00042360058675209683\n4467/10000. loss: 0.0002890940910826127\n4468/10000. loss: 0.0003441295896967252\n4469/10000. loss: 0.00035445337804655236\n4470/10000. loss: 0.00033310509752482176\n4471/10000. loss: 0.0002529071255897482\n4472/10000. loss: 0.0004155741383632024\n4473/10000. loss: 0.000344921446715792\n4474/10000. loss: 0.00040771331017216045\n4475/10000. loss: 0.0005384492687880993\n4476/10000. loss: 0.0004099914027998845\n4477/10000. loss: 0.0003400923063357671\n4478/10000. loss: 0.0004645644997557004\n4479/10000. loss: 0.0003159827707956235\n4480/10000. loss: 0.0004604979573438565\n4481/10000. loss: 0.00048133102245628834\n4482/10000. loss: 0.0002614982076920569\n4483/10000. loss: 0.0005161444035669168\n4484/10000. loss: 0.00025015204058339197\n4485/10000. loss: 0.0005012591524670521\n4486/10000. loss: 0.00038724986370652914\n4487/10000. loss: 0.0003164667868986726\n4488/10000. loss: 0.0006157769821584225\n4489/10000. loss: 0.00034138928943624097\n4490/10000. loss: 0.0005166528280824423\n4491/10000. loss: 0.0003608835395425558\n4492/10000. loss: 0.00027631196038176614\n4493/10000. loss: 0.0003843854259078701\n4494/10000. loss: 0.00039573883016904193\n4495/10000. loss: 0.00047743467924495536\n4496/10000. loss: 0.00027410998397196334\n4497/10000. loss: 0.0003412287915125489\n4498/10000. loss: 0.00037039835782100755\n4499/10000. loss: 0.0002719239564612508\n4500/10000. loss: 0.00025172570409874123\n4501/10000. loss: 0.00023381390686457357\n4502/10000. loss: 0.00025505851954221725\n4503/10000. loss: 0.00048365688417106867\n4504/10000. loss: 0.00028816833704089123\n4505/10000. loss: 0.00039445531244079274\n4506/10000. loss: 0.00027027446776628494\n4507/10000. loss: 0.00032324332278221846\n4508/10000. loss: 0.0002598344775227209\n4509/10000. loss: 0.00027691064557681483\n4510/10000. loss: 0.0002963928272947669\n4511/10000. loss: 0.00032221642322838306\n4512/10000. loss: 0.0002552705506483714\n4513/10000. loss: 0.00029565409446756047\n4514/10000. loss: 0.00025551482879867155\n4515/10000. loss: 0.0004143485178550084\n4516/10000. loss: 0.0004540301936989029\n4517/10000. loss: 0.0003065952332690358\n4518/10000. loss: 0.0002662703045643866\n4519/10000. loss: 0.00024090800434350967\n4520/10000. loss: 0.0003454852073142926\n4521/10000. loss: 0.00037161501434942085\n4522/10000. loss: 0.0004120029043406248\n4523/10000. loss: 0.0003938676090911031\n4524/10000. loss: 0.0003890144483496745\n4525/10000. loss: 0.0003591240771735708\n4526/10000. loss: 0.00032680054816106957\n4527/10000. loss: 0.0002500881867793699\n4528/10000. loss: 0.00027421757113188505\n4529/10000. loss: 0.0005031062755733728\n4530/10000. loss: 0.00040239332399020594\n4531/10000. loss: 0.0003013356084314485\n4532/10000. loss: 0.00024175748694688082\n4533/10000. loss: 0.00030411997189124423\n4534/10000. loss: 0.00042273441795259714\n4535/10000. loss: 0.00046153638201455277\n4536/10000. loss: 0.00041321106255054474\n4537/10000. loss: 0.0002575322675208251\n4538/10000. loss: 0.0002936361512790124\n4539/10000. loss: 0.0003129120450466871\n4540/10000. loss: 0.0002271527579675118\n4541/10000. loss: 0.0002794372267089784\n4542/10000. loss: 0.0003523706303288539\n4543/10000. loss: 0.000525445289288958\n4544/10000. loss: 0.00043060847868522006\n4545/10000. loss: 0.0003271264722570777\n4546/10000. loss: 0.0003978208793948094\n4547/10000. loss: 0.00043088783665249747\n4548/10000. loss: 0.000608458649367094\n4549/10000. loss: 0.00045877912392218906\n4550/10000. loss: 0.00030784370998541516\n4551/10000. loss: 0.0005163540287564198\n4552/10000. loss: 0.0006672504047552744\n4553/10000. loss: 0.0005707543653746446\n4554/10000. loss: 0.0004658023438726862\n4555/10000. loss: 0.0008960272340724865\n4556/10000. loss: 0.0010072668083012104\n4557/10000. loss: 0.001092735289906462\n4558/10000. loss: 0.0011004675179719925\n4559/10000. loss: 0.0009334557689726353\n4560/10000. loss: 0.0012942079920321703\n4561/10000. loss: 0.0011059828102588654\n4562/10000. loss: 0.0016497462056577206\n4563/10000. loss: 0.0012171033304184675\n4564/10000. loss: 0.0008845408447086811\n4565/10000. loss: 0.0011995097156614065\n4566/10000. loss: 0.0011617598744730155\n4567/10000. loss: 0.0007802981417626143\n4568/10000. loss: 0.0009532716746131579\n4569/10000. loss: 0.0008977100563546022\n4570/10000. loss: 0.0009318330946067969\n4571/10000. loss: 0.0009001699897150198\n4572/10000. loss: 0.0005520180954287449\n4573/10000. loss: 0.0005294910321633021\n4574/10000. loss: 0.0004048748717953761\n4575/10000. loss: 0.0006038220599293709\n4576/10000. loss: 0.0006546006382753452\n4577/10000. loss: 0.0003421635289366047\n4578/10000. loss: 0.000525639004384478\n4579/10000. loss: 0.00033948795559505623\n4580/10000. loss: 0.00035460673583050567\n4581/10000. loss: 0.00036931049544364214\n4582/10000. loss: 0.0004345151052499811\n4583/10000. loss: 0.000370414888796707\n4584/10000. loss: 0.00039450637996196747\n4585/10000. loss: 0.0003243470564484596\n4586/10000. loss: 0.0003602541983127594\n4587/10000. loss: 0.0004546344668293993\n4588/10000. loss: 0.000529094870823125\n4589/10000. loss: 0.0005485585425049067\n4590/10000. loss: 0.00043105279716352624\n4591/10000. loss: 0.0004917818199222287\n4592/10000. loss: 0.0005185238551348448\n4593/10000. loss: 0.00044275967714687187\n4594/10000. loss: 0.0005375004839152098\n4595/10000. loss: 0.0004259547373900811\n4596/10000. loss: 0.00044929337066908676\n4597/10000. loss: 0.0004271285142749548\n4598/10000. loss: 0.0003693287338440617\n4599/10000. loss: 0.00041035248432308435\n4600/10000. loss: 0.00039337521108488244\n4601/10000. loss: 0.00035503716208040714\n4602/10000. loss: 0.00038857727001110714\n4603/10000. loss: 0.0003347208645815651\n4604/10000. loss: 0.00041113949070374173\n4605/10000. loss: 0.00033982024372865755\n4606/10000. loss: 0.0004422339067483942\n4607/10000. loss: 0.0002769787291375299\n4608/10000. loss: 0.0005918468038241068\n4609/10000. loss: 0.0002526244691883524\n4610/10000. loss: 0.00026469764998182654\n4611/10000. loss: 0.00027292576851323247\n4612/10000. loss: 0.0003289705685650309\n4613/10000. loss: 0.000384889116200308\n4614/10000. loss: 0.00039631372783333063\n4615/10000. loss: 0.00033737307724853355\n4616/10000. loss: 0.0003633858480801185\n4617/10000. loss: 0.00026407015199462575\n4618/10000. loss: 0.00026343815261498094\n4619/10000. loss: 0.0002598752810930212\n4620/10000. loss: 0.0003332240351786216\n4621/10000. loss: 0.00042332347948104143\n4622/10000. loss: 0.00029335326204697293\n4623/10000. loss: 0.00039611927544077236\n4624/10000. loss: 0.000482499444236358\n4625/10000. loss: 0.0003604512894526124\n4626/10000. loss: 0.00044913093249003094\n4627/10000. loss: 0.0002557527623139322\n4628/10000. loss: 0.0004634583213677009\n4629/10000. loss: 0.0006023632983366648\n4630/10000. loss: 0.0004052928028007348\n4631/10000. loss: 0.0002877262692588071\n4632/10000. loss: 0.0002735953506392737\n4633/10000. loss: 0.00023522746050730348\n4634/10000. loss: 0.0003731773079683383\n4635/10000. loss: 0.0003766341833397746\n4636/10000. loss: 0.00037879877102871734\n4637/10000. loss: 0.0002884683587277929\n4638/10000. loss: 0.0003051986374581854\n4639/10000. loss: 0.00041986846675475437\n4640/10000. loss: 0.0003660661168396473\n4641/10000. loss: 0.0002685252499456207\n4642/10000. loss: 0.00032111672529329854\n4643/10000. loss: 0.0003929345402866602\n4644/10000. loss: 0.0004141267854720354\n4645/10000. loss: 0.0004394888722648223\n4646/10000. loss: 0.0002919487693967919\n4647/10000. loss: 0.0003312228946015239\n4648/10000. loss: 0.000238895610285302\n4649/10000. loss: 0.0004322874204566081\n4650/10000. loss: 0.0004071272366369764\n4651/10000. loss: 0.00034098509543885786\n4652/10000. loss: 0.0003542771252493064\n4653/10000. loss: 0.00030356701851511997\n4654/10000. loss: 0.00030185918634136516\n4655/10000. loss: 0.00029521359829232097\n4656/10000. loss: 0.0004344662108148138\n4657/10000. loss: 0.0005216974144180616\n4658/10000. loss: 0.00039020021601269644\n4659/10000. loss: 0.0005685638170689344\n4660/10000. loss: 0.00038552094095697004\n4661/10000. loss: 0.0006831448214749495\n4662/10000. loss: 0.0012994000377754371\n4663/10000. loss: 0.0006296364590525627\n4664/10000. loss: 0.0027228382726510367\n4665/10000. loss: 0.002126647780338923\n4666/10000. loss: 0.0032209555308024087\n4667/10000. loss: 0.002727604160706202\n4668/10000. loss: 0.0016836868599057198\n4669/10000. loss: 0.002969016321003437\n4670/10000. loss: 0.0011413992227365573\n4671/10000. loss: 0.0018680981981257598\n4672/10000. loss: 0.002146637998521328\n4673/10000. loss: 0.0017856502284606297\n4674/10000. loss: 0.0019893180578947067\n4675/10000. loss: 0.0013593841964999835\n4676/10000. loss: 0.0011777248388777177\n4677/10000. loss: 0.0013537810494502385\n4678/10000. loss: 0.0016483149180809658\n4679/10000. loss: 0.001049073413014412\n4680/10000. loss: 0.00148064736276865\n4681/10000. loss: 0.0012520930419365566\n4682/10000. loss: 0.000914829084649682\n4683/10000. loss: 0.0007795124935607115\n4684/10000. loss: 0.0007195073024680217\n4685/10000. loss: 0.000791782590871056\n4686/10000. loss: 0.0005287180344263712\n4687/10000. loss: 0.000607451229977111\n4688/10000. loss: 0.0005501771423344811\n4689/10000. loss: 0.0005978616730620464\n4690/10000. loss: 0.0005789901285121838\n4691/10000. loss: 0.0004592235976209243\n4692/10000. loss: 0.0006637523571650187\n4693/10000. loss: 0.0006306817134221395\n4694/10000. loss: 0.00046258656463275355\n4695/10000. loss: 0.00045814551413059235\n4696/10000. loss: 0.0004506956708307068\n4697/10000. loss: 0.00035683248036851484\n4698/10000. loss: 0.00032118931024645764\n4699/10000. loss: 0.000332582703170677\n4700/10000. loss: 0.0004913355223834515\n4701/10000. loss: 0.000325273082125932\n4702/10000. loss: 0.000382625808318456\n4703/10000. loss: 0.0004302241140976548\n4704/10000. loss: 0.00028464900484929484\n4705/10000. loss: 0.0003030887455679476\n4706/10000. loss: 0.0003298299852758646\n4707/10000. loss: 0.0006276893739899\n4708/10000. loss: 0.0003119351846786837\n4709/10000. loss: 0.00047259808828433353\n4710/10000. loss: 0.0003962862150122722\n4711/10000. loss: 0.0002852230876063307\n4712/10000. loss: 0.0004514534957706928\n4713/10000. loss: 0.0003248447513518234\n4714/10000. loss: 0.0003797166670362155\n4715/10000. loss: 0.00041661737486720085\n4716/10000. loss: 0.0003146092058159411\n4717/10000. loss: 0.00042237372448047\n4718/10000. loss: 0.00042209789777795476\n4719/10000. loss: 0.00026960587517047924\n4720/10000. loss: 0.00041283678729087114\n4721/10000. loss: 0.0004123247927054763\n4722/10000. loss: 0.0005347709326694409\n4723/10000. loss: 0.0004777483797321717\n4724/10000. loss: 0.000288551712098221\n4725/10000. loss: 0.00026324589271098375\n4726/10000. loss: 0.0002646217471919954\n4727/10000. loss: 0.00032610058163603145\n4728/10000. loss: 0.00031768082408234477\n4729/10000. loss: 0.00048476011337091524\n4730/10000. loss: 0.0004325847451885541\n4731/10000. loss: 0.0004007273431246479\n4732/10000. loss: 0.00029247205626840395\n4733/10000. loss: 0.0002644471436118086\n4734/10000. loss: 0.00024176656734198332\n4735/10000. loss: 0.0003314835097019871\n4736/10000. loss: 0.0002506104065105319\n4737/10000. loss: 0.0002769312316862245\n4738/10000. loss: 0.00025510735576972365\n4739/10000. loss: 0.000294775003567338\n4740/10000. loss: 0.00029716974434753257\n4741/10000. loss: 0.0003167116083204746\n4742/10000. loss: 0.0003379432794948419\n4743/10000. loss: 0.0003828657791018486\n4744/10000. loss: 0.00028920026185611886\n4745/10000. loss: 0.0004322691820561886\n4746/10000. loss: 0.0003979602673401435\n4747/10000. loss: 0.0002432413942491015\n4748/10000. loss: 0.0002551879539775352\n4749/10000. loss: 0.0002516797200466196\n4750/10000. loss: 0.00030922592850402\n4751/10000. loss: 0.0002892017364501953\n4752/10000. loss: 0.0003530017177884777\n4753/10000. loss: 0.000282040040474385\n4754/10000. loss: 0.00021146676347901425\n4755/10000. loss: 0.00028148233347261947\n4756/10000. loss: 0.00022954212424034873\n4757/10000. loss: 0.00041634938679635525\n4758/10000. loss: 0.00037815053171167773\n4759/10000. loss: 0.00044716956714789074\n4760/10000. loss: 0.0002650613508497675\n4761/10000. loss: 0.00030822209858645994\n4762/10000. loss: 0.00029373927585159737\n4763/10000. loss: 0.00044966857725133497\n4764/10000. loss: 0.0004677871086945136\n4765/10000. loss: 0.0002634033638363083\n4766/10000. loss: 0.0003033074705551068\n4767/10000. loss: 0.00036127733377118904\n4768/10000. loss: 0.00045599051130314666\n4769/10000. loss: 0.0003610887409498294\n4770/10000. loss: 0.000347873584056894\n4771/10000. loss: 0.0002738079832245906\n4772/10000. loss: 0.00027820873462284607\n4773/10000. loss: 0.00025140505749732256\n4774/10000. loss: 0.0003368222775558631\n4775/10000. loss: 0.00027197308372706175\n4776/10000. loss: 0.00035452454661329585\n4777/10000. loss: 0.00025224809845288593\n4778/10000. loss: 0.0003854154298702876\n4779/10000. loss: 0.0002738469047471881\n4780/10000. loss: 0.0002549190927917759\n4781/10000. loss: 0.0002259208122268319\n4782/10000. loss: 0.00024343324669947228\n4783/10000. loss: 0.0002519437499965231\n4784/10000. loss: 0.0004419178779547413\n4785/10000. loss: 0.0002765599638223648\n4786/10000. loss: 0.00027481633393714827\n4787/10000. loss: 0.00036325732556482154\n4788/10000. loss: 0.00022688705939799547\n4789/10000. loss: 0.00022321844395870963\n4790/10000. loss: 0.00029593131815393764\n4791/10000. loss: 0.00028179438474277657\n4792/10000. loss: 0.00024248958410074314\n4793/10000. loss: 0.00034060659042249125\n4794/10000. loss: 0.0006201779857898752\n4795/10000. loss: 0.00034613492122540873\n4796/10000. loss: 0.0003355403120319049\n4797/10000. loss: 0.0004366911016404629\n4798/10000. loss: 0.00029799351856733364\n4799/10000. loss: 0.0002558465348556638\n4800/10000. loss: 0.0002379187111121913\n4801/10000. loss: 0.0003014332226788004\n4802/10000. loss: 0.00048297686347117025\n4803/10000. loss: 0.00036846078000962734\n4804/10000. loss: 0.0003481458794946472\n4805/10000. loss: 0.00045084689433375996\n4806/10000. loss: 0.0003283795279761155\n4807/10000. loss: 0.00030387806085248786\n4808/10000. loss: 0.00027258698052416247\n4809/10000. loss: 0.0003788030395905177\n4810/10000. loss: 0.00029218371491879225\n4811/10000. loss: 0.0003645824423680703\n4812/10000. loss: 0.00040207124159981805\n4813/10000. loss: 0.0004956741274024049\n4814/10000. loss: 0.00034840327377120656\n4815/10000. loss: 0.00048490543849766254\n4816/10000. loss: 0.0008739407639950514\n4817/10000. loss: 0.0010679892729967833\n4818/10000. loss: 0.0008913450874388218\n4819/10000. loss: 0.0014498654442528884\n4820/10000. loss: 0.0008735436325271925\n4821/10000. loss: 0.0020054147268335023\n4822/10000. loss: 0.0013786077809830506\n4823/10000. loss: 0.0018522217869758606\n4824/10000. loss: 0.0013285245125492413\n4825/10000. loss: 0.001588505848000447\n4826/10000. loss: 0.002178476812938849\n4827/10000. loss: 0.00188984132061402\n4828/10000. loss: 0.0019562182327111564\n4829/10000. loss: 0.001767276165386041\n4830/10000. loss: 0.0017430405132472515\n4831/10000. loss: 0.0013822875916957855\n4832/10000. loss: 0.0007267002171526352\n4833/10000. loss: 0.0011508410486082237\n4834/10000. loss: 0.001244679403801759\n4835/10000. loss: 0.00044245234069724876\n4836/10000. loss: 0.0006117433464775482\n4837/10000. loss: 0.0008285025445123514\n4838/10000. loss: 0.0007501294215520223\n4839/10000. loss: 0.0005474241139988104\n4840/10000. loss: 0.0005840785646190246\n4841/10000. loss: 0.0006067246043433746\n4842/10000. loss: 0.0006373721019675335\n4843/10000. loss: 0.0005107819257924954\n4844/10000. loss: 0.00047794686785588664\n4845/10000. loss: 0.0005390862158189217\n4846/10000. loss: 0.0004618677000204722\n4847/10000. loss: 0.0004912522078181306\n4848/10000. loss: 0.0004145080844561259\n4849/10000. loss: 0.000392434885725379\n4850/10000. loss: 0.00035868026316165924\n4851/10000. loss: 0.00029766889444241923\n4852/10000. loss: 0.0004917026963084936\n4853/10000. loss: 0.0003307056613266468\n4854/10000. loss: 0.0004983303369954228\n4855/10000. loss: 0.0004345094785094261\n4856/10000. loss: 0.00037822201071927947\n4857/10000. loss: 0.00042317272163927555\n4858/10000. loss: 0.0004096524401878317\n4859/10000. loss: 0.0005029375897720456\n4860/10000. loss: 0.0004327603771040837\n4861/10000. loss: 0.0003956099196026723\n4862/10000. loss: 0.00032729352824389935\n4863/10000. loss: 0.00033328453234086436\n4864/10000. loss: 0.0003361701965332031\n4865/10000. loss: 0.0002680474426597357\n4866/10000. loss: 0.00027877532799417776\n4867/10000. loss: 0.0003056831192225218\n4868/10000. loss: 0.00030025622497002286\n4869/10000. loss: 0.00024865645294388133\n4870/10000. loss: 0.000434335321187973\n4871/10000. loss: 0.00037300330586731434\n4872/10000. loss: 0.0002725244654963414\n4873/10000. loss: 0.00031495821895077825\n4874/10000. loss: 0.00042279049133261043\n4875/10000. loss: 0.0003817407414317131\n4876/10000. loss: 0.000499569073629876\n4877/10000. loss: 0.0003224750010607143\n4878/10000. loss: 0.00031527547010531026\n4879/10000. loss: 0.00048724030299733084\n4880/10000. loss: 0.0002770441157432894\n4881/10000. loss: 0.0002193947439081967\n4882/10000. loss: 0.00034913156802455586\n4883/10000. loss: 0.00037789678511520225\n4884/10000. loss: 0.0002668515274611612\n4885/10000. loss: 0.00027208162161211175\n4886/10000. loss: 0.00023281362761432925\n4887/10000. loss: 0.00030074264698972303\n4888/10000. loss: 0.0002868598676286638\n4889/10000. loss: 0.00029672142894317705\n4890/10000. loss: 0.00026070326566696167\n4891/10000. loss: 0.00038238056004047394\n4892/10000. loss: 0.00038144493009895086\n4893/10000. loss: 0.0003036020401244362\n4894/10000. loss: 0.0003630415303632617\n4895/10000. loss: 0.00020487924727300802\n4896/10000. loss: 0.0003919457085430622\n4897/10000. loss: 0.00031511309013391536\n4898/10000. loss: 0.00023507381168504557\n4899/10000. loss: 0.0003820282096664111\n4900/10000. loss: 0.00030804276078318554\n4901/10000. loss: 0.0004139827797189355\n4902/10000. loss: 0.00026266420415292185\n4903/10000. loss: 0.00027812904833505553\n4904/10000. loss: 0.00042321587291856605\n4905/10000. loss: 0.00041458515139917534\n4906/10000. loss: 0.00035440622984121245\n4907/10000. loss: 0.0003864610722909371\n4908/10000. loss: 0.00025384964343781274\n4909/10000. loss: 0.0003846765806277593\n4910/10000. loss: 0.00042171737489600975\n4911/10000. loss: 0.00023421323082099357\n4912/10000. loss: 0.00031773575271169346\n4913/10000. loss: 0.00030669471016153693\n4914/10000. loss: 0.0003235215942064921\n4915/10000. loss: 0.00038606316472093266\n4916/10000. loss: 0.0002594148584951957\n4917/10000. loss: 0.0003931936031828324\n4918/10000. loss: 0.0003918157114336888\n4919/10000. loss: 0.000314806355163455\n4920/10000. loss: 0.0002372542512603104\n4921/10000. loss: 0.0002687303737426798\n4922/10000. loss: 0.0002940026655172308\n4923/10000. loss: 0.00026168945866326493\n4924/10000. loss: 0.00022359200132389864\n4925/10000. loss: 0.00027475015182668966\n4926/10000. loss: 0.00027230502261469763\n4927/10000. loss: 0.0003544225279862682\n4928/10000. loss: 0.00034734210930764675\n4929/10000. loss: 0.00022478452107558647\n4930/10000. loss: 0.00030241801869124174\n4931/10000. loss: 0.0003570876627539595\n4932/10000. loss: 0.0003610054263845086\n4933/10000. loss: 0.00025495083536952734\n4934/10000. loss: 0.0002841448488955696\n4935/10000. loss: 0.0003990872452656428\n4936/10000. loss: 0.00033073779195547104\n4937/10000. loss: 0.00021721236407756805\n4938/10000. loss: 0.0003066789358854294\n4939/10000. loss: 0.0002891644835472107\n4940/10000. loss: 0.0003390849257508914\n4941/10000. loss: 0.0003106882407640417\n4942/10000. loss: 0.00031299039255827665\n4943/10000. loss: 0.00027084948184589547\n4944/10000. loss: 0.00029882503440603614\n4945/10000. loss: 0.00035113942188521224\n4946/10000. loss: 0.00036702537909150124\n4947/10000. loss: 0.00026215494532758993\n4948/10000. loss: 0.00026298826560378075\n4949/10000. loss: 0.000383701641112566\n4950/10000. loss: 0.0003919146644572417\n4951/10000. loss: 0.00023631827207282186\n4952/10000. loss: 0.00045740992451707524\n4953/10000. loss: 0.0003566356996695201\n4954/10000. loss: 0.00022977701155468822\n4955/10000. loss: 0.0002876539947465062\n4956/10000. loss: 0.00019790387401978174\n4957/10000. loss: 0.0003045642127593358\n4958/10000. loss: 0.00026699041094010073\n4959/10000. loss: 0.0003213476932918032\n4960/10000. loss: 0.0002859055530279875\n4961/10000. loss: 0.0003932448647295435\n4962/10000. loss: 0.0003162948026632269\n4963/10000. loss: 0.00024706524952004355\n4964/10000. loss: 0.00023146206513047218\n4965/10000. loss: 0.00024190442248558006\n4966/10000. loss: 0.0004164399579167366\n4967/10000. loss: 0.00030183788233747083\n4968/10000. loss: 0.0003634841414168477\n4969/10000. loss: 0.00033305174050231773\n4970/10000. loss: 0.00024803246681888896\n4971/10000. loss: 0.00037310194845000905\n4972/10000. loss: 0.0002491573492685954\n4973/10000. loss: 0.0004391266265884042\n4974/10000. loss: 0.0002647757452602188\n4975/10000. loss: 0.0004644059808924794\n4976/10000. loss: 0.00034291332121938467\n4977/10000. loss: 0.0004815417341887951\n4978/10000. loss: 0.0004653555030624072\n4979/10000. loss: 0.0006348446477204561\n4980/10000. loss: 0.000757971623291572\n4981/10000. loss: 0.0009343018755316734\n4982/10000. loss: 0.0012078801325211923\n4983/10000. loss: 0.0024651912972331047\n4984/10000. loss: 0.002818754563728968\n4985/10000. loss: 0.002048961818218231\n4986/10000. loss: 0.0034037018194794655\n4987/10000. loss: 0.0018187786142031352\n4988/10000. loss: 0.006145484124620755\n4989/10000. loss: 0.009111234297355017\n4990/10000. loss: 0.007926490157842636\n4991/10000. loss: 0.006054213270545006\n4992/10000. loss: 0.0051442719995975494\n4993/10000. loss: 0.002999681979417801\n4994/10000. loss: 0.0017630488922198613\n4995/10000. loss: 0.0017874638239542644\n4996/10000. loss: 0.0011753848909089963\n4997/10000. loss: 0.0016008568927645683\n4998/10000. loss: 0.0016202710879345734\n4999/10000. loss: 0.0019514026741186778\n5000/10000. loss: 0.0019268634108205636\n5001/10000. loss: 0.0014273825411995251\n5002/10000. loss: 0.0007945074855039517\n5003/10000. loss: 0.0005911609623581171\n5004/10000. loss: 0.0010298423003405333\n5005/10000. loss: 0.0011868342602004607\n5006/10000. loss: 0.0010447204113006592\n5007/10000. loss: 0.0012203583028167486\n5008/10000. loss: 0.0015501235611736774\n5009/10000. loss: 0.0010631806217133999\n5010/10000. loss: 0.000799254048615694\n5011/10000. loss: 0.0006210351518044869\n5012/10000. loss: 0.00046132543745140236\n5013/10000. loss: 0.000552831799723208\n5014/10000. loss: 0.0007445299221823612\n5015/10000. loss: 0.0006582324082652727\n5016/10000. loss: 0.0006739948876202106\n5017/10000. loss: 0.0007309522479772568\n5018/10000. loss: 0.00047042981411019963\n5019/10000. loss: 0.0005868722607071201\n5020/10000. loss: 0.0006542378881325325\n5021/10000. loss: 0.0004794620132694642\n5022/10000. loss: 0.0004656002080688874\n5023/10000. loss: 0.0006013287541766962\n5024/10000. loss: 0.0006350287003442645\n5025/10000. loss: 0.00038256604845325154\n5026/10000. loss: 0.0005134072853252292\n5027/10000. loss: 0.00040825378770629567\n5028/10000. loss: 0.000297459385668238\n5029/10000. loss: 0.00048039923422038555\n5030/10000. loss: 0.0005003466503694654\n5031/10000. loss: 0.0004651531344279647\n5032/10000. loss: 0.0005188739160075784\n5033/10000. loss: 0.00047921397102375823\n5034/10000. loss: 0.0004804901157816251\n5035/10000. loss: 0.0004341355136906107\n5036/10000. loss: 0.00034764544883122045\n5037/10000. loss: 0.0004898547194898129\n5038/10000. loss: 0.0005127176797638336\n5039/10000. loss: 0.0006020818836987019\n5040/10000. loss: 0.000493364369807144\n5041/10000. loss: 0.0005395128779734174\n5042/10000. loss: 0.00032826350070536137\n5043/10000. loss: 0.0005150019812087218\n5044/10000. loss: 0.0005170946630338827\n5045/10000. loss: 0.00028846711696436006\n5046/10000. loss: 0.00027100507092351717\n5047/10000. loss: 0.00033698099044462043\n5048/10000. loss: 0.00035625459471096593\n5049/10000. loss: 0.00033153694433470565\n5050/10000. loss: 0.0002930233410249154\n5051/10000. loss: 0.00047466251999139786\n5052/10000. loss: 0.00029570880966881913\n5053/10000. loss: 0.0004006827560563882\n5054/10000. loss: 0.00032164928658554953\n5055/10000. loss: 0.00036526126010964316\n5056/10000. loss: 0.00027675476546088856\n5057/10000. loss: 0.00036048260517418385\n5058/10000. loss: 0.00034766364842653275\n5059/10000. loss: 0.0002843738184310496\n5060/10000. loss: 0.0002547729527577758\n5061/10000. loss: 0.00028021843172609806\n5062/10000. loss: 0.0004934313474223018\n5063/10000. loss: 0.0002794628186772267\n5064/10000. loss: 0.000424872695778807\n5065/10000. loss: 0.0005141445435583591\n5066/10000. loss: 0.00037335685919970274\n5067/10000. loss: 0.00034473165093610686\n5068/10000. loss: 0.00034783365360150736\n5069/10000. loss: 0.0003761936289568742\n5070/10000. loss: 0.0002767155723025401\n5071/10000. loss: 0.00031411383921901387\n5072/10000. loss: 0.00025380647275596857\n5073/10000. loss: 0.0003422932156051199\n5074/10000. loss: 0.0003912680937598149\n5075/10000. loss: 0.00032064570890118677\n5076/10000. loss: 0.00025509381278728444\n5077/10000. loss: 0.0005148851778358221\n5078/10000. loss: 0.00035360098506013554\n5079/10000. loss: 0.0003023058719312151\n5080/10000. loss: 0.00035010188973198336\n5081/10000. loss: 0.0003457206379001339\n5082/10000. loss: 0.00041459434820959967\n5083/10000. loss: 0.0002480072046940525\n5084/10000. loss: 0.00027231222096209723\n5085/10000. loss: 0.000438997366776069\n5086/10000. loss: 0.0004692791650692622\n5087/10000. loss: 0.0003045954314681391\n5088/10000. loss: 0.0004795818046356241\n5089/10000. loss: 0.00033872760832309723\n5090/10000. loss: 0.0004914384723330537\n5091/10000. loss: 0.00047852044614652794\n5092/10000. loss: 0.0008378264804681143\n5093/10000. loss: 0.0003077764025268455\n5094/10000. loss: 0.0002914682263508439\n5095/10000. loss: 0.00042708714803059894\n5096/10000. loss: 0.0002922395554681619\n5097/10000. loss: 0.0005169415380805731\n5098/10000. loss: 0.00033534085378050804\n5099/10000. loss: 0.00026905946045493084\n5100/10000. loss: 0.0004065598671634992\n5101/10000. loss: 0.00038754551981886226\n5102/10000. loss: 0.0003411495126783848\n5103/10000. loss: 0.0002709589898586273\n5104/10000. loss: 0.00048505708885689575\n5105/10000. loss: 0.0003395703000326951\n5106/10000. loss: 0.00024557098125418025\n5107/10000. loss: 0.0003950671913723151\n5108/10000. loss: 0.0003146775028047462\n5109/10000. loss: 0.0003881348141779502\n5110/10000. loss: 0.0003235334297642112\n5111/10000. loss: 0.000284852731662492\n5112/10000. loss: 0.0002453236180978517\n5113/10000. loss: 0.0003225117106921971\n5114/10000. loss: 0.0003487707969422142\n5115/10000. loss: 0.0003769976319745183\n5116/10000. loss: 0.0004106983154391249\n5117/10000. loss: 0.00045489434463282424\n5118/10000. loss: 0.00030146845771620673\n5119/10000. loss: 0.00027719840484981734\n5120/10000. loss: 0.000296779558993876\n5121/10000. loss: 0.00023929301338891187\n5122/10000. loss: 0.0003581060639893015\n5123/10000. loss: 0.0004240424605086446\n5124/10000. loss: 0.00032293301774188876\n5125/10000. loss: 0.00029762131938089925\n5126/10000. loss: 0.0003143542368585865\n5127/10000. loss: 0.00028313614893704653\n5128/10000. loss: 0.00025588227435946465\n5129/10000. loss: 0.00026833302884673077\n5130/10000. loss: 0.0002645777227977912\n5131/10000. loss: 0.00033756446403761703\n5132/10000. loss: 0.0003719078376889229\n5133/10000. loss: 0.00023973461550970873\n5134/10000. loss: 0.000334413954988122\n5135/10000. loss: 0.0004192998943229516\n5136/10000. loss: 0.0002593526927133401\n5137/10000. loss: 0.00025011108179266256\n5138/10000. loss: 0.00023175093034903208\n5139/10000. loss: 0.00032937084324657917\n5140/10000. loss: 0.0002526236736836533\n5141/10000. loss: 0.0003111778642050922\n5142/10000. loss: 0.0002924579700144629\n5143/10000. loss: 0.00036924821324646473\n5144/10000. loss: 0.00024145729063699642\n5145/10000. loss: 0.00025884931286176044\n5146/10000. loss: 0.0005806592913965384\n5147/10000. loss: 0.00035600488384564716\n5148/10000. loss: 0.00027212521915013593\n5149/10000. loss: 0.0005165229085832834\n5150/10000. loss: 0.0002725356607697904\n5151/10000. loss: 0.00040545384399592876\n5152/10000. loss: 0.0003736203070729971\n5153/10000. loss: 0.00037751505927493173\n5154/10000. loss: 0.0003858314206202825\n5155/10000. loss: 0.0003352131849775712\n5156/10000. loss: 0.0002777241364431878\n5157/10000. loss: 0.0003607507872705658\n5158/10000. loss: 0.0004434071791668733\n5159/10000. loss: 0.0003547228795165817\n5160/10000. loss: 0.00023173408893247446\n5161/10000. loss: 0.00023056782083585858\n5162/10000. loss: 0.0003101370530202985\n5163/10000. loss: 0.0004055133710304896\n5164/10000. loss: 0.0002887304872274399\n5165/10000. loss: 0.000254539637050281\n5166/10000. loss: 0.0003692536459614833\n5167/10000. loss: 0.00027424481231719255\n5168/10000. loss: 0.0002963246079161763\n5169/10000. loss: 0.00022198702208697796\n5170/10000. loss: 0.0002913657420625289\n5171/10000. loss: 0.00020267323513204852\n5172/10000. loss: 0.00035687935693810385\n5173/10000. loss: 0.00021938985446467996\n5174/10000. loss: 0.00028724485309794545\n5175/10000. loss: 0.0002407108355934421\n5176/10000. loss: 0.0004024492421497901\n5177/10000. loss: 0.00041341436250756186\n5178/10000. loss: 0.00022718523784230152\n5179/10000. loss: 0.0002508399193175137\n5180/10000. loss: 0.00027322908863425255\n5181/10000. loss: 0.00034811611597736675\n5182/10000. loss: 0.0002636506299798687\n5183/10000. loss: 0.00028964046699305374\n5184/10000. loss: 0.0004399063376088937\n5185/10000. loss: 0.00027864820246274274\n5186/10000. loss: 0.0002747187584949036\n5187/10000. loss: 0.000539289826216797\n5188/10000. loss: 0.00046438327990472317\n5189/10000. loss: 0.00035248879188050825\n5190/10000. loss: 0.0003946001719062527\n5191/10000. loss: 0.00028821952097738784\n5192/10000. loss: 0.00022785126930102706\n5193/10000. loss: 0.00023167939313376942\n5194/10000. loss: 0.0003476267835746209\n5195/10000. loss: 0.00022474499807382622\n5196/10000. loss: 0.00025288791706164676\n5197/10000. loss: 0.0002871314063668251\n5198/10000. loss: 0.00037593274222066003\n5199/10000. loss: 0.00021065210845942298\n5200/10000. loss: 0.00027367424142236513\n5201/10000. loss: 0.00037434852371613186\n5202/10000. loss: 0.00035081105306744576\n5203/10000. loss: 0.00020758887209619084\n5204/10000. loss: 0.00037322208906213444\n5205/10000. loss: 0.0002962848520837724\n5206/10000. loss: 0.00029796922657017905\n5207/10000. loss: 0.00032135141858210164\n5208/10000. loss: 0.00032856858645876247\n5209/10000. loss: 0.00034209884082277614\n5210/10000. loss: 0.00029408198315650225\n5211/10000. loss: 0.00027692220949878293\n5212/10000. loss: 0.00031274677409480017\n5213/10000. loss: 0.000256804710564514\n5214/10000. loss: 0.00026053341571241617\n5215/10000. loss: 0.0004204648236433665\n5216/10000. loss: 0.0003089381692310174\n5217/10000. loss: 0.0003840560869624217\n5218/10000. loss: 0.0002756679702239732\n5219/10000. loss: 0.00024476660958801705\n5220/10000. loss: 0.00034349287549654645\n5221/10000. loss: 0.00029289487671727937\n5222/10000. loss: 0.0003291563286135594\n5223/10000. loss: 0.0003732257367422183\n5224/10000. loss: 0.0004506797607367237\n5225/10000. loss: 0.00036174590544154245\n5226/10000. loss: 0.000498431579520305\n5227/10000. loss: 0.0005652594457690915\n5228/10000. loss: 0.0005176448418448368\n5229/10000. loss: 0.0007010740227997303\n5230/10000. loss: 0.0006506384427969655\n5231/10000. loss: 0.001562963395069043\n5232/10000. loss: 0.0010763274816175301\n5233/10000. loss: 0.0017394668733080227\n5234/10000. loss: 0.0014735587562123935\n5235/10000. loss: 0.0016107203749318917\n5236/10000. loss: 0.0012943653855472803\n5237/10000. loss: 0.001439354692896207\n5238/10000. loss: 0.0012630828811476629\n5239/10000. loss: 0.0012831790372729301\n5240/10000. loss: 0.0011600932727257411\n5241/10000. loss: 0.0012105318407217662\n5242/10000. loss: 0.0009902624879032373\n5243/10000. loss: 0.0008950574944416682\n5244/10000. loss: 0.0007182907623549303\n5245/10000. loss: 0.0005215289226422707\n5246/10000. loss: 0.0004540647690494855\n5247/10000. loss: 0.0005246219225227833\n5248/10000. loss: 0.0004478262271732092\n5249/10000. loss: 0.0004392530924330155\n5250/10000. loss: 0.0003770398519312342\n5251/10000. loss: 0.0003992456089084347\n5252/10000. loss: 0.0003932479303330183\n5253/10000. loss: 0.0005482578029235204\n5254/10000. loss: 0.00039640022441744804\n5255/10000. loss: 0.00036584818735718727\n5256/10000. loss: 0.0005695118258396784\n5257/10000. loss: 0.0005087251774966717\n5258/10000. loss: 0.0006210101613154014\n5259/10000. loss: 0.0003309420232350628\n5260/10000. loss: 0.0003149878078450759\n5261/10000. loss: 0.0003343020798638463\n5262/10000. loss: 0.0002988699513177077\n5263/10000. loss: 0.0004378278584529956\n5264/10000. loss: 0.0003622284857556224\n5265/10000. loss: 0.00024633501501133043\n5266/10000. loss: 0.00043544452637434006\n5267/10000. loss: 0.00026552365549529594\n5268/10000. loss: 0.0002584791897485654\n5269/10000. loss: 0.00041763562088211376\n5270/10000. loss: 0.0002964394516311586\n5271/10000. loss: 0.0002429874730296433\n5272/10000. loss: 0.00038747854220370453\n5273/10000. loss: 0.00044902639153103036\n5274/10000. loss: 0.0003832561584810416\n5275/10000. loss: 0.0003706393763422966\n5276/10000. loss: 0.0005328014958649874\n5277/10000. loss: 0.0004462773213163018\n5278/10000. loss: 0.00031078742661823827\n5279/10000. loss: 0.00037409675618012744\n5280/10000. loss: 0.00032184711502244073\n5281/10000. loss: 0.0004107433681686719\n5282/10000. loss: 0.00032275455305352807\n5283/10000. loss: 0.0003011665927867095\n5284/10000. loss: 0.0003996805753558874\n5285/10000. loss: 0.0002290105912834406\n5286/10000. loss: 0.00022447248920798302\n5287/10000. loss: 0.0003376854971672098\n5288/10000. loss: 0.00037227986225237447\n5289/10000. loss: 0.00027054625873764354\n5290/10000. loss: 0.0003693730492765705\n5291/10000. loss: 0.0003321185164774458\n5292/10000. loss: 0.0002948274292672674\n5293/10000. loss: 0.0002071090469447275\n5294/10000. loss: 0.00027139401451374095\n5295/10000. loss: 0.0003611311161269744\n5296/10000. loss: 0.0003120808784539501\n5297/10000. loss: 0.00039382255636155605\n5298/10000. loss: 0.0002588850911706686\n5299/10000. loss: 0.0002888421295210719\n5300/10000. loss: 0.00035712406194458407\n5301/10000. loss: 0.00025854783598333597\n5302/10000. loss: 0.0003657380196576317\n5303/10000. loss: 0.0002276379382237792\n5304/10000. loss: 0.0003485390916466713\n5305/10000. loss: 0.0002572741359472275\n5306/10000. loss: 0.0004622383664051692\n5307/10000. loss: 0.000241531563612322\n5308/10000. loss: 0.00024132810843487582\n5309/10000. loss: 0.00025236187502741814\n5310/10000. loss: 0.00022295107676958045\n5311/10000. loss: 0.00039702172701557476\n5312/10000. loss: 0.00040119261636088294\n5313/10000. loss: 0.0003374510755141576\n5314/10000. loss: 0.00027545207800964516\n5315/10000. loss: 0.00022123825813954076\n5316/10000. loss: 0.00039852239812413853\n5317/10000. loss: 0.00020966837958743176\n5318/10000. loss: 0.000387316569685936\n5319/10000. loss: 0.0003078015288338065\n5320/10000. loss: 0.00025251628054926795\n5321/10000. loss: 0.0003830937979122003\n5322/10000. loss: 0.00022291005977119008\n5323/10000. loss: 0.00033279388056447107\n5324/10000. loss: 0.0003088028752245009\n5325/10000. loss: 0.0003523821554457148\n5326/10000. loss: 0.0002996175123068194\n5327/10000. loss: 0.00033634835078070563\n5328/10000. loss: 0.0003239032036314408\n5329/10000. loss: 0.00022921509419878325\n5330/10000. loss: 0.00023070540434370437\n5331/10000. loss: 0.00027369150969510275\n5332/10000. loss: 0.0002979059354402125\n5333/10000. loss: 0.0003138820951183637\n5334/10000. loss: 0.0002888360759243369\n5335/10000. loss: 0.0004028525048245986\n5336/10000. loss: 0.00032642961014062166\n5337/10000. loss: 0.00031724195772161085\n5338/10000. loss: 0.0002777691697701812\n5339/10000. loss: 0.00032091581185037893\n5340/10000. loss: 0.00033963751047849655\n5341/10000. loss: 0.00028744762918601435\n5342/10000. loss: 0.00025831857540955144\n5343/10000. loss: 0.00026219662201280397\n5344/10000. loss: 0.0002871988496432702\n5345/10000. loss: 0.0003146586629251639\n5346/10000. loss: 0.00019667105516418815\n5347/10000. loss: 0.00024003745056688786\n5348/10000. loss: 0.0003623971715569496\n5349/10000. loss: 0.00019491921799878278\n5350/10000. loss: 0.00025372952222824097\n5351/10000. loss: 0.00023352079248676697\n5352/10000. loss: 0.00024869201782469946\n5353/10000. loss: 0.00021277383590737978\n5354/10000. loss: 0.0002021079029267033\n5355/10000. loss: 0.00019747835661595067\n5356/10000. loss: 0.0003632044730087121\n5357/10000. loss: 0.00020658071540916959\n5358/10000. loss: 0.00022778037237003446\n5359/10000. loss: 0.0003358001510302226\n5360/10000. loss: 0.00030673797785614926\n5361/10000. loss: 0.00038102602896591026\n5362/10000. loss: 0.00026202830485999584\n5363/10000. loss: 0.0002501128668275972\n5364/10000. loss: 0.0002876791792611281\n5365/10000. loss: 0.0004562627679357926\n5366/10000. loss: 0.00031053109948212904\n5367/10000. loss: 0.0005654554503659407\n5368/10000. loss: 0.000648214171330134\n5369/10000. loss: 0.00037522835191339254\n5370/10000. loss: 0.0012592715211212635\n5371/10000. loss: 0.0006510211775700251\n5372/10000. loss: 0.0007358717266470194\n5373/10000. loss: 0.001493242879708608\n5374/10000. loss: 0.0028677061200141907\n5375/10000. loss: 0.0018087057396769524\n5376/10000. loss: 0.0033861640840768814\n5377/10000. loss: 0.0035245887314279876\n5378/10000. loss: 0.002176467174043258\n5379/10000. loss: 0.0031711204598347345\n5380/10000. loss: 0.002065946658452352\n5381/10000. loss: 0.001181885755310456\n5382/10000. loss: 0.0008043434160451094\n5383/10000. loss: 0.0008502217630545298\n5384/10000. loss: 0.0008765371361126503\n5385/10000. loss: 0.001172558559725682\n5386/10000. loss: 0.0010744696483016014\n5387/10000. loss: 0.0010608471930027008\n5388/10000. loss: 0.001165551133453846\n5389/10000. loss: 0.0006203156275053819\n5390/10000. loss: 0.0006504409248009324\n5391/10000. loss: 0.00040355635186036426\n5392/10000. loss: 0.0004140659002587199\n5393/10000. loss: 0.0005349515316387018\n5394/10000. loss: 0.0007440287154167891\n5395/10000. loss: 0.0007236403568337361\n5396/10000. loss: 0.0005941752654810747\n5397/10000. loss: 0.0005923310139526924\n5398/10000. loss: 0.00045483435193697613\n5399/10000. loss: 0.0003142242591517667\n5400/10000. loss: 0.0005066981539130211\n5401/10000. loss: 0.00038262953360875446\n5402/10000. loss: 0.0004579065910850962\n5403/10000. loss: 0.0006097054186587533\n5404/10000. loss: 0.000557575219621261\n5405/10000. loss: 0.0005126065419365963\n5406/10000. loss: 0.0005118285771459341\n5407/10000. loss: 0.00045452189321319264\n5408/10000. loss: 0.0003203724045306444\n5409/10000. loss: 0.0002670448545056085\n5410/10000. loss: 0.00042477366514503956\n5411/10000. loss: 0.0003043260464134316\n5412/10000. loss: 0.00040103499001512927\n5413/10000. loss: 0.0003692254734536012\n5414/10000. loss: 0.0004449512731904785\n5415/10000. loss: 0.0004853555001318455\n5416/10000. loss: 0.0003136591015694042\n5417/10000. loss: 0.0002701640672360857\n5418/10000. loss: 0.00040234788320958614\n5419/10000. loss: 0.00040723072985808056\n5420/10000. loss: 0.0003449218347668648\n5421/10000. loss: 0.00043023711380859214\n5422/10000. loss: 0.0003477058295781414\n5423/10000. loss: 0.00043669416724393767\n5424/10000. loss: 0.00028962583746761084\n5425/10000. loss: 0.0004919440252706409\n5426/10000. loss: 0.0003451225347816944\n5427/10000. loss: 0.0003946940414607525\n5428/10000. loss: 0.00041736817608277005\n5429/10000. loss: 0.00027325879394387204\n5430/10000. loss: 0.00047745462507009506\n5431/10000. loss: 0.00039406058688958484\n5432/10000. loss: 0.0002521731269856294\n5433/10000. loss: 0.0003873356617987156\n5434/10000. loss: 0.0003094455460086465\n5435/10000. loss: 0.0004733833872402708\n5436/10000. loss: 0.0003690406447276473\n5437/10000. loss: 0.00042585508587459725\n5438/10000. loss: 0.0003151520698641737\n5439/10000. loss: 0.0004745327169075608\n5440/10000. loss: 0.00025127418727303546\n5441/10000. loss: 0.0003977002343162894\n5442/10000. loss: 0.0002579275945511957\n5443/10000. loss: 0.0002641896329199274\n5444/10000. loss: 0.0002428595907986164\n5445/10000. loss: 0.0002885763921464483\n5446/10000. loss: 0.0002829148822153608\n5447/10000. loss: 0.0002663214496957759\n5448/10000. loss: 0.0004178187033782403\n5449/10000. loss: 0.00025250075850635767\n5450/10000. loss: 0.0006533832444498936\n5451/10000. loss: 0.0002767296585564812\n5452/10000. loss: 0.00041383702773600817\n5453/10000. loss: 0.0004367487272247672\n5454/10000. loss: 0.00021056425369655093\n5455/10000. loss: 0.0003002348627584676\n5456/10000. loss: 0.00026547868037596345\n5457/10000. loss: 0.00040559199017783004\n5458/10000. loss: 0.0003448993278046449\n5459/10000. loss: 0.0002253253866607944\n5460/10000. loss: 0.0003770275895173351\n5461/10000. loss: 0.00043923450478663045\n5462/10000. loss: 0.00043478491716086864\n5463/10000. loss: 0.000305261656952401\n5464/10000. loss: 0.0004425977822393179\n5465/10000. loss: 0.0003436827488864462\n5466/10000. loss: 0.00027482966349149746\n5467/10000. loss: 0.0002750113296012084\n5468/10000. loss: 0.00043754628859460354\n5469/10000. loss: 0.00024784230239068467\n5470/10000. loss: 0.00025914710325499374\n5471/10000. loss: 0.00029337790329009295\n5472/10000. loss: 0.00033354243108381826\n5473/10000. loss: 0.0004068951044852535\n5474/10000. loss: 0.0002823350951075554\n5475/10000. loss: 0.00034497429927190143\n5476/10000. loss: 0.0003404459760834773\n5477/10000. loss: 0.00037656833107272786\n5478/10000. loss: 0.00035599009909977514\n5479/10000. loss: 0.000337798148393631\n5480/10000. loss: 0.00037735017637411755\n5481/10000. loss: 0.0002259592292830348\n5482/10000. loss: 0.000375781402302285\n5483/10000. loss: 0.0003258670525004466\n5484/10000. loss: 0.0002496110779854159\n5485/10000. loss: 0.00026980154992391664\n5486/10000. loss: 0.00020248842580864826\n5487/10000. loss: 0.00022632624798764786\n5488/10000. loss: 0.0003429900001113613\n5489/10000. loss: 0.0003691720776259899\n5490/10000. loss: 0.0002813396858982742\n5491/10000. loss: 0.00029590997534493607\n5492/10000. loss: 0.0003413773374632001\n5493/10000. loss: 0.0003284758422523737\n5494/10000. loss: 0.00029492861358448863\n5495/10000. loss: 0.000290268372433881\n5496/10000. loss: 0.00022864291289200386\n5497/10000. loss: 0.0002840591866212587\n5498/10000. loss: 0.00043191803464045125\n5499/10000. loss: 0.000531437573954463\n5500/10000. loss: 0.0003596460058664282\n5501/10000. loss: 0.00030253819810847443\n5502/10000. loss: 0.0002882269521554311\n5503/10000. loss: 0.00019664874222750464\n5504/10000. loss: 0.00021949115519722304\n5505/10000. loss: 0.00027915980899706483\n5506/10000. loss: 0.0002136571565642953\n5507/10000. loss: 0.00035248149652034044\n5508/10000. loss: 0.0002661812080380817\n5509/10000. loss: 0.0002427372382953763\n5510/10000. loss: 0.0004146801463017861\n5511/10000. loss: 0.00029964120282481116\n5512/10000. loss: 0.00038795227495332557\n5513/10000. loss: 0.0004064335177342097\n5514/10000. loss: 0.0002483629311124484\n5515/10000. loss: 0.0002792190061882138\n5516/10000. loss: 0.00036596083858360845\n5517/10000. loss: 0.00035246329692502815\n5518/10000. loss: 0.00038302297859142226\n5519/10000. loss: 0.00032095483038574457\n5520/10000. loss: 0.0003936550347134471\n5521/10000. loss: 0.0003876828122884035\n5522/10000. loss: 0.00031369532613704604\n5523/10000. loss: 0.0005959509871900082\n5524/10000. loss: 0.0004635967779904604\n5525/10000. loss: 0.000619435915723443\n5526/10000. loss: 0.0004829512909054756\n5527/10000. loss: 0.0006119218499710163\n5528/10000. loss: 0.0004894110218932232\n5529/10000. loss: 0.0006465286326905092\n5530/10000. loss: 0.0006090238457545638\n5531/10000. loss: 0.0007144640355060498\n5532/10000. loss: 0.000662059678385655\n5533/10000. loss: 0.0005997682067876061\n5534/10000. loss: 0.000615120322133104\n5535/10000. loss: 0.00039765890687704086\n5536/10000. loss: 0.0004926297115162015\n5537/10000. loss: 0.000255144783295691\n5538/10000. loss: 0.00030110155542691547\n5539/10000. loss: 0.0004290224363406499\n5540/10000. loss: 0.00028686415559301776\n5541/10000. loss: 0.0003654203998545806\n5542/10000. loss: 0.0003047256808107098\n5543/10000. loss: 0.00037525113051136333\n5544/10000. loss: 0.00025370569589237374\n5545/10000. loss: 0.00035461624308178824\n5546/10000. loss: 0.0003815947954232494\n5547/10000. loss: 0.0002554304664954543\n5548/10000. loss: 0.0002492566127330065\n5549/10000. loss: 0.0004240860774492224\n5550/10000. loss: 0.00027147723206629354\n5551/10000. loss: 0.00024231705659379563\n5552/10000. loss: 0.00038778805173933506\n5553/10000. loss: 0.0002574930355573694\n5554/10000. loss: 0.0004108496941626072\n5555/10000. loss: 0.00029189535416662693\n5556/10000. loss: 0.0003893450290585558\n5557/10000. loss: 0.00026514796384920675\n5558/10000. loss: 0.00044021177260826033\n5559/10000. loss: 0.0004030768759548664\n5560/10000. loss: 0.00021376363777865967\n5561/10000. loss: 0.0002818664846320947\n5562/10000. loss: 0.00042433202421913546\n5563/10000. loss: 0.00022363812119389573\n5564/10000. loss: 0.0003820090399434169\n5565/10000. loss: 0.00036687908383707207\n5566/10000. loss: 0.0003556861774995923\n5567/10000. loss: 0.0002343268133699894\n5568/10000. loss: 0.00022348115453496575\n5569/10000. loss: 0.00022406026255339384\n5570/10000. loss: 0.0002671550416077177\n5571/10000. loss: 0.00030211937458564836\n5572/10000. loss: 0.00026831659488379955\n5573/10000. loss: 0.0003120706727107366\n5574/10000. loss: 0.00030320089232797426\n5575/10000. loss: 0.0002617131103761494\n5576/10000. loss: 0.00041332336453100044\n5577/10000. loss: 0.0001999032295619448\n5578/10000. loss: 0.00032547143443177146\n5579/10000. loss: 0.00039512186776846647\n5580/10000. loss: 0.0002568843774497509\n5581/10000. loss: 0.00019669224275276065\n5582/10000. loss: 0.0003575435063491265\n5583/10000. loss: 0.00021369827057545385\n5584/10000. loss: 0.00019772405115266642\n5585/10000. loss: 0.00024694895061353844\n5586/10000. loss: 0.0003022099457060297\n5587/10000. loss: 0.00037371056775252026\n5588/10000. loss: 0.00040775145559261244\n5589/10000. loss: 0.0003617775704090794\n5590/10000. loss: 0.00021843990543857217\n5591/10000. loss: 0.0003014798276126385\n5592/10000. loss: 0.0004218871084352334\n5593/10000. loss: 0.00042994172933200997\n5594/10000. loss: 0.000335159944370389\n5595/10000. loss: 0.0004729783395305276\n5596/10000. loss: 0.0004825668875128031\n5597/10000. loss: 0.0005248229329784712\n5598/10000. loss: 0.00039541011210530996\n5599/10000. loss: 0.0004850845628728469\n5600/10000. loss: 0.0006343773178135356\n5601/10000. loss: 0.0005355271666000286\n5602/10000. loss: 0.0007963267465432485\n5603/10000. loss: 0.0007069098452727\n5604/10000. loss: 0.001322082243859768\n5605/10000. loss: 0.0005620569766809543\n5606/10000. loss: 0.0011732003185898066\n5607/10000. loss: 0.001160273017982642\n5608/10000. loss: 0.0015890300273895264\n5609/10000. loss: 0.00098087793836991\n5610/10000. loss: 0.0019592344760894775\n5611/10000. loss: 0.0017060586251318455\n5612/10000. loss: 0.0018044128082692623\n5613/10000. loss: 0.0023451953505476317\n5614/10000. loss: 0.0014128067220250766\n5615/10000. loss: 0.0018358798697590828\n5616/10000. loss: 0.0013462072238326073\n5617/10000. loss: 0.001028815284371376\n5618/10000. loss: 0.0008840002119541168\n5619/10000. loss: 0.0005858850587780277\n5620/10000. loss: 0.00033171459411581356\n5621/10000. loss: 0.0004626536586632331\n5622/10000. loss: 0.0005464266675213972\n5623/10000. loss: 0.0005103656246016423\n5624/10000. loss: 0.000435127061791718\n5625/10000. loss: 0.0005857707001268864\n5626/10000. loss: 0.0006021206888059775\n5627/10000. loss: 0.000492495057793955\n5628/10000. loss: 0.0005221936541299025\n5629/10000. loss: 0.000531164308389028\n5630/10000. loss: 0.0003627489398544033\n5631/10000. loss: 0.000401758976901571\n5632/10000. loss: 0.0004524306083718936\n5633/10000. loss: 0.0003248436648088197\n5634/10000. loss: 0.0003650953294709325\n5635/10000. loss: 0.00040806690230965614\n5636/10000. loss: 0.00029174213220054906\n5637/10000. loss: 0.0004528588615357876\n5638/10000. loss: 0.0003177836964217325\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车 增加LSTM\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\nimport warnings\n# 忽略特定类型的警告\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n                ('l1',nn.Linear(input_size,256)),\n                ('relu1',nn.ReLU()),\n                ('l2',nn.Linear(256,256)),\n                ('relu2',nn.ReLU()),\n                ('l3',nn.Linear(256,1))\n            ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n# LSTM模型\nclass lstm_model(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_model, self).__init__()\n        self.net1=nn.Sequential(OrderedDict([\n            ('lstm',nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout))\n        ]))\n        self.net2=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(hidden_size,hidden_size)),\n            ('relu',nn.ReLU()),\n            ('l2',nn.Linear(hidden_size,1))\n        ]))\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.lstm_layers = lstm_layers\n        self.dropout = dropout\n#         # LSTM编码器，接受输入序列并输出隐藏状态\n#         self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n#         # 线性层，将LSTM的输出映射到1维\n#         self.linear1 = nn.Linear(hidden_size, hidden_size)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.linear2 = nn.Linear(hidden_size, 1)\n        # 初始化线性层的权重和偏置\n#         nn.init.normal_(self.linear1.weight, 0, 0.02)\n#         nn.init.constant_(self.linear1.bias, 0.0)\n#         nn.init.normal_(self.linear2.weight, 0, 0.02)\n#         nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    def forward(self, x, weights):\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        lstm_encoder = nn.LSTM(self.input_size, self.hidden_size, self.lstm_layers, batch_first=False, dropout=self.dropout)\n        # 将已知的权重替换为模型的权重\n        lstm_encoder.weight_ih_l0.data = weights[0]\n        lstm_encoder.weight_hh_l0.data = weights[1]\n        lstm_encoder.bias_ih_l0.data = weights[2]\n        lstm_encoder.bias_hh_l0.data = weights[3]\n#         # 线性层，将LSTM的输出映射到1维\n#         self.linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.linear2 = nn.Linear(self.hidden_size, 1)\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = lstm_encoder(x)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out = F.linear(h_n, weights[4], weights[5])\n        x = F.relu(out)\n        out = F.linear(h_n, weights[6], weights[7])\n        return out\n\n\nclass lstm_test(nn.Module):\n    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n    def __init__(self, input_size=3, hidden_size=16, lstm_layers=1, dropout=0.1):\n        super(lstm_test, self).__init__()\n        # LSTM编码器，接受输入序列并输出隐藏状态\n        self.encoder = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=False, dropout=dropout)\n        # 线性层，将LSTM的输出映射到1维\n        # self.linear = nn.Linear(hidden_size, 1)\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, 1)\n        self.relu = nn.ReLU(inplace=True)\n        # 初始化线性层的权重和偏置\n#         nn.init.normal_(self.linear1.weight, 0, 0.02)\n#         nn.init.constant_(self.linear1.bias, 0.0)\n#         nn.init.normal_(self.linear2.weight, 0, 0.02)\n#         nn.init.constant_(self.linear2.bias, 0.0)\n    # LSTM 模型由编码器和线性层组成。编码器接收输入数据并计算编码表示和最终层的隐藏状态。\n    # 随后，线性层层将隐藏状态映射到输出值，输出值经过 tanh 激活函数并乘以代表加速度极限的常数\n    def forward(self, src):\n        # 将输入序列通过LSTM编码器得到隐藏状态  src(10,32,3)\n        # enc_x(10,32,16)是模型在每个时间步的输出，(h_n, c_n)((1,32,16), (1,32,16))) 是模型的隐藏状态和记忆单元状态\n        enc_x, (h_n, c_n) = self.encoder(src)\n        # 如果隐藏状态的维度为3，则取最后一层的隐藏状态 即(batch_size, hidden_size)\n        if len(h_n.shape) == 3:\n            h_n = h_n[-1] # (32,16)\n        # 通过线性层和激活函数得到最终输出\n        out_0 = self.linear1(h_n) # (32,16)\n        out = self.linear2(self.relu(out_0)) # (32, 1)\n        # out = self.linear(h_n)\n        # out = torch.tanh(out) * ACC_LIMIT\n        return out\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len)\n        return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = int(self.k/2)\n        self.models = []\n\n    def inner_loop(self, task):\n        learner = lstm_test(input_size = 3, lstm_layers = 1).to(device)\n        state_dict = copy.deepcopy(self.net.state_dict())\n        learner.load_state_dict(state_dict)\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = dict((name, param) for (name, param) in learner.named_parameters())  # 复制当前网络参数作为临时参数\n#         weightz = copy.deepcopy(temp_weights)\n        dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程\n        loss = inner_train(dataloader1, learner, temp_weights, self.criterion) / self.batch_size\n        g = torch.autograd.grad(loss, learner.parameters(), create_graph=True)\n\n        g = tuple(g[i] if i != 0 else torch.zeros(g[0].shape, requires_grad=True) for i in range(len(g)))\n        \n#         temp_weights = dict((name, param - self.alpha * g) for ((name, param), g) in zip(temp_weights.items(), g))\n        \n        for ((name, param), g_tuple) in zip(temp_weights.items(), g):\n            # 对梯度元组中的每个张量进行设备移动，并计算新的参数值\n            new_param = param.to(device) - self.alpha * g_tuple.to(device)\n            temp_weights[name] = new_param\n        # 测试weight有没有变化\n#         for (name, param) in temp_weights.items():\n#             print(temp_weights[name] == weightz[name])    \n        learner.load_state_dict(temp_weights)\n        \n        dataloader2 = DataLoader(\n                dataset_loader2,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        metaloss = inner_train(dataloader2, learner, temp_weights, self.criterion) / self.batch_size\n        return metaloss ,learner\n\n    def outer_loop(self, num_epochs):  # epoch 500\n        total_loss = 0\n        for epoch in range(1, num_epochs + 1):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()  # 从任务分布中采样一个元任务\n            grads = []\n            for i in tasks:\n                task = DataTask(i)\n                metaloss, learner = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                g = torch.autograd.grad(metaloss, learner.parameters(), create_graph=True)\n\n                #zero the gradients\n                g = tuple(g[i] for i in range(len(g)))\n\n                meta_grads = {name:g for ((name, _), g) in zip(learner.named_parameters(), g)}\n\n                #pass the meta-task gradients to the \n                grads.append(meta_grads)\n                metaloss_sum += metaloss  # mete_loss求和\n            dataset1,dataset2 = task.sample_data(size=20)  # 从任务中采样数据集 D\n            dataloader = DataLoader(\n                    dataset1,\n                    batch_size=10,\n                    shuffle=True,\n                    num_workers=1,\n                    drop_last=False)\n            loss = inner_train(dataloader, self.net, grads, self.criterion) / 10\n            loss.backward(retain_graph=True)\n\n            # Unpack the gradients from dictionary of meta-gradients\n            gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n\n            hooks = []\n            for(k,v) in self.net.named_parameters():\n                def get_closure():\n                    key = k\n                    def replace_grad(grad):\n                        return gradients[key]\n                    return replace_grad\n                hooks.append(v.register_hook(get_closure()))\n            torch.optim.Adam(self.net.parameters(), lr=self.beta).zero_grad()\n            loss.backward()\n\n            torch.optim.Adam(self.net.parameters(), lr=self.beta).step()\n\n            for h in hooks:\n                h.remove()\n\n            self.models.append(copy.deepcopy(self.net.state_dict()))\n\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'lstm'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'lstm':\n    net = lstm_test(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n# print(list(net.parameters()))\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=300)\n\n\nmaml.outer_loop(num_epochs=5000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n# print(list(net.parameters()))\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn_model(input_size = his_horizon*3)\nelif model_type == 'lstm':\n    dummy_net = lstm_test(input_size = 3, lstm_layers = 1)\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=5\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.Adam\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n        opt.zero_grad()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n        opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T00:46:07.258848Z","iopub.execute_input":"2024-03-31T00:46:07.259664Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda:0\n1/5000. loss: 0.001435397503276666\n2/5000. loss: 0.0011475549545139074\n3/5000. loss: 0.00138047244399786\n4/5000. loss: 0.0013754662747184436\n5/5000. loss: 0.002124756264189879\n6/5000. loss: 0.002204833241800467\n7/5000. loss: 0.002136906608939171\n8/5000. loss: 0.0021589193493127823\n9/5000. loss: 0.001954034281273683\n10/5000. loss: 0.0018329080194234848\n11/5000. loss: 0.0020344884445269904\n12/5000. loss: 0.002271635147432486\n13/5000. loss: 0.001988455497970184\n14/5000. loss: 0.002149212329337994\n15/5000. loss: 0.0012424499727785587\n16/5000. loss: 0.0024165508026878038\n17/5000. loss: 0.0012527085685481627\n18/5000. loss: 0.0013146384929617245\n19/5000. loss: 0.002215956337749958\n20/5000. loss: 0.0017124880105257034\n21/5000. loss: 0.0015735048800706863\n22/5000. loss: 0.001778071125348409\n23/5000. loss: 0.0012910581814746063\n24/5000. loss: 0.0015506517762939136\n25/5000. loss: 0.001510181153813998\n26/5000. loss: 0.0016583927596608798\n27/5000. loss: 0.0020579624300201735\n28/5000. loss: 0.00151081383228302\n29/5000. loss: 0.0014616744592785835\n30/5000. loss: 0.0008891841862350702\n31/5000. loss: 0.0013686465099453926\n32/5000. loss: 0.001432351612796386\n33/5000. loss: 0.0019302054618795712\n34/5000. loss: 0.0007578710404535135\n35/5000. loss: 0.001695140264928341\n36/5000. loss: 0.0015036522721250851\n37/5000. loss: 0.0011485995880017679\n38/5000. loss: 0.0011870671684543292\n39/5000. loss: 0.0020684621607263884\n40/5000. loss: 0.002146949991583824\n41/5000. loss: 0.0017846332242091496\n42/5000. loss: 0.0015289631361762683\n43/5000. loss: 0.0012108038645237684\n44/5000. loss: 0.0012493463388333719\n45/5000. loss: 0.0010595975909382105\n46/5000. loss: 0.0014260027868052323\n47/5000. loss: 0.0018608076497912407\n48/5000. loss: 0.0017010831894973915\n49/5000. loss: 0.0017982575421531994\n50/5000. loss: 0.0011512764419118564\n51/5000. loss: 0.002237008884549141\n52/5000. loss: 0.0017998353578150272\n53/5000. loss: 0.0019682361744344234\n54/5000. loss: 0.0007068435661494732\n55/5000. loss: 0.0015616392095883687\n56/5000. loss: 0.0010605931747704744\n57/5000. loss: 0.0007309893456598123\n58/5000. loss: 0.0014155738366146882\n59/5000. loss: 0.0013907095417380333\n60/5000. loss: 0.0011431243425856035\n61/5000. loss: 0.0007869009083757798\n62/5000. loss: 0.002052716755618652\n63/5000. loss: 0.0008813493574659029\n64/5000. loss: 0.0011374771129339933\n65/5000. loss: 0.002098296924183766\n66/5000. loss: 0.0015058231850465138\n67/5000. loss: 0.001973648245135943\n68/5000. loss: 0.0008626645430922508\n69/5000. loss: 0.0013704315448800723\n70/5000. loss: 0.0008183961423734824\n71/5000. loss: 0.0014485685775677364\n72/5000. loss: 0.0014019898759822051\n73/5000. loss: 0.001991329714655876\n74/5000. loss: 0.0020866532189150653\n75/5000. loss: 0.001448382002611955\n76/5000. loss: 0.0014568553306162357\n77/5000. loss: 0.0015584599847594898\n78/5000. loss: 0.0010695566112796466\n79/5000. loss: 0.0016210381872951984\n80/5000. loss: 0.001921716146171093\n81/5000. loss: 0.0021736910566687584\n82/5000. loss: 0.000825835857540369\n83/5000. loss: 0.0008385463152080774\n84/5000. loss: 0.0016624474277098973\n85/5000. loss: 0.0018573820901413758\n86/5000. loss: 0.0018164664506912231\n87/5000. loss: 0.0008381918693582217\n88/5000. loss: 0.0010590610715250175\n89/5000. loss: 0.001581253328671058\n90/5000. loss: 0.0017562288170059521\n91/5000. loss: 0.001776234246790409\n92/5000. loss: 0.002092310848335425\n93/5000. loss: 0.0016956847781936328\n94/5000. loss: 0.0011201199765006702\n95/5000. loss: 0.001019572140648961\n96/5000. loss: 0.0014811063495775063\n97/5000. loss: 0.002094703105588754\n98/5000. loss: 0.0011145117071767647\n99/5000. loss: 0.002238948829472065\n100/5000. loss: 0.001341133068005244\n101/5000. loss: 0.0013833694780866306\n102/5000. loss: 0.0014080793286363285\n103/5000. loss: 0.0017846423822144668\n104/5000. loss: 0.001116510946303606\n105/5000. loss: 0.0022305333986878395\n106/5000. loss: 0.0008688884942481915\n107/5000. loss: 0.0020082108676433563\n108/5000. loss: 0.0011710300265500944\n109/5000. loss: 0.00159828985730807\n110/5000. loss: 0.0018745133032401402\n111/5000. loss: 0.0010999211420615513\n112/5000. loss: 0.0022324112554391227\n113/5000. loss: 0.0007965038530528545\n114/5000. loss: 0.0019305208697915077\n115/5000. loss: 0.0009719206330676874\n116/5000. loss: 0.001567314223696788\n117/5000. loss: 0.0011094561778008938\n118/5000. loss: 0.0022543038552006087\n119/5000. loss: 0.0021338158597548804\n120/5000. loss: 0.0013474527125557263\n121/5000. loss: 0.001368298816184203\n122/5000. loss: 0.0013580598557988803\n123/5000. loss: 0.0017573692214985688\n124/5000. loss: 0.0012769273792703946\n125/5000. loss: 0.0013507744297385216\n126/5000. loss: 0.0013069737081726391\n127/5000. loss: 0.00209438723201553\n128/5000. loss: 0.00078915199264884\n129/5000. loss: 0.0014104767081638177\n130/5000. loss: 0.0012303524029751618\n131/5000. loss: 0.0011594418125847976\n132/5000. loss: 0.0018293432270487149\n133/5000. loss: 0.0020195900773008666\n134/5000. loss: 0.001657121970007817\n135/5000. loss: 0.0014419762107233207\n136/5000. loss: 0.0020260331220924854\n137/5000. loss: 0.0016477457247674465\n138/5000. loss: 0.0010008242291708787\n139/5000. loss: 0.0012002494186162949\n140/5000. loss: 0.0007189168439557155\n141/5000. loss: 0.0019973510255416236\n142/5000. loss: 0.0014009270817041397\n143/5000. loss: 0.0013104965910315514\n144/5000. loss: 0.001279769620547692\n145/5000. loss: 0.0016298874591787655\n146/5000. loss: 0.0009473960381001234\n147/5000. loss: 0.002007594952980677\n148/5000. loss: 0.0013691784503559272\n149/5000. loss: 0.0021885656751692295\n150/5000. loss: 0.0012688807522257168\n151/5000. loss: 0.0019044339035948117\n152/5000. loss: 0.0013738293200731277\n153/5000. loss: 0.0015436058553556602\n154/5000. loss: 0.0018560569733381271\n155/5000. loss: 0.002074537022660176\n156/5000. loss: 0.0012079422983030479\n157/5000. loss: 0.001306180376559496\n158/5000. loss: 0.0012684798178573449\n159/5000. loss: 0.0014252960681915283\n160/5000. loss: 0.0014041235360006492\n161/5000. loss: 0.0013709412887692451\n162/5000. loss: 0.0017461736376086872\n163/5000. loss: 0.0020774475609262786\n164/5000. loss: 0.0014916295185685158\n165/5000. loss: 0.0021363707880179086\n166/5000. loss: 0.0010122041373203199\n167/5000. loss: 0.0020578651068111262\n168/5000. loss: 0.001157197558010618\n169/5000. loss: 0.0015010667654375236\n170/5000. loss: 0.0021089004973570504\n171/5000. loss: 0.0015780267616113026\n172/5000. loss: 0.0020734270413716636\n173/5000. loss: 0.0013242258379856746\n174/5000. loss: 0.0014488110318779945\n175/5000. loss: 0.0017231542927523453\n176/5000. loss: 0.0021849777549505234\n177/5000. loss: 0.0013561975210905075\n178/5000. loss: 0.0018057130898038547\n179/5000. loss: 0.0021226045986016593\n180/5000. loss: 0.0013563999285300572\n181/5000. loss: 0.0008473320243259271\n182/5000. loss: 0.0019519465665022533\n183/5000. loss: 0.0013642939738929272\n184/5000. loss: 0.0012898984520385663\n185/5000. loss: 0.002139465572933356\n186/5000. loss: 0.0013248842830459278\n187/5000. loss: 0.002143841547270616\n188/5000. loss: 0.0007557825495799383\n189/5000. loss: 0.001396024910112222\n190/5000. loss: 0.0014396728947758675\n191/5000. loss: 0.0007341591020425161\n192/5000. loss: 0.002185581251978874\n193/5000. loss: 0.0011840644292533398\n194/5000. loss: 0.0020905261238416037\n195/5000. loss: 0.0013034967705607414\n196/5000. loss: 0.0010952276643365622\n197/5000. loss: 0.001409259159117937\n198/5000. loss: 0.0016648873376349609\n199/5000. loss: 0.0013516855736573536\n200/5000. loss: 0.0014615982460478942\n201/5000. loss: 0.0011626843673487504\n","output_type":"stream"}]},{"cell_type":"code","source":"# MAML-跟车  version2   内循环使用adam\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport copy\n\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        # 根据索引划分数据集\n        data_indices =shuffled_indices[:data_size] # 测试 (1881)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_indices = shuffled_indices[:data_ratio]\n    return data[data_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# 定义模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(OrderedDict([\n            ('l1',nn.Linear(input_size,256)),\n            ('relu1',nn.ReLU()),\n            ('l2',nn.Linear(256,256)),\n            ('relu2',nn.ReLU()),\n            ('l3',nn.Linear(256,1))\n        ]))\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def argforward(self,x,weights): \n        x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n        x=F.relu(x)\n        x=F.linear(x,weights[2],weights[3])\n        x=F.relu(x)\n        x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion):\n    # 初始化变量\n    train_loss_his = []  # 训练损失\n    best_train_loss = None  # 最佳训练损失\n    best_epoch = None  # 最佳验证损失时的轮次\n    # 训练过程\n    train_losses = []  # 记录每个epoch的训练损失\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,20,3)，y->(149,20)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n#         print(T,B,d)  # 149  20  3\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net.argforward(x, temp_weights).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        return loss\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n#             print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD_DAS1_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD_DAS2_data\n            Ts = 0.1\n            max_len = 150\n#             print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n#             print('--Waymo--')\n        meta_data_set = split_train(meta_data, size)\n#         print(data_set[0].shape)\n        dataset_loader = ImitationCarFolData(meta_data_set, max_len = max_len)\n        return dataset_loader\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 元数据集的数量\n        self.n = n  # 每次获取数据集的数量\n    # 随机生成一个跟车数据集\n    def sample_task(self):\n        # 从1到5中随机选择3个不重复的数\n        data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k):  # (net,alpha=0.01,beta=0.001,tasks=data_tasks,k=100)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.tasks = tasks  # 任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_optimiser = torch.optim.Adam(self.weights, self.beta)  # 外循环优化器（Adam）\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.plot_every = 1  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.batch_size = self.k\n\n    def inner_loop(self, task):\n        # 内循环更新参数，用于计算元学习损失\n#         temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        temp_weights = self.weights\n        inner_optim = torch.optim.Adam(temp_weights, self.alpha)\n        dataset_loader = task.sample_data(size=self.k)  # 从任务中采样数据集 D\n        dataloader = DataLoader(\n                dataset_loader,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        # 训练过程  第一步\n        loss = inner_train(dataloader, self.net, temp_weights, self.criterion) / self.k\n        # 优化过程\n        inner_optim.zero_grad()\n        grads = torch.autograd.grad(loss, temp_weights)  # 计算损失对参数的梯度\n        for w, g in zip(temp_weights, grads):\n            w.grad = g\n        inner_optim.step()\n#         temp_weights = [w - self.alpha * g for w, g in zip(temp_weights, grads)]  # 临时参数更新 梯度下降\n        # 第二步\n        metaloss = inner_train(dataloader, self.net, temp_weights, self.criterion) / self.k\n        return metaloss\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        total_loss = 0\n        for epoch in tqdm(range(1, num_epochs + 1)):\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task()  # 从任务分布中采样一个元任务\n            for i in tasks:\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights)  # 计算元学习损失对参数的梯度\n            # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n            if epoch % self.plot_every == 0:\n                self.meta_losses.append(total_loss / self.plot_every)\n                total_loss = 0\n\ndataset = 'NGSIM'\nmodel_type = 'nn'\nsave = f'{model_type}_{dataset}.pt' # 保存模型文件\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.0001,beta=0.0001,tasks=data_tasks,k=150)\n\n\nmaml.outer_loop(num_epochs=5000)\nplt.plot(maml.meta_losses)\nplt.show()\nprint('----------------------')\n\n\n# Train\nTs = 0.1\nmax_len = 150\nog_net = maml.net.net\n# 创建一个与原始网络结构相同的虚拟网络\nif model_type == 'nn':\n    dummy_net = nn.Sequential(OrderedDict([\n        ('l1', nn.Linear(his_horizon*3,256)),\n        ('relu1', nn.ReLU()),\n        ('l2', nn.Linear(256,256)),\n        ('relu2', nn.ReLU()),\n        ('l3', nn.Linear(256,1))\n    ]))\ndummy_net=dummy_net.to(device)\n# 加载原始网络的权重\ndummy_net.load_state_dict(og_net.state_dict())\n# 进行迭代，每次更新虚拟网络的参数\nnum_shots=10\nlr = 0.01\nloss_fn=nn.MSELoss()\noptim=torch.optim.SGD\nopt=optim(dummy_net.parameters(),lr=lr)\n\n\n# 数据集划分\ndef split_data(data,data_ratio):\n    if data_ratio < 0:\n        # 如果小于零，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n    else:\n        # 如果大于等于零，则data_ratio为获取数据集中的数量\n        data_size = data_ratio\n    return data[:data_size]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n# 获取数据集的数量\nK=40\ndataset_train = split_data(NGSIM_train, K)\ndataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len)\ntrain_loader = DataLoader(\n            dataset_loader_train,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_val = NGSIM_val\ndataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len)\nval_loader = DataLoader(\n            dataset_loader_val,\n            batch_size=10,\n            shuffle=True,\n            num_workers=1,\n            drop_last=True)\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=10,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\n# 训练过程\nfor epoch in tqdm(range(num_shots)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    jerk_val = 0\n    dummy_net.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(374,32,3)，y->(374,32)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            # 根据his_horizon个数据预测加速度\n            acc_pre = dummy_net(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = loss_fn(y_pre, y_label)\n        opt.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        opt.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    dummy_net.eval()\n    error_list = []\n    for i, item in enumerate(val_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        # lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n            if model_type == 'nn':\n              x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n            acc_pre = dummy_net(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = loss_fn(y_pre, y_label)\n        # jerk_val = np.mean(np.abs(np.diff(torch.tensor(y_pre))/Ts))/batch_size\n        # print(\"Val jerk：\", jerk_val)\n        opt.zero_grad()\n        torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25)\n        opt.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    train_val_error = (mean_validation_error + train_loss)/2\n    if best_validation_loss is None or best_validation_loss > train_val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = train_val_error\n        # save the best model\n    with open(save, 'wb') as f:\n        torch.save(dummy_net, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\n# print(list(dummy_net.parameters()))\nprint(\"Epoch:{0}| Best Train+Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\nplt.show()\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nmodel = dummy_net\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device) # x(374,32,3)  y(374,32)\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10,32,3)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测的车距\n    spacing_pre = x_data[..., 0] # (374,32)\n    # 真实的车距\n    spacing_obs = x_data_orig[..., 0] # (374,32)\n\n    error = loss_fn(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.1\n# max_len = 150\ndataset_test = NGSIM_test\ndataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len)\ntest_loader = DataLoader(\n            dataset_loader_test,\n            batch_size=1,\n            shuffle=False,\n            num_workers=1,\n            drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\njerk_set = []\n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d)\n        if model_type == 'nn':\n          x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = loss_fn(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T01:16:16.519658Z","iopub.execute_input":"2024-02-25T01:16:16.519947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 原跟车训练测试过程\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom collections import OrderedDict\nimport os\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom functools import partial\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\nimport learn2learn as l2l\nfrom pyts.image import GramianAngularField\nfrom pyts.image import MarkovTransitionField\nfrom PIL import Image\nimport warnings\n# 忽略特定类型的警告\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/to_img_log\")\n\nACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD_DAS2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# car_following_data = HighD_data\n# the data shape format (number of car following event, 4 dimension data, lenth of each dimension)数据形状格式（跟车事件的车辆数、4 维数据、各维数据的长度）\n# 4 dimension data= [spacing, subject_vehicle_speed, relative_speed, leading_vehicle_speed]4 维数据= [车距、后车车速、相对车速、前车车速］\n# print(car_following_data)\n# print(car_following_data.shape)   # HighD(12541, 4, 375)\n\n# 保存日志\ndef get_logger(filename, verbosity=1, name=None):\n    # 设置不同verbosity对应的日志级别\n    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n    # 设置日志输出格式\n    formatter = logging.Formatter(\n        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level_dict[verbosity])\n    # 创建文件处理器，将日志写入文件\n    fh = logging.FileHandler(filename, \"w\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # 创建控制台处理器，将日志输出到控制台\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n# 日志保存路径\n# logger = get_logger('/kaggle/working/to_img_main.log')\n\n\n# split the date into train, validation, test 数据集划分\ndef split_train(data,test_ratio,val_ratio):\n    # np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    # 根据输入百分比（test_ratio）计算测试集大小\n    test_set_size=int(len(data)*test_ratio)\n    # 根据输入百分比（val_ratio）计算验证集大小\n    val_number=int(len(data)*(test_ratio+val_ratio))\n    # 根据索引划分数据集\n    test_indices =shuffled_indices[:test_set_size] # 测试 (1881)\n    val_indices=shuffled_indices[test_set_size:val_number] # 验证 (1881)\n    train_indices=shuffled_indices[val_number:] # 训练 (8779)\n    return data[train_indices],data[test_indices],data[val_indices]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# Define NN and lstm models\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\n\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, split: str, max_len = max_len):\n        if split == 'train':\n            self.data = train_data\n        elif split == 'test':\n            self.data = test_data\n        elif split == 'validation':\n            self.data = val_data\n        self.max_len = max_len # Max length of a car following event.\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / Ts  # (max_len - 1)\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\nclass CNN(nn.Module):\n    def __init__(self, output_size, in_channels=3, hid_dim=60, embedding_size=None):\n        super(CNN, self).__init__()\n\n        if embedding_size is None:\n            embedding_size = hid_dim\n\n        self.encoder = l2l.vision.models.ConvBase(channels=in_channels, hidden=hid_dim, max_pool=True, layers=3,\n                                                  max_pool_factor=1)\n#         self.encoder = torch.nn.Sequential(\n#             nn.Conv2d(3,8,3,1,1),\n#             nn.BatchNorm2d(8),\n#             nn.ReLU(),\n#             nn.Dropout(0.3)\n#         )\n\n        self.features = torch.nn.Sequential(\n            self.encoder,\n            nn.Dropout(0.2),\n            nn.Flatten(),\n        )\n        self.classifier = torch.nn.Linear(\n            embedding_size,\n            output_size,\n            bias=True,\n        )\n#         self.classifier.weight.data.normal_()\n#         self.classifier.bias.data.mul_(0.0)\n\n        self.hidden_size = hid_dim\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n# Train\n# 获取划分数据集\n# (8779,4,375)，(1881,4,375)，(1881,4,375)\n# train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# train_data, test_data, val_data = split_train(car_following_data,0.8,0.1)\ntrain_data = SPMD1_train\ntest_data = SPMD1_test\nval_data = SPMD1_val\n# train_data = NGSIM_train\n# test_data = NGSIM_test\n# val_data = NGSIM_val\nprint(train_data.shape, test_data.shape, val_data.shape)\ndataset = 'SPMD1'\nmodel_type = 'cnn'\nbatch_size = 32\ntotal_epochs = 10\n# 创建训练集 DataLoader\ntrain_dataset = ImitationCarFolData(split = 'train', max_len = max_len)\ntrain_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建验证集 DataLoader\nvalidation_dataset = ImitationCarFolData(split = 'validation', max_len = max_len)\nvalidation_loader = DataLoader(\n        validation_dataset ,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n        drop_last=True)\n# 创建测试集 DataLoader\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=10,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n\n# 使用训练设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nhis_horizon = 10 # number of time steps as history data\nlr = 1e-3 # learning rate\n# lr = 0.001\nmtf = MarkovTransitionField(image_size=his_horizon)\ngasf = GramianAngularField(method='summation')\ngadf = GramianAngularField(method='difference')\n\n# 定义保存文件的文件夹路径\nsave_folder = '/kaggle/input/model-state'\nsave = f'img_{model_type}_{dataset}.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n# 定义文件保存路径\nsave_path = os.path.join(save_folder, save)\n\n# 根据名称定义模型\nif model_type == 'cnn':\n    model = CNN(output_size = 1).to(device)\nmodel_state = list(model.parameters())\n# print(model_state)\n# 定义优化器和损失函数\nmodel_optim = optim.Adam(model.parameters(), lr=lr, weight_decay=3e-4)\ncriterion = nn.MSELoss()\n\n# 初始化变量\ntrain_loss_his = [] # 训练损失\ntest_error_his = [] # 测试误差\nvalidation_error_his = [] # 验证误差\nbest_train_loss = None # 最佳训练损失\nbest_validation_loss = None # 最佳验证损失\nbest_epoch = None # 最佳验证损失时的轮次\n\nprint(\"----\")\n# 训练过程\nfor epoch in tqdm(range(total_epochs)):\n    train_losses = [] # 记录每个epoch的训练损失\n    validation_losses = [] # 记录每个epoch的验证损失\n    model.train()\n    # 遍历数据集\n    for i, item in enumerate(train_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(32,374)连接，转换成(3,32,374)\n        x_data = torch.stack(x_data)\n        if model_type == 'cnn':\n            space = x_data[0].clone().detach() # (bitch_size,149)\n            sv = x_data[1].clone().detach()\n            relSpeed = x_data[2].clone().detach()\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,bs,3)，y->(149,bs)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,bs)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,bs)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, bs, 3)\n            if model_type == 'cnn':\n                a = space[:, frame-his_horizon:frame] # (bs, 10)\n                b = sv[:, frame-his_horizon:frame]\n                c = relSpeed[:, frame-his_horizon:frame]\n                # 进行转换\n                # space_change = torch.tensor(mtf.transform(a))\n                # sv_change = torch.tensor(mtf.transform(b))\n                # relS_change = torch.tensor(mtf.transform(c))\n                # space_change = torch.tensor(gasf.transform(a))\n                # sv_change = torch.tensor(gasf.transform(b))\n                # relS_change = torch.tensor(gasf.transform(c))\n                space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n                sv_change = torch.tensor(gadf.transform(b))\n                relS_change = torch.tensor(gadf.transform(c))\n                x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = model(x).squeeze() # (32)\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n        model_optim.step()\n\n        train_losses.append(loss.item())\n    # 计算本轮平均损失\n    train_loss = np.mean(train_losses)\n\n    train_loss_his.append(train_loss)\n    print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n    # 验证过程\n    model.eval()\n    error_list = []\n    for i, item in enumerate(validation_loader):\n        x_data, y_data = item['inputs'], item['label']\n        # Put T into the first dimension, B, T, d -> T, B, d\n        x_data = torch.stack(x_data)\n        if model_type == 'cnn':\n            space = x_data[0].clone().detach() # (bitch_size,149)\n            sv = x_data[1].clone().detach()\n            relSpeed = x_data[2].clone().detach()\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        # x->(149,bs,3)，y->(149,bs)\n        x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n        T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        # save a copy of x_data\n        x_data_orig = x_data.clone().detach()\n        y_label = y_data[his_horizon:]\n        for frame in range(his_horizon, T):\n            x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'cnn':\n                a = space[:, frame-his_horizon:frame] # (bs, 10)\n                b = sv[:, frame-his_horizon:frame]\n                c = relSpeed[:, frame-his_horizon:frame]\n                # 进行转换\n                # space_change = torch.tensor(mtf.transform(a))\n                # sv_change = torch.tensor(mtf.transform(b))\n                # relS_change = torch.tensor(mtf.transform(c))\n                # space_change = torch.tensor(gasf.transform(a))\n                # sv_change = torch.tensor(gasf.transform(b))\n                # relS_change = torch.tensor(gasf.transform(c))\n                space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n                sv_change = torch.tensor(gadf.transform(b))\n                relS_change = torch.tensor(gadf.transform(c))\n                x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            acc_pre = model(x).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        validation_loss = criterion(y_pre, y_label)\n        model_optim.zero_grad()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n        model_optim.step()\n        validation_losses.append(validation_loss.item())\n    # 计算平均验证误差\n    mean_validation_error = np.mean(validation_losses)\n    val_error = mean_validation_error\n    if best_validation_loss is None or best_validation_loss > val_error:\n        best_epoch = epoch + 1\n        best_validation_loss = val_error\n        # save the best model\n        with open(save, 'wb') as f:\n            torch.save(model, f)\n\n    validation_error_his.append(mean_validation_error)\n    print(\"Epoch: {0}| Validation error: {1:.7f}\".format(epoch + 1, mean_validation_error))\n\nprint(\"Epoch:{0}| Best Val Loss：{1:.7f}\".format(best_epoch, best_validation_loss))\n# plt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(train_loss_his, label = 'train_loss')\nplt.plot(validation_error_his, label = 'val_loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('loss (MSE for acceleration prediction)')\n# plt.savefig(os.path.join(plt_path, 'img_train_val_loss.png'))\nplt.show()\nplt.close()\n\n\n# 用该数据集训练得到的模型去对其他数据集进行测试\n# Ts = 0.04\n# max_len = 375\n# # car_following_data = NGSIM_data\n# # print(car_following_data.shape)\n# # train_data, test_data, val_data = split_train(car_following_data,0.15,0.15)\n# test_data = HighD_test\n# # 创建测试集 DataLoader\n# test_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\n# test_loader = DataLoader(\n#         test_dataset,\n#         batch_size=batch_size,\n#         shuffle=False,\n#         num_workers=1,\n#         drop_last=True)\n\n\n# Testing, closed-loop prediction\n# Load the best model saved\nwith open(f'{save}', 'rb') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\nerror_list = []\n\nfor i, item in enumerate(test_loader):\n    x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data) # (3,32,374)\n    if model_type == 'cnn':\n        space = x_data[0].clone().detach() # (bitch_size,149)\n        sv = x_data[1].clone().detach()\n        relSpeed = x_data[2].clone().detach()\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    # x->(149,bs,3)，y->(149,bs)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n    # print(T,B,d)  # 149  20  3\n    # 获取前车速度\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1) # (375,32)\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach() # (374,32,3)\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n        if model_type == 'cnn':\n            a = space[:, frame-his_horizon:frame] # (bs, 10)\n            b = sv[:, frame-his_horizon:frame]\n            c = relSpeed[:, frame-his_horizon:frame]\n            # 进行转换\n            # space_change = torch.tensor(mtf.transform(a))\n            # sv_change = torch.tensor(mtf.transform(b))\n            # relS_change = torch.tensor(mtf.transform(c))\n            # space_change = torch.tensor(gasf.transform(a))\n            # sv_change = torch.tensor(gasf.transform(b))\n            # relS_change = torch.tensor(gasf.transform(c))\n            space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n            sv_change = torch.tensor(gadf.transform(b))\n            relS_change = torch.tensor(gadf.transform(c))\n            x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n\n        acc_pre = model(x).squeeze() # (32,1)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            # 根据当前速度和加速度计算下一时间速度\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts # (32)\n            # 计算下一时间速度的相对速度\n            delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n            # 该时刻真实相对速度\n            delta_v = x_data[frame, :, -1] # (32)\n            # 通过两车车距加上相对位移得到下一时间段车距 ？\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2 # (32)\n\n            # update 根据计算得到的值，更新下一时间的值\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n\n    error = criterion(spacing_pre, spacing_obs).item()\n    error_list.append(error)\nmodel.train()\n# 计算MSE - 间距均方误差（Mean square error of spacing）\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n\n\n# plot for one event\nevent_no = 2\nplt.plot(spacing_obs[:, event_no].cpu().detach().numpy(), label = 'GT')\nplt.plot(spacing_pre[:, event_no].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('Spacing (m)')\n# plt.savefig(os.path.join(plt_path, 'img_Spacing.png'))\nplt.show()\nplt.close()\n\n# speed part\nplt.figure()\nplt.plot(x_data_orig[:, event_no, 1].cpu().detach().numpy(), label = 'GT')\nplt.plot(x_data[:, event_no, 1].cpu().detach().numpy(), '--', label = 'prediction')\nplt.legend()\nplt.xlabel(f'Time ({Ts}s)')\nplt.ylabel('SV speed (m/s)')\n# plt.savefig(os.path.join(plt_path, 'img_Speed.png'))\nplt.show()\nplt.close()\n\n\n# Updated for the four key elvaluation metrics.  Note that: The loss design in FollowNet is for the reduction of the mse of spacing.\n# Please note that The TTC results presented in the table of the paper indicate the average TTC value calculated.\n# However, a drawback of this approach is that if the relative velocity in just one frame is extremely small, it will result in a significantly large computed TTC when reproducing paper.\n# To address this issue, it may be beneficial to consider plotting the distribution of TTC as a way to present the results.\n# 间距均方误差（Mean square error of spacing (MSE))\n# 碰撞率（Collision rate）\n# 驾驶舒适度指标(颠簸)（Driving comfort metric (jerk)）\n# 碰撞时间(TTC)指标（Time-To-Collision (TTC) metric）\n# for test\n# 创建测试集 DataLoader\n# Ts = 0.04\n# max_len = 375\nbatch_size = 1\ntest_dataset = ImitationCarFolData(split = 'test', max_len = max_len)\ntest_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=1,\n        drop_last=True)\n# 记录跟车事件数和发生碰撞事件数\ncount = 0\ncol = 0\nmin_error = 10000\nmin_index = 0\nmax_error = 0\nmax_index = 0\njerk_set = [] \n# TTC_set = []\nminimum_ttc_set = []\nmodel.eval()\n# ttc = [1, 2, 3, 4, 5, 6, 7, 8, -1, -3, 7]\ndef calculate_safety(ttc):\n    # Remove any negative values (assuming TTC cannot be negative)\n    # ttc = [x for x in ttc if x >= 0]\n    # 计算ttc中最小值\n    minimum_ttc = min(ttc)\n    # 计算ttc中第1百分位的值\n    first_percentile_ttc = np.percentile(ttc, 1)\n    # print(1111)\n    return minimum_ttc, first_percentile_ttc\n\n# # Example usage\n# minimum_ttc, first_percentile_ttc = calculate_safety(ttc)\n# print(\"Minimum TTC:\", minimum_ttc)\n# print(\"1st Percentile TTC:\", first_percentile_ttc)\n\nfor i, item in tqdm(enumerate(test_loader)):\n\n    # jerk\n    # jerk = np.diff(sv)/Ts\n    acc_single = []\n    TTC_single = []\n\n    count += 1\n    x_data, y_data = item['inputs'], item['label']\n    # Put T into the first dimension, B, T, d -> T, B, d\n    x_data = torch.stack(x_data)\n    if model_type == 'cnn':\n        space = x_data[0].clone().detach() # (bitch_size,149)\n        sv = x_data[1].clone().detach()\n        relSpeed = x_data[2].clone().detach()\n    # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n    # x->(149,bs,3)，y->(149,bs)\n    x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n    T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n    # print(T,B,d)  # 149  20  3\n\n    lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n\n    # save a copy of x_data\n    x_data_orig = x_data.clone().detach()\n\n    for frame in range(his_horizon, T):\n        x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n        if model_type == 'cnn':\n            a = space[:, frame-his_horizon:frame] # (bs, 10)\n            b = sv[:, frame-his_horizon:frame]\n            c = relSpeed[:, frame-his_horizon:frame]\n            # 进行转换\n            # space_change = torch.tensor(mtf.transform(a))\n            # sv_change = torch.tensor(mtf.transform(b))\n            # relS_change = torch.tensor(mtf.transform(c))\n            # space_change = torch.tensor(gasf.transform(a))\n            # sv_change = torch.tensor(gasf.transform(b))\n            # relS_change = torch.tensor(gasf.transform(c))\n            space_change = torch.tensor(gadf.transform(a)) # (bs, 10, 10)\n            sv_change = torch.tensor(gadf.transform(b))\n            relS_change = torch.tensor(gadf.transform(c))\n            x = torch.stack([space_change, sv_change, relS_change], axis=1).float().to(device)\n        if model_type == 'nn':\n            x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n\n        acc_pre = model(x).squeeze()\n\n        acc_single.append(acc_pre)\n\n        # update next data\n        # 根据前一次计算得到的值，计算后面的值\n        if frame < T-1:\n            sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts\n            # print(sv_spd_)\n            # MyDevice = torch.device('cuda:0')\n            # 速度取值大于0.001的\n            sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n            delta_v_ = lv_spd[frame + 1] - sv_spd_\n            delta_v = x_data[frame, :, -1]\n            spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2\n\n            spacing = np.array(spacing_.cpu())\n            # 如果spacing<=0，表示发生碰撞\n            if float(np.array(spacing_.cpu())) <= 0:\n                col += 1\n                break\n            next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n            x_data[frame + 1] = next_frame_data\n        # 计算TTC\n        TTC_single.append(-spacing_ / delta_v_) # 正常情况(139)\n    # Mean absolute jerk\n    jerk =np.mean(np.abs(np.diff(torch.tensor(acc_single))/Ts))\n\n    # if np.sum(torch.tensor(TTC_single).cuda().cpu().numpy()>0) == 0:\n    #     continue\n    TTC_single = torch.tensor(TTC_single).to(device).cpu().numpy()\n    TTC_single = [x for x in TTC_single if x >= 0]\n    if len(TTC_single) > 0:\n        minimum_ttc, _ = calculate_safety(TTC_single)\n        minimum_ttc_set.append(minimum_ttc)  # 大小为测试数据总数减去发生了碰撞的事件\n\n    # Calculating spacing error for the closed-loop simulation\n    # 预测车距\n    spacing_pre = x_data[..., 0]\n    # 真实车距\n    spacing_obs = x_data_orig[..., 0]\n    # for index, (space_obs, space_pre) in enumerate(zip(spacing_obs[:, 0].cpu().detach().numpy(), spacing_pre[:, 0].cpu().detach().numpy())):\n    #     writer.add_scalars('NGSIM:{}/Spacing'.format(count), {'GT': space_obs, 'prediction': space_pre}, global_step=index)\n    # for index, (speed_obs, speed_pre) in enumerate(zip(x_data_orig[:, 0, 1].cpu().detach().numpy(), x_data[:, 0, 1].cpu().detach().numpy())):\n    #     writer.add_scalars('NGSIM:{}/Speed'.format(count), {'GT': speed_obs, 'prediction': speed_pre}, global_step=index)\n    # writer.close()\n\n    # plt.plot(np.array(spacing_obs.cpu()), label = 'GT')\n    # plt.plot(np.array(spacing_pre.cpu()), '--', label = 'Prediction')\n    # plt.legend()\n    # plt.xlabel(f'Time ({Ts}s)');\n    # plt.ylabel('Spacing (m)');\n    # plt.show()\n    error = criterion(spacing_pre[:(frame+1)], spacing_obs[:(frame+1)]).item()\n    error_list.append(error)\n    if error > max_error:\n        max_error = error\n        max_index = count\n    if error < min_error:\n        min_error = error\n        min_index = count\n\n    jerk_set.append(jerk)\n# 输出指标\nprint(\"index（{}）min_error：{}, index（{}）max_error：{}\".format(min_index, min_error, max_index, max_error))\n# logger.info(\"index（{}）min_error：{}, index（{}）max_error：{}\".format(min_index, min_error, max_index, max_error))\nprint(\"count=\",count,\"col=\",col,\"rate\",col/count,\"jerk\", np.mean(jerk_set), \"miniumu_ttc\", np.mean(minimum_ttc_set))\n# logger.info(\"count={}，col={}，rate={:.5f}%\".format(count, col, col/count*100))\n# logger.info(\"jerk={:.5f}，miniumu_ttc={:.5f}\".format(np.mean(jerk_set), np.mean(minimum_ttc_set)))\nmodel.train()\nmean_test_error = np.mean(error_list)\nprint('mean_test_error',mean_test_error)\n# logger.info(\"mean_test_error：{:.5f}\".format(mean_test_error))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:33:39.638721Z","iopub.execute_input":"2024-04-25T13:33:39.638995Z","iopub.status.idle":"2024-04-25T13:33:56.102746Z","shell.execute_reply.started":"2024-04-25T13:33:39.638970Z","shell.execute_reply":"2024-04-25T13:33:56.101533Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlearn2learn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01ml2l\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GramianAngularField\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkovTransitionField\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'learn2learn'"],"ename":"ModuleNotFoundError","evalue":"No module named 'learn2learn'","output_type":"error"}]},{"cell_type":"code","source":"import numbers\nfrom copy import copy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport random\n\n\ndef extract_top_level_dict(current_dict):\n    \"\"\"\n    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n    :param value: Param value\n    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n    :return: A dictionary graph of the params already added to the graph.\n    \"\"\"\n    output_dict = dict()\n    for key in current_dict.keys():\n        name = key.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"block_dict.\", \"\")\n        name = name.replace(\"module-\", \"\")\n        top_level = name.split(\".\")[0]\n        sub_level = \".\".join(name.split(\".\")[1:])\n\n        if top_level not in output_dict:\n            if sub_level == \"\":\n                output_dict[top_level] = current_dict[key]\n            else:\n                output_dict[top_level] = {sub_level: current_dict[key]}\n        else:\n            new_item = {key: value for key, value in output_dict[top_level].items()}\n            new_item[sub_level] = current_dict[key]\n            output_dict[top_level] = new_item\n\n    #print(current_dict.keys(), output_dict.keys())\n    return output_dict\n\n\nclass MetaLinearLayer(nn.Module):\n    def __init__(self, input_shape, num_filters, use_bias):\n        \"\"\"\n        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n        learning setting.\n        :param input_shape: The shape of the input data, in the form (b, f)\n        :param num_filters: Number of output filters\n        :param use_bias: Whether to use biases or not.\n        \"\"\"\n        super(MetaLinearLayer, self).__init__()\n        b, c = input_shape\n\n        self.use_bias = use_bias\n        self.weights = nn.Parameter(torch.ones(num_filters, c))\n        nn.init.xavier_uniform_(self.weights)\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.zeros(num_filters))\n\n    def forward(self, x, params=None):\n        \"\"\"\n        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n        Otherwise passed params will be used to execute the function.\n        :param x: Input data batch, in the form (b, f)\n        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n        Otherwise the external are used.\n        :return: The result of the linear function.\n        \"\"\"\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n            if self.use_bias:\n                (weight, bias) = params[\"weights\"], params[\"bias\"]\n            else:\n                (weight) = params[\"weights\"]\n                bias = None\n        else:\n            pass\n            #print('no inner loop params', self)\n\n            if self.use_bias:\n                weight, bias = self.weights, self.bias\n            else:\n                weight = self.weights\n                bias = None\n        # print(x.shape)\n        out = F.linear(input=x, weight=weight, bias=bias)\n        return out\n\n\nclass MetaStepLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        super(MetaStepLossNetwork, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        out = x\n\n        self.linear1 = MetaLinearLayer(input_shape=self.input_shape,\n                                                    num_filters=self.input_dim, use_bias=True)\n\n        self.linear2 = MetaLinearLayer(input_shape=(1, self.input_dim),\n                                                    num_filters=1, use_bias=True)\n\n\n        out = self.linear1(out)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n    def forward(self, x, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n\n        linear1_params = None\n        linear2_params = None\n\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n\n            linear1_params = params['linear1']\n            linear2_params = params['linear2']\n\n        out = x\n        \n        out = self.linear1(out, linear1_params)\n        out = F.relu_(out)\n        out = self.linear2(out, linear2_params)\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass MetaLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        \"\"\"\n        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n        used at inference time. Enables inner loop optimization readily.\n        :param im_shape: The input image batch shape.\n        :param num_output_classes: The number of output classes of the network.\n        :param args: A named tuple containing the system's hyperparameters.\n        :param device: The device to run this on.\n        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n        be enabled. \n        \"\"\"\n        super(MetaLossNetwork, self).__init__()\n        \n        self.device = device\n        # self.args = args\n        self.notspi = notspi\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        self.layer_dict = nn.ModuleDict()\n        # ф\n        for i in range(self.num_steps): \n            self.layer_dict['step{}'.format(i)] = MetaStepLossNetwork(self.input_dim, notspi=self.notspi, device=self.device)\n\n            out = self.layer_dict['step{}'.format(i)](x)\n\n    def forward(self, x, num_step, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n        param_dict = dict()\n\n        if params is not None: \n            # params = {key: value[0] for key, value in params.items()}\n            param_dict = extract_top_level_dict(current_dict=params)\n            \n        for name, param in self.layer_dict.named_parameters():\n            path_bits = name.split(\".\")\n            layer_name = path_bits[0]\n            if layer_name not in param_dict:\n                param_dict[layer_name] = None\n\n            \n        out = x\n        \n        out = self.layer_dict['step{}'.format(num_step)](out, param_dict['step{}'.format(num_step)])\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass StepLossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(StepLossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n        output_dim = num_loss_net_layers * 2 * 2 # 2 for weight and bias, another 2 for multiplier and offset\n\n        self.linear1 = nn.Linear(input_dim, input_dim)\n        self.activation = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(input_dim, output_dim)\n\n        self.multiplier_bias = nn.Parameter(torch.zeros(output_dim // 2))\n        self.offset_bias = nn.Parameter(torch.zeros(output_dim // 2))\n\n    def forward(self, task_state, num_step, loss_params):\n        # ψ\n        out = self.linear1(task_state)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n        generated_multiplier, generated_offset = torch.chunk(out, chunks=2, dim=-1)\n\n        i = 0\n        updated_loss_weights = dict()\n        for key, val in loss_params.items():\n            if 'step{}'.format(num_step) in key:\n                updated_loss_weights[key] = (1 + self.multiplier_bias[i] * generated_multiplier[i]) * val + \\\n                                             self.offset_bias[i] * generated_offset[i]\n                i+=1\n\n        return updated_loss_weights\n\n\nclass LossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(LossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.loss_adapter = nn.ModuleList()\n        for i in range(self.num_steps): \n            self.loss_adapter.append(StepLossAdapter(input_dim, num_loss_net_layers, notspi=notspi, device=device))\n\n    def forward(self, task_state, num_step, loss_params):\n        return self.loss_adapter[num_step](task_state, num_step, loss_params)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nfrom torch import _weight_norm\nimport numpy as np\nfrom math import sqrt\nimport random\n\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim_in = input_dim\n        self.dim_k = input_dim//2\n        self.dim_v = input_dim//2\n        self.num_heads = num_heads\n        self.linear_q = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_k = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_v = nn.Linear(input_dim, input_dim, bias=True)\n        self._norm_fact = 1 / sqrt((input_dim//2) // num_heads)\n\n    def forward(self, x, param=None):\n        batch, n, dim_in = x.shape\n        assert dim_in == self.dim_in\n\n        nh = self.num_heads\n        dk = self.dim_k // nh  # dim_k of each head\n        dv = self.dim_v // nh  # dim_v of each head\n        if param == None:\n            q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            v = self.linear_v(x).reshape(batch, n, nh, dv*2).transpose(1, 2)  # (batch, nh, n, dv)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1]).reshape(batch, n, nh, dk).transpose(1, 2)\n            k = F.linear(x, weight=param[2], bias=param[3]).reshape(batch, n, nh, dk).transpose(1, 2)\n            v = F.linear(x, weight=param[4], bias=param[5]).reshape(batch, n, nh, dv*2).transpose(1, 2)\n        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact  # batch, nh, n, n\n        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n\n        att = torch.matmul(dist, v)  # batch, nh, n, dv\n        out = att.transpose(1, 2).reshape(batch, n, self.dim_in)  # batch, n, dim_v\n        \n        # out = att.reshape(att.shape[0], -1)\n\n        return out\n    \n\nclass conv1d(nn.Module):\n    def _init_(self):\n        super(conv1d, self).__init__()\n    \n    def forward(self, x, weight, bias, stride, padding, dilation):\n        return F.conv1d(x, weight, bias=bias, stride=stride, padding=padding, dilation=dilation)\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm1 = nn.BatchNorm1d(n_outputs)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        # self.relu1 = nn.Tanh()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm2 = nn.BatchNorm1d(n_outputs)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        # self.relu2 = nn.Tanh()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.batchnorm1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.batchnorm2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        # self.relu = nn.Tanh()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        self.kernel_size = kernel_size\n        self.dropout = dropout\n        self.num_inputs = num_inputs\n        self.num_channels = num_channels\n        layers = []\n        # self.vars_bn = nn.ParameterList()\n        num_levels = len(num_channels)\n        # num_levels = num_channels\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n        self.token = nn.Parameter(torch.ones(1, num_channels[-1]))\n        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=10, nhead= 5,\\\n        #                     dim_feedforward=10, batch_first=True)\n        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n        # self.features = nn.LSTM(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.features = nn.GRU(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.attention = TemporalAttention(num_channels[-1])\n        self.attention = MultiHeadSelfAttention(num_channels[-1])\n        self.mlp = nn.Sequential(\n            nn.Linear(num_channels[-1], num_channels[-1]),\n            # nn.LeakyReLU(negative_slope=0.01),\n            nn.ReLU(), \n            nn.Linear(num_channels[-1], 1)\n        )\n        # self.features = nn.Linear(num_channels[-1], num_channels[-1])\n        # self.decoder = nn.Linear(num_channels[-1], 1)\n\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=num_channels[i-1], nhead= 4,\\\n        #                     dim_feedforward=num_channels[i-1], batch_first=False)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        # self.tanh = nn.Tanh()\n\n    def forward(self, x, weights=None):\n        device =  x[0].device\n        if weights == None:\n            # transformer output:\n            # x = self.transformer_encoder(x) + x\n            \n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n            # output = self.attention(output)\n            # linear output:\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n            # h_n = self.features(output)\n            \n            # LSTM output:\n            # enc_x, (h_n, c_n) = self.features(output)\n            \n            # GRU output:\n            # enc_x, h_n = self.features(output)\n\n            # if len(h_n.shape) == 3:\n            #     h_n = h_n[-1] # (32,16)\n            \n            # 通过线性层和激活函数得到最终输出\n            # out = self.decoder(F.relu(h_n))\n            # out = self.decoder(F.leaky_relu(h_n, negative_slope=0.01))\n            out = self.mlp(output)\n        else:\n            # TemporalBlock0\n            # 实际权重 = 模数 * 方向向量\n            ks = self.kernel_size\n            weight1 = _weight_norm(weights[3], weights[2], 0)\n            conv1d1 = weight_norm(nn.Conv1d(self.num_inputs, self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d1._conv_forward(x, weight=weight1, bias=weights[1])\n            # output1 = F.conv1d(x, weight=weights[1], bias=weights[2], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean1 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp1(output1), running_mean=running_mean1, running_var=running_var1, weight=weights[4], bias=weights[5])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            weight2 = _weight_norm(weights[8], weights[7], 0)\n            conv1d2 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d2._conv_forward(output1, weight=weight2, bias=weights[6])\n            # output1 = F.conv1d(output1, weight=weights[5], bias=weights[6], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean2 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp2(output1), running_mean=running_mean2, running_var=running_var2, weight=weights[9], bias=weights[10])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            res1 = F.conv1d(x, weights[11], bias=weights[12], stride=1)\n            # output1 = F.tanh(output1 + res1)\n            output1 = F.relu(output1 + res1)\n\n            # TemporalBlock1\n            weight3 = _weight_norm(weights[15], weights[14], 0)\n            conv1d3 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d3._conv_forward(output1, weight=weight3, bias=weights[13])\n            # output2 = F.conv1d(output1, weight=weights[11], bias=weights[12], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean3 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp1(output2), running_mean=running_mean3, running_var=running_var3, weight=weights[16], bias=weights[17])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            weight4 = _weight_norm(weights[20], weights[19], 0)\n            conv1d4 = weight_norm(nn.Conv1d(self.num_channels[1], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d4._conv_forward(output2, weight=weight4, bias=weights[18])\n            # output2 = F.conv1d(output2, weight=weights[15], bias=weights[16], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean4 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var4 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp2(output2), running_mean=running_mean4, running_var=running_var4, weight=weights[21], bias=weights[22])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            res2 = F.conv1d(output1, weights[23], bias=weights[24], stride=1)\n            # output2 = F.tanh(output2 + res2)\n            output = F.relu(output2 + res2)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[25:31])\n            output = output[:, -1, :]\n\n            # output = output.transpose(1, 2)\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n\n            # out = F.leaky_relu(F.linear(output, weight=weights[31] , bias=weights[32]), negative_slope=0.01)\n            out = F.relu(F.linear(output, weight=weights[31] , bias=weights[32]))\n            out = F.linear(out, weight=weights[33], bias=weights[34])\n\n        return out\n    \n\n# MAML-跟车  version1\n# %matplotlib inline\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport copy\nimport random\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\n# from net.TCN import TemporalConvNet\n# from net.TCN_wn_bn import TemporalConvNet\n# from net.TCN_layer import TemporalConvNet\n# from net.TCN import MultiHeadSelfAttention\n# from net.cbrd import cbrd\n# from net.meta_neural_network_architectures import MetaLossNetwork, LossAdapter\n\n# 设置随机种子\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/MAML_alfa_log\")\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\n# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\ngpu = 0\ndevice = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu')\nprint(device)\n\n\n# # 保存日志\n# def get_logger(filename, verbosity=1, name=None):\n#     # 设置不同verbosity对应的日志级别\n#     level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n#     # 设置日志输出格式\n#     formatter = logging.Formatter(\n#         \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n#     )\n#     logger = logging.getLogger(name)\n#     logger.setLevel(level_dict[verbosity])\n#     # 创建文件处理器，将日志写入文件\n#     fh = logging.FileHandler(filename, \"w\")\n#     fh.setFormatter(formatter)\n#     logger.addHandler(fh)\n#     # 创建控制台处理器，将日志输出到控制台\n#     sh = logging.StreamHandler()\n#     sh.setFormatter(formatter)\n#     logger.addHandler(sh)\n#     return logger\n# # 日志保存路径\n# logger = get_logger('/home/ubuntu/fedavg/FollowNet-car/log/MAML_alfa_oooollllldddd.log')\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len, Ts = 0.1):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n        self.Ts = Ts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / self.Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(\n                nn.Linear(input_size,256),\n                nn.ReLU(),\n                nn.Linear(256,256),\n                nn.ReLU(),\n                nn.Linear(256,1)\n            )\n        self.input_size = input_size\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights=None):\n        if weights == None:\n            x = self.net(x)\n        else:\n            x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n            x=F.relu(x)\n            x=F.linear(x,weights[2],weights[3])\n            x=F.relu(x)\n            x=F.linear(x,weights[4],weights[5])\n        return x\n\n\n# 计算ttc\ndef calculate_safety(ttc):\n    minimum_ttc = min(ttc)\n    return minimum_ttc\n# 多数据同时验证\ndef model_evaluate(model,his_horizon,testdata):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    model.eval()\n\n    jerk_set = torch.empty(0).to(device)\n    error_set = torch.empty(0).to(device)\n    minimum_ttc_set = torch.empty(0).to(device)\n\n    criterion = nn.MSELoss()\n    count = 0\n    col = 0\n    for i, item in enumerate(testdata):#每个样本挨个出来\n        \n        x_data, y_data = item['inputs'], item['label']\n        x_data = torch.stack(x_data)#3x149\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        count += B\n        x_data_orig = x_data.clone().detach()#提前克隆的副本\n        \n        col_list = torch.full((B,), -1).to(device)\n        acc_batch = torch.empty(B,0).to(device)\n        ttc_batch = torch.empty(B,0).to(device)\n\n        \n        for frame in range(his_horizon, T):#对32个样本检测\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]   # (20, 3, 10)\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            #x = x.transpose(0,1).reshape(B, -1)\n            acc_pre = model(x).squeeze()\n            if acc_pre.dim() == 0:\n                acc_pre = acc_pre.unsqueeze(0)\n            acc_batch = torch.cat((acc_batch, acc_pre.unsqueeze(1)), dim=1)#记录所有acc     \n            if model_type == 'cnn1d' or model_type == 'tcn':\n                if frame < T-1:\n                    # 根据当前速度和加速度计算下一时间速度\n                    sv_spd_ = x_data[:, 1, frame] + acc_pre*Ts # (32)\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n                    # 计算下一时间速度的相对速度\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n                    # 该时刻真实相对速度\n                    delta_v = x_data[:, -1, frame] # (32)\n                    # 通过两车车距加上相对位移得到下一时间段车距 ？\n                    spacing_ = x_data[:, 0, frame] + Ts*(delta_v + delta_v_)/2 # (32)\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    # update 根据计算得到的值，更新下一时间的值\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[:, :, frame + 1] = next_frame_data\n            else:\n                if frame < T-1:#开环训练，不断依照预测值更新下一个时间节点的数据\n                    sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts#新的自车速度\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)#保证速度不小于等于0\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_#vt临时计算的\n                    delta_v = x_data[frame, :, -1]#v0原有的\n                    spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2#新的距离\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[frame + 1] = next_frame_data\n            if spacing_.dim() == 0:\n                ttc_batch_ = (-spacing_ / delta_v_).unsqueeze(0)\n            else:\n                ttc_batch_ = (-spacing_ / delta_v_)\n                \n            ttc_batch = torch.cat((ttc_batch, ttc_batch_.unsqueeze(1)), dim=1)\n        for i in range(B):#对32个样本统一处理\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                spacing_obs = x_data_orig[i,0,...]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[i, 0, :col_list[i]]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[i, 0, :]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            else:\n                spacing_obs = x_data_orig[...,i,0]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[:col_list[i],i, 0]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[:,i, 0]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            acc_batch_ = acc_batch_.cpu().detach().numpy()\n            jerk_single = np.mean(np.abs(np.diff(acc_batch_)/Ts))\n            TTC_single = [x for x in ttc_batch_ if x >= 0]#除去其中TTC小于0的\n            if len(TTC_single) > 0:\n                minimum_ttc_single = calculate_safety(TTC_single)#该样本的最小TTC\n                minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)      \n            jerk_set= torch.cat((jerk_set,torch.tensor(jerk_single).unsqueeze(0).to(device)), dim=0)\n            error_set= torch.cat((error_set,torch.tensor(error_single).unsqueeze(0).to(device)), dim=0)\n        col = col + torch.sum(col_list != -1).item()\n    if len(minimum_ttc_set) == 0:\n        ttc = 0\n    else:\n        ttc = sum(minimum_ttc_set)/len(minimum_ttc_set)\n    error = sum(error_set)/len(error_set)\n    return error,col,count,sum(jerk_set)/len(jerk_set),ttc#还需修改（以MSE为基准）\n\n\ndef finetuning():\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    # Train\n    Ts = 0.1\n    max_len = 150\n    og_net = maml.net\n    # 创建一个与原始网络结构相同的虚拟网络\n    if model_type == 'nn':\n        dummy_net = nn_model(input_size = his_horizon*3)\n    elif model_type == 'tcn':\n        dummy_net = TemporalConvNet(num_inputs=3, num_channels=(16,32))\n#     elif model_type == 'cnn1d':\n#         dummy_net = CNN1D()\n        # dummy_net = cbrd(num_inputs=3, num_output=1)\n    dummy_net=dummy_net.to(device)\n\n    # 多gpu训练\n    # if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n    #     dummy_net = torch.nn.DataParallel(dummy_net)  # 自动选择gpu\n    # dummy_net.to(device)\n\n    # 加载原始网络的权重\n    # weight_path = torch.load(state_path)\n    # dummy_net.load_state_dict(weight_path.state_dict())\n    dummy_net.load_state_dict(og_net.state_dict())\n    # 进行迭代，每次更新虚拟网络的参数\n    num_shots=5\n    lr = 0.01\n    loss_fn=nn.MSELoss()\n    optim=torch.optim.Adam\n    opt=optim(dummy_net.parameters(),lr=lr, weight_decay=3e-4)\n\n\n    # 数据集划分\n    def split_data(data,data_ratio):\n        # random.seed(SEED)\n        # np.random.seed(SEED)\n        # torch.manual_seed(SEED)\n        if data_ratio > 0 and data_ratio <= 1:\n            # 如果小于等于1，根据输入百分比计算获取数据集的数量\n            data_size=int(len(data)*data_ratio)\n        else:\n            # 如果大于1，则data_ratio为获取数据集中的数量\n            data_size = data_ratio\n        return data[:data_size]\n    # np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n    # 获取数据集的数量\n    K=40\n    dataset_train = split_data(train_data, K)\n    dataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len, Ts=Ts)\n    train_loader = DataLoader(\n                dataset_loader_train,\n                batch_size=10,\n                shuffle=True,\n                num_workers=1,\n                drop_last=True)\n    dataset_test = test_data\n    dataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len, Ts=Ts)\n    test_loader = DataLoader(\n                dataset_loader_test,\n                batch_size=64,\n                shuffle=False,\n                num_workers=1,\n                drop_last=False)\n    # dataset_val = NGSIM_val\n    # dataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len, Ts=Ts)\n    # val_loader = DataLoader(\n    #             dataset_loader_val,\n    #             batch_size=10,\n    #             shuffle=True,\n    #             num_workers=1,\n    #             drop_last=True)\n\n\n    # 初始化变量\n    train_loss_his = [] # 训练损失\n    test_error_his = [] # 测试误差\n    best_train_loss = None # 最佳训练损失\n\n    # 训练过程\n    best_error = 10000\n    for epoch in range(num_shots):\n        train_losses = [] # 记录每个epoch的训练损失\n        validation_losses = [] # 记录每个epoch的验证损失\n        jerk_val = 0\n        dummy_net.train()\n        # 遍历数据集\n        for i, item in enumerate(train_loader):\n            # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n            # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n            x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n            # Put T into the first dimension, B, T, d -> T, B, d\n            # 将x_data中3个(32,374)连接，转换成(3,32,374)\n            x_data = torch.stack(x_data)\n            # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                # x->(bs,3,149)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n                B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            else:\n                # x->(149,bs,3)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n                T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            # print(T,B,d)  # 149  20  3\n                \n            y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n            y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n            # 从历史数据时间步开始遍历\n            for frame in range(his_horizon, T):\n                if model_type == 'cnn1d' or model_type == 'tcn':\n                    x= x_data[:, :, frame-his_horizon:frame]\n                else:\n                    x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n                if model_type == 'nn':\n                    x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n                # 根据his_horizon个数据预测加速度\n                acc_pre = dummy_net(x).squeeze() # (32)\n                y_pre[frame - his_horizon] = acc_pre\n            #计算损失并进行反传及优化\n            loss = loss_fn(y_pre, y_label)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n            opt.step()\n\n            train_losses.append(loss.item())\n        # 计算本轮平均损失\n        train_loss = np.mean(train_losses)\n\n        train_loss_his.append(train_loss)\n        print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n#         logger.info(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n        mean_spacing_error,col,count,jerk,miniumu_ttc = model_evaluate(dummy_net,his_horizon,test_loader)\n        print(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n#         logger.info(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n        if mean_spacing_error < best_error:\n            best_error = mean_spacing_error\n            # save the best model\n#             with open(save_path, 'wb') as f:\n#                 torch.save(dummy_net, f)\n    return best_error\n\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion, metal=None):\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n            \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x, temp_weights).squeeze() # (32)\n            # acc_pre = net.module(x, temp_weights).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        if metal == None:\n            return loss\n        else:\n            return loss, y_pre\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n            if adapt_tasks:\n                task_net = HighD_net\n            # print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Lyft_net\n            # print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD1_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD1_net\n            # print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = NGSIM_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD2_net\n            # print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = Waymo_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Waymo_net\n            # print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len, Ts = Ts)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len, Ts = Ts)\n        if adapt_tasks:\n            return dataset_loader1, dataset_loader2, task_net\n        else:\n            return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self, epoch):\n        # 从1到5中随机选择3个不重复的数\n        # 创建一个新的随机数生成器实例\n        rng = np.random.RandomState(epoch)\n        data_choice = rng.choice(self.data_range, size=self.n, replace=False) + 1\n        # data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\ndef attenuate_init(attenuator, panning, task_embeddings, names_weights_copy):\n        ## Attenuate\n        updated_names_weights_copy = list()\n        \n        # 生成衰减参数和偏移参数\n        if attenuator == None:\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(key + beta[i])\n                i+=1\n        elif panning == None:\n            gamma = attenuator(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key)\n                i+=1\n        else:\n            gamma = attenuator(task_embeddings)\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key + beta[i])\n                i+=1\n            \n        return updated_names_weights_copy\n\n\ndef get_inner_loop_parameter_dict(params):\n    \"\"\"\n    Returns a dictionary with the parameters to use for inner loop updates.\n    :param params: A dictionary of the network's parameters.\n    :return: A dictionary of the parameters to use for the inner loop optimization process.\n    \"\"\"\n    param_dict = dict()\n    for name, param in params:\n        if param.requires_grad:\n            param_dict[name] = param.to(device)\n\n    return param_dict\n\ndef get_per_step_loss_importance_vector(inner_step, current_epoch):\n        \"\"\"\n        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n        loss towards the optimization loss.\n        :return: A tensor to be used to compute the weighted average of the loss, useful for\n        the MSL (Multi Step Loss) mechanism.\n        \"\"\"\n        multi_step_loss_num_epochs = 20\n        loss_weights = np.ones(shape=(inner_step)) * (\n                1.0 / inner_step)\n        decay_rate = 1.0 / inner_step / multi_step_loss_num_epochs\n        min_value_for_non_final_losses = 0.03 / inner_step\n        for i in range(len(loss_weights) - 1):\n            curr_value = np.maximum(loss_weights[i] - (current_epoch * decay_rate), min_value_for_non_final_losses)\n            loss_weights[i] = curr_value\n\n        curr_value = np.minimum(\n            loss_weights[-1] + (current_epoch * (inner_step - 1) * decay_rate),\n            1.0 - ((inner_step - 1) * min_value_for_non_final_losses))\n        loss_weights[-1] = curr_value\n        loss_weights = torch.Tensor(loss_weights).to(device)\n        return loss_weights\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k, weight_decay, inner_step):  # (net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=150)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.inner_step = inner_step  # 内部循环更新的步数\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.meta_test = []\n        self.finetuning_error = []\n        self.plot_every = 10  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.first_add = 0  # 用于添加优化器优化的参数\n        self.batch_size = int(self.k/2)\n        # 设置权重衰减因子\n        self.weight_decay = weight_decay\n        self.num_layers = len(self.weights)\n        params = [{'params': self.weights}]\n        if MeTAL:\n            base_learner_num_layers = len(self.weights)\n            support_meta_loss_num_dim = base_learner_num_layers + 2\n            support_adapter_num_dim = base_learner_num_layers + 1\n\n            self.meta_loss = MetaLossNetwork(support_meta_loss_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_loss_adapter = LossAdapter(support_adapter_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_loss_adapter.parameters()}\n            params.append(new_params)\n\n            if semi_supervised:\n                query_num_dim = base_learner_num_layers + 1\n                self.meta_query_loss = MetaLossNetwork(query_num_dim, notspi=self.inner_step, device=device).to(device=device)\n                self.meta_query_loss_adapter = LossAdapter(query_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n                new_params = {'params': self.meta_query_loss.parameters()}\n                params.append(new_params)\n                new_params = {'params': self.meta_query_loss_adapter.parameters()}\n                params.append(new_params)\n\n        if attenuate:  #L2F\n            self.attenuator = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Sigmoid()\n            ).to(device=device)\n            new_params = {'params': self.attenuator.parameters()}\n            params.append(new_params)\n        if tapt:  #bias\n            self.panning = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Tanh()\n            ).to(device=device)\n            new_params = {'params': self.panning.parameters()}\n            params.append(new_params)\n        if alfa:  # ALFA\n            input_dim = self.num_layers*2\n            self.regularizer = nn.Sequential(\n                nn.Linear(input_dim, input_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(input_dim, input_dim)\n            ).to(device=device)\n            new_params = {'params': self.regularizer.parameters()}\n            params.append(new_params)\n        self.meta_optimiser = torch.optim.Adam(params,lr=self.beta, weight_decay=3e-4)\n\n    def inner_loop(self, task):\n        taskloss = []\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        if adapt_tasks:\n            dataset_loader1, dataset_loader2, task_net = task.sample_data(size=self.k)\n            task_optimiser = torch.optim.Adam([{'params':task_net.parameters()}],lr=0.001, weight_decay=3e-4)\n        else:\n            dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        for step in range(self.inner_step):\n            if MeTAL:\n                support_task_state = []\n                support_loss, support_preds = inner_train(dataloader1, self.net, temp_weights, self.criterion, metal=1)\n                support_loss /= self.batch_size\n                support_task_state.append(support_loss)\n\n                for v in temp_weights:\n                    support_task_state.append(v.mean())\n\n                support_task_state = torch.stack(support_task_state)\n                adapt_support_task_state = (support_task_state - support_task_state.mean())/(support_task_state.std() + 1e-12)                                                 \n\n                updated_meta_loss_weights = self.meta_loss_adapter(adapt_support_task_state, step, self.names_loss_weights_copy)\n\n                # support_y = torch.zeros(support_preds.shape).to(device)\n                # support_y[torch.arange(support_y.size(0)), y] = 1\n                support_task_state = torch.cat((\n                    support_task_state.view(1, -1).expand(support_preds.size(1), -1),\n                    support_preds.transpose(0, 1).mean(dim=1, keepdim=True)\n                    # support_y\n                ), -1)\n\n                support_task_state = (support_task_state - support_task_state.mean()) / (support_task_state.std() + 1e-12)\n                meta_support_loss = self.meta_loss(support_task_state, step, params=updated_meta_loss_weights).mean().squeeze()\n                if semi_supervised:\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    _, query_preds = inner_train(dataloader2, self.net, temp_weights, self.criterion, metal=1)\n                    query_task_state = []\n                    for v in temp_weights:\n                        query_task_state.append(v.mean())\n                    # out_prob = F.log_softmax(query_preds)\n                    # instance_entropy = torch.sum(torch.exp(out_prob) * out_prob, dim=-1)\n                    query_task_state = torch.stack(query_task_state)\n                    query_task_state = torch.cat((\n                                query_task_state.view(1, -1).expand(query_preds.size(1), -1), \n                                query_preds.transpose(0, 1).mean(dim=1, keepdim=True)\n                                # instance_entropy.view(-1, 1)\n                    ), -1)\n\n                    query_task_state = (query_task_state - query_task_state.mean())/(query_task_state.std() + 1e-12)\n                    updated_meta_query_loss_weights = self.meta_query_loss_adapter(query_task_state.mean(0), step, self.names_query_loss_weights_copy)\n\n                    meta_query_loss = self.meta_query_loss(query_task_state, step, params=updated_meta_query_loss_weights).mean().squeeze()\n\n                    loss = support_loss + meta_support_loss + meta_query_loss\n                else:\n                    loss = support_loss + meta_support_loss\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            else:\n                # 训练过程\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            if step == 0:\n                if adapt_tasks:     # every tasks\n                    grads = tuple(grads[i] for i in range(len(grads)))\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    temp_weights = attenuate_init(task_net, None, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    new_loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                    grads = torch.autograd.grad(new_loss, temp_weights, create_graph=True, retain_graph=True)\n\n                if attenuate or tapt:    #L2F\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    if attenuate and tapt:\n                        temp_weights = attenuate_init(self.attenuator, self.panning, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    elif attenuate and not tapt:\n                        temp_weights = attenuate_init(self.attenuator, None, layerwise_mean_grads, temp_weights)\n                    elif not attenuate and tapt:\n                        temp_weights = attenuate_init(None, self.panning, layerwise_mean_grads, temp_weights)\n                    new_loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                    grads = torch.autograd.grad(new_loss, temp_weights, create_graph=True, retain_graph=True)\n\n            if alfa:    #ALFA\n                # 用于存储权重的平均值和梯度的平均值\n                per_step_task_embedding = []\n                for v in temp_weights:\n                    per_step_task_embedding.append(v.mean())\n            \n                for i in range(len(grads)):\n                    per_step_task_embedding.append(grads[i].mean())\n\n                per_step_task_embedding = torch.stack(per_step_task_embedding)\n\n                generated_params = self.regularizer(per_step_task_embedding)\n\n                generated_alpha, generated_beta = torch.split(generated_params, split_size_or_sections=self.num_layers)\n                generated_alpha_params = []\n                generated_beta_params = []\n                g = 0\n                for key in temp_weights:\n                    generated_alpha_params.append(generated_alpha[g])\n                    generated_beta_params.append(generated_beta[g])\n                    g+=1\n                # 初始化ALFA自适应参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    if random_init:\n                        self.names_beta_dict_per_param = nn.ParameterDict()\n                    self.names_alpha_dict = nn.ParameterDict()\n                    self.names_beta_dict = nn.ParameterDict()\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    for idx, param in enumerate(temp_weights):\n\n                        if random_init:\n                        # per-param weight decay for random init\n                            self.names_beta_dict_per_param[str(idx)] = nn.Parameter(\n                                data=torch.ones(param.shape) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_learning_rates)\n\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step),\n                                requires_grad=use_learnable_beta)\n                            \n                            self.names_beta_dict_per_param.to(device)\n                        else:\n                            # per-step per-layer meta-learnable weight decay bias term (for more stable training and better performance by 2~3%)\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_beta)\n                        \n                        # per-step per-layer meta-learnable learning rate bias term (for more stable training and better performance by 2~3%)\n                        self.names_alpha_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                        \n                        self.names_beta_dict.to(device)\n                        self.names_alpha_dict.to(device)\n                    # 将额外的参数添加到优化器中\n                    if random_init:\n                        self.meta_optimiser.add_param_group({'params': self.names_beta_dict_per_param.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_beta_dict.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_alpha_dict.parameters()})\n            else:\n                # 使用可学习的参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    self.names_learning_rates_dict = nn.ParameterDict()\n                    for idx, param in enumerate(temp_weights):\n                        self.names_learning_rates_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                    self.names_learning_rates_dict.to(device)\n                    if use_learnable_alpha:\n                        self.meta_optimiser.add_param_group({'params': self.names_learning_rates_dict.parameters()})\n                        \n                    if inner_update == \"L2\" and self.weight_decay != None:\n                        self.names_weight_decay_dict = nn.ParameterDict()\n                        for idx, param in enumerate(temp_weights):\n                            self.names_weight_decay_dict[str(idx)] = nn.Parameter(\n                                    data=torch.ones(self.inner_step) * init_weight_decay,\n                                    requires_grad=use_learnable_beta)\n                        self.names_weight_decay_dict.to(device)\n                        if use_learnable_beta:\n                            self.meta_optimiser.add_param_group({'params': self.names_weight_decay_dict.parameters()})\n\n            if inner_update == \"L2\" and self.weight_decay != None:\n                # temp_weights = [w - self.alpha * (g + (self.weight_decay * w)) for w, g in zip(temp_weights, grads)]\n                # temp_weights = [((1 - self.alpha * self.weight_decay) * w) - (self.alpha * g) for w, g in zip(temp_weights, grads)]\n                temp_weights = [((1 - self.names_learning_rates_dict[str(key)][step] * self.names_weight_decay_dict[str(key)][step]) * w) - (self.names_learning_rates_dict[str(key)][step] * g) for key, (w, g) in enumerate(zip(temp_weights, grads))]\n            elif inner_update == \"ALFA\" and alfa:\n                if random_init:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step] * self.names_beta_dict_per_param[str(key)]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n                else:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n            else:\n                temp_weights = [w - self.names_learning_rates_dict[str(key)][step] * g for key, (w, g) in enumerate(zip(temp_weights, grads))]  # 临时参数更新 梯度下降\n\n            if use_multi_step_loss_optimization and self.epoch < 20:\n                dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                taskloss.append(self.per_step_loss_importance_vectors[step] * metaloss)\n            else:\n                if step == (self.inner_step - 1):\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                    taskloss.append(metaloss)\n        task_losses = torch.sum(torch.stack(taskloss))\n        if adapt_tasks:\n            task_optimiser.zero_grad()\n            metaloss.backward(retain_graph=True)\n            task_optimiser.step()\n        return task_losses\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        best_loss = 10000\n        best_index = 0\n        for epoch in range(1, num_epochs + 1):\n            self.epoch = epoch\n            if MeTAL:\n                self.names_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_loss.named_parameters())\n                if semi_supervised:\n                    self.names_query_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_query_loss.named_parameters())\n            total_loss = 0\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task(epoch)\n            for i in tasks:\n                if use_multi_step_loss_optimization:\n                    self.per_step_loss_importance_vectors = get_per_step_loss_importance_vector(self.inner_step, epoch)\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            # metaloss_sum.backward(retain_graph=True)\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights, retain_graph=True)  # 计算元学习损失对参数的梯度\n            # # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n\n            if MeTAL:\n                meta_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss.parameters(), meta_loss_grads):\n                    w.grad = g\n\n                meta_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss_adapter.parameters(), meta_loss_adapter_grads):\n                    w.grad = g\n\n                if semi_supervised:\n                    meta_query_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss.parameters(), meta_query_loss_grads):\n                        w.grad = g\n\n                    meta_query_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss_adapter.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss_adapter.parameters(), meta_query_loss_adapter_grads):\n                        w.grad = g\n\n            if not alfa and use_learnable_alpha:\n                learning_rates_grads = torch.autograd.grad(metaloss_sum, list(self.names_learning_rates_dict.parameters()), retain_graph=True)\n                for w, g in zip(self.names_learning_rates_dict.parameters(), learning_rates_grads):\n                    w.grad = g\n                if inner_update == \"L2\" and self.weight_decay != None and use_learnable_beta:\n                    weight_decay_grads = torch.autograd.grad(metaloss_sum, list(self.names_weight_decay_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_weight_decay_dict.parameters(), weight_decay_grads):\n                        w.grad = g\n\n            if attenuate:\n                attenuate_grads = torch.autograd.grad(metaloss_sum, list(self.attenuator.parameters()), retain_graph=True)\n                for w, g in zip(self.attenuator.parameters(), attenuate_grads):\n                    w.grad = g\n            if tapt:\n                tapt_grads = torch.autograd.grad(metaloss_sum, list(self.panning.parameters()), retain_graph=True)\n                for w, g in zip(self.panning.parameters(), tapt_grads):\n                    w.grad = g\n            if alfa:\n                alfa_grads = torch.autograd.grad(metaloss_sum, list(self.regularizer.parameters()), retain_graph=True)\n                for w, g in zip(self.regularizer.parameters(), alfa_grads):\n                    w.grad = g\n                if use_learnable_beta:\n                    names_beta_grads = torch.autograd.grad(metaloss_sum, list(self.names_beta_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict.parameters(), names_beta_grads):\n                        w.grad = g\n                if use_learnable_alpha:\n                    names_alpha_grads = torch.autograd.grad(metaloss_sum, self.names_alpha_dict.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_alpha_dict.parameters(), names_alpha_grads):\n                        w.grad = g\n                if random_init:\n                    random_init_grads = torch.autograd.grad(metaloss_sum, self.names_beta_dict_per_param.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict_per_param.parameters(), random_init_grads):\n                        w.grad = g\n            \n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            mserror,col,count,jerk,ttc = model_evaluate(self.net,his_horizon,meta_loader)\n            if epoch % self.print_every == 0:\n                print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                print(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n#                 logger.info(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n#                 logger.info(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n                if mserror < best_loss:\n                    best_loss = mserror\n                    best_index = epoch\n                    # with open(file_path, 'wb') as f:\n                    #     torch.save(self.net, f)\n                self.meta_losses.append(total_loss / self.print_every)\n                self.meta_test.append(mserror.cpu().detach().numpy() / self.print_every)\n            self.finetuning_error.append(finetuning())\n            print(\"   \")\n            # fine_error = finetuning()\n            # self.finetuning_error.append(fine_error)\n            # if epoch % self.plot_every == 0:\n            #     epoch_path = f\"/home/ubuntu/fedavg/FollowNet-car/MAML_state/maml_l2f_test/MAML_nn_a0.01_b0.0001_{epoch}E.pt\" # 元初始化参数\n            #     # weight_change = torch.load(file_path)\n            #     with open(epoch_path, 'wb') as f:\n            #         torch.save(self.net, f)\n        print(\"(\", best_index, \")\",\"best_loss:\", best_loss)\n\ndataset = 'SPMD2'\nmodel_type = 'tcn'\nif dataset == 'SPMD1':\n    train_data = SPMD1_train\n    test_data = SPMD1_test\nelif dataset == 'SPMD2':\n    train_data = SPMD2_train\n    test_data = SPMD2_test\nelif dataset == 'Waymo':\n    train_data = Waymo_train\n    test_data = Waymo_test\nelif dataset == 'NGSIM':\n    train_data = NGSIM_train\n    test_data = NGSIM_test\nelif dataset == 'Lyft':\n    train_data = Lyft_train\n    test_data = Lyft_test\nelif dataset == 'HighD':\n    train_data = HighD_train\n    test_data = HighD_test\n# 定义保存文件的文件夹路径\n# file_path = \"/home/ubuntu/fedavg/FollowNet-car/MAML_state/MAML_alfa_nn_a0.01_b0.0001_k300_E50.pt\" # 元初始化参数\n# save_folder = '/home/ubuntu/fedavg/FollowNet-car/state'\n# save = f'MAML_ALFA_{model_type}_{dataset}_oooollllldddd.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\n# if not os.path.exists(save_folder):\n#     os.makedirs(save_folder)\n# 定义文件保存路径\n# save_path = os.path.join(save_folder, save)\nmeta_test = test_data\nmeta_loader_test = ImitationCarFolData(meta_test, max_len = max_len, Ts=Ts)\nmeta_loader = DataLoader(\n            meta_loader_test,\n            batch_size=96,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n# 模型初始化\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'tcn':\n    net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\n# elif model_type == 'cnn1d':\n#     net = CNN1D().to(device)\n    # net = cbrd(num_inputs=3, num_output=1).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n\n# model_state = list(net.parameters())\n# print(model_state)\n\n# 多gpu训练\n# if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n#     net = torch.nn.DataParallel(net)  # 自动选择gpu\n# net.to(device)\n\n# 内部循环更新策略\nMeTAL = False   # 任务自适应loss函数\nsemi_supervised = False  # 使用查询集 作为半监督\nadapt_tasks = False  # 权重初始化 每个任务用一个\nattenuate = False  # 权重初始化 所有任务共用 权重衰减\ntapt = False  # 权重初始化 所有任务共用 权重偏差\nalfa = False        # 自适应学习优化超参数\nrandom_init = False\nuse_learnable_learning_rates = random_init\nuse_learnable_beta = False  # 使优化参数beta可学习\nuse_learnable_alpha = False  # 使优化参数alpha可学习\nuse_multi_step_loss_optimization = False  # 内部循环多步时，使用最后一步的Loss，还是使用多步的Loss。\ninner_update = 'ALFA'\nif adapt_tasks:\n    num_layers = len(list(net.parameters()))\n    HighD_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Lyft_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD1_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD2_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Waymo_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.01,beta=0.01,tasks=data_tasks,k=200,weight_decay=0.0001,inner_step=1)\n\n\nmaml.outer_loop(num_epochs=500)\nplt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(maml.meta_losses)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_Loss.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.meta_test)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_mserror.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.finetuning_error)\n# plt.savefig(os.path.join(plt_path, 'MAML_alfa_oooollllldddd_finetuning_mserror.png'))\nplt.show()\nplt.close()\n# # 将数据保存到文件中\nnp.save('/kaggle/working/kaggle_maml_meta_losses.npy', np.array(maml.meta_losses))\nnp.save('/kaggle/working/kaggle_maml_meta_test.npy', np.array(maml.meta_test))\nnp.save('/kaggle/working/kaggle_maml_finetuning_error.npy', np.array(maml.finetuning_error))\n\n\nprint('----------------------')\n# logger.info(\"--------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:21:27.314557Z","iopub.execute_input":"2024-06-03T01:21:27.314897Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_26/3337634293.py:867: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)\n","output_type":"stream"},{"name":"stdout","text":"1/500. loss: 0.0030535062154134116\n1/500. mserror: 102.7873306274414  col: 1087  count: 3637  jerk: 0.00438933540135622  ttc: 36.295753479003906\nEpoch: 1| Train Loss: 0.1696991\nmean_spacing_error：97.53593，col=304，count=3637，rate=8.35854%，jerk=0.00040，miniumu_ttc=134.02794\nEpoch: 2| Train Loss: 0.1518969\nmean_spacing_error：96.88409，col=340，count=3637，rate=9.34836%，jerk=0.00327，miniumu_ttc=232.09081\nEpoch: 3| Train Loss: 0.1512302\nmean_spacing_error：95.04176，col=303，count=3637，rate=8.33104%，jerk=0.00330，miniumu_ttc=105.82082\nEpoch: 4| Train Loss: 0.1518357\nmean_spacing_error：84.95623，col=362，count=3637，rate=9.95326%，jerk=0.01029，miniumu_ttc=86.44263\nEpoch: 5| Train Loss: 0.1476017\nmean_spacing_error：63.69027，col=243，count=3637，rate=6.68133%，jerk=0.01057，miniumu_ttc=110.82883\n   \n2/500. loss: 0.003802728528777758\n2/500. mserror: 76.08035278320312  col: 477  count: 3637  jerk: 0.0006114662974141538  ttc: 81.97216033935547\nEpoch: 1| Train Loss: 0.1650520\nmean_spacing_error：93.45181，col=313，count=3637，rate=8.60599%，jerk=0.00046，miniumu_ttc=152.57030\nEpoch: 2| Train Loss: 0.1517792\nmean_spacing_error：91.36572，col=308，count=3637，rate=8.46852%，jerk=0.00107，miniumu_ttc=104.26459\nEpoch: 3| Train Loss: 0.1476234\nmean_spacing_error：87.66953，col=168，count=3637，rate=4.61919%，jerk=0.00845，miniumu_ttc=576.83191\nEpoch: 4| Train Loss: 0.1369270\nmean_spacing_error：87.86489，col=72，count=3637，rate=1.97965%，jerk=0.02108，miniumu_ttc=115.93440\nEpoch: 5| Train Loss: 0.1279370\nmean_spacing_error：75.88918，col=132，count=3637，rate=3.62936%，jerk=0.01982，miniumu_ttc=80.55441\n   \n3/500. loss: 0.005185063928365707\n3/500. mserror: 82.82811737060547  col: 398  count: 3637  jerk: 9.706171113066375e-05  ttc: 127.0958023071289\nEpoch: 1| Train Loss: 0.1705745\nmean_spacing_error：81.79623，col=406，count=3637，rate=11.16305%，jerk=0.00042，miniumu_ttc=94.42758\nEpoch: 2| Train Loss: 0.1526331\nmean_spacing_error：92.14896，col=302，count=3637，rate=8.30355%，jerk=0.00116，miniumu_ttc=89.82133\nEpoch: 3| Train Loss: 0.1476943\nmean_spacing_error：77.60785，col=116，count=3637，rate=3.18944%，jerk=0.01398，miniumu_ttc=75.12600\nEpoch: 4| Train Loss: 0.1378521\nmean_spacing_error：74.30209，col=190，count=3637，rate=5.22409%，jerk=0.01623，miniumu_ttc=111.29977\nEpoch: 5| Train Loss: 0.1331098\nmean_spacing_error：64.53221，col=300，count=3637，rate=8.24856%，jerk=0.02192，miniumu_ttc=373.03033\n   \n4/500. loss: 0.004086076902846496\n4/500. mserror: 88.57457733154297  col: 358  count: 3637  jerk: 6.075452984077856e-05  ttc: 159.3597869873047\nEpoch: 1| Train Loss: 0.1729297\nmean_spacing_error：86.34361，col=371，count=3637，rate=10.20071%，jerk=0.00059，miniumu_ttc=127.34771\nEpoch: 2| Train Loss: 0.1527433\nmean_spacing_error：85.73493，col=343，count=3637，rate=9.43085%，jerk=0.00126，miniumu_ttc=89.70692\nEpoch: 3| Train Loss: 0.1496352\nmean_spacing_error：74.79703，col=285，count=3637，rate=7.83613%，jerk=0.00507，miniumu_ttc=123.81218\nEpoch: 4| Train Loss: 0.1413666\nmean_spacing_error：54.81345，col=53，count=3637，rate=1.45724%，jerk=0.03282，miniumu_ttc=187.42487\nEpoch: 5| Train Loss: 0.1274566\nmean_spacing_error：55.57704，col=403，count=3637，rate=11.08056%，jerk=0.02796，miniumu_ttc=49.42076\n   \n5/500. loss: 0.0029972984145085015\n5/500. mserror: 94.32530975341797  col: 321  count: 3637  jerk: 7.883879879955202e-05  ttc: 103.52875518798828\nEpoch: 1| Train Loss: 0.1713433\nmean_spacing_error：85.68188，col=372，count=3637，rate=10.22821%，jerk=0.00040，miniumu_ttc=80.21905\nEpoch: 2| Train Loss: 0.1531347\nmean_spacing_error：92.23515，col=337，count=3637，rate=9.26588%，jerk=0.00058，miniumu_ttc=116.24204\nEpoch: 3| Train Loss: 0.1512677\nmean_spacing_error：96.28854，col=297，count=3637，rate=8.16607%，jerk=0.00089，miniumu_ttc=225.43843\nEpoch: 4| Train Loss: 0.1497704\nmean_spacing_error：68.79221，col=318，count=3637，rate=8.74347%，jerk=0.00465，miniumu_ttc=240.32031\nEpoch: 5| Train Loss: 0.1442489\nmean_spacing_error：50.91695，col=389，count=3637，rate=10.69563%，jerk=0.01498，miniumu_ttc=103.29002\n   \n6/500. loss: 0.004524751255909602\n6/500. mserror: 95.28851318359375  col: 316  count: 3637  jerk: 4.476325193536468e-05  ttc: 123.28595733642578\nEpoch: 1| Train Loss: 0.1721206\nmean_spacing_error：87.82501，col=352，count=3637，rate=9.67831%，jerk=0.00066，miniumu_ttc=101.26574\nEpoch: 2| Train Loss: 0.1524787\nmean_spacing_error：90.67455，col=324，count=3637，rate=8.90844%，jerk=0.00068，miniumu_ttc=85.78259\nEpoch: 3| Train Loss: 0.1502633\nmean_spacing_error：85.09480，col=292，count=3637，rate=8.02859%，jerk=0.00294，miniumu_ttc=243.45312\nEpoch: 4| Train Loss: 0.1456226\nmean_spacing_error：55.08740，col=265，count=3637，rate=7.28622%，jerk=0.01733，miniumu_ttc=108.61851\nEpoch: 5| Train Loss: 0.1386368\nmean_spacing_error：51.01949，col=117，count=3637，rate=3.21694%，jerk=0.04091，miniumu_ttc=47.65339\n   \n7/500. loss: 0.0035893029222885766\n7/500. mserror: 93.2220687866211  col: 331  count: 3637  jerk: 0.00017305636720266193  ttc: 143.7620086669922\nEpoch: 1| Train Loss: 0.1699223\nmean_spacing_error：85.32935，col=336，count=3637，rate=9.23838%，jerk=0.00136，miniumu_ttc=93.61397\nEpoch: 2| Train Loss: 0.1523237\nmean_spacing_error：90.86434，col=318，count=3637，rate=8.74347%，jerk=0.00067，miniumu_ttc=103.93442\nEpoch: 3| Train Loss: 0.1504669\nmean_spacing_error：87.48333，col=295，count=3637，rate=8.11108%，jerk=0.00272，miniumu_ttc=110.98349\nEpoch: 4| Train Loss: 0.1462063\nmean_spacing_error：50.29774，col=206，count=3637，rate=5.66401%，jerk=0.02065，miniumu_ttc=82.77686\nEpoch: 5| Train Loss: 0.1352085\nmean_spacing_error：44.13921，col=33，count=3637，rate=0.90734%，jerk=0.06117，miniumu_ttc=55.93787\n   \n8/500. loss: 0.0034927095596988997\n8/500. mserror: 89.49050903320312  col: 352  count: 3637  jerk: 0.00027506871265359223  ttc: 97.63641357421875\nEpoch: 1| Train Loss: 0.1668874\nmean_spacing_error：92.82344，col=308，count=3637，rate=8.46852%，jerk=0.00108，miniumu_ttc=133.68672\nEpoch: 2| Train Loss: 0.1522390\nmean_spacing_error：87.03185，col=347，count=3637，rate=9.54083%，jerk=0.00062，miniumu_ttc=106.62947\nEpoch: 3| Train Loss: 0.1503811\nmean_spacing_error：92.48135，col=287，count=3637，rate=7.89112%，jerk=0.00229，miniumu_ttc=179.79294\nEpoch: 4| Train Loss: 0.1464633\nmean_spacing_error：60.11114，col=308，count=3637，rate=8.46852%，jerk=0.01446，miniumu_ttc=89.56214\nEpoch: 5| Train Loss: 0.1352791\nmean_spacing_error：57.04828，col=179，count=3637，rate=4.92164%，jerk=0.03384，miniumu_ttc=96.27278\n   \n9/500. loss: 0.003940950768689315\n9/500. mserror: 88.09210968017578  col: 364  count: 3637  jerk: 0.00029283235198818147  ttc: 170.2154083251953\nEpoch: 1| Train Loss: 0.1621754\nmean_spacing_error：97.10872，col=292，count=3637，rate=8.02859%，jerk=0.00126，miniumu_ttc=142.23074\nEpoch: 2| Train Loss: 0.1519148\nmean_spacing_error：88.87791，col=357，count=3637，rate=9.81578%，jerk=0.00014，miniumu_ttc=179.70201\nEpoch: 3| Train Loss: 0.1512247\nmean_spacing_error：100.23952，col=294，count=3637，rate=8.08359%，jerk=0.00051，miniumu_ttc=151.41209\nEpoch: 4| Train Loss: 0.1509273\nmean_spacing_error：87.04676，col=316，count=3637，rate=8.68848%，jerk=0.00274，miniumu_ttc=107.19322\nEpoch: 5| Train Loss: 0.1497427\nmean_spacing_error：71.95567，col=343，count=3637，rate=9.43085%，jerk=0.00664，miniumu_ttc=159.28714\n   \n10/500. loss: 0.0036664381623268127\n10/500. mserror: 86.38362884521484  col: 372  count: 3637  jerk: 0.0004402940394356847  ttc: 73.56668090820312\nEpoch: 1| Train Loss: 0.1548301\nmean_spacing_error：94.40276，col=323，count=3637，rate=8.88095%，jerk=0.00005，miniumu_ttc=106.08024\nEpoch: 2| Train Loss: 0.1516276\nmean_spacing_error：91.91949，col=339，count=3637，rate=9.32087%，jerk=0.00010，miniumu_ttc=92.81734\nEpoch: 3| Train Loss: 0.1512521\nmean_spacing_error：99.83772，col=298，count=3637，rate=8.19357%，jerk=0.00010，miniumu_ttc=117.57861\nEpoch: 4| Train Loss: 0.1513827\nmean_spacing_error：97.63049，col=299，count=3637，rate=8.22106%，jerk=0.00059，miniumu_ttc=129.62941\nEpoch: 5| Train Loss: 0.1512971\nmean_spacing_error：89.67513，col=315，count=3637，rate=8.66098%，jerk=0.00100，miniumu_ttc=95.34422\n   \n11/500. loss: 0.0024369433522224426\n11/500. mserror: 82.99295043945312  col: 398  count: 3637  jerk: 0.0006378381513059139  ttc: 124.02587127685547\nEpoch: 1| Train Loss: 0.1535901\nmean_spacing_error：95.17117，col=311，count=3637，rate=8.55100%，jerk=0.00014，miniumu_ttc=251.75540\nEpoch: 2| Train Loss: 0.1516783\nmean_spacing_error：80.56559，col=340，count=3637，rate=9.34836%，jerk=0.00200，miniumu_ttc=104.21780\nEpoch: 3| Train Loss: 0.1497986\nmean_spacing_error：77.26541，col=342，count=3637，rate=9.40335%，jerk=0.00341，miniumu_ttc=124.80921\nEpoch: 4| Train Loss: 0.1466421\nmean_spacing_error：48.86593，col=389，count=3637，rate=10.69563%，jerk=0.02234，miniumu_ttc=39.32040\nEpoch: 5| Train Loss: 0.1421344\nmean_spacing_error：31.04092，col=175，count=3637，rate=4.81166%，jerk=0.03516，miniumu_ttc=76.38074\n   \n12/500. loss: 0.0022939788177609444\n12/500. mserror: 80.53028869628906  col: 424  count: 3637  jerk: 0.0005686376825906336  ttc: 78.040771484375\nEpoch: 1| Train Loss: 0.1537041\nmean_spacing_error：97.27780，col=306，count=3637，rate=8.41353%，jerk=0.00010，miniumu_ttc=105.24493\nEpoch: 2| Train Loss: 0.1520759\nmean_spacing_error：85.48347，col=348，count=3637，rate=9.56833%，jerk=0.00088，miniumu_ttc=133.76175\nEpoch: 3| Train Loss: 0.1502032\nmean_spacing_error：72.53421，col=306，count=3637，rate=8.41353%，jerk=0.00421，miniumu_ttc=106.86133\nEpoch: 4| Train Loss: 0.1475378\nmean_spacing_error：52.09934，col=342，count=3637，rate=9.40335%，jerk=0.01111，miniumu_ttc=185.44748\nEpoch: 5| Train Loss: 0.1390884\nmean_spacing_error：45.04301，col=180，count=3637，rate=4.94913%，jerk=0.04327，miniumu_ttc=53.23900\n   \n13/500. loss: 0.004987208793560664\n13/500. mserror: 78.49961853027344  col: 440  count: 3637  jerk: 0.0009577086893841624  ttc: 89.92930603027344\nEpoch: 1| Train Loss: 0.1547549\nmean_spacing_error：96.45742，col=322，count=3637，rate=8.85345%，jerk=0.00077，miniumu_ttc=152.37587\nEpoch: 2| Train Loss: 0.1518473\nmean_spacing_error：86.15362，col=345，count=3637，rate=9.48584%，jerk=0.00070，miniumu_ttc=106.05378\nEpoch: 3| Train Loss: 0.1508671\nmean_spacing_error：84.39636，col=315，count=3637，rate=8.66098%，jerk=0.00170，miniumu_ttc=120.63660\nEpoch: 4| Train Loss: 0.1484108\nmean_spacing_error：62.04908，col=346，count=3637，rate=9.51334%，jerk=0.00825，miniumu_ttc=222.20915\nEpoch: 5| Train Loss: 0.1454219\nmean_spacing_error：57.51263，col=464，count=3637，rate=12.75777%，jerk=0.05085，miniumu_ttc=21.81549\n   \n14/500. loss: 0.0032044326265652976\n14/500. mserror: 77.47481536865234  col: 458  count: 3637  jerk: 0.0008278219611383975  ttc: 76.77747344970703\nEpoch: 1| Train Loss: 0.1552983\nmean_spacing_error：100.61021，col=300，count=3637，rate=8.24856%，jerk=0.00029，miniumu_ttc=161.81744\nEpoch: 2| Train Loss: 0.1516476\nmean_spacing_error：93.14678，col=329，count=3637，rate=9.04592%，jerk=0.00056，miniumu_ttc=85.37141\nEpoch: 3| Train Loss: 0.1512284\nmean_spacing_error：94.06799，col=314，count=3637，rate=8.63349%，jerk=0.00039，miniumu_ttc=135.04488\nEpoch: 4| Train Loss: 0.1512471\nmean_spacing_error：82.84679，col=320，count=3637，rate=8.79846%，jerk=0.00173，miniumu_ttc=99.06104\nEpoch: 5| Train Loss: 0.1488646\nmean_spacing_error：35.97729，col=350，count=3637，rate=9.62332%，jerk=0.01755，miniumu_ttc=98.56312\n   \n15/500. loss: 0.0048014043519894285\n15/500. mserror: 79.40843200683594  col: 430  count: 3637  jerk: 0.0008481849799863994  ttc: 73.26435852050781\nEpoch: 1| Train Loss: 0.1553443\nmean_spacing_error：97.73239，col=306，count=3637，rate=8.41353%，jerk=0.00015，miniumu_ttc=99.86611\nEpoch: 2| Train Loss: 0.1515002\nmean_spacing_error：94.22684，col=314，count=3637，rate=8.63349%，jerk=0.00039，miniumu_ttc=132.12892\nEpoch: 3| Train Loss: 0.1513876\nmean_spacing_error：93.09621，col=305，count=3637，rate=8.38603%，jerk=0.00093，miniumu_ttc=233.67476\nEpoch: 4| Train Loss: 0.1510281\nmean_spacing_error：77.83120，col=319，count=3637，rate=8.77097%，jerk=0.00251，miniumu_ttc=113.22604\nEpoch: 5| Train Loss: 0.1481419\nmean_spacing_error：59.59193，col=299，count=3637，rate=8.22106%，jerk=0.00859，miniumu_ttc=99.19364\n   \n16/500. loss: 0.004740680878361066\n16/500. mserror: 81.78983306884766  col: 400  count: 3637  jerk: 0.000610643473919481  ttc: 152.85633850097656\nEpoch: 1| Train Loss: 0.1539769\nmean_spacing_error：97.99487，col=302，count=3637，rate=8.30355%，jerk=0.00021，miniumu_ttc=102.67019\nEpoch: 2| Train Loss: 0.1523583\nmean_spacing_error：86.85938，col=347，count=3637，rate=9.54083%，jerk=0.00133，miniumu_ttc=184.20561\nEpoch: 3| Train Loss: 0.1505998\nmean_spacing_error：92.94995，col=299，count=3637，rate=8.22106%，jerk=0.00111，miniumu_ttc=101.42381\nEpoch: 4| Train Loss: 0.1497830\nmean_spacing_error：66.76736，col=303，count=3637，rate=8.33104%，jerk=0.00573，miniumu_ttc=305.02798\nEpoch: 5| Train Loss: 0.1446109\nmean_spacing_error：46.55676，col=188，count=3637，rate=5.16910%，jerk=0.02231，miniumu_ttc=89.61107\n   \n17/500. loss: 0.0026665482049187026\n17/500. mserror: 83.82951354980469  col: 388  count: 3637  jerk: 0.0003488227666821331  ttc: 95.78008270263672\nEpoch: 1| Train Loss: 0.1540103\nmean_spacing_error：98.07986，col=299，count=3637，rate=8.22106%，jerk=0.00026，miniumu_ttc=108.57625\nEpoch: 2| Train Loss: 0.1513640\nmean_spacing_error：91.10730，col=314，count=3637，rate=8.63349%，jerk=0.00103，miniumu_ttc=131.02513\nEpoch: 3| Train Loss: 0.1509564\nmean_spacing_error：85.00590，col=307，count=3637，rate=8.44102%，jerk=0.00207，miniumu_ttc=113.23962\nEpoch: 4| Train Loss: 0.1490350\nmean_spacing_error：67.11719，col=299，count=3637，rate=8.22106%，jerk=0.00743，miniumu_ttc=88.18562\nEpoch: 5| Train Loss: 0.1421430\nmean_spacing_error：30.40499，col=237，count=3637，rate=6.51636%，jerk=0.03306，miniumu_ttc=210.56602\n   \n18/500. loss: 0.002921776846051216\n18/500. mserror: 84.98087310791016  col: 379  count: 3637  jerk: 0.00024333561304956675  ttc: 83.41893005371094\nEpoch: 1| Train Loss: 0.1542314\nmean_spacing_error：100.21955，col=290，count=3637，rate=7.97360%，jerk=0.00045，miniumu_ttc=130.98210\nEpoch: 2| Train Loss: 0.1510849\nmean_spacing_error：90.31365，col=307，count=3637，rate=8.44102%，jerk=0.00115，miniumu_ttc=88.34064\nEpoch: 3| Train Loss: 0.1497097\nmean_spacing_error：70.14082，col=319，count=3637，rate=8.77097%，jerk=0.00719，miniumu_ttc=108.47666\nEpoch: 4| Train Loss: 0.1391656\nmean_spacing_error：39.17331，col=259，count=3637，rate=7.12125%，jerk=0.04917，miniumu_ttc=44.11666\nEpoch: 5| Train Loss: 0.1364344\nmean_spacing_error：34.66758，col=202，count=3637，rate=5.55403%，jerk=0.04980，miniumu_ttc=110.81538\n   \n19/500. loss: 0.005733139192064603\n19/500. mserror: 85.23595428466797  col: 371  count: 3637  jerk: 0.00031342654256150126  ttc: 348.7688293457031\nEpoch: 1| Train Loss: 0.1536961\nmean_spacing_error：98.20832，col=299，count=3637，rate=8.22106%，jerk=0.00030，miniumu_ttc=108.88686\nEpoch: 2| Train Loss: 0.1513334\nmean_spacing_error：89.60099，col=336，count=3637，rate=9.23838%，jerk=0.00076，miniumu_ttc=781.61536\nEpoch: 3| Train Loss: 0.1499785\nmean_spacing_error：83.16642，col=314，count=3637，rate=8.63349%，jerk=0.00336，miniumu_ttc=138.87067\nEpoch: 4| Train Loss: 0.1436369\nmean_spacing_error：46.01668，col=222，count=3637，rate=6.10393%，jerk=0.02269，miniumu_ttc=90.51126\nEpoch: 5| Train Loss: 0.1303369\nmean_spacing_error：46.04172，col=149，count=3637，rate=4.09678%，jerk=0.03149，miniumu_ttc=78.01100\n   \n20/500. loss: 0.002968748720983664\n20/500. mserror: 83.94390106201172  col: 381  count: 3637  jerk: 0.0004809123638551682  ttc: 120.38370513916016\nEpoch: 1| Train Loss: 0.1539353\nmean_spacing_error：98.97549，col=295，count=3637，rate=8.11108%，jerk=0.00033，miniumu_ttc=109.67924\nEpoch: 2| Train Loss: 0.1506981\nmean_spacing_error：90.08784，col=320，count=3637，rate=8.79846%，jerk=0.00111，miniumu_ttc=975.55914\nEpoch: 3| Train Loss: 0.1470238\nmean_spacing_error：79.34660，col=234，count=3637，rate=6.43387%，jerk=0.00697，miniumu_ttc=90.09544\nEpoch: 4| Train Loss: 0.1397863\nmean_spacing_error：56.38278，col=98，count=3637，rate=2.69453%，jerk=0.03350，miniumu_ttc=48.97762\nEpoch: 5| Train Loss: 0.1245348\nmean_spacing_error：71.56333，col=123，count=3637，rate=3.38191%，jerk=0.02931，miniumu_ttc=57.57840\n   \n21/500. loss: 0.0027093676229317984\n21/500. mserror: 82.508544921875  col: 389  count: 3637  jerk: 0.0006311776814982295  ttc: 140.3699951171875\nEpoch: 1| Train Loss: 0.1538207\nmean_spacing_error：99.16809，col=299，count=3637，rate=8.22106%，jerk=0.00010，miniumu_ttc=129.34656\nEpoch: 2| Train Loss: 0.1513306\nmean_spacing_error：93.47839，col=321，count=3637，rate=8.82596%，jerk=0.00026，miniumu_ttc=114.68929\nEpoch: 3| Train Loss: 0.1511664\nmean_spacing_error：91.36127，col=322，count=3637，rate=8.85345%，jerk=0.00081，miniumu_ttc=104.14388\nEpoch: 4| Train Loss: 0.1491767\nmean_spacing_error：67.29208，col=274，count=3637，rate=7.53368%，jerk=0.00874，miniumu_ttc=120.81081\nEpoch: 5| Train Loss: 0.1394213\nmean_spacing_error：26.70077，col=88，count=3637，rate=2.41958%，jerk=0.06037，miniumu_ttc=82.96432\n   \n22/500. loss: 0.004493815203507741\n22/500. mserror: 80.89179992675781  col: 402  count: 3637  jerk: 0.0007812160765752196  ttc: 88.61747741699219\nEpoch: 1| Train Loss: 0.1539202\nmean_spacing_error：100.19469，col=296，count=3637，rate=8.13858%，jerk=0.00016，miniumu_ttc=132.11536\nEpoch: 2| Train Loss: 0.1521794\nmean_spacing_error：79.77383，col=387，count=3637，rate=10.64064%，jerk=0.00226，miniumu_ttc=101.12457\nEpoch: 3| Train Loss: 0.1501701\nmean_spacing_error：71.88746，col=345，count=3637，rate=9.48584%，jerk=0.00405，miniumu_ttc=91.97778\nEpoch: 4| Train Loss: 0.1471833\nmean_spacing_error：35.79649，col=293，count=3637，rate=8.05609%，jerk=0.02607，miniumu_ttc=197.12161\nEpoch: 5| Train Loss: 0.1469400\nmean_spacing_error：69.41682，col=785，count=3637，rate=21.58372%，jerk=0.08530，miniumu_ttc=34.12219\n   \n23/500. loss: 0.0029156357049942017\n23/500. mserror: 79.60511779785156  col: 410  count: 3637  jerk: 0.00098420272115618  ttc: 106.18547821044922\nEpoch: 1| Train Loss: 0.1539487\nmean_spacing_error：101.71779，col=294，count=3637，rate=8.08359%，jerk=0.00009，miniumu_ttc=202.60178\nEpoch: 2| Train Loss: 0.1515008\nmean_spacing_error：91.73110，col=323，count=3637，rate=8.88095%，jerk=0.00074，miniumu_ttc=86.18508\nEpoch: 3| Train Loss: 0.1514889\nmean_spacing_error：89.36285，col=330，count=3637，rate=9.07341%，jerk=0.00087，miniumu_ttc=95.05872\nEpoch: 4| Train Loss: 0.1507531\nmean_spacing_error：83.72162，col=305，count=3637，rate=8.38603%，jerk=0.00234，miniumu_ttc=99.77307\nEpoch: 5| Train Loss: 0.1487786\nmean_spacing_error：59.16972，col=325，count=3637，rate=8.93594%，jerk=0.00998，miniumu_ttc=132.84563\n   \n24/500. loss: 0.002914740703999996\n24/500. mserror: 78.79322052001953  col: 417  count: 3637  jerk: 0.001175831537693739  ttc: 112.64387512207031\nEpoch: 1| Train Loss: 0.1542601\nmean_spacing_error：102.36275，col=288，count=3637，rate=7.91861%，jerk=0.00028，miniumu_ttc=127.49862\nEpoch: 2| Train Loss: 0.1515225\nmean_spacing_error：89.56829，col=326，count=3637，rate=8.96343%，jerk=0.00091，miniumu_ttc=157.04643\nEpoch: 3| Train Loss: 0.1509299\nmean_spacing_error：88.18002，col=307，count=3637，rate=8.44102%，jerk=0.00175，miniumu_ttc=169.56320\nEpoch: 4| Train Loss: 0.1503324\nmean_spacing_error：76.28562，col=395，count=3637，rate=10.86060%，jerk=0.00473，miniumu_ttc=112.36195\nEpoch: 5| Train Loss: 0.1492321\nmean_spacing_error：62.53739，col=307，count=3637，rate=8.44102%，jerk=0.00876，miniumu_ttc=126.60744\n   \n25/500. loss: 0.0044002945845325785\n25/500. mserror: 78.2872543334961  col: 409  count: 3637  jerk: 0.001430098433047533  ttc: 803.7254028320312\nEpoch: 1| Train Loss: 0.1539692\nmean_spacing_error：102.76330，col=288，count=3637，rate=7.91861%，jerk=0.00020，miniumu_ttc=113.96797\nEpoch: 2| Train Loss: 0.1519213\nmean_spacing_error：87.71977，col=358，count=3637，rate=9.84328%，jerk=0.00099，miniumu_ttc=83.63340\nEpoch: 3| Train Loss: 0.1510199\nmean_spacing_error：89.20575，col=331，count=3637，rate=9.10091%，jerk=0.00097，miniumu_ttc=109.30256\nEpoch: 4| Train Loss: 0.1502240\nmean_spacing_error：60.82899，col=313，count=3637，rate=8.60599%，jerk=0.00699，miniumu_ttc=135.58813\nEpoch: 5| Train Loss: 0.1472806\nmean_spacing_error：37.43665，col=390，count=3637，rate=10.72312%，jerk=0.02236，miniumu_ttc=105.02756\n   \n26/500. loss: 0.004646396264433861\n26/500. mserror: 78.82299041748047  col: 391  count: 3637  jerk: 0.0015770923346281052  ttc: 95.00653076171875\nEpoch: 1| Train Loss: 0.1536991\nmean_spacing_error：102.74430，col=289，count=3637，rate=7.94611%，jerk=0.00014，miniumu_ttc=110.36714\nEpoch: 2| Train Loss: 0.1516997\nmean_spacing_error：81.86116，col=387，count=3637，rate=10.64064%，jerk=0.00211，miniumu_ttc=85.00655\nEpoch: 3| Train Loss: 0.1505276\nmean_spacing_error：74.49625，col=307，count=3637，rate=8.44102%，jerk=0.00393，miniumu_ttc=99.95345\nEpoch: 4| Train Loss: 0.1487535\nmean_spacing_error：46.09145，col=347，count=3637，rate=9.54083%，jerk=0.01361，miniumu_ttc=86.54803\nEpoch: 5| Train Loss: 0.1417151\nmean_spacing_error：70.35894，col=330，count=3637，rate=9.07341%，jerk=0.06535，miniumu_ttc=17.16047\n   \n27/500. loss: 0.003008659929037094\n27/500. mserror: 78.00595092773438  col: 384  count: 3637  jerk: 0.001965821487829089  ttc: 90.688720703125\nEpoch: 1| Train Loss: 0.1536104\nmean_spacing_error：101.94184，col=294，count=3637，rate=8.08359%，jerk=0.00009，miniumu_ttc=202.11520\nEpoch: 2| Train Loss: 0.1521786\nmean_spacing_error：71.60715，col=408，count=3637，rate=11.21804%，jerk=0.00267，miniumu_ttc=74.80044\nEpoch: 3| Train Loss: 0.1498580\nmean_spacing_error：59.45754，col=324，count=3637，rate=8.90844%，jerk=0.00623，miniumu_ttc=75.17194\nEpoch: 4| Train Loss: 0.1482120\nmean_spacing_error：52.85865，col=295，count=3637，rate=8.11108%，jerk=0.01364，miniumu_ttc=202.20703\nEpoch: 5| Train Loss: 0.1405255\nmean_spacing_error：44.97582，col=302，count=3637，rate=8.30355%，jerk=0.05781，miniumu_ttc=78.76225\n   \n28/500. loss: 0.003372702126701673\n28/500. mserror: 78.23465728759766  col: 365  count: 3637  jerk: 0.002300162799656391  ttc: 252.8479461669922\nEpoch: 1| Train Loss: 0.1534300\nmean_spacing_error：100.04431，col=295，count=3637，rate=8.11108%，jerk=0.00028，miniumu_ttc=142.38730\nEpoch: 2| Train Loss: 0.1528699\nmean_spacing_error：62.40240，col=374，count=3637，rate=10.28320%，jerk=0.00488，miniumu_ttc=89.20727\nEpoch: 3| Train Loss: 0.1481016\nmean_spacing_error：43.30891，col=333，count=3637，rate=9.15590%，jerk=0.01302，miniumu_ttc=960.23254\nEpoch: 4| Train Loss: 0.1453089\nmean_spacing_error：42.85191，col=285，count=3637，rate=7.83613%，jerk=0.02752，miniumu_ttc=1662.83557\nEpoch: 5| Train Loss: 0.1445511\nmean_spacing_error：42.30104，col=253，count=3637，rate=6.95628%，jerk=0.04491，miniumu_ttc=54.96389\n   \n29/500. loss: 0.00321886253853639\n29/500. mserror: 75.93679809570312  col: 362  count: 3637  jerk: 0.003076269757002592  ttc: 151.91748046875\nEpoch: 1| Train Loss: 0.1534004\nmean_spacing_error：100.45361，col=295，count=3637，rate=8.11108%，jerk=0.00022，miniumu_ttc=139.54338\nEpoch: 2| Train Loss: 0.1529373\nmean_spacing_error：54.75948，col=405，count=3637，rate=11.13555%，jerk=0.00586，miniumu_ttc=134.88820\nEpoch: 3| Train Loss: 0.1466438\nmean_spacing_error：45.72026，col=292，count=3637，rate=8.02859%，jerk=0.01582，miniumu_ttc=98.35896\nEpoch: 4| Train Loss: 0.1440902\nmean_spacing_error：38.47937，col=279，count=3637，rate=7.67116%，jerk=0.05660，miniumu_ttc=228.58119\nEpoch: 5| Train Loss: 0.1382440\nmean_spacing_error：58.31510，col=263，count=3637，rate=7.23123%，jerk=0.05273，miniumu_ttc=123.38338\n   \n30/500. loss: 0.004178135966261228\n30/500. mserror: 80.90946197509766  col: 341  count: 3637  jerk: 0.0022345345932990313  ttc: 129.78396606445312\nEpoch: 1| Train Loss: 0.1535680\nmean_spacing_error：99.31557，col=295，count=3637，rate=8.11108%，jerk=0.00047，miniumu_ttc=135.51530\nEpoch: 2| Train Loss: 0.1523341\nmean_spacing_error：84.25363，col=358，count=3637，rate=9.84328%，jerk=0.00075，miniumu_ttc=140.05878\nEpoch: 3| Train Loss: 0.1509489\nmean_spacing_error：83.53442，col=311，count=3637，rate=8.55100%，jerk=0.00236，miniumu_ttc=92.85617\nEpoch: 4| Train Loss: 0.1490484\nmean_spacing_error：53.54178，col=315，count=3637，rate=8.66098%，jerk=0.01210，miniumu_ttc=148.90192\nEpoch: 5| Train Loss: 0.1455201\nmean_spacing_error：83.40806，col=605，count=3637，rate=16.63459%，jerk=0.04474，miniumu_ttc=42.87903\n   \n31/500. loss: 0.0028827162459492683\n31/500. mserror: 70.13404083251953  col: 343  count: 3637  jerk: 0.004504697863012552  ttc: 161.99192810058594\nEpoch: 1| Train Loss: 0.1538134\nmean_spacing_error：99.61141，col=288，count=3637，rate=7.91861%，jerk=0.00115，miniumu_ttc=111.47665\nEpoch: 2| Train Loss: 0.1517306\nmean_spacing_error：74.81141，col=356，count=3637，rate=9.78829%，jerk=0.00280，miniumu_ttc=104.68356\nEpoch: 3| Train Loss: 0.1501706\nmean_spacing_error：59.95894，col=319，count=3637，rate=8.77097%，jerk=0.00735，miniumu_ttc=94.69952\nEpoch: 4| Train Loss: 0.1476340\nmean_spacing_error：34.75521，col=408，count=3637，rate=11.21804%，jerk=0.02837，miniumu_ttc=102.17580\nEpoch: 5| Train Loss: 0.1440748\nmean_spacing_error：83.56663，col=436，count=3637，rate=11.98790%，jerk=0.10125，miniumu_ttc=24.09686\n   \n32/500. loss: 0.002576524702211221\n32/500. mserror: 68.24241638183594  col: 412  count: 3637  jerk: 0.008644243702292442  ttc: 77.72881317138672\nEpoch: 1| Train Loss: 0.1512751\nmean_spacing_error：86.95234，col=279，count=3637，rate=7.67116%，jerk=0.00462，miniumu_ttc=213.02223\nEpoch: 2| Train Loss: 0.1609215\nmean_spacing_error：58.11436，col=502，count=3637，rate=13.80258%，jerk=0.06010，miniumu_ttc=43.12289\nEpoch: 3| Train Loss: 0.1373941\nmean_spacing_error：41.94718，col=204，count=3637，rate=5.60902%，jerk=0.02770，miniumu_ttc=152.28072\nEpoch: 4| Train Loss: 0.1342895\nmean_spacing_error：29.90165，col=193，count=3637，rate=5.30657%，jerk=0.03991，miniumu_ttc=140.59929\nEpoch: 5| Train Loss: 0.1284536\nmean_spacing_error：27.38161，col=95，count=3637，rate=2.61204%，jerk=0.06986，miniumu_ttc=72.96951\n   \n33/500. loss: 0.002557754827042421\n33/500. mserror: 73.26528930664062  col: 309  count: 3637  jerk: 0.004417503252625465  ttc: 106.31784057617188\nEpoch: 1| Train Loss: 0.1543494\nmean_spacing_error：103.75248，col=288，count=3637，rate=7.91861%，jerk=0.00063，miniumu_ttc=137.85127\nEpoch: 2| Train Loss: 0.1516500\nmean_spacing_error：82.88104，col=345，count=3637，rate=9.48584%，jerk=0.00146，miniumu_ttc=88.76954\nEpoch: 3| Train Loss: 0.1496551\nmean_spacing_error：78.78715，col=306，count=3637，rate=8.41353%，jerk=0.00333，miniumu_ttc=133.90013\nEpoch: 4| Train Loss: 0.1452443\nmean_spacing_error：58.85835，col=245，count=3637，rate=6.73632%，jerk=0.01119，miniumu_ttc=99.09615\nEpoch: 5| Train Loss: 0.1306700\nmean_spacing_error：43.07108，col=138，count=3637，rate=3.79434%，jerk=0.03711，miniumu_ttc=74.56319\n   \n34/500. loss: 0.004445925354957581\n34/500. mserror: 88.7467269897461  col: 287  count: 3637  jerk: 0.003064086427912116  ttc: 747.9163208007812\nEpoch: 1| Train Loss: 0.1523980\nmean_spacing_error：84.44227，col=324，count=3637，rate=8.90844%，jerk=0.00166，miniumu_ttc=118.95368\nEpoch: 2| Train Loss: 0.1505514\nmean_spacing_error：90.96824，col=272，count=3637，rate=7.47869%，jerk=0.00293，miniumu_ttc=127.96126\nEpoch: 3| Train Loss: 0.1472619\nmean_spacing_error：83.65337，col=257，count=3637，rate=7.06626%，jerk=0.00479，miniumu_ttc=329.36087\nEpoch: 4| Train Loss: 0.1387647\nmean_spacing_error：67.55582，col=78，count=3637，rate=2.14462%，jerk=0.02039，miniumu_ttc=118.12071\nEpoch: 5| Train Loss: 0.1263411\nmean_spacing_error：65.10314，col=307，count=3637，rate=8.44102%，jerk=0.02011，miniumu_ttc=128.72438\n   \n35/500. loss: 0.0024173809215426445\n35/500. mserror: 94.51864624023438  col: 280  count: 3637  jerk: 0.00262144161388278  ttc: 97.98469543457031\nEpoch: 1| Train Loss: 0.1531363\nmean_spacing_error：84.95043，col=325，count=3637，rate=8.93594%，jerk=0.00150，miniumu_ttc=91.92732\nEpoch: 2| Train Loss: 0.1505218\nmean_spacing_error：89.37307，col=283，count=3637，rate=7.78114%，jerk=0.00311，miniumu_ttc=106.49889\nEpoch: 3| Train Loss: 0.1475569\nmean_spacing_error：85.62508，col=209，count=3637，rate=5.74649%，jerk=0.00607，miniumu_ttc=88.58833\nEpoch: 4| Train Loss: 0.1391129\nmean_spacing_error：66.86388，col=102，count=3637，rate=2.80451%，jerk=0.01859，miniumu_ttc=93.25542\nEpoch: 5| Train Loss: 0.1262882\nmean_spacing_error：76.31236，col=21，count=3637，rate=0.57740%，jerk=0.03437，miniumu_ttc=78.18887\n   \n36/500. loss: 0.004045011475682259\n36/500. mserror: 92.0654067993164  col: 279  count: 3637  jerk: 0.002962361555546522  ttc: 93.5755844116211\nEpoch: 1| Train Loss: 0.1522966\nmean_spacing_error：84.11667，col=324，count=3637，rate=8.90844%，jerk=0.00175，miniumu_ttc=137.15237\nEpoch: 2| Train Loss: 0.1516696\nmean_spacing_error：90.58380，col=238，count=3637，rate=6.54385%，jerk=0.00430，miniumu_ttc=118.59355\nEpoch: 3| Train Loss: 0.1454199\nmean_spacing_error：94.35172，col=103，count=3637，rate=2.83200%，jerk=0.01606，miniumu_ttc=275.70346\nEpoch: 4| Train Loss: 0.1337569\nmean_spacing_error：78.61076，col=123，count=3637，rate=3.38191%，jerk=0.01551，miniumu_ttc=117.41612\nEpoch: 5| Train Loss: 0.1268308\nmean_spacing_error：73.44790，col=118，count=3637，rate=3.24443%，jerk=0.02176，miniumu_ttc=94.34594\n   \n37/500. loss: 0.002989154619475206\n37/500. mserror: 83.95724487304688  col: 289  count: 3637  jerk: 0.0031654774211347103  ttc: 156.22726440429688\nEpoch: 1| Train Loss: 0.1509658\nmean_spacing_error：83.63383，col=300，count=3637，rate=8.24856%，jerk=0.00273，miniumu_ttc=411.92349\nEpoch: 2| Train Loss: 0.1498149\nmean_spacing_error：88.17590，col=150，count=3637，rate=4.12428%，jerk=0.00936，miniumu_ttc=182.26897\nEpoch: 3| Train Loss: 0.1435343\nmean_spacing_error：76.01310，col=199，count=3637，rate=5.47154%，jerk=0.01043，miniumu_ttc=99.83004\nEpoch: 4| Train Loss: 0.1290782\nmean_spacing_error：62.52157，col=156，count=3637，rate=4.28925%，jerk=0.01801，miniumu_ttc=69.34026\nEpoch: 5| Train Loss: 0.1230174\nmean_spacing_error：49.00718，col=149，count=3637，rate=4.09678%，jerk=0.02924，miniumu_ttc=67.74425\n   \n38/500. loss: 0.0025710522507627807\n38/500. mserror: 76.77284240722656  col: 315  count: 3637  jerk: 0.002959060249850154  ttc: 113.26288604736328\nEpoch: 1| Train Loss: 0.1502475\nmean_spacing_error：83.11616，col=312，count=3637，rate=8.57850%，jerk=0.00242，miniumu_ttc=247.81401\nEpoch: 2| Train Loss: 0.1494138\nmean_spacing_error：78.33868，col=167，count=3637，rate=4.59170%，jerk=0.00955，miniumu_ttc=103.11589\nEpoch: 3| Train Loss: 0.1364465\nmean_spacing_error：74.82788，col=104，count=3637，rate=2.85950%，jerk=0.02019，miniumu_ttc=100.45607\nEpoch: 4| Train Loss: 0.1229405\nmean_spacing_error：69.47222，col=115，count=3637，rate=3.16195%，jerk=0.03275，miniumu_ttc=726.71143\nEpoch: 5| Train Loss: 0.1218351\nmean_spacing_error：45.22374，col=99，count=3637，rate=2.72202%，jerk=0.03344，miniumu_ttc=67.75886\n   \n39/500. loss: 0.00331171415746212\n39/500. mserror: 74.07515716552734  col: 330  count: 3637  jerk: 0.003034336259588599  ttc: 95.9425048828125\nEpoch: 1| Train Loss: 0.1502709\nmean_spacing_error：76.67480，col=326，count=3637，rate=8.96343%，jerk=0.00328，miniumu_ttc=101.09412\nEpoch: 2| Train Loss: 0.1472457\nmean_spacing_error：57.47651，col=176，count=3637，rate=4.83915%，jerk=0.01513，miniumu_ttc=301.73630\nEpoch: 3| Train Loss: 0.1345119\nmean_spacing_error：41.47879，col=225，count=3637，rate=6.18642%，jerk=0.02830，miniumu_ttc=76.80132\nEpoch: 4| Train Loss: 0.1265679\nmean_spacing_error：36.13990，col=93，count=3637，rate=2.55705%，jerk=0.05520，miniumu_ttc=113.43834\nEpoch: 5| Train Loss: 0.1126366\nmean_spacing_error：26.80572，col=66，count=3637，rate=1.81468%，jerk=0.06042，miniumu_ttc=75.67647\n   \n40/500. loss: 0.0020421138033270836\n40/500. mserror: 72.54841613769531  col: 327  count: 3637  jerk: 0.0034728222526609898  ttc: 101.7700424194336\nEpoch: 1| Train Loss: 0.1499291\nmean_spacing_error：75.24237，col=332，count=3637，rate=9.12840%，jerk=0.00383，miniumu_ttc=103.57649\nEpoch: 2| Train Loss: 0.1534727\nmean_spacing_error：48.33587，col=348，count=3637，rate=9.56833%，jerk=0.01011，miniumu_ttc=76.36801\nEpoch: 3| Train Loss: 0.1391299\nmean_spacing_error：36.31036，col=175，count=3637，rate=4.81166%，jerk=0.03093，miniumu_ttc=539.53943\nEpoch: 4| Train Loss: 0.1330079\nmean_spacing_error：38.71288，col=307，count=3637，rate=8.44102%，jerk=0.04301，miniumu_ttc=47.89498\nEpoch: 5| Train Loss: 0.1229039\nmean_spacing_error：39.47184，col=338，count=3637，rate=9.29337%，jerk=0.07490，miniumu_ttc=180.34447\n   \n41/500. loss: 0.0030879070982337\n41/500. mserror: 70.1072769165039  col: 315  count: 3637  jerk: 0.004543699789792299  ttc: 87.20397186279297\nEpoch: 1| Train Loss: 0.1507251\nmean_spacing_error：82.12086，col=284，count=3637，rate=7.80863%，jerk=0.00333，miniumu_ttc=283.27408\nEpoch: 2| Train Loss: 0.1429260\nmean_spacing_error：65.76689，col=105，count=3637，rate=2.88699%，jerk=0.01861，miniumu_ttc=120.69521\nEpoch: 3| Train Loss: 0.1335969\nmean_spacing_error：57.92402，col=115，count=3637，rate=3.16195%，jerk=0.02043，miniumu_ttc=85.42035\nEpoch: 4| Train Loss: 0.1236347\nmean_spacing_error：47.93155，col=93，count=3637，rate=2.55705%，jerk=0.03122，miniumu_ttc=110.24922\nEpoch: 5| Train Loss: 0.1178702\nmean_spacing_error：39.05317，col=118，count=3637，rate=3.24443%，jerk=0.03229，miniumu_ttc=106.16514\n   \n42/500. loss: 0.003032894184192022\n42/500. mserror: 67.6624526977539  col: 293  count: 3637  jerk: 0.0061064050532877445  ttc: 76.43575286865234\nEpoch: 1| Train Loss: 0.1504498\nmean_spacing_error：85.71410，col=251，count=3637，rate=6.90129%，jerk=0.00482，miniumu_ttc=90.47839\nEpoch: 2| Train Loss: 0.1423718\nmean_spacing_error：84.44269，col=54，count=3637，rate=1.48474%，jerk=0.02052，miniumu_ttc=124.09344\nEpoch: 3| Train Loss: 0.1316032\nmean_spacing_error：78.87687，col=141，count=3637，rate=3.87682%，jerk=0.01170，miniumu_ttc=103.75390\nEpoch: 4| Train Loss: 0.1276139\nmean_spacing_error：77.36591，col=425，count=3637，rate=11.68546%，jerk=0.00766，miniumu_ttc=57.04075\nEpoch: 5| Train Loss: 0.1250665\nmean_spacing_error：77.31038，col=123，count=3637，rate=3.38191%，jerk=0.01759，miniumu_ttc=110.20380\n   \n43/500. loss: 0.003946265205740929\n43/500. mserror: 70.98152923583984  col: 226  count: 3637  jerk: 0.008045045658946037  ttc: 141.11810302734375\nEpoch: 1| Train Loss: 0.1525776\nmean_spacing_error：86.88976，col=276，count=3637，rate=7.58867%，jerk=0.00408，miniumu_ttc=176.95404\nEpoch: 2| Train Loss: 0.1450824\nmean_spacing_error：88.08922，col=137，count=3637，rate=3.76684%，jerk=0.00970，miniumu_ttc=102.46744\nEpoch: 3| Train Loss: 0.1373333\nmean_spacing_error：78.86701，col=60，count=3637，rate=1.64971%，jerk=0.02035，miniumu_ttc=116.51707\nEpoch: 4| Train Loss: 0.1284714\nmean_spacing_error：74.83984，col=82，count=3637，rate=2.25461%，jerk=0.02075，miniumu_ttc=101.47533\nEpoch: 5| Train Loss: 0.1252598\nmean_spacing_error：88.14140，col=40，count=3637，rate=1.09981%，jerk=0.02747，miniumu_ttc=90.90925\n   \n44/500. loss: 0.0021658016679187617\n44/500. mserror: 71.59859466552734  col: 203  count: 3637  jerk: 0.009938279166817665  ttc: 122.37863159179688\nEpoch: 1| Train Loss: 0.1555490\nmean_spacing_error：126.37312，col=171，count=3637，rate=4.70168%，jerk=0.00436，miniumu_ttc=109.43980\nEpoch: 2| Train Loss: 0.1491848\nmean_spacing_error：79.99571，col=284，count=3637，rate=7.80863%，jerk=0.00405，miniumu_ttc=136.13048\nEpoch: 3| Train Loss: 0.1390593\nmean_spacing_error：77.91895，col=87，count=3637，rate=2.39208%，jerk=0.01895，miniumu_ttc=107.91434\nEpoch: 4| Train Loss: 0.1287328\nmean_spacing_error：69.80122，col=123，count=3637，rate=3.38191%，jerk=0.01692，miniumu_ttc=88.62584\nEpoch: 5| Train Loss: 0.1240773\nmean_spacing_error：63.40550，col=128，count=3637，rate=3.51938%，jerk=0.02469，miniumu_ttc=103.51534\n   \n45/500. loss: 0.00272444107880195\n45/500. mserror: 60.84756088256836  col: 208  count: 3637  jerk: 0.011811522766947746  ttc: 102.53515625\nEpoch: 1| Train Loss: 0.1544617\nmean_spacing_error：129.00983，col=169，count=3637，rate=4.64669%，jerk=0.00467，miniumu_ttc=121.48515\nEpoch: 2| Train Loss: 0.1499565\nmean_spacing_error：82.39219，col=316，count=3637，rate=8.68848%，jerk=0.00324，miniumu_ttc=149.70374\nEpoch: 3| Train Loss: 0.1413370\nmean_spacing_error：88.09019，col=118，count=3637，rate=3.24443%，jerk=0.01115，miniumu_ttc=88.88004\nEpoch: 4| Train Loss: 0.1353975\nmean_spacing_error：69.50732，col=96，count=3637，rate=2.63954%，jerk=0.01768，miniumu_ttc=87.61097\nEpoch: 5| Train Loss: 0.1251639\nmean_spacing_error：71.86434，col=36，count=3637，rate=0.98983%，jerk=0.03335，miniumu_ttc=73.13857\n   \n46/500. loss: 0.003907659401496251\n46/500. mserror: 57.144657135009766  col: 217  count: 3637  jerk: 0.013955539092421532  ttc: 171.8362274169922\nEpoch: 1| Train Loss: 0.1542691\nmean_spacing_error：135.05807，col=121，count=3637，rate=3.32692%，jerk=0.00802，miniumu_ttc=181.40192\nEpoch: 2| Train Loss: 0.1472560\nmean_spacing_error：83.76700，col=254，count=3637，rate=6.98378%，jerk=0.00557，miniumu_ttc=75.05116\nEpoch: 3| Train Loss: 0.1343590\nmean_spacing_error：76.46722，col=93，count=3637，rate=2.55705%，jerk=0.02490，miniumu_ttc=172.13350\nEpoch: 4| Train Loss: 0.1286601\nmean_spacing_error：68.74984，col=96，count=3637，rate=2.63954%，jerk=0.01903，miniumu_ttc=117.10099\nEpoch: 5| Train Loss: 0.1241146\nmean_spacing_error：72.84143，col=80，count=3637，rate=2.19962%，jerk=0.02549，miniumu_ttc=125.49506\n   \n47/500. loss: 0.003969701627890269\n47/500. mserror: 54.023372650146484  col: 192  count: 3637  jerk: 0.016893574967980385  ttc: 63.795841217041016\nEpoch: 1| Train Loss: 0.1545553\nmean_spacing_error：121.81144，col=162，count=3637，rate=4.45422%，jerk=0.00617，miniumu_ttc=383.36069\nEpoch: 2| Train Loss: 0.1488228\nmean_spacing_error：80.02396，col=302，count=3637，rate=8.30355%，jerk=0.00392，miniumu_ttc=91.54432\nEpoch: 3| Train Loss: 0.1393982\nmean_spacing_error：85.04627，col=135，count=3637，rate=3.71185%，jerk=0.01038，miniumu_ttc=121.63499\nEpoch: 4| Train Loss: 0.1347300\nmean_spacing_error：74.67804，col=129，count=3637，rate=3.54688%，jerk=0.01357，miniumu_ttc=303.99081\nEpoch: 5| Train Loss: 0.1287776\nmean_spacing_error：70.61585，col=160，count=3637，rate=4.39923%，jerk=0.01494，miniumu_ttc=73.92185\n   \n48/500. loss: 0.0028591115648547807\n48/500. mserror: 50.78409957885742  col: 161  count: 3637  jerk: 0.020687604323029518  ttc: 104.08137512207031\nEpoch: 1| Train Loss: 0.1558428\nmean_spacing_error：120.95435，col=145，count=3637，rate=3.98680%，jerk=0.00762，miniumu_ttc=184.43176\nEpoch: 2| Train Loss: 0.1470375\nmean_spacing_error：76.72838，col=283，count=3637，rate=7.78114%，jerk=0.00517，miniumu_ttc=192.89857\nEpoch: 3| Train Loss: 0.1363832\nmean_spacing_error：77.28969，col=107，count=3637，rate=2.94199%，jerk=0.01446，miniumu_ttc=80.26495\nEpoch: 4| Train Loss: 0.1320714\nmean_spacing_error：70.57167，col=79，count=3637，rate=2.17212%，jerk=0.01987，miniumu_ttc=185.04237\nEpoch: 5| Train Loss: 0.1284890\nmean_spacing_error：72.76685，col=181，count=3637，rate=4.97663%，jerk=0.01369，miniumu_ttc=93.49479\n   \n49/500. loss: 0.0037970508759220443\n49/500. mserror: 46.064239501953125  col: 122  count: 3637  jerk: 0.02852196805179119  ttc: 79.84973907470703\nEpoch: 1| Train Loss: 0.1621880\nmean_spacing_error：117.32661，col=127，count=3637，rate=3.49189%，jerk=0.00943，miniumu_ttc=130.92815\nEpoch: 2| Train Loss: 0.1451858\nmean_spacing_error：76.40243，col=267，count=3637，rate=7.34122%，jerk=0.00616，miniumu_ttc=173.16818\nEpoch: 3| Train Loss: 0.1347413\nmean_spacing_error：76.96751，col=104，count=3637，rate=2.85950%，jerk=0.01455，miniumu_ttc=308.79666\nEpoch: 4| Train Loss: 0.1296382\nmean_spacing_error：70.23351，col=126，count=3637，rate=3.46439%，jerk=0.01768，miniumu_ttc=85.73679\nEpoch: 5| Train Loss: 0.1292928\nmean_spacing_error：59.83448，col=86，count=3637，rate=2.36459%，jerk=0.02496，miniumu_ttc=68.76526\n   \n50/500. loss: 0.0035931256910165152\n50/500. mserror: 51.33969497680664  col: 77  count: 3637  jerk: 0.03916085138916969  ttc: 131.19358825683594\nEpoch: 1| Train Loss: 0.1759617\nmean_spacing_error：114.11243，col=105，count=3637，rate=2.88699%，jerk=0.01331，miniumu_ttc=121.14376\nEpoch: 2| Train Loss: 0.1404592\nmean_spacing_error：66.98067，col=346，count=3637，rate=9.51334%，jerk=0.00901，miniumu_ttc=87.63435\nEpoch: 3| Train Loss: 0.1307388\nmean_spacing_error：54.35859，col=78，count=3637，rate=2.14462%，jerk=0.02691，miniumu_ttc=69.49150\nEpoch: 4| Train Loss: 0.1292070\nmean_spacing_error：58.98960，col=159，count=3637，rate=4.37173%，jerk=0.02121，miniumu_ttc=204.74840\nEpoch: 5| Train Loss: 0.1220127\nmean_spacing_error：75.66677，col=72，count=3637，rate=1.97965%，jerk=0.02097，miniumu_ttc=169.95758\n   \n51/500. loss: 0.0033144718036055565\n51/500. mserror: 46.39248275756836  col: 145  count: 3637  jerk: 0.03861693665385246  ttc: 538.3751220703125\nEpoch: 1| Train Loss: 0.1716746\nmean_spacing_error：106.25961，col=106，count=3637，rate=2.91449%，jerk=0.01331，miniumu_ttc=118.72169\nEpoch: 2| Train Loss: 0.1365118\nmean_spacing_error：59.74098，col=175，count=3637，rate=4.81166%，jerk=0.01512，miniumu_ttc=80.82700\nEpoch: 3| Train Loss: 0.1276673\nmean_spacing_error：58.43862，col=127，count=3637，rate=3.49189%，jerk=0.02046，miniumu_ttc=115.97770\nEpoch: 4| Train Loss: 0.1254373\nmean_spacing_error：64.36872，col=75，count=3637，rate=2.06214%，jerk=0.02840，miniumu_ttc=120.32495\nEpoch: 5| Train Loss: 0.1184002\nmean_spacing_error：78.82876，col=31，count=3637，rate=0.85235%，jerk=0.03026，miniumu_ttc=120.98629\n   \n52/500. loss: 0.0021882842605312667\n52/500. mserror: 48.541770935058594  col: 182  count: 3637  jerk: 0.03459133207798004  ttc: 67.13408660888672\nEpoch: 1| Train Loss: 0.1698160\nmean_spacing_error：113.47961，col=105，count=3637，rate=2.88699%，jerk=0.01262，miniumu_ttc=294.07440\nEpoch: 2| Train Loss: 0.1370420\nmean_spacing_error：68.04179，col=230，count=3637，rate=6.32389%，jerk=0.01064，miniumu_ttc=83.94726\nEpoch: 3| Train Loss: 0.1308982\nmean_spacing_error：56.59648，col=73，count=3637，rate=2.00715%，jerk=0.02833，miniumu_ttc=126.76203\nEpoch: 4| Train Loss: 0.1264181\nmean_spacing_error：52.11577，col=126，count=3637，rate=3.46439%，jerk=0.02575，miniumu_ttc=84.99698\nEpoch: 5| Train Loss: 0.1199446\nmean_spacing_error：56.31674，col=51，count=3637，rate=1.40225%，jerk=0.02997，miniumu_ttc=123.79629\n   \n53/500. loss: 0.0035582507650057473\n53/500. mserror: 47.55051803588867  col: 114  count: 3637  jerk: 0.035244978964328766  ttc: 57.88154983520508\nEpoch: 1| Train Loss: 0.1727250\nmean_spacing_error：111.97270，col=106，count=3637，rate=2.91449%，jerk=0.01201，miniumu_ttc=162.44798\nEpoch: 2| Train Loss: 0.1351455\nmean_spacing_error：68.41648，col=127，count=3637，rate=3.49189%，jerk=0.01531，miniumu_ttc=116.29649\nEpoch: 3| Train Loss: 0.1271797\nmean_spacing_error：59.58206，col=118，count=3637，rate=3.24443%，jerk=0.02256，miniumu_ttc=65.30013\nEpoch: 4| Train Loss: 0.1264344\nmean_spacing_error：56.72854，col=111，count=3637，rate=3.05197%，jerk=0.02386，miniumu_ttc=73.27570\nEpoch: 5| Train Loss: 0.1174896\nmean_spacing_error：72.16238，col=89，count=3637，rate=2.44707%，jerk=0.01911，miniumu_ttc=89.69836\n   \n54/500. loss: 0.00211419848104318\n54/500. mserror: 57.55512619018555  col: 67  count: 3637  jerk: 0.04001368582248688  ttc: 70.92349243164062\nEpoch: 1| Train Loss: 0.1833173\nmean_spacing_error：148.03273，col=66，count=3637，rate=1.81468%，jerk=0.01656，miniumu_ttc=102.17375\nEpoch: 2| Train Loss: 0.1441156\nmean_spacing_error：73.32082，col=221，count=3637，rate=6.07644%，jerk=0.00934，miniumu_ttc=375.12326\nEpoch: 3| Train Loss: 0.1322424\nmean_spacing_error：72.79613，col=71，count=3637，rate=1.95216%，jerk=0.02409，miniumu_ttc=156.19350\nEpoch: 4| Train Loss: 0.1335847\nmean_spacing_error：69.88148，col=119，count=3637，rate=3.27193%，jerk=0.01531，miniumu_ttc=105.51482\nEpoch: 5| Train Loss: 0.1250105\nmean_spacing_error：73.77391，col=44，count=3637，rate=1.20979%，jerk=0.02388，miniumu_ttc=182.43614\n   \n55/500. loss: 0.002292043219010035\n55/500. mserror: 52.24038314819336  col: 89  count: 3637  jerk: 0.03626560792326927  ttc: 67.64595794677734\nEpoch: 1| Train Loss: 0.1721913\nmean_spacing_error：120.92731，col=84，count=3637，rate=2.30960%，jerk=0.01410，miniumu_ttc=81.04434\nEpoch: 2| Train Loss: 0.1345904\nmean_spacing_error：70.41281，col=139，count=3637，rate=3.82183%，jerk=0.01504，miniumu_ttc=78.31742\nEpoch: 3| Train Loss: 0.1264885\nmean_spacing_error：61.25758，col=74，count=3637，rate=2.03464%，jerk=0.02494，miniumu_ttc=142.95743\nEpoch: 4| Train Loss: 0.1188479\nmean_spacing_error：60.45088，col=35，count=3637，rate=0.96233%，jerk=0.02828，miniumu_ttc=80.40566\nEpoch: 5| Train Loss: 0.1276438\nmean_spacing_error：66.43373，col=155，count=3637，rate=4.26175%，jerk=0.01635，miniumu_ttc=122.00223\n   \n56/500. loss: 0.0021897529562314353\n56/500. mserror: 50.833763122558594  col: 128  count: 3637  jerk: 0.030159883201122284  ttc: 68.52078247070312\nEpoch: 1| Train Loss: 0.1638863\nmean_spacing_error：115.99631，col=99，count=3637，rate=2.72202%，jerk=0.01423，miniumu_ttc=111.36246\nEpoch: 2| Train Loss: 0.1314470\nmean_spacing_error：68.20724，col=94，count=3637，rate=2.58455%，jerk=0.01862，miniumu_ttc=141.34941\nEpoch: 3| Train Loss: 0.1235276\nmean_spacing_error：61.76332，col=144，count=3637，rate=3.95931%，jerk=0.01958，miniumu_ttc=93.09670\nEpoch: 4| Train Loss: 0.1168627\nmean_spacing_error：56.27463，col=54，count=3637，rate=1.48474%，jerk=0.02729，miniumu_ttc=77.62228\nEpoch: 5| Train Loss: 0.1275244\nmean_spacing_error：61.73321，col=107，count=3637，rate=2.94199%，jerk=0.01945，miniumu_ttc=89.55198\n   \n57/500. loss: 0.0038497066125273705\n57/500. mserror: 50.40228271484375  col: 144  count: 3637  jerk: 0.03054259531199932  ttc: 69.42203521728516\nEpoch: 1| Train Loss: 0.1612330\nmean_spacing_error：118.54721，col=98，count=3637，rate=2.69453%，jerk=0.01260，miniumu_ttc=81.60621\nEpoch: 2| Train Loss: 0.1334504\nmean_spacing_error：72.47297，col=121，count=3637，rate=3.32692%，jerk=0.01493，miniumu_ttc=135.56662\nEpoch: 3| Train Loss: 0.1217427\nmean_spacing_error：60.66418，col=97，count=3637，rate=2.66703%，jerk=0.02307，miniumu_ttc=87.08930\nEpoch: 4| Train Loss: 0.1175522\nmean_spacing_error：64.33984，col=55，count=3637，rate=1.51224%，jerk=0.02439，miniumu_ttc=81.51795\nEpoch: 5| Train Loss: 0.1268913\nmean_spacing_error：70.30257，col=179，count=3637，rate=4.92164%，jerk=0.01539，miniumu_ttc=275.60501\n   \n58/500. loss: 0.0026268468548854194\n58/500. mserror: 50.31437683105469  col: 155  count: 3637  jerk: 0.03128548339009285  ttc: 112.84695434570312\nEpoch: 1| Train Loss: 0.1628483\nmean_spacing_error：122.50794，col=90，count=3637，rate=2.47457%，jerk=0.01123，miniumu_ttc=94.71845\nEpoch: 2| Train Loss: 0.1353970\nmean_spacing_error：76.46762，col=144，count=3637，rate=3.95931%，jerk=0.01261，miniumu_ttc=108.10487\nEpoch: 3| Train Loss: 0.1236435\nmean_spacing_error：63.09845，col=98，count=3637，rate=2.69453%，jerk=0.02181，miniumu_ttc=80.10799\nEpoch: 4| Train Loss: 0.1186374\nmean_spacing_error：62.15788，col=56，count=3637，rate=1.53973%，jerk=0.02821，miniumu_ttc=86.35677\nEpoch: 5| Train Loss: 0.1172244\nmean_spacing_error：65.66537，col=85，count=3637，rate=2.33709%，jerk=0.02747，miniumu_ttc=81.75269\n   \n59/500. loss: 0.003749329907198747\n59/500. mserror: 47.428611755371094  col: 118  count: 3637  jerk: 0.03607858717441559  ttc: 82.44718933105469\nEpoch: 1| Train Loss: 0.1699148\nmean_spacing_error：131.61520，col=72，count=3637，rate=1.97965%，jerk=0.01213，miniumu_ttc=134.55807\nEpoch: 2| Train Loss: 0.1376559\nmean_spacing_error：75.15955，col=154，count=3637，rate=4.23426%，jerk=0.01194，miniumu_ttc=107.95680\nEpoch: 3| Train Loss: 0.1248848\nmean_spacing_error：65.25283，col=84，count=3637，rate=2.30960%，jerk=0.02037，miniumu_ttc=80.18330\nEpoch: 4| Train Loss: 0.1198878\nmean_spacing_error：64.97556，col=41，count=3637，rate=1.12730%，jerk=0.02539，miniumu_ttc=95.80672\nEpoch: 5| Train Loss: 0.1191982\nmean_spacing_error：68.74429，col=75，count=3637，rate=2.06214%，jerk=0.02220，miniumu_ttc=94.26417\n   \n60/500. loss: 0.003317865232626597\n60/500. mserror: 46.110069274902344  col: 58  count: 3637  jerk: 0.04695505276322365  ttc: 77.27132415771484\nEpoch: 1| Train Loss: 0.1755579\nmean_spacing_error：116.66380，col=82，count=3637，rate=2.25461%，jerk=0.01313，miniumu_ttc=112.59136\nEpoch: 2| Train Loss: 0.1380846\nmean_spacing_error：69.79912，col=129，count=3637，rate=3.54688%，jerk=0.01427，miniumu_ttc=128.98294\nEpoch: 3| Train Loss: 0.1281341\nmean_spacing_error：62.28162，col=91，count=3637，rate=2.50206%，jerk=0.02013，miniumu_ttc=91.44766\nEpoch: 4| Train Loss: 0.1193968\nmean_spacing_error：67.82788，col=86，count=3637，rate=2.36459%，jerk=0.02105，miniumu_ttc=89.64648\nEpoch: 5| Train Loss: 0.1332706\nmean_spacing_error：72.74670，col=65，count=3637，rate=1.78719%，jerk=0.01967，miniumu_ttc=125.58679\n   \n61/500. loss: 0.0022828769870102406\n61/500. mserror: 45.07323455810547  col: 37  count: 3637  jerk: 0.053341567516326904  ttc: 78.70695495605469\nEpoch: 1| Train Loss: 0.1785399\nmean_spacing_error：114.84796，col=75，count=3637，rate=2.06214%，jerk=0.01510，miniumu_ttc=144.88867\nEpoch: 2| Train Loss: 0.1446765\nmean_spacing_error：72.74336，col=209，count=3637，rate=5.74649%，jerk=0.01032，miniumu_ttc=91.40881\nEpoch: 3| Train Loss: 0.1289709\nmean_spacing_error：66.89309，col=70，count=3637，rate=1.92466%，jerk=0.02415，miniumu_ttc=79.47062\nEpoch: 4| Train Loss: 0.1353768\nmean_spacing_error：61.24255，col=96，count=3637，rate=2.63954%，jerk=0.02100，miniumu_ttc=69.50243\nEpoch: 5| Train Loss: 0.1216442\nmean_spacing_error：64.14681，col=51，count=3637，rate=1.40225%，jerk=0.02478，miniumu_ttc=226.60526\n   \n62/500. loss: 0.0038041993975639343\n62/500. mserror: 40.6983642578125  col: 50  count: 3637  jerk: 0.055933404713869095  ttc: 73.12564849853516\nEpoch: 1| Train Loss: 0.1902230\nmean_spacing_error：114.14180，col=69，count=3637，rate=1.89717%，jerk=0.01756，miniumu_ttc=126.48901\nEpoch: 2| Train Loss: 0.1455123\nmean_spacing_error：70.73583，col=198，count=3637，rate=5.44405%，jerk=0.01143，miniumu_ttc=72.45978\nEpoch: 3| Train Loss: 0.1263705\nmean_spacing_error：57.39997，col=98，count=3637，rate=2.69453%，jerk=0.02241，miniumu_ttc=93.07013\nEpoch: 4| Train Loss: 0.1223621\nmean_spacing_error：47.17982，col=82，count=3637，rate=2.25461%，jerk=0.03056，miniumu_ttc=57.16385\nEpoch: 5| Train Loss: 0.1261698\nmean_spacing_error：47.12490，col=65，count=3637，rate=1.78719%，jerk=0.03085，miniumu_ttc=63.62006\n   \n63/500. loss: 0.003635251894593239\n63/500. mserror: 41.28016662597656  col: 75  count: 3637  jerk: 0.051126714795827866  ttc: 37.46453094482422\nEpoch: 1| Train Loss: 0.1946285\nmean_spacing_error：97.00992，col=92，count=3637，rate=2.52956%，jerk=0.01434，miniumu_ttc=140.98035\nEpoch: 2| Train Loss: 0.1373223\nmean_spacing_error：65.67834，col=185，count=3637，rate=5.08661%，jerk=0.01400，miniumu_ttc=87.38033\nEpoch: 3| Train Loss: 0.1251179\nmean_spacing_error：66.94489，col=115，count=3637，rate=3.16195%，jerk=0.01732，miniumu_ttc=70.33894\nEpoch: 4| Train Loss: 0.1219280\nmean_spacing_error：63.77096，col=104，count=3637，rate=2.85950%，jerk=0.02027，miniumu_ttc=82.34354\nEpoch: 5| Train Loss: 0.1184532\nmean_spacing_error：60.10212，col=75，count=3637，rate=2.06214%，jerk=0.02689，miniumu_ttc=62.76544\n   \n64/500. loss: 0.003314133733510971\n64/500. mserror: 39.13681411743164  col: 60  count: 3637  jerk: 0.04810861870646477  ttc: 67.22215270996094\nEpoch: 1| Train Loss: 0.1928113\nmean_spacing_error：108.03517，col=73，count=3637，rate=2.00715%，jerk=0.01551，miniumu_ttc=373.18082\nEpoch: 2| Train Loss: 0.1353503\nmean_spacing_error：66.71078，col=178，count=3637，rate=4.89414%，jerk=0.01422，miniumu_ttc=64.04572\nEpoch: 3| Train Loss: 0.1277146\nmean_spacing_error：59.93311，col=91，count=3637，rate=2.50206%，jerk=0.02303，miniumu_ttc=65.31462\nEpoch: 4| Train Loss: 0.1209241\nmean_spacing_error：56.72793，col=66，count=3637，rate=1.81468%，jerk=0.02673，miniumu_ttc=86.17446\nEpoch: 5| Train Loss: 0.1173189\nmean_spacing_error：57.96495，col=48，count=3637，rate=1.31977%，jerk=0.02644，miniumu_ttc=136.56979\n   \n65/500. loss: 0.003718021015326182\n65/500. mserror: 40.34596252441406  col: 82  count: 3637  jerk: 0.0407969206571579  ttc: 53.160430908203125\nEpoch: 1| Train Loss: 0.1819147\nmean_spacing_error：133.97075，col=58，count=3637，rate=1.59472%，jerk=0.01684，miniumu_ttc=995.46277\nEpoch: 2| Train Loss: 0.1404302\nmean_spacing_error：70.52866，col=565，count=3637，rate=15.53478%，jerk=0.00218，miniumu_ttc=46.64431\nEpoch: 3| Train Loss: 0.1251225\nmean_spacing_error：59.07109，col=164，count=3637，rate=4.50921%，jerk=0.01945，miniumu_ttc=125.00069\nEpoch: 4| Train Loss: 0.1216679\nmean_spacing_error：59.03927，col=81，count=3637，rate=2.22711%，jerk=0.02506，miniumu_ttc=63.38790\nEpoch: 5| Train Loss: 0.1176637\nmean_spacing_error：65.03202，col=89，count=3637，rate=2.44707%，jerk=0.02562，miniumu_ttc=89.55515\n   \n66/500. loss: 0.003565241893132528\n66/500. mserror: 45.41195297241211  col: 118  count: 3637  jerk: 0.032623741775751114  ttc: 52.343318939208984\nEpoch: 1| Train Loss: 0.1759527\nmean_spacing_error：148.89722，col=55，count=3637，rate=1.51224%，jerk=0.02098，miniumu_ttc=111.12112\nEpoch: 2| Train Loss: 0.1443799\nmean_spacing_error：62.74659，col=457，count=3637，rate=12.56530%，jerk=0.00847，miniumu_ttc=87.58258\nEpoch: 3| Train Loss: 0.1255959\nmean_spacing_error：54.70660，col=194，count=3637，rate=5.33407%，jerk=0.01825，miniumu_ttc=64.96593\nEpoch: 4| Train Loss: 0.1193059\nmean_spacing_error：45.09050，col=41，count=3637，rate=1.12730%，jerk=0.03550，miniumu_ttc=126.89909\nEpoch: 5| Train Loss: 0.1173585\nmean_spacing_error：44.81219，col=93，count=3637，rate=2.55705%，jerk=0.03094，miniumu_ttc=96.14487\n   \n67/500. loss: 0.00202500664939483\n67/500. mserror: 44.454010009765625  col: 115  count: 3637  jerk: 0.029587818309664726  ttc: 56.270172119140625\nEpoch: 1| Train Loss: 0.1741479\nmean_spacing_error：131.23843，col=83，count=3637，rate=2.28210%，jerk=0.01311，miniumu_ttc=113.05918\nEpoch: 2| Train Loss: 0.1399619\nmean_spacing_error：69.32363，col=600，count=3637，rate=16.49711%，jerk=0.00522，miniumu_ttc=105.23430\nEpoch: 3| Train Loss: 0.1254533\nmean_spacing_error：45.77821，col=131，count=3637，rate=3.60187%，jerk=0.02615，miniumu_ttc=85.36882\nEpoch: 4| Train Loss: 0.1170876\nmean_spacing_error：36.08892，col=91，count=3637，rate=2.50206%，jerk=0.03747，miniumu_ttc=109.15752\nEpoch: 5| Train Loss: 0.1142847\nmean_spacing_error：42.48248，col=59，count=3637，rate=1.62222%，jerk=0.03493，miniumu_ttc=113.36601\n   \n68/500. loss: 0.002100233609477679\n68/500. mserror: 43.63017654418945  col: 88  count: 3637  jerk: 0.029867159202694893  ttc: 146.64195251464844\nEpoch: 1| Train Loss: 0.1757004\nmean_spacing_error：103.57162，col=157，count=3637，rate=4.31674%，jerk=0.00782，miniumu_ttc=246.54822\nEpoch: 2| Train Loss: 0.1330345\nmean_spacing_error：74.33985，col=235，count=3637，rate=6.46137%，jerk=0.01073，miniumu_ttc=116.48442\nEpoch: 3| Train Loss: 0.1244580\nmean_spacing_error：58.59227，col=134，count=3637，rate=3.68436%，jerk=0.02269，miniumu_ttc=123.92056\nEpoch: 4| Train Loss: 0.1173524\nmean_spacing_error：51.93096，col=28，count=3637，rate=0.76987%，jerk=0.03669，miniumu_ttc=71.66429\nEpoch: 5| Train Loss: 0.1217006\nmean_spacing_error：53.75066，col=92，count=3637，rate=2.52956%，jerk=0.02707，miniumu_ttc=146.72081\n   \n69/500. loss: 0.002032784124215444\n69/500. mserror: 44.507713317871094  col: 89  count: 3637  jerk: 0.029015224426984787  ttc: 84.41484832763672\nEpoch: 1| Train Loss: 0.1663762\nmean_spacing_error：225.09181，col=48，count=3637，rate=1.31977%，jerk=0.02521，miniumu_ttc=144.79333\nEpoch: 2| Train Loss: 0.1579723\nmean_spacing_error：60.21634，col=139，count=3637，rate=3.82183%，jerk=0.01495，miniumu_ttc=67.14331\nEpoch: 3| Train Loss: 0.1309662\nmean_spacing_error：53.65032，col=84，count=3637，rate=2.30960%，jerk=0.02192，miniumu_ttc=168.93123\nEpoch: 4| Train Loss: 0.1190827\nmean_spacing_error：42.49509，col=40，count=3637，rate=1.09981%，jerk=0.03436，miniumu_ttc=104.85059\nEpoch: 5| Train Loss: 0.1164780\nmean_spacing_error：44.03352，col=66，count=3637，rate=1.81468%，jerk=0.03700，miniumu_ttc=77.06644\n   \n70/500. loss: 0.003929259876410167\n70/500. mserror: 43.3633918762207  col: 83  count: 3637  jerk: 0.030394306406378746  ttc: 179.97119140625\nEpoch: 1| Train Loss: 0.1744982\nmean_spacing_error：188.83612，col=29，count=3637，rate=0.79736%，jerk=0.03036，miniumu_ttc=103.96609\nEpoch: 2| Train Loss: 0.1518571\nmean_spacing_error：45.04771，col=261，count=3637，rate=7.17624%，jerk=0.01878，miniumu_ttc=74.48534\nEpoch: 3| Train Loss: 0.1250891\nmean_spacing_error：39.14014，col=21，count=3637，rate=0.57740%，jerk=0.04469，miniumu_ttc=101.58842\nEpoch: 4| Train Loss: 0.1240521\nmean_spacing_error：29.81999，col=145，count=3637，rate=3.98680%，jerk=0.06029，miniumu_ttc=76.56154\nEpoch: 5| Train Loss: 0.1137785\nmean_spacing_error：28.15511，col=88，count=3637，rate=2.41958%，jerk=0.08247，miniumu_ttc=123.21739\n   \n71/500. loss: 0.0021761351575454078\n71/500. mserror: 41.019588470458984  col: 111  count: 3637  jerk: 0.030646758154034615  ttc: 85.60215759277344\nEpoch: 1| Train Loss: 0.1814238\nmean_spacing_error：193.64059，col=35，count=3637，rate=0.96233%，jerk=0.02635，miniumu_ttc=183.63608\nEpoch: 2| Train Loss: 0.1484882\nmean_spacing_error：56.49508，col=226，count=3637，rate=6.21391%，jerk=0.01538，miniumu_ttc=2009.34351\nEpoch: 3| Train Loss: 0.1268359\nmean_spacing_error：43.42747，col=34，count=3637，rate=0.93484%，jerk=0.03824，miniumu_ttc=88.86755\nEpoch: 4| Train Loss: 0.1195752\nmean_spacing_error：35.07698，col=116，count=3637，rate=3.18944%，jerk=0.04016，miniumu_ttc=98.09070\nEpoch: 5| Train Loss: 0.1130540\nmean_spacing_error：30.91172，col=70，count=3637，rate=1.92466%，jerk=0.04686，miniumu_ttc=80.90501\n   \n72/500. loss: 0.0024676574394106865\n72/500. mserror: 41.43697738647461  col: 140  count: 3637  jerk: 0.031343236565589905  ttc: 48.06175994873047\nEpoch: 1| Train Loss: 0.1894359\nmean_spacing_error：218.52896，col=48，count=3637，rate=1.31977%，jerk=0.02033，miniumu_ttc=110.81691\nEpoch: 2| Train Loss: 0.1563883\nmean_spacing_error：75.11957，col=438，count=3637，rate=12.04289%，jerk=0.00400，miniumu_ttc=70.88219\nEpoch: 3| Train Loss: 0.1373457\nmean_spacing_error：68.01894，col=105，count=3637，rate=2.88699%，jerk=0.01670，miniumu_ttc=106.98920\nEpoch: 4| Train Loss: 0.1311138\nmean_spacing_error：61.46925，col=197，count=3637，rate=5.41655%，jerk=0.01578，miniumu_ttc=460.95074\nEpoch: 5| Train Loss: 0.1234396\nmean_spacing_error：50.61165，col=158，count=3637，rate=4.34424%，jerk=0.02408，miniumu_ttc=65.05457\n   \n73/500. loss: 0.0033237350483735404\n73/500. mserror: 37.80166244506836  col: 97  count: 3637  jerk: 0.03665076196193695  ttc: 77.30755615234375\nEpoch: 1| Train Loss: 0.1793284\nmean_spacing_error：181.07944，col=48，count=3637，rate=1.31977%，jerk=0.02706，miniumu_ttc=128.27747\nEpoch: 2| Train Loss: 0.1481106\nmean_spacing_error：56.89648，col=175，count=3637，rate=4.81166%，jerk=0.01486，miniumu_ttc=147.85495\nEpoch: 3| Train Loss: 0.1259491\nmean_spacing_error：45.28094，col=90，count=3637，rate=2.47457%，jerk=0.02693，miniumu_ttc=76.14444\nEpoch: 4| Train Loss: 0.1205485\nmean_spacing_error：37.33999，col=55，count=3637，rate=1.51224%，jerk=0.04219，miniumu_ttc=95.91102\nEpoch: 5| Train Loss: 0.1169022\nmean_spacing_error：33.68426，col=63，count=3637，rate=1.73220%，jerk=0.04273，miniumu_ttc=793.91467\n   \n74/500. loss: 0.00205199575672547\n74/500. mserror: 38.429996490478516  col: 79  count: 3637  jerk: 0.04102948307991028  ttc: 185.31158447265625\nEpoch: 1| Train Loss: 0.1783692\nmean_spacing_error：165.73470，col=50，count=3637，rate=1.37476%，jerk=0.02994，miniumu_ttc=149.35835\nEpoch: 2| Train Loss: 0.1527819\nmean_spacing_error：43.05853，col=251，count=3637，rate=6.90129%，jerk=0.01939，miniumu_ttc=120.09402\nEpoch: 3| Train Loss: 0.1280434\nmean_spacing_error：38.96620，col=87，count=3637，rate=2.39208%，jerk=0.03267，miniumu_ttc=116.32418\nEpoch: 4| Train Loss: 0.1213059\nmean_spacing_error：36.88362，col=55，count=3637，rate=1.51224%，jerk=0.05677，miniumu_ttc=43.78418\nEpoch: 5| Train Loss: 0.1137807\nmean_spacing_error：46.28355，col=62，count=3637，rate=1.70470%，jerk=0.03378，miniumu_ttc=98.52984\n   \n75/500. loss: 0.0035900926838318505\n75/500. mserror: 35.887115478515625  col: 87  count: 3637  jerk: 0.04203170910477638  ttc: 113.76644897460938\nEpoch: 1| Train Loss: 0.1797495\nmean_spacing_error：185.12105，col=40，count=3637，rate=1.09981%，jerk=0.03000，miniumu_ttc=131.25516\nEpoch: 2| Train Loss: 0.1491476\nmean_spacing_error：43.56262，col=218，count=3637，rate=5.99395%，jerk=0.01935，miniumu_ttc=66.49136\nEpoch: 3| Train Loss: 0.1309109\nmean_spacing_error：38.31199，col=63，count=3637，rate=1.73220%，jerk=0.04081，miniumu_ttc=231.06575\nEpoch: 4| Train Loss: 0.1237944\nmean_spacing_error：28.94777，col=115，count=3637，rate=3.16195%，jerk=0.04696，miniumu_ttc=78.62834\nEpoch: 5| Train Loss: 0.1155200\nmean_spacing_error：33.17390，col=19，count=3637，rate=0.52241%，jerk=0.05624，miniumu_ttc=1175.99780\n   \n76/500. loss: 0.0020981493095556893\n76/500. mserror: 34.499332427978516  col: 111  count: 3637  jerk: 0.04118307679891586  ttc: 105.37380981445312\nEpoch: 1| Train Loss: 0.1673264\nmean_spacing_error：65.22062，col=132，count=3637，rate=3.62936%，jerk=0.01655，miniumu_ttc=116.97066\nEpoch: 2| Train Loss: 0.1306558\nmean_spacing_error：60.88748，col=69，count=3637，rate=1.89717%，jerk=0.02254，miniumu_ttc=139.73090\nEpoch: 3| Train Loss: 0.1231874\nmean_spacing_error：43.29113，col=96，count=3637，rate=2.63954%，jerk=0.03377，miniumu_ttc=92.53834\nEpoch: 4| Train Loss: 0.1192395\nmean_spacing_error：39.65148，col=156，count=3637，rate=4.28925%，jerk=0.03147，miniumu_ttc=76.87246\nEpoch: 5| Train Loss: 0.1137206\n","output_type":"stream"}]},{"cell_type":"code","source":"import numbers\nfrom copy import copy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport random\n\n\ndef extract_top_level_dict(current_dict):\n    \"\"\"\n    Builds a graph dictionary from the passed depth_keys, value pair. Useful for dynamically passing external params\n    :param depth_keys: A list of strings making up the name of a variable. Used to make a graph for that params tree.\n    :param value: Param value\n    :param key_exists: If none then assume new dict, else load existing dict and add new key->value pairs to it.\n    :return: A dictionary graph of the params already added to the graph.\n    \"\"\"\n    output_dict = dict()\n    for key in current_dict.keys():\n        name = key.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"layer_dict.\", \"\")\n        name = name.replace(\"block_dict.\", \"\")\n        name = name.replace(\"module-\", \"\")\n        top_level = name.split(\".\")[0]\n        sub_level = \".\".join(name.split(\".\")[1:])\n\n        if top_level not in output_dict:\n            if sub_level == \"\":\n                output_dict[top_level] = current_dict[key]\n            else:\n                output_dict[top_level] = {sub_level: current_dict[key]}\n        else:\n            new_item = {key: value for key, value in output_dict[top_level].items()}\n            new_item[sub_level] = current_dict[key]\n            output_dict[top_level] = new_item\n\n    #print(current_dict.keys(), output_dict.keys())\n    return output_dict\n\n\nclass MetaLinearLayer(nn.Module):\n    def __init__(self, input_shape, num_filters, use_bias):\n        \"\"\"\n        A MetaLinear layer. Applies the same functionality of a standard linearlayer with the added functionality of\n        being able to receive a parameter dictionary at the forward pass which allows the convolution to use external\n        weights instead of the internal ones stored in the linear layer. Useful for inner loop optimization in the meta\n        learning setting.\n        :param input_shape: The shape of the input data, in the form (b, f)\n        :param num_filters: Number of output filters\n        :param use_bias: Whether to use biases or not.\n        \"\"\"\n        super(MetaLinearLayer, self).__init__()\n        b, c = input_shape\n\n        self.use_bias = use_bias\n        self.weights = nn.Parameter(torch.ones(num_filters, c))\n        nn.init.xavier_uniform_(self.weights)\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.zeros(num_filters))\n\n    def forward(self, x, params=None):\n        \"\"\"\n        Forward propagates by applying a linear function (Wx + b). If params are none then internal params are used.\n        Otherwise passed params will be used to execute the function.\n        :param x: Input data batch, in the form (b, f)\n        :param params: A dictionary containing 'weights' and 'bias'. If params are none then internal params are used.\n        Otherwise the external are used.\n        :return: The result of the linear function.\n        \"\"\"\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n            if self.use_bias:\n                (weight, bias) = params[\"weights\"], params[\"bias\"]\n            else:\n                (weight) = params[\"weights\"]\n                bias = None\n        else:\n            pass\n            #print('no inner loop params', self)\n\n            if self.use_bias:\n                weight, bias = self.weights, self.bias\n            else:\n                weight = self.weights\n                bias = None\n        # print(x.shape)\n        out = F.linear(input=x, weight=weight, bias=bias)\n        return out\n\n\nclass MetaStepLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        super(MetaStepLossNetwork, self).__init__()\n\n        self.device = device\n        # self.args = args\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        out = x\n\n        self.linear1 = MetaLinearLayer(input_shape=self.input_shape,\n                                                    num_filters=self.input_dim, use_bias=True)\n\n        self.linear2 = MetaLinearLayer(input_shape=(1, self.input_dim),\n                                                    num_filters=1, use_bias=True)\n\n\n        out = self.linear1(out)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n    def forward(self, x, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n\n        linear1_params = None\n        linear2_params = None\n\n        if params is not None:\n            params = extract_top_level_dict(current_dict=params)\n\n            linear1_params = params['linear1']\n            linear2_params = params['linear2']\n\n        out = x\n        \n        out = self.linear1(out, linear1_params)\n        out = F.relu_(out)\n        out = self.linear2(out, linear2_params)\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass MetaLossNetwork(nn.Module):\n    def __init__(self, input_dim, notspi, device):\n        \"\"\"\n        Builds a multilayer convolutional network. It also provides functionality for passing external parameters to be\n        used at inference time. Enables inner loop optimization readily.\n        :param im_shape: The input image batch shape.\n        :param num_output_classes: The number of output classes of the network.\n        :param args: A named tuple containing the system's hyperparameters.\n        :param device: The device to run this on.\n        :param meta_classifier: A flag indicating whether the system's meta-learning (inner-loop) functionalities should\n        be enabled. \n        \"\"\"\n        super(MetaLossNetwork, self).__init__()\n        \n        self.device = device\n        # self.args = args\n        self.notspi = notspi\n        self.input_dim = input_dim\n        self.input_shape = (1, input_dim)\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.build_network()\n        # print(\"meta network params\")\n        # for name, param in self.named_parameters():\n        #     print(name, param.shape)\n\n    def build_network(self):\n        \"\"\" \n        Builds the network before inference is required by creating some dummy inputs with the same input as the\n        self.im_shape tuple. Then passes that through the network and dynamically computes input shapes and\n        sets output shapes for each layer.\n        \"\"\"\n        x = torch.zeros(self.input_shape)\n        self.layer_dict = nn.ModuleDict()\n        # ф\n        for i in range(self.num_steps): \n            self.layer_dict['step{}'.format(i)] = MetaStepLossNetwork(self.input_dim, notspi=self.notspi, device=self.device)\n\n            out = self.layer_dict['step{}'.format(i)](x)\n\n    def forward(self, x, num_step, params=None):\n        \"\"\"                                  \n        Forward propages through the network. If any params are passed then they are used instead of stored params.\n        :param x: Input image batch.\n        :param num_step: The current inner loop step number\n        :param params: If params are None then internal parameters are used. If params are a dictionary with keys the\n         same as the layer names then they will be used instead.\n        :param training: Whether this is training (True) or eval time.\n        :param backup_running_statistics: Whether to backup the running statistics in their backup store. Which is\n        then used to reset the stats back to a previous state (usually after an eval loop, when we want to throw away stored statistics)\n        :return: Logits of shape b, num_output_classes.\n        \"\"\" \n        param_dict = dict()\n\n        if params is not None: \n            # params = {key: value[0] for key, value in params.items()}\n            param_dict = extract_top_level_dict(current_dict=params)\n            \n        for name, param in self.layer_dict.named_parameters():\n            path_bits = name.split(\".\")\n            layer_name = path_bits[0]\n            if layer_name not in param_dict:\n                param_dict[layer_name] = None\n\n            \n        out = x\n        \n        out = self.layer_dict['step{}'.format(num_step)](out, param_dict['step{}'.format(num_step)])\n\n        return out\n\n    def zero_grad(self, params=None):\n        if params is None:\n            for param in self.parameters():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n        else:\n            for name, param in params.items():\n                if param.requires_grad == True:\n                    if param.grad is not None:\n                        if torch.sum(param.grad) > 0:\n                            # print(param.grad)\n                            param.grad.zero_()\n                            params[name].grad = None\n\n    def restore_backup_stats(self):\n        \"\"\"\n        Reset stored batch statistics from the stored backup.\n        \"\"\"\n        for i in range(self.num_stages):\n            self.layer_dict['conv{}'.format(i)].restore_backup_stats()\n\n\nclass StepLossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(StepLossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n        output_dim = num_loss_net_layers * 2 * 2 # 2 for weight and bias, another 2 for multiplier and offset\n\n        self.linear1 = nn.Linear(input_dim, input_dim)\n        self.activation = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(input_dim, output_dim)\n\n        self.multiplier_bias = nn.Parameter(torch.zeros(output_dim // 2))\n        self.offset_bias = nn.Parameter(torch.zeros(output_dim // 2))\n\n    def forward(self, task_state, num_step, loss_params):\n        # ψ\n        out = self.linear1(task_state)\n        out = F.relu_(out)\n        out = self.linear2(out)\n\n        generated_multiplier, generated_offset = torch.chunk(out, chunks=2, dim=-1)\n\n        i = 0\n        updated_loss_weights = dict()\n        for key, val in loss_params.items():\n            if 'step{}'.format(num_step) in key:\n                updated_loss_weights[key] = (1 + self.multiplier_bias[i] * generated_multiplier[i]) * val + \\\n                                             self.offset_bias[i] * generated_offset[i]\n                i+=1\n\n        return updated_loss_weights\n\n\nclass LossAdapter(nn.Module):\n    def __init__(self, input_dim, num_loss_net_layers, notspi, device):\n        super(LossAdapter, self).__init__()\n\n        self.device = device\n        # self.args = args\n\n        self.num_steps = notspi # number of inner-loop steps\n\n        self.loss_adapter = nn.ModuleList()\n        for i in range(self.num_steps): \n            self.loss_adapter.append(StepLossAdapter(input_dim, num_loss_net_layers, notspi=notspi, device=device))\n\n    def forward(self, task_state, num_step, loss_params):\n        return self.loss_adapter[num_step](task_state, num_step, loss_params)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nfrom torch import _weight_norm\nimport numpy as np\nfrom math import sqrt\nimport random\n\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim_in = input_dim\n        self.dim_k = input_dim//2\n        self.dim_v = input_dim//2\n        self.num_heads = num_heads\n        self.linear_q = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_k = nn.Linear(input_dim, input_dim//2, bias=True)\n        self.linear_v = nn.Linear(input_dim, input_dim, bias=True)\n        self._norm_fact = 1 / sqrt((input_dim//2) // num_heads)\n\n    def forward(self, x, param=None):\n        batch, n, dim_in = x.shape\n        assert dim_in == self.dim_in\n\n        nh = self.num_heads\n        dk = self.dim_k // nh  # dim_k of each head\n        dv = self.dim_v // nh  # dim_v of each head\n        if param == None:\n            q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)  # (batch, nh, n, dk)\n            v = self.linear_v(x).reshape(batch, n, nh, dv*2).transpose(1, 2)  # (batch, nh, n, dv)\n        else:\n            q = F.linear(x, weight=param[0], bias=param[1]).reshape(batch, n, nh, dk).transpose(1, 2)\n            k = F.linear(x, weight=param[2], bias=param[3]).reshape(batch, n, nh, dk).transpose(1, 2)\n            v = F.linear(x, weight=param[4], bias=param[5]).reshape(batch, n, nh, dv*2).transpose(1, 2)\n        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact  # batch, nh, n, n\n        dist = torch.softmax(dist, dim=-1)  # batch, nh, n, n\n\n        att = torch.matmul(dist, v)  # batch, nh, n, dv\n        out = att.transpose(1, 2).reshape(batch, n, self.dim_in)  # batch, n, dim_v\n        \n        # out = att.reshape(att.shape[0], -1)\n\n        return out\n    \n\nclass conv1d(nn.Module):\n    def _init_(self):\n        super(conv1d, self).__init__()\n    \n    def forward(self, x, weight, bias, stride, padding, dilation):\n        return F.conv1d(x, weight, bias=bias, stride=stride, padding=padding, dilation=dilation)\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm1 = nn.BatchNorm1d(n_outputs)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        # self.relu1 = nn.Tanh()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        # self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n        #                                    stride=stride, padding=padding, dilation=dilation)\n        self.batchnorm2 = nn.BatchNorm1d(n_outputs)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        # self.relu2 = nn.Tanh()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.batchnorm1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.batchnorm2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        # self.relu = nn.Tanh()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        self.kernel_size = kernel_size\n        self.dropout = dropout\n        self.num_inputs = num_inputs\n        self.num_channels = num_channels\n        layers = []\n        # self.vars_bn = nn.ParameterList()\n        num_levels = len(num_channels)\n        # num_levels = num_channels\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n        self.token = nn.Parameter(torch.ones(1, num_channels[-1]))\n        # self.encoder_layer = nn.TransformerEncoderLayer(d_model=10, nhead= 5,\\\n        #                     dim_feedforward=10, batch_first=True)\n        # self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n        # self.features = nn.LSTM(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.features = nn.GRU(input_size=num_channels[-1], hidden_size=16, num_layers=1, batch_first=True)\n        # self.attention = TemporalAttention(num_channels[-1])\n        self.attention = MultiHeadSelfAttention(num_channels[-1])\n        self.mlp = nn.Sequential(\n            nn.Linear(num_channels[-1], num_channels[-1]),\n            # nn.LeakyReLU(negative_slope=0.01),\n            nn.ReLU(), \n            nn.Linear(num_channels[-1], 1)\n        )\n        # self.features = nn.Linear(num_channels[-1], num_channels[-1])\n        # self.decoder = nn.Linear(num_channels[-1], 1)\n\n        # encoder_layer = nn.TransformerEncoderLayer(d_model=num_channels[i-1], nhead= 4,\\\n        #                     dim_feedforward=num_channels[i-1], batch_first=False)\n        # self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        # self.tanh = nn.Tanh()\n\n    def forward(self, x, weights=None):\n        device =  x[0].device\n        if weights == None:\n            # transformer output:\n            # x = self.transformer_encoder(x) + x\n            \n            output = self.network(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n            # output = self.attention(output)\n            # linear output:\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n            # h_n = self.features(output)\n            \n            # LSTM output:\n            # enc_x, (h_n, c_n) = self.features(output)\n            \n            # GRU output:\n            # enc_x, h_n = self.features(output)\n\n            # if len(h_n.shape) == 3:\n            #     h_n = h_n[-1] # (32,16)\n            \n            # 通过线性层和激活函数得到最终输出\n            # out = self.decoder(F.relu(h_n))\n            # out = self.decoder(F.leaky_relu(h_n, negative_slope=0.01))\n            out = self.mlp(output)\n        else:\n            # TemporalBlock0\n            # 实际权重 = 模数 * 方向向量\n            ks = self.kernel_size\n            weight1 = _weight_norm(weights[3], weights[2], 0)\n            conv1d1 = weight_norm(nn.Conv1d(self.num_inputs, self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d1._conv_forward(x, weight=weight1, bias=weights[1])\n            # output1 = F.conv1d(x, weight=weights[1], bias=weights[2], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean1 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp1(output1), running_mean=running_mean1, running_var=running_var1, weight=weights[4], bias=weights[5])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            weight2 = _weight_norm(weights[8], weights[7], 0)\n            conv1d2 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[0], kernel_size=ks, stride=1, padding=(ks-1)*1, dilation=1))\n            output1 = conv1d2._conv_forward(output1, weight=weight2, bias=weights[6])\n            # output1 = F.conv1d(output1, weight=weights[5], bias=weights[6], stride=1, padding=(ks-1)*1, dilation=1)\n            running_mean2 = nn.Parameter(torch.zeros(self.num_channels[0]), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(self.num_channels[0]), requires_grad=False).to(device)\n            output1 = F.batch_norm(self.network[0].chomp2(output1), running_mean=running_mean2, running_var=running_var2, weight=weights[9], bias=weights[10])\n            # output1 = F.tanh(output1)\n            output1 = F.relu(output1)\n            output1 = F.dropout(output1, self.dropout)\n            res1 = F.conv1d(x, weights[11], bias=weights[12], stride=1)\n            # output1 = F.tanh(output1 + res1)\n            output1 = F.relu(output1 + res1)\n\n            # TemporalBlock1\n            weight3 = _weight_norm(weights[15], weights[14], 0)\n            conv1d3 = weight_norm(nn.Conv1d(self.num_channels[0], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d3._conv_forward(output1, weight=weight3, bias=weights[13])\n            # output2 = F.conv1d(output1, weight=weights[11], bias=weights[12], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean3 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp1(output2), running_mean=running_mean3, running_var=running_var3, weight=weights[16], bias=weights[17])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            weight4 = _weight_norm(weights[20], weights[19], 0)\n            conv1d4 = weight_norm(nn.Conv1d(self.num_channels[1], self.num_channels[1], kernel_size=ks, stride=1, padding=(ks-1)*2, dilation=2))\n            output2 = conv1d4._conv_forward(output2, weight=weight4, bias=weights[18])\n            # output2 = F.conv1d(output2, weight=weights[15], bias=weights[16], stride=1, padding=(ks-1)*2, dilation=2)\n            running_mean4 = nn.Parameter(torch.zeros(self.num_channels[1]), requires_grad=False).to(device)\n            running_var4 = nn.Parameter(torch.ones(self.num_channels[1]), requires_grad=False).to(device)\n            output2 = F.batch_norm(self.network[1].chomp2(output2), running_mean=running_mean4, running_var=running_var4, weight=weights[21], bias=weights[22])\n            # output2 = F.tanh(output2)\n            output2 = F.relu(output2)\n            output2 = F.dropout(output2, self.dropout)\n            res2 = F.conv1d(output1, weights[23], bias=weights[24], stride=1)\n            # output2 = F.tanh(output2 + res2)\n            output = F.relu(output2 + res2)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[25:31])\n            output = output[:, -1, :]\n\n            # output = output.transpose(1, 2)\n            # output_shape = output.shape\n            # output = output.reshape(output_shape[0], output_shape[1] * output_shape[2])\n\n            # out = F.leaky_relu(F.linear(output, weight=weights[31] , bias=weights[32]), negative_slope=0.01)\n            out = F.relu(F.linear(output, weight=weights[31] , bias=weights[32]))\n            out = F.linear(out, weight=weights[33], bias=weights[34])\n\n        return out\n    \n\n\n# MAML-跟车  version1\n# %matplotlib inline\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport copy\nimport random\nimport logging\nfrom torch.utils.tensorboard import SummaryWriter\n# from net.TCN import TemporalConvNet\n# from net.TCN_wn_bn import TemporalConvNet\n# from net.TCN_layer import TemporalConvNet\n# from net.TCN import MultiHeadSelfAttention\n# from net.meta_neural_network_architectures import MetaLossNetwork, LossAdapter\n\n# 设置随机种子\nSEED = 22\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# writer = SummaryWriter(\"/home/ubuntu/fedavg/FollowNet-car/SummaryWriter/MAML_MeTAL_log\")\n\n# ACC_LIMIT = 3 # the limit of acceleration, this can be calibrated based on the data 加速度极限\n# Ts = 0.04 # time interval for data sampling for HighD is 0.04 for other datasets are 0.1 数据采样时间间隔 15/375s\nTs = 0.1\n\n# load data    加载包含Python对象的.npy或.npz文件，需要显式地设置allow_pickle参数为True\nHighD_data = np.load('/kaggle/input/follow-car-datasets/data/HighD_car_fol_event_set_dhw.npy', allow_pickle=True)\nNGSIM_data = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_car_fol_event_set.npy', allow_pickle=True)\nSPMD1_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das1_car_fol_event_set.npy', allow_pickle=True)\nSPMD2_data = np.load('/kaggle/input/follow-car-datasets/data/SPMD_das2_car_fol_event_set.npy', allow_pickle=True)\nWaymo_data = np.load('/kaggle/input/follow-car-datasets/data/waymo_filter_carfo_info_150.npy', allow_pickle=True)\nLyft_data = np.load('/kaggle/input/lyft-data/Lyft_car_fol_event_set.npy', allow_pickle=True)\nLyft_test = np.load('/kaggle/input/follow-car-datasets/data/Lyft_test_data.npy', allow_pickle=True)\nLyft_train = np.load('/kaggle/input/follow-car-datasets/data/Lyft_train_data.npy', allow_pickle=True)\nLyft_val = np.load('/kaggle/input/follow-car-datasets/data/Lyft_val_data.npy', allow_pickle=True)\nHighD_test = np.load('/kaggle/input/follow-car-datasets/data/HighD_test_data.npy', allow_pickle=True)\nHighD_train = np.load('/kaggle/input/follow-car-datasets/data/HighD_train_data.npy', allow_pickle=True)\nHighD_val = np.load('/kaggle/input/follow-car-datasets/data/HighD_val_data.npy', allow_pickle=True)\nNGSIM_test = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_test_data.npy', allow_pickle=True)\nNGSIM_train = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_train_data.npy', allow_pickle=True)\nNGSIM_val = np.load('/kaggle/input/follow-car-datasets/data/NGSIM_I_80_val_data.npy', allow_pickle=True)\nSPMD1_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_test_data.npy', allow_pickle=True)\nSPMD1_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_train_data.npy', allow_pickle=True)\nSPMD1_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD1_val_data.npy', allow_pickle=True)\nSPMD2_test = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_test_data.npy', allow_pickle=True)\nSPMD2_train = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_train_data.npy', allow_pickle=True)\nSPMD2_val = np.load('/kaggle/input/follow-car-datasets/data/SPMD2_val_data.npy', allow_pickle=True)\nWaymo_test = np.load('/kaggle/input/follow-car-datasets/data/Waymo_test_data.npy', allow_pickle=True)\nWaymo_train = np.load('/kaggle/input/follow-car-datasets/data/Waymo_train_data.npy', allow_pickle=True)\nWaymo_val = np.load('/kaggle/input/follow-car-datasets/data/Waymo_val_data.npy', allow_pickle=True)\n# 使用训练设备\n# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\ngpu = 0\ndevice = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() and gpu != -1 else 'cpu')\nprint(device)\n\n\n# 保存日志\ndef get_logger(filename, verbosity=1, name=None):\n    # 设置不同verbosity对应的日志级别\n    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n    # 设置日志输出格式\n    formatter = logging.Formatter(\n        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(level_dict[verbosity])\n    # 创建文件处理器，将日志写入文件\n    fh = logging.FileHandler(filename, \"w\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # 创建控制台处理器，将日志输出到控制台\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n# 日志保存路径\n# dataset = 'NGSIM'\nlogger = get_logger(f'/kaggle/working/MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_ht.log')\n\n# 任务数据集划分\ndef split_train(data,data_ratio):\n#     np.random.seed(43)\n    # 随机排列数据集的索引\n    shuffled_indices=np.random.permutation(len(data))\n    if data_ratio > 0 and data_ratio <= 1:\n        # 如果小于等于1，根据输入百分比计算获取数据集的数量\n        data_size=int(len(data)*data_ratio)\n        data_num = int(data_size/2)\n        # 根据索引划分数据集\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_size]   # query set\n    else:\n        # 如果大于1，则data_ratio为获取数据集中的数量\n        data_num = int(data_ratio/2)\n        data_indices1 = shuffled_indices[:data_num]             # support set\n        data_indices2 = shuffled_indices[data_num:data_ratio]   # query set\n    return data[data_indices1], data[data_indices2]\n# np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n\n# max_len = 375 # for HighD dataset is 375 for others are 150\nmax_len = 150\nhis_horizon = 10\n\n\n# 将数据集划分成样本\nclass ImitationCarFolData(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset class for imitation learning (state -> action fitting) based car-following models.\n    \"\"\"\n    def __init__(self, data, max_len = max_len, Ts = 0.1):\n        self.data = data\n        self.max_len = max_len # Max length of a car following event.\n        self.Ts = Ts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # 获取数据集中指定索引的车辆跟驰事件\n        event = self.data[idx]\n        # 截取事件中的max_len个时间步数据\n        sv = event[1][:self.max_len] # 后车速度\n        lv = event[3][:self.max_len] # 前车速度\n        spacing = event[0][:self.max_len] # 车距\n        relSpeed = event[2][:self.max_len] # 相对速度\n        # 构建输入数据，包括 spacing, sv, relSpeed，截取除最后一项的全部数据\n        inputs = [spacing[:-1], sv[:-1], relSpeed[:-1]]\n        # 计算加速度标签（当前速度减去上一个时间步的速度）\n        acc_label = np.diff(sv) / self.Ts\n        # 保存 lv 作为速度标签\n        lv_spd = lv\n        return {'inputs': inputs, 'label': acc_label, 'lv_spd': lv_spd}\n\n\n# nn模型\nclass nn_model(nn.Module):\n    def __init__(self, input_size = 3):\n        super(nn_model,self).__init__()\n        self.net=nn.Sequential(\n                nn.Linear(input_size,256),\n                nn.ReLU(),\n                nn.Linear(256,256),\n                nn.ReLU(),\n                nn.Linear(256,1)\n            )\n        self.input_size = input_size\n        self.state = list(self.net.parameters())\n    # 用于元学习的内循环中更新权重的方法，用于计算元损失，然后重置下一个元任务的权重。\n    def forward(self,x,weights=None):\n        if weights == None:\n            x = self.net(x)\n        else:\n            x=F.linear(x,weights[0],weights[1])  # (input,weight,bias)\n            x=F.relu(x)\n            x=F.linear(x,weights[2],weights[3])\n            x=F.relu(x)\n            x=F.linear(x,weights[4],weights[5])\n        return x\n\n# 1dcnn模型\nclass CNN1D(nn.Module):\n    def __init__(self, input_size=3, num_classes=1, dropout=0.2):\n        super(CNN1D, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(in_channels=input_size, out_channels=8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(8),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout))\n        self.attention = MultiHeadSelfAttention(32)\n        self.mlp = nn.Sequential(\n            nn.Linear(32, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes)\n        )\n        self.token = nn.Parameter(torch.ones(1, 32))\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.dropout = dropout\n\n    def forward(self, x, weights=None): # input x(150, 3, 10)\n        if weights == None:\n            output = self.net(x)\n            output = output.transpose(1, 2)\n            result = torch.cat([output, self.token.unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result)\n            output = output[:, -1, :]\n\n            out = self.mlp(output)\n        else:\n            # bn = nn.BatchNorm1d(32)\n            # print(bn.running_mean, bn.running_var, bn.weight, bn.bias)\n            running_mean1 = nn.Parameter(torch.zeros(8), requires_grad=False).to(device)\n            running_var1 = nn.Parameter(torch.ones(8), requires_grad=False).to(device)\n            output = F.conv1d(x, weights[1],weights[2], stride=1, padding=1)   # (150, 32, 10)\n            output = F.batch_norm(output, running_mean=running_mean1, running_var=running_var1, weight=weights[3], bias=weights[4])\n            output = F.relu(output)   # (150, 32, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            running_mean2 = nn.Parameter(torch.zeros(16), requires_grad=False).to(device)\n            running_var2 = nn.Parameter(torch.ones(16), requires_grad=False).to(device)\n            output = F.conv1d(output, weights[5],weights[6], stride=1, padding=1)   # (150, 64, 10)\n            output = F.batch_norm(output, running_mean=running_mean2, running_var=running_var2, weight=weights[7], bias=weights[8])\n            output = F.relu(output)   # (150, 64, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            running_mean3 = nn.Parameter(torch.zeros(32), requires_grad=False).to(device)\n            running_var3 = nn.Parameter(torch.ones(32), requires_grad=False).to(device)\n            output = F.conv1d(output, weights[9],weights[10], stride=1, padding=1)   # (150, 128, 10)\n            output = F.batch_norm(output, running_mean=running_mean3, running_var=running_var3, weight=weights[11], bias=weights[12])\n            output = F.relu(output)   # (150, 128, 10)\n            output = F.dropout(output, p=self.dropout)\n\n            output = output.transpose(1, 2)\n            result = torch.cat([output, weights[0].unsqueeze(0).expand(x.shape[0], -1, -1)], dim=1)\n            output = self.attention(result, weights[13:19])\n            output = output[:, -1, :]\n\n            out = F.linear(output, weights[19], weights[20]) # (150, 128)\n            out = F.relu(out)   # (150, 128)\n            out = F.linear(out, weights[21], weights[22]) # (150, 1)\n        return out\n\n# 计算ttc\ndef calculate_safety(ttc):\n    minimum_ttc = min(ttc)\n    return minimum_ttc\n# 多数据同时验证\ndef model_evaluate(model,his_horizon,testdata):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    model.eval()\n\n    jerk_set = torch.empty(0).to(device)\n    error_set = torch.empty(0).to(device)\n    minimum_ttc_set = torch.empty(0).to(device)\n\n    criterion = nn.MSELoss()\n    count = 0\n    col = 0\n    for i, item in tqdm(enumerate(testdata)):#每个样本挨个出来\n        \n        x_data, y_data = item['inputs'], item['label']\n        x_data = torch.stack(x_data)#3x149\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n        lv_spd = item['lv_spd'].float().to(device).transpose(0, 1)\n        count += B\n        x_data_orig = x_data.clone().detach()#提前克隆的副本\n        \n        col_list = torch.full((B,), -1).to(device)\n        acc_batch = torch.empty(B,0).to(device)\n        ttc_batch = torch.empty(B,0).to(device)\n\n        \n        for frame in range(his_horizon, T):#对32个样本检测\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]   # (20, 3, 10)\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            #x = x.transpose(0,1).reshape(B, -1)\n            acc_pre = model(x).squeeze()\n            if acc_pre.dim() == 0:\n                acc_pre = acc_pre.unsqueeze(0)\n            acc_batch = torch.cat((acc_batch, acc_pre.unsqueeze(1)), dim=1)#记录所有acc     \n            if model_type == 'cnn1d' or model_type == 'tcn':\n                if frame < T-1:\n                    # 根据当前速度和加速度计算下一时间速度\n                    sv_spd_ = x_data[:, 1, frame] + acc_pre*Ts # (32)\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)\n                    # 计算下一时间速度的相对速度\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_ # (32)\n                    # 该时刻真实相对速度\n                    delta_v = x_data[:, -1, frame] # (32)\n                    # 通过两车车距加上相对位移得到下一时间段车距 ？\n                    spacing_ = x_data[:, 0, frame] + Ts*(delta_v + delta_v_)/2 # (32)\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    # update 根据计算得到的值，更新下一时间的值\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[:, :, frame + 1] = next_frame_data\n            else:\n                if frame < T-1:#开环训练，不断依照预测值更新下一个时间节点的数据\n                    sv_spd_ = x_data[frame, :, 1] + acc_pre*Ts#新的自车速度\n                    sv_spd_ = torch.tensor(np.maximum(np.array(sv_spd_.detach().cpu()),0.001), device=device)#保证速度不小于等于0\n                    delta_v_ = lv_spd[frame + 1] - sv_spd_#vt临时计算的\n                    delta_v = x_data[frame, :, -1]#v0原有的\n                    spacing_ = x_data[frame, :, 0] + Ts*(delta_v + delta_v_)/2#新的距离\n                    for i in range(len(spacing_)):  # 遍历每一行\n                        if spacing_[i] < 0 and col_list[i] == -1:    # 如果这一行包含0\n                            col_list[i] = frame+1\n                    next_frame_data = torch.stack((spacing_, sv_spd_, delta_v_)).transpose(0, 1) # B, 3\n                    x_data[frame + 1] = next_frame_data\n            if spacing_.dim() == 0:\n                ttc_batch_ = (-spacing_ / delta_v_).unsqueeze(0)\n            else:\n                ttc_batch_ = (-spacing_ / delta_v_)\n                \n            ttc_batch = torch.cat((ttc_batch, ttc_batch_.unsqueeze(1)), dim=1)\n        for i in range(B):#对32个样本统一处理\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                spacing_obs = x_data_orig[i,0,...]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[i, 0, :col_list[i]]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[i, 0, :]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            else:\n                spacing_obs = x_data_orig[...,i,0]\n                if(col_list[i] != -1):\n                    acc_batch_ = acc_batch[i,:col_list[i]-his_horizon]\n                    ttc_batch_ = ttc_batch[i,:col_list[i]-his_horizon]#修改\n                    spacing_pre_ = x_data[:col_list[i],i, 0]\n                    error_single = criterion(spacing_pre_[:col_list[i]], spacing_obs[:col_list[i]]).item()\n                else:\n                    acc_batch_ = acc_batch[i,:]\n                    ttc_batch_ = ttc_batch[i,:]\n                    spacing_pre_ = x_data[:,i, 0]\n                    error_single = criterion(spacing_pre_[:], spacing_obs[:]).item()\n            acc_batch_ = acc_batch_.cpu().detach().numpy()\n            jerk_single = np.mean(np.abs(np.diff(acc_batch_)/Ts))\n            TTC_single = [x for x in ttc_batch_ if x >= 0]#除去其中TTC小于0的\n            if len(TTC_single) > 0:\n                minimum_ttc_single = calculate_safety(TTC_single)#该样本的最小TTC\n                minimum_ttc_set= torch.cat((minimum_ttc_set,torch.tensor(minimum_ttc_single).unsqueeze(0).to(device)), dim=0)      \n            jerk_set= torch.cat((jerk_set,torch.tensor(jerk_single).unsqueeze(0).to(device)), dim=0)\n            error_set= torch.cat((error_set,torch.tensor(error_single).unsqueeze(0).to(device)), dim=0)\n        col = col + torch.sum(col_list != -1).item()\n    if len(minimum_ttc_set) == 0:\n        ttc = 0\n    else:\n        ttc = sum(minimum_ttc_set)/len(minimum_ttc_set)\n    error = sum(error_set)/len(error_set)\n    return error,col,count,sum(jerk_set)/len(jerk_set),ttc#还需修改（以MSE为基准）\n\n\ndef finetuning(best_mse_state):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    # Train\n    Ts = 0.1\n    max_len = 150\n    og_net = maml.net\n    # 创建一个与原始网络结构相同的虚拟网络\n    if model_type == 'nn':\n        dummy_net = nn_model(input_size = his_horizon*3)\n    elif model_type == 'tcn':\n        dummy_net = TemporalConvNet(num_inputs=3, num_channels=(16,32))\n    elif model_type == 'cnn1d':\n        dummy_net = CNN1D()\n        # dummy_net = cbrd(num_inputs=3, num_output=1)\n    dummy_net=dummy_net.to(device)\n\n    # 多gpu训练\n    # if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n    #     dummy_net = torch.nn.DataParallel(dummy_net)  # 自动选择gpu\n    # dummy_net.to(device)\n\n    # 加载原始网络的权重\n    # weight_path = torch.load(state_path)\n    # dummy_net.load_state_dict(weight_path.state_dict())\n    dummy_net.load_state_dict(og_net.state_dict())\n    # 进行迭代，每次更新虚拟网络的参数\n    num_shots=5\n    lr = 0.01\n    loss_fn=nn.MSELoss()\n    optim=torch.optim.Adam\n    opt=optim(dummy_net.parameters(),lr=lr, weight_decay=3e-4)\n\n\n    # 数据集划分\n    def split_data(data,data_ratio):\n        # random.seed(SEED)\n        # np.random.seed(SEED)\n        # torch.manual_seed(SEED)\n        if data_ratio > 0 and data_ratio <= 1:\n            # 如果小于等于1，根据输入百分比计算获取数据集的数量\n            data_size=int(len(data)*data_ratio)\n        else:\n            # 如果大于1，则data_ratio为获取数据集中的数量\n            data_size = data_ratio\n        return data[:data_size]\n    # np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n\n    # 获取数据集的数量\n    K=40\n    dataset_train = split_data(train_data, K)\n    dataset_loader_train = ImitationCarFolData(dataset_train, max_len = max_len, Ts = Ts)\n    train_loader = DataLoader(\n                dataset_loader_train,\n                batch_size=10,\n                shuffle=True,\n                num_workers=1,\n                drop_last=True)\n    dataset_test = test_data\n    dataset_loader_test = ImitationCarFolData(dataset_test, max_len = max_len, Ts = Ts)\n    test_loader = DataLoader(\n                dataset_loader_test,\n                batch_size=96,\n                shuffle=False,\n                num_workers=1,\n                drop_last=False)\n    # dataset_val = NGSIM_val\n    # dataset_loader_val = ImitationCarFolData(dataset_val, max_len = max_len, Ts = Ts)\n    # val_loader = DataLoader(\n    #             dataset_loader_val,\n    #             batch_size=10,\n    #             shuffle=True,\n    #             num_workers=1,\n    #             drop_last=True)\n\n\n    # 初始化变量\n    train_loss_his = [] # 训练损失\n    test_error_his = [] # 测试误差\n    best_train_loss = None # 最佳训练损失\n\n    # 训练过程\n    best_error = 10000\n    for epoch in tqdm(range(num_shots)):\n        train_losses = [] # 记录每个epoch的训练损失\n        validation_losses = [] # 记录每个epoch的验证损失\n        jerk_val = 0\n        dummy_net.train()\n        # 遍历数据集\n        for i, item in enumerate(train_loader):\n            # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n            # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n            x_data, y_data = item['inputs'], item['label'] # x_data((32,374)，(32,374)，(32,374))  y_data(32,374)\n            # Put T into the first dimension, B, T, d -> T, B, d\n            # 将x_data中3个(32,374)连接，转换成(3,32,374)\n            x_data = torch.stack(x_data)\n            # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                # x->(bs,3,149)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n                B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            else:\n                # x->(149,bs,3)，y->(149,bs)\n                x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n                T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n            # print(T,B,d)  # 149  20  3\n                \n            y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (364,32)\n            y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (364,32)\n            # 从历史数据时间步开始遍历\n            for frame in range(his_horizon, T):\n                if model_type == 'cnn1d' or model_type == 'tcn':\n                    x= x_data[:, :, frame-his_horizon:frame]\n                else:\n                    x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n                if model_type == 'nn':\n                    x = x.transpose(0,1).reshape(B, -1) # flatten for history data\n                # 根据his_horizon个数据预测加速度\n                acc_pre = dummy_net(x).squeeze() # (32)\n                y_pre[frame - his_horizon] = acc_pre\n            #计算损失并进行反传及优化\n            loss = loss_fn(y_pre, y_label)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(dummy_net.parameters(), 0.25) # 对模型梯度进行裁剪，避免梯度爆炸\n            opt.step()\n\n            train_losses.append(loss.item())\n        # 计算本轮平均损失\n        train_loss = np.mean(train_losses)\n\n        train_loss_his.append(train_loss)\n#         print(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n        logger.info(\"Epoch: {0}| Train Loss: {1:.7f}\".format(epoch + 1, train_loss))\n\n        mean_spacing_error,col,count,jerk,miniumu_ttc = model_evaluate(dummy_net,his_horizon,test_loader)\n        logger.info(\"mean_spacing_error：{:.5f}，col={}，count={}，rate={:.5f}%，jerk={:.5f}，miniumu_ttc={:.5f}\".format(mean_spacing_error, col, count, (col/count)*100, jerk, miniumu_ttc))\n        if epoch == 0:\n            first_step_error = mean_spacing_error\n        if mean_spacing_error < best_error:\n            best_error = mean_spacing_error\n        if mean_spacing_error < best_mse_state:\n            best_mse_state = mean_spacing_error\n            # save the best model\n            with open(save_path, 'wb') as f:\n                torch.save(dummy_net, f)\n    return best_error, first_step_error, best_mse_state\n\n\n\n# 元学习内循环训练过程\ndef inner_train(data_loader, net ,temp_weights, criterion, metal=None):\n    net.train()\n    # 遍历数据集\n    for i, item in enumerate(data_loader):\n        # x_data, y_data = item['inputs'].float().to(device), item['label'].float().to(device)\n        # 获取输入数据及标签，input包括[距离，后车速度，相对速度]，label为加速度\n        x_data, y_data = item['inputs'], item['label'] # x_data((20,149)，(20,149)，(20,149))  y_data(20,149)\n        # Put T into the first dimension, B, T, d -> T, B, d\n        # 将x_data中3个(20,149)连接，转换成(3,20,149)\n        x_data = torch.stack(x_data)\n        # x_data, y_data = x_data.transpose(0, 1), y_data.transpose(0, 1)\n        if model_type == 'cnn1d' or model_type == 'tcn':\n            # x->(bs,3,149)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 1).float().to(device), y_data.transpose(0, 1).float().to(device)\n            B, d, T = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        else:\n            # x->(149,bs,3)，y->(149,bs)\n            x_data, y_data = x_data.transpose(0, 2).float().to(device), y_data.transpose(0, 1).float().to(device)\n            T, B, d = x_data.shape # (total steps, batch_size, d) as the shape of the data\n        # print(T,B,d)  # 149  20  3\n            \n        y_pre = torch.zeros(T - his_horizon, B).to(device) # 存储预测加速度 (139,20)\n        y_label = y_data[his_horizon:] # 存储从his_horizon开始的真实加速度 (139,20)\n        # 从历史数据时间步开始遍历\n        for frame in range(his_horizon, T):\n            if model_type == 'cnn1d' or model_type == 'tcn':\n                x= x_data[:, :, frame-his_horizon:frame]\n            else:\n                x = x_data[frame-his_horizon:frame] # (his_horizon, B, d) (10, 20, 3)\n            if model_type == 'nn':\n                x = x.transpose(0,1).reshape(B, -1) # flatten for history data (20, 30)\n            # 根据his_horizon个数据预测加速度\n            acc_pre = net(x, temp_weights).squeeze() # (32)\n            # acc_pre = net.module(x, temp_weights).squeeze()\n            y_pre[frame - his_horizon] = acc_pre\n        #计算损失并进行反传及优化\n        loss = criterion(y_pre, y_label)\n        if metal == None:\n            return loss\n        else:\n            return loss, y_pre\n\n\n# 生成不同数据集的任务类\nclass DataTask():\n    def __init__(self,data_range):\n        self.data_range = data_range\n  # 生成任务数据，默认生成一个数据点，可以通过指定size参数生成多个数据点\n    def sample_data(self,size=1):\n        if self.data_range == 1:\n            meta_data = HighD_data\n            Ts = 0.04\n            max_len = 375\n            if adapt_tasks:\n                task_net = HighD_net\n            # print('--HighD--')\n        elif self.data_range == 2:\n            meta_data = Lyft_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Lyft_net\n            # print('--Lyft---')\n        elif self.data_range == 3:\n            meta_data = SPMD1_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD1_net\n            # print('SPMD_DAS1')\n        elif self.data_range == 4:\n            meta_data = SPMD2_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = SPMD2_net\n            # print('SPMD_DAS2')\n        elif self.data_range == 5:\n            meta_data = NGSIM_data\n            Ts = 0.1\n            max_len = 150\n            if adapt_tasks:\n                task_net = Waymo_net\n            # print('--Waymo--')\n        meta_data_set1, meta_data_set2 = split_train(meta_data, size)\n#         print(len(meta_data_set1),len(meta_data_set2))\n        dataset_loader1 = ImitationCarFolData(meta_data_set1, max_len = max_len, Ts = Ts)\n        dataset_loader2 = ImitationCarFolData(meta_data_set2, max_len = max_len, Ts = Ts)\n        if adapt_tasks:\n            return dataset_loader1, dataset_loader2, task_net\n        else:\n            return dataset_loader1, dataset_loader2\n\n\n# 任务分布，及选择不同的数据\nclass DataDistribution():\n    def __init__(self, data_range=5, n=3):\n        self.data_range = data_range    # 获取数据集的数值\n        self.n = n\n    # 随机生成一个跟车数据集\n    def sample_task(self, epoch):\n        # 从1到5中随机选择3个不重复的数\n        # 创建一个新的随机数生成器实例\n        rng = np.random.RandomState(epoch)\n        data_choice = rng.choice(self.data_range, size=self.n, replace=False) + 1\n        # data_choice = np.random.choice(self.data_range, size=self.n, replace=False) + 1\n        return data_choice\n\n\ndef attenuate_init(attenuator, panning, task_embeddings, names_weights_copy):\n        ## Attenuate\n        updated_names_weights_copy = list()\n        \n        # 生成衰减参数和偏移参数\n        if attenuator == None:\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(key + beta[i])\n                i+=1\n        elif panning == None:\n            gamma = attenuator(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key)\n                i+=1\n        else:\n            gamma = attenuator(task_embeddings)\n            beta = panning(task_embeddings)\n            i = 0\n            for key in names_weights_copy:\n                updated_names_weights_copy.append(gamma[i] * key + beta[i])\n                i+=1\n            \n        return updated_names_weights_copy\n\n\ndef get_inner_loop_parameter_dict(params):\n    \"\"\"\n    Returns a dictionary with the parameters to use for inner loop updates.\n    :param params: A dictionary of the network's parameters.\n    :return: A dictionary of the parameters to use for the inner loop optimization process.\n    \"\"\"\n    param_dict = dict()\n    for name, param in params:\n        if param.requires_grad:\n            param_dict[name] = param.to(device)\n\n    return param_dict\n\ndef get_per_step_loss_importance_vector(inner_step, current_epoch):\n        \"\"\"\n        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n        loss towards the optimization loss.\n        :return: A tensor to be used to compute the weighted average of the loss, useful for\n        the MSL (Multi Step Loss) mechanism.\n        \"\"\"\n        multi_step_loss_num_epochs = 20\n        loss_weights = np.ones(shape=(inner_step)) * (\n                1.0 / inner_step)\n        decay_rate = 1.0 / inner_step / multi_step_loss_num_epochs\n        min_value_for_non_final_losses = 0.03 / inner_step\n        for i in range(len(loss_weights) - 1):\n            curr_value = np.maximum(loss_weights[i] - (current_epoch * decay_rate), min_value_for_non_final_losses)\n            loss_weights[i] = curr_value\n\n        curr_value = np.minimum(\n            loss_weights[-1] + (current_epoch * (inner_step - 1) * decay_rate),\n            1.0 - ((inner_step - 1) * min_value_for_non_final_losses))\n        loss_weights[-1] = curr_value\n        loss_weights = torch.Tensor(loss_weights).to(device)\n        return loss_weights\n\n\n# MAML元学习训练过程\nclass MAML():\n    def __init__(self, net, alpha, beta, tasks, k, weight_decay, inner_step):  # (net,alpha=0.01,beta=0.0001,tasks=data_tasks,k=150)\n        # 初始化 MAML 算法的元参数\n        self.net = net  # 元学习使用的神经网络模型\n        self.weights = list(net.parameters())  # 获取神经网络的参数\n        self.alpha = alpha  # 内循环学习率\n        self.beta = beta  # 外循环学习率\n        self.inner_step = inner_step  # 内部循环更新的步数\n        self.tasks = tasks  # 正弦任务的分布\n        self.k = k  # 内循环（任务采样）的数据集大小\n        self.criterion = nn.MSELoss()  # 均方误差损失函数\n        self.meta_losses = []  # 用于存储元学习的损失历史\n        self.meta_test = []\n        self.finetuning_error = []\n        self.plot_every = 10  # 每隔多少次迭代记录一次损失，用于绘图\n        self.print_every = 1  # 每隔多少次迭代输出一次损失信息，用于打印\n        self.first_add = 0  # 用于添加优化器优化的参数\n        self.all_best_state = 10000 # 用于存储微调后最好mse时的模型权重\n        self.batch_size = int(self.k/2)\n        # 设置权重衰减因子\n        self.weight_decay = weight_decay\n        self.num_layers = len(self.weights)\n        params = [{'params': self.weights}]\n        if MeTAL:\n            base_learner_num_layers = len(self.weights)\n            support_meta_loss_num_dim = base_learner_num_layers + 2 + 1\n            support_adapter_num_dim = base_learner_num_layers + 1\n\n            self.meta_loss = MetaLossNetwork(support_meta_loss_num_dim, notspi=self.inner_step, device=device).to(device=device)\n            self.meta_loss_adapter = LossAdapter(support_adapter_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n            new_params = {'params': self.meta_loss.parameters()}\n            params.append(new_params)\n            new_params = {'params': self.meta_loss_adapter.parameters()}\n            params.append(new_params)\n\n            if semi_supervised:\n                query_num_dim = base_learner_num_layers + 1 + 1\n                self.meta_query_loss = MetaLossNetwork(query_num_dim, notspi=self.inner_step, device=device).to(device=device)\n                self.meta_query_loss_adapter = LossAdapter(query_num_dim, num_loss_net_layers=2, notspi=self.inner_step, device=device).to(device=device)\n                new_params = {'params': self.meta_query_loss.parameters()}\n                params.append(new_params)\n                new_params = {'params': self.meta_query_loss_adapter.parameters()}\n                params.append(new_params)\n\n        if attenuate:  #L2F\n            self.attenuator = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Sigmoid()\n            ).to(device=device)\n            new_params = {'params': self.attenuator.parameters()}\n            params.append(new_params)\n        if tapt:  #bias\n            self.panning = nn.Sequential(\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.num_layers, self.num_layers),\n                nn.Tanh()\n            ).to(device=device)\n            new_params = {'params': self.panning.parameters()}\n            params.append(new_params)\n        if alfa:  # ALFA\n            input_dim = self.num_layers*2\n            self.regularizer = nn.Sequential(\n                nn.Linear(input_dim, input_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(input_dim, input_dim)\n            ).to(device=device)\n            new_params = {'params': self.regularizer.parameters()}\n            params.append(new_params)\n        self.meta_optimiser = torch.optim.Adam(params,lr=self.beta, weight_decay=3e-4)\n\n    def inner_loop(self, task):\n        taskloss = []\n        # 内循环更新参数，用于计算元学习损失\n        temp_weights = [w.clone() for w in self.weights]  # 复制当前网络参数作为临时参数\n        if adapt_tasks:\n            dataset_loader1, dataset_loader2, task_net = task.sample_data(size=self.k)\n            task_optimiser = torch.optim.Adam([{'params':task_net.parameters()}],lr=0.001, weight_decay=3e-4)\n        else:\n            dataset_loader1, dataset_loader2 = task.sample_data(size=self.k)  # 从任务中采样数据集 D 和 D'\n        dataloader1 = DataLoader(\n                dataset_loader1,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=1,\n                drop_last=False)\n        for step in range(self.inner_step):\n            if step == 0 and adapt_tasks or attenuate or tapt:\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n                grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n                if adapt_tasks:     # every tasks\n                    grads = tuple(grads[i] for i in range(len(grads)))\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    temp_weights = attenuate_init(task_net, None, layerwise_mean_grads, temp_weights)  # 得到新的权重\n\n                if attenuate or tapt:    #L2F\n                    layerwise_mean_grads = []\n                    for i in range(len(grads)):\n                        layerwise_mean_grads.append(grads[i].mean())  # 每一层的梯度取平均值\n                    layerwise_mean_grads = torch.stack(layerwise_mean_grads)\n                    if attenuate and tapt:\n                        temp_weights = attenuate_init(self.attenuator, self.panning, layerwise_mean_grads, temp_weights)  # 得到新的权重\n                    elif attenuate and not tapt:\n                        temp_weights = attenuate_init(self.attenuator, None, layerwise_mean_grads, temp_weights)\n                    elif not attenuate and tapt:\n                        temp_weights = attenuate_init(None, self.panning, layerwise_mean_grads, temp_weights)\n\n            if MeTAL:\n                support_task_state = []\n                # 获取支持集 Loss 和 预测值\n                support_loss, support_preds = inner_train(dataloader1, self.net, temp_weights, self.criterion, metal=1)\n                support_loss /= int(self.batch_size)\n                support_task_state.append(support_loss)\n\n                # 将每层模型权重的平均值添加到task_state中\n                for v in temp_weights:\n                    support_task_state.append(v.mean())\n\n                support_task_state = torch.stack(support_task_state)\n                # 归一化\n                adapt_support_task_state = (support_task_state - support_task_state.mean())/(support_task_state.std() + 1e-12)                                                 \n                # 损失函数网络参数更新\n                updated_meta_loss_weights = self.meta_loss_adapter(adapt_support_task_state, step, self.names_loss_weights_copy)\n                for i, item in enumerate(dataloader1):\n                    acceleration = item['label']\n                # 将预测值添加到task_state\n                support_task_state = torch.cat((\n                    support_task_state.view(1, -1).expand(support_preds.size(1), -1),\n                    support_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                    acceleration.mean(dim=1, keepdim=True).float().to(device)\n                ), -1)\n\n                support_task_state = (support_task_state - support_task_state.mean()) / (support_task_state.std() + 1e-12)\n                # 计算support_loss\n                meta_support_loss = self.meta_loss(support_task_state, step, params=updated_meta_loss_weights).mean().squeeze()\n                if semi_supervised:\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    query_loss, query_preds = inner_train(dataloader2, self.net, temp_weights, self.criterion, metal=1)\n                    query_loss /= int(self.batch_size)\n                    query_task_state = []\n                    \n                    for i, item in enumerate(dataloader2):\n                        acceleration = item['label']\n                    # query_task_state.append(query_loss)\n\n                    for v in temp_weights:\n                        query_task_state.append(v.mean())\n                    query_task_state = torch.stack(query_task_state)\n                    query_task_state = torch.cat((\n                                query_task_state.view(1, -1).expand(query_preds.size(1), -1),\n                                query_preds.transpose(0, 1).mean(dim=1, keepdim=True),\n                                acceleration.mean(dim=1, keepdim=True).float().to(device)\n                    ), -1)\n\n                    query_task_state = (query_task_state - query_task_state.mean())/(query_task_state.std() + 1e-12)\n                    updated_meta_query_loss_weights = self.meta_query_loss_adapter(query_task_state.mean(0), step, self.names_query_loss_weights_copy)\n\n                    meta_query_loss = self.meta_query_loss(query_task_state, step, params=updated_meta_query_loss_weights).mean().squeeze()\n\n                    loss = support_loss + meta_support_loss + meta_query_loss\n                else:\n                    loss = support_loss + meta_support_loss\n            else:\n                # 训练过程\n                loss = inner_train(dataloader1, self.net, temp_weights, self.criterion) / self.batch_size\n            grads = torch.autograd.grad(loss, temp_weights, create_graph=True, retain_graph=True)  # 计算损失对参数的梯度\n\n            if alfa:    #ALFA\n                # 用于存储权重的平均值和梯度的平均值\n                per_step_task_embedding = []\n                for v in temp_weights:\n                    per_step_task_embedding.append(v.mean())\n            \n                for i in range(len(grads)):\n                    per_step_task_embedding.append(grads[i].mean())\n\n                per_step_task_embedding = torch.stack(per_step_task_embedding)\n\n                generated_params = self.regularizer(per_step_task_embedding)\n\n                generated_alpha, generated_beta = torch.split(generated_params, split_size_or_sections=self.num_layers)\n                generated_alpha_params = []\n                generated_beta_params = []\n                g = 0\n                for key in temp_weights:\n                    generated_alpha_params.append(generated_alpha[g])\n                    generated_beta_params.append(generated_beta[g])\n                    g+=1\n                # 初始化ALFA自适应参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    if random_init:\n                        self.names_beta_dict_per_param = nn.ParameterDict()\n                    self.names_alpha_dict = nn.ParameterDict()\n                    self.names_beta_dict = nn.ParameterDict()\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    for idx, param in enumerate(temp_weights):\n\n                        if random_init:\n                        # per-param weight decay for random init\n                            self.names_beta_dict_per_param[str(idx)] = nn.Parameter(\n                                data=torch.ones(param.shape) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_learning_rates)\n\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step),\n                                requires_grad=use_learnable_beta)\n                            \n                            self.names_beta_dict_per_param.to(device)\n                        else:\n                            # per-step per-layer meta-learnable weight decay bias term (for more stable training and better performance by 2~3%)\n                            self.names_beta_dict[str(idx)] = nn.Parameter(\n                                data=torch.ones(self.inner_step) * init_weight_decay * init_learning_rate,\n                                requires_grad=use_learnable_beta)\n                        \n                        # per-step per-layer meta-learnable learning rate bias term (for more stable training and better performance by 2~3%)\n                        self.names_alpha_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                        \n                        self.names_beta_dict.to(device)\n                        self.names_alpha_dict.to(device)\n                    # 将额外的参数添加到优化器中\n                    if random_init:\n                        self.meta_optimiser.add_param_group({'params': self.names_beta_dict_per_param.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_beta_dict.parameters()})\n                    self.meta_optimiser.add_param_group({'params': self.names_alpha_dict.parameters()})\n            else:\n                # 使用可学习的参数\n                if self.first_add == 0:\n                    self.first_add += 1\n\n                    init_learning_rate = torch.ones(1) * self.alpha\n                    init_learning_rate.to(device)\n                    init_weight_decay = torch.ones(1)* self.weight_decay\n                    init_weight_decay.to(device)\n\n                    self.names_learning_rates_dict = nn.ParameterDict()\n                    for idx, param in enumerate(temp_weights):\n                        self.names_learning_rates_dict[str(idx)] = nn.Parameter(\n                            data=torch.ones(self.inner_step) * init_learning_rate,\n                            requires_grad=use_learnable_alpha)\n                    self.names_learning_rates_dict.to(device)\n                    if use_learnable_alpha:\n                        self.meta_optimiser.add_param_group({'params': self.names_learning_rates_dict.parameters()})\n                        \n                    if inner_update == \"L2\" and self.weight_decay != None:\n                        self.names_weight_decay_dict = nn.ParameterDict()\n                        for idx, param in enumerate(temp_weights):\n                            self.names_weight_decay_dict[str(idx)] = nn.Parameter(\n                                    data=torch.ones(self.inner_step) * init_weight_decay,\n                                    requires_grad=use_learnable_beta)\n                        self.names_weight_decay_dict.to(device)\n                        if use_learnable_beta:\n                            self.meta_optimiser.add_param_group({'params': self.names_weight_decay_dict.parameters()})\n\n            if inner_update == \"L2\" and self.weight_decay != None:\n                # temp_weights = [w - self.alpha * (g + (self.weight_decay * w)) for w, g in zip(temp_weights, grads)]\n                # temp_weights = [((1 - self.alpha * self.weight_decay) * w) - (self.alpha * g) for w, g in zip(temp_weights, grads)]\n                temp_weights = [((1 - self.names_learning_rates_dict[str(key)][step] * self.names_weight_decay_dict[str(key)][step]) * w) - (self.names_learning_rates_dict[str(key)][step] * g) for key, (w, g) in enumerate(zip(temp_weights, grads))]\n            elif inner_update == \"ALFA\" and alfa:\n                if random_init:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step] * self.names_beta_dict_per_param[str(key)]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n                else:\n                    temp_weights = [(1 - beta * self.names_beta_dict[str(key)][step]) * w - (alpha * self.names_alpha_dict[str(key)][step] * g) for key, (w, g, beta, alpha) in enumerate(zip(temp_weights, grads, generated_beta_params, generated_alpha_params))]\n            else:\n                temp_weights = [w - self.names_learning_rates_dict[str(key)][step] * g for key, (w, g) in enumerate(zip(temp_weights, grads))]  # 临时参数更新 梯度下降\n\n            if use_multi_step_loss_optimization and self.epoch < 20:\n                dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                taskloss.append(self.per_step_loss_importance_vectors[step] * metaloss)\n            else:\n                if step == (self.inner_step - 1):\n                    dataloader2 = DataLoader(\n                            dataset_loader2,\n                            batch_size=self.batch_size,\n                            shuffle=True,\n                            num_workers=1,\n                            drop_last=False)\n                    metaloss = inner_train(dataloader2, self.net, temp_weights, self.criterion) / self.batch_size\n                    taskloss.append(metaloss)\n        task_losses = torch.sum(torch.stack(taskloss))\n        if adapt_tasks:\n            task_optimiser.zero_grad()\n            metaloss.backward(retain_graph=True)\n            task_optimiser.step()\n        return task_losses\n\n    def outer_loop(self, num_epochs):  # epoch 5000\n        best_loss = 10000\n        best_first_step_error = 10000\n        best_index = 0\n        for epoch in tqdm(range(1, num_epochs + 1)):\n            self.epoch = epoch\n            if MeTAL:\n                self.names_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_loss.named_parameters())\n                if semi_supervised:\n                    self.names_query_loss_weights_copy = get_inner_loop_parameter_dict(self.meta_query_loss.named_parameters())\n            total_loss = 0\n            metaloss_sum = 0\n            tasks = self.tasks.sample_task(epoch)\n            for i in tasks:\n                if use_multi_step_loss_optimization:\n                    self.per_step_loss_importance_vectors = get_per_step_loss_importance_vector(self.inner_step, epoch)\n                task = DataTask(i)\n                metaloss = self.inner_loop(task)  # 内循环更新参数，计算元学习损失\n                metaloss_sum += metaloss  # mete_loss求和\n            self.meta_optimiser.zero_grad()\n            # metaloss_sum.backward(retain_graph=True)\n            metagrads = torch.autograd.grad(metaloss_sum, self.weights, retain_graph=True)  # 计算元学习损失对参数的梯度\n            # # 重要步骤：使用元学习梯度更新网络参数\n            for w, g in zip(self.weights, metagrads):\n                w.grad = g\n\n            if MeTAL:\n                meta_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss.parameters(), meta_loss_grads):\n                    w.grad = g\n\n                meta_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_loss_adapter.parameters()), retain_graph=True)\n                for w, g in zip(self.meta_loss_adapter.parameters(), meta_loss_adapter_grads):\n                    w.grad = g\n\n                if semi_supervised:\n                    meta_query_loss_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss.parameters(), meta_query_loss_grads):\n                        w.grad = g\n\n                    meta_query_loss_adapter_grads = torch.autograd.grad(metaloss_sum, list(self.meta_query_loss_adapter.parameters()), retain_graph=True)\n                    for w, g in zip(self.meta_query_loss_adapter.parameters(), meta_query_loss_adapter_grads):\n                        w.grad = g\n\n            if not alfa and use_learnable_alpha:\n                learning_rates_grads = torch.autograd.grad(metaloss_sum, list(self.names_learning_rates_dict.parameters()), retain_graph=True)\n                for w, g in zip(self.names_learning_rates_dict.parameters(), learning_rates_grads):\n                    w.grad = g\n                if inner_update == \"L2\" and self.weight_decay != None and use_learnable_beta:\n                    weight_decay_grads = torch.autograd.grad(metaloss_sum, list(self.names_weight_decay_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_weight_decay_dict.parameters(), weight_decay_grads):\n                        w.grad = g\n\n            if attenuate:\n                attenuate_grads = torch.autograd.grad(metaloss_sum, list(self.attenuator.parameters()), retain_graph=True)\n                for w, g in zip(self.attenuator.parameters(), attenuate_grads):\n                    w.grad = g\n            if tapt:\n                tapt_grads = torch.autograd.grad(metaloss_sum, list(self.panning.parameters()), retain_graph=True)\n                for w, g in zip(self.panning.parameters(), tapt_grads):\n                    w.grad = g\n            if alfa:\n                alfa_grads = torch.autograd.grad(metaloss_sum, list(self.regularizer.parameters()), retain_graph=True)\n                for w, g in zip(self.regularizer.parameters(), alfa_grads):\n                    w.grad = g\n                if use_learnable_beta:\n                    names_beta_grads = torch.autograd.grad(metaloss_sum, list(self.names_beta_dict.parameters()), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict.parameters(), names_beta_grads):\n                        w.grad = g\n                if use_learnable_alpha:\n                    names_alpha_grads = torch.autograd.grad(metaloss_sum, self.names_alpha_dict.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_alpha_dict.parameters(), names_alpha_grads):\n                        w.grad = g\n                if random_init:\n                    random_init_grads = torch.autograd.grad(metaloss_sum, self.names_beta_dict_per_param.parameters(), retain_graph=True)\n                    for w, g in zip(self.names_beta_dict_per_param.parameters(), random_init_grads):\n                        w.grad = g\n            \n            self.meta_optimiser.step()  # 使用外循环优化器更新网络参数\n            total_loss += metaloss_sum.item() / len(tasks)\n            mserror,col,count,jerk,ttc = model_evaluate(self.net,his_horizon,meta_loader)\n            if epoch % self.print_every == 0:\n                # print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                logger.info(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / self.print_every))\n                logger.info(\"{}/{}. mserror: {}  col: {}  count: {}  jerk: {}  ttc: {}\".format(epoch, num_epochs, mserror, col, count, jerk, ttc))\n                if mserror < best_loss:\n                    best_loss = mserror\n                    best_index = epoch\n                    # with open(file_path, 'wb') as f:\n                    #     torch.save(self.net, f)\n                self.meta_losses.append(total_loss / self.print_every)\n                self.meta_test.append(mserror.cpu().detach().numpy() / self.print_every)\n            # finetuning()\n            fine_error, first_step_error, best_mse_state = finetuning(self.all_best_state)\n            self.all_best_state = best_mse_state\n            if first_step_error < best_first_step_error:\n                best_first_step_epoch = epoch\n                best_first_step_error = first_step_error\n            self.finetuning_error.append(fine_error.cpu().detach().numpy())\n            # if epoch % self.plot_every == 0:\n            #     epoch_path = f\"/home/ubuntu/fedavg/FollowNet-car/MAML_state/maml_l2f_test/MAML_nn_a0.01_b0.0001_{epoch}E.pt\" # 元初始化参数\n            #     # weight_change = torch.load(file_path)\n            #     with open(epoch_path, 'wb') as f:\n            #         torch.save(self.net, f)\n        logger.info(\"(\", best_index, \")\",\"best_loss:\", best_loss)\n        logger.info(\"(\", best_first_step_epoch, \")\", \"best_first_step_error:\",best_first_step_error)\n\ndataset = 'Waymo'\nmodel_type = 'tcn'\nif dataset == 'SPMD1':\n    train_data = SPMD1_train\n    test_data = SPMD1_test\nelif dataset == 'SPMD2':\n    train_data = SPMD2_train\n    test_data = SPMD2_test\nelif dataset == 'Waymo':\n    train_data = Waymo_train\n    test_data = Waymo_test\nelif dataset == 'NGSIM':\n    train_data = NGSIM_train\n    test_data = NGSIM_test\nelif dataset == 'Lyft':\n    train_data = Lyft_train\n    test_data = Lyft_test\nelif dataset == 'HighD':\n    train_data = HighD_train\n    test_data = HighD_test\n# 定义保存文件的文件夹路径\n# file_path = \"/home/ubuntu/fedavg/FollowNet-car/MAML_state/MAML_MeTAL_nn_a0.01_b0.0001_k300_E50.pt\" # 元初始化参数\nsave_folder = '/kaggle/working/'\nsave = f'MeTAL_Waymo_{model_type}_{dataset}_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc.pt' # 保存模型文件\n# 确保保存文件的文件夹存在，如果不存在，则创建\nif not os.path.exists(save_folder):\n    os.makedirs(save_folder)\n# 定义文件保存路径\nsave_path = os.path.join(save_folder, save)\nmeta_test = test_data\nmeta_loader_test = ImitationCarFolData(meta_test, max_len = max_len, Ts = Ts)\nmeta_loader = DataLoader(\n            meta_loader_test,\n            batch_size=96,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n# 模型初始化\nif model_type == 'nn':\n    net = nn_model(input_size = his_horizon*3).to(device)\nelif model_type == 'tcn':\n    net = TemporalConvNet(num_inputs=3, num_channels=(16,32)).to(device)\nelif model_type == 'cnn1d':\n    net = CNN1D().to(device)\n    # net = cbrd(num_inputs=3, num_output=1).to(device)\n# elif model_type == 'lstm':\n#     net = lstm_model(input_size = 3, lstm_layers = 1).to(device) # single layer lstm\n\n# model_state = list(net.parameters())\n# print(model_state)\n\n# 多gpu训练\n# if torch.cuda.device_count() > 1:   # 查看当前电脑的可用的gpu的数量，若gpu数量>1,就多gpu训练\n#     net = torch.nn.DataParallel(net)  # 自动选择gpu\n# net.to(device)\n\n# 内部循环更新策略\nMeTAL = True   # 任务自适应loss函数\nsemi_supervised = True  # 使用查询集 作为半监督\nadapt_tasks = False  # 权重初始化 每个任务用一个\nattenuate = False  # 权重初始化 所有任务共用 权重衰减\ntapt = False  # 权重初始化 所有任务共用 权重偏差\nalfa = False        # 自适应学习优化超参数\nrandom_init = False\nuse_learnable_learning_rates = random_init\nuse_learnable_beta = False  # 使优化参数beta可学习\nuse_learnable_alpha = False  # 使优化参数alpha可学习\nuse_multi_step_loss_optimization = False  # 内部循环多步时，使用最后一步的Loss，还是使用多步的Loss\ninner_update = 'ALFA'\nif adapt_tasks:\n    num_layers = len(list(net.parameters()))\n    HighD_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Lyft_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD1_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    SPMD2_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\n    Waymo_net = nn.Sequential(\n        nn.Linear(num_layers, num_layers),\n        nn.ReLU(inplace=True),\n        nn.Linear(num_layers, num_layers),\n        nn.Sigmoid()\n    ).to(device=device)\ndata_tasks=DataDistribution(data_range=5, n=3)\nmaml=MAML(net,alpha=0.001,beta=0.01,tasks=data_tasks,k=200,weight_decay=0.0001,inner_step=1)\nmaml.outer_loop(num_epochs=2)\n\n# plt_path = \"/home/ubuntu/fedavg/FollowNet-car/test_metrics\"\nplt.plot(maml.meta_losses)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_Loss.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.meta_test)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_mserror.png'))\nplt.show()\nplt.close()\n\nplt.plot(maml.finetuning_error)\n# plt.savefig(os.path.join(plt_path, f'MeTAL_Waymo_{device}_tcnks5_1632_a0.001_b0.01_step1_k200_K40_Mese_acc_finetuning_mserror.png'))\nplt.show()\nplt.close()\n# 将数据保存到文件中\nnp.save('/kaggle/working/kaggle_maml_meta_losses.npy', np.array(maml.meta_losses))\nnp.save('/kaggle/working/kaggle_maml_meta_test.npy', np.array(maml.meta_test))\nnp.save('/kaggle/working/kaggle_maml_finetuning_error.npy', np.array(maml.finetuning_error))\n\nprint('----------------------')\nlogger.info(\"----------------------\")\n","metadata":{},"execution_count":null,"outputs":[]}]}